{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea - closing remarks of https://www.lesswrong.com/posts/FgjcHiWvADgsocE34/a-descriptive-not-prescriptive-overview-of-current-ai\n",
    "\n",
    "a semantic search service that returns relevant literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create aiproject python\n",
    "# conda activate aiproject\n",
    "# !pip install  anthropic\n",
    "# https://github.com/anthropics/anthropic-cookbook/blob/main/misc/pdf_upload_summarization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = 'alignment_texts.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PUT API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "import os\n",
    "anthropic = Anthropic(api_key= 'INSERT_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1024,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, world\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion = anthropic.completions.create(    model=\"claude-3-haiku-20240307\",    \n",
    "#                                         max_tokens_to_sample=1024,    \n",
    "#                                         prompt=f\"{HUMAN_PROMPT} How do I?{AI_PROMPT}\",)\n",
    "# print(completion.completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train Claude on https://github.com/moirage/alignment-research-dataset \n",
    "2. Prompt Claude to \n",
    "3. Ask Claude to create a markdown file with the top 10 clusters as topics and the top 10 papers in each cluster as subtopics.\n",
    "4. Feed markdowns into quartz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = anthropic.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1024,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How to train Claude on a dataset stored in jsonl?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_01ETFj9cmQSqfcW3nh8CjNCW', content=[TextBlock(text=\"I don't actually have the ability to be trained on new datasets. I'm Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't have access to external datasets or the capability to update my own knowledge or capabilities.\", type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=19, output_tokens=58))\n"
     ]
    }
   ],
   "source": [
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"claude-3-5-sonnet-20240620\"\n",
    "def get_completion(client, prompt):\n",
    "    return client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2048,\n",
    "        messages=[{\n",
    "            \"role\": 'user', \"content\":  prompt\n",
    "        }]\n",
    "    ).content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path, num_lines=None):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if num_lines and len(data) >= num_lines:\n",
    "                break\n",
    "            item = json.loads(line.strip())\n",
    "            if item.get('source_type') == 'latex':\n",
    "                data.append(item)\n",
    "    return data\n",
    "    \n",
    "def extract_relevant_fields(data):\n",
    "    combined_entries = []\n",
    "    for item in data:\n",
    "        combined_entry = {\n",
    "            'title': item.get('title'),\n",
    "            'abstract': item.get('abstract'),\n",
    "            # 'summary': item.get('summary')\n",
    "        }\n",
    "        combined_entries.append(combined_entry)\n",
    "    return combined_entries\n",
    "\n",
    "data = load_jsonl(file_path, num_lines=20)\n",
    "combined_entries = extract_relevant_fields(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1806.09055v2',\n",
       "  'title': 'DARTS: Differentiable Architecture Search',\n",
       "  'authors': ['Hanxiao Liu', 'Karen Simonyan', 'Yiming Yang'],\n",
       "  'date_published': '2018-06-24 00:06:13+00:00',\n",
       "  'data_last_modified': '2019-04-23 06:29:32+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1806.09055v2',\n",
       "  'abstract': 'This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.',\n",
       "  'author_comment': 'Published at ICLR 2019; Code and pretrained models available at\\n  https://github.com/quark0/darts',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': 'main.tex',\n",
       "  'text': '---\\nabstract: |\\n  This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.\\nauthor:\\n- |\\n  Hanxiao Liu[^1]\\\\\\n  CMU\\\\\\n  `hanxiaol@cs.cmu.com`\\\\\\n  Karen Simonyan\\\\\\n  DeepMind\\\\\\n  `simonyan@google.com`\\\\\\n  Yiming Yang\\\\\\n  CMU\\\\\\n  `yiming@cs.cmu.edu`\\nbibliography:\\n- reference.bib\\ntitle: \"DARTS: Differentiable Architecture Search\"\\n---\\n\\nIntroduction\\n============\\n\\nDiscovering state-of-the-art neural network architectures requires substantial effort of human experts. Recently, there has been a growing interest in developing algorithmic solutions to automate the manual process of architecture design. The automatically searched architectures have achieved highly competitive performance in tasks such as image classification [@zoph2016neural; @zoph2017learning; @liu2017hierarchical; @liu2017progressive; @real2018regularized] and object detection [@zoph2017learning].\\n\\nThe best existing architecture search algorithms are computationally demanding despite their remarkable performance. For example, obtaining a state-of-the-art architecture for CIFAR-10 and ImageNet required 2000 GPU days of reinforcement learning (RL) [@zoph2017learning] or 3150 GPU days of evolution [@real2018regularized]. Several approaches for speeding up have been proposed, such as imposing a particular structure of the search space [@liu2017hierarchical; @liu2017progressive], weights or performance prediction for each individual architecture [@brock2017smash; @baker2018accelerating] and weight sharing/inheritance across multiple architectures [@elsken2017simple; @pham2018efficient; @cai2018efficient; @bender2018understanding], but the fundamental challenge of scalability remains. An inherent cause of inefficiency for the dominant approaches, e.g. based on RL, evolution, MCTS [@negrinho2017deeparchitect], SMBO [@liu2017progressive] or Bayesian optimization [@kandasamy2018neural], is the fact that architecture search is treated as a black-box optimization problem over a discrete domain, which leads to a large number of architecture evaluations required.\\n\\nIn this work, we approach the problem from a different angle, and propose a method for efficient architecture search called DARTS (Differentiable ARchiTecture Search). Instead of searching over a discrete set of candidate architectures, we relax the search space to be continuous, so that the architecture can be optimized with respect to its validation set performance by gradient descent. The data efficiency of gradient-based optimization, as opposed to inefficient black-box search, allows DARTS to achieve competitive performance with the state of the art using orders of magnitude less computation resources. It also outperforms another recent efficient architecture search method, ENAS\\xa0[@pham2018efficient]. Notably, DARTS is simpler than many existing approaches as it does not involve controllers [@zoph2016neural; @baker2016designing; @zoph2017learning; @pham2018efficient; @zhong2018practical], hypernetworks [@brock2017smash] or performance predictors [@liu2017progressive], yet it is generic enough handle both convolutional and recurrent architectures.\\n\\nThe idea of searching architectures within a continuous domain is not new [@saxena2016convolutional; @ahmed2017connectivity; @veniat2017learning; @shin2018differentiable], but there are several major distinctions. While prior works seek to fine-tune a specific aspect of an architecture, such as filter shapes or branching patterns in a convolutional network, DARTS is able to learn high-performance architecture building blocks with complex graph topologies within a rich search space. Moreover, DARTS is not restricted to any specific architecture family, and is applicable to both convolutional and recurrent networks.\\n\\nIn our experiments (Sect.\\xa0[3](#sec:experiments){reference-type=\"ref\" reference=\"sec:experiments\"}) we show that DARTS is able to design a convolutional cell that achieves 2.76 $\\\\pm$ 0.09% test error on CIFAR-10 for image classification using 3.3M parameters, which is competitive with the state-of-the-art result by regularized evolution [@real2018regularized] obtained using three orders of magnitude more computation resources. The same convolutional cell also achieves 26.7% top-1 error when transferred to ImageNet (mobile setting), which is comparable to the best RL method [@zoph2017learning]. On the language modeling task, DARTS efficiently discovers a recurrent cell that achieves 55.7 test perplexity on Penn Treebank (PTB), outperforming both extensively tuned LSTM [@melis2017state] and all the existing automatically searched cells based on NAS [@zoph2016neural] and ENAS [@pham2018efficient].\\n\\nOur contributions can be summarized as follows:\\n\\n-   We introduce a novel algorithm for differentiable network architecture search based on bilevel optimization, which is applicable to both convolutional and recurrent architectures.\\n\\n-   Through extensive experiments on image classification and language modeling tasks we show that gradient-based architecture search achieves highly competitive results on CIFAR-10 and outperforms the state of the art on PTB. This is a very interesting result, considering that so far the best architecture search methods used non-differentiable search techniques, e.g. based on RL [@zoph2017learning] or evolution [@real2018regularized; @liu2017hierarchical].\\n\\n-   We achieve remarkable efficiency improvement (reducing the cost of architecture discovery to a few GPU days), which we attribute to the use of gradient-based optimization as opposed to non-differentiable search techniques.\\n\\n-   We show that the architectures learned by DARTS on CIFAR-10 and PTB are transferable to ImageNet and WikiText-2, respectively.\\n\\nThe implementation of DARTS is available at <https://github.com/quark0/darts>\\n\\nDifferentiable Architecture Search\\n==================================\\n\\nWe describe our search space in general form in Sect.\\xa0[2.1](#sec:sec:search-space){reference-type=\"ref\" reference=\"sec:sec:search-space\"}, where the computation procedure for an architecture (or a cell in it) is represented as a directed acyclic graph. We then introduce a simple continuous relaxation scheme for our search space which leads to a differentiable learning objective for the joint optimization of the architecture and its weights (Sect.\\xa0[2.2](#sec:sec:relaxation-and-optimization){reference-type=\"ref\" reference=\"sec:sec:relaxation-and-optimization\"}). Finally, we propose an approximation technique to make the algorithm computationally feasible and efficient (Sect.\\xa0[2.3](#sec:sec:approximation){reference-type=\"ref\" reference=\"sec:sec:approximation\"}).\\n\\nSearch Space {#sec:sec:search-space}\\n------------\\n\\nFollowing [@zoph2017learning; @real2018regularized; @liu2017progressive; @liu2017hierarchical], we search for a computation cell as the building block of the final architecture. The learned cell could either be stacked to form a convolutional network or recursively connected to form a recurrent network.\\n\\nA cell is a directed acyclic graph consisting of an ordered sequence of $N$ nodes. Each node $x^{(i)}$ is a latent representation (e.g. a feature map in convolutional networks) and each directed edge $(i,j)$ is associated with some operation $o^{(i,j)}$ that transforms $x^{(i)}$. We assume the cell to have two input nodes and a single output node. For convolutional cells, the input nodes are defined as the cell outputs in the previous two layers [@zoph2017learning]. For recurrent cells, these are defined as the input at the current step and the state carried from the previous step. The output of the cell is obtained by applying a reduction operation (e.g. concatenation) to all the intermediate nodes.\\n\\nEach intermediate node is computed based on all of its predecessors: $$x^{(j)} = \\\\sum_{i<j} o^{(i, j)}(x^{(i)})$$ A special *zero* operation is also included to indicate a lack of connection between two nodes. The task of learning the cell therefore reduces to learning the operations on its edges.\\n\\nContinuous Relaxation and Optimization {#sec:sec:relaxation-and-optimization}\\n--------------------------------------\\n\\nLet $\\\\mathcal{O}$ be a set of candidate operations (e.g., convolution, max pooling, *zero*) where each operation represents some function $o(\\\\cdot)$ to be applied to $x^{(i)}$. To make the search space continuous, we relax the categorical choice of a particular operation to a softmax over all possible operations: $$\\\\begin{aligned}\\n    \\\\bar{o}^{(i,j)}(x) = \\\\sum_{o \\\\in \\\\mathcal{O}} \\\\frac{\\\\exp(\\\\alpha_o^{(i,j)})}{\\\\sum_{o\\' \\\\in \\\\mathcal{O}} \\\\exp(\\\\alpha_{o\\'}^{(i,j)})} o(x)\\n    \\\\label{eq:soft}\\\\end{aligned}$$ where the operation mixing weights for a pair of nodes $(i,j)$ are parameterized by a vector $\\\\alpha^{(i,j)}$ of dimension $|\\\\mathcal{O}|$. The task of architecture search then reduces to learning a set of continuous variables $\\\\alpha = \\\\big\\\\{ \\\\alpha^{(i,j)} \\\\big\\\\}$, as illustrated in Fig.\\xa0[1](#fig:darts){reference-type=\"ref\" reference=\"fig:darts\"}. At the end of search, a discrete architecture can be obtained by replacing each mixed operation $\\\\bar{o}^{(i,j)}$ with the most likely operation, i.e., $o^{(i,j)} = \\n    \\\\mathrm{argmax}_{o \\\\in \\\\mathcal{O}} \\\\enskip \\\\alpha^{(i,j)}_o$. In the following, we refer to $\\\\alpha$ as the (encoding of the) architecture.\\n\\n![An overview of DARTS: (a) Operations on the edges are initially unknown. (b) Continuous relaxation of the search space by placing a mixture of candidate operations on each edge. (c) Joint optimization of the mixing probabilities and the network weights by solving a bilevel optimization problem. (d) Inducing the final architecture from the learned mixing probabilities.](darts.png){#fig:darts width=\"0.75\\\\\\\\linewidth\"}\\n\\nAfter relaxation, our goal is to jointly learn the architecture $\\\\alpha$ and the weights $w$ within all the mixed operations (e.g. weights of the convolution filters). Analogous to architecture search using RL [@zoph2016neural; @zoph2017learning; @pham2018efficient] or evolution [@liu2017hierarchical; @real2018regularized] where the validation set performance is treated as the reward or fitness, DARTS aims to optimize the validation loss, but using gradient descent.\\n\\nDenote by $\\\\mathcal{L}_{train}$ and $\\\\mathcal{L}_{val}$ the training and the validation loss, respectively. Both losses are determined not only by the architecture $\\\\alpha$, but also the weights $w$ in the network. The goal for architecture search is to find $\\\\alpha^*$ that minimizes the validation loss $\\\\mathcal{L}_{val}(w^*, \\\\alpha^*)$, where the weights $w^*$ associated with the architecture are obtained by minimizing the training loss $w^* = \\\\mathrm{argmin}_w\\\\ \\\\mathcal{L}_{train}(w, \\\\alpha^*)$.\\n\\nThis implies a bilevel optimization problem [@anandalingam1992hierarchical; @colson2007overview] with $\\\\alpha$ as the upper-level variable and $w$ as the lower-level variable: $$\\\\begin{aligned}\\n    \\\\min_\\\\alpha \\\\quad & \\\\mathcal{L}_{val}(w^*(\\\\alpha), \\\\alpha) \\\\label{eq:outer} \\\\\\\\\\n    \\\\text{s.t.} \\\\quad &w^*(\\\\alpha) = \\\\mathrm{argmin}_w \\\\enskip \\\\mathcal{L}_{train}(w, \\\\alpha) \\\\label{eq:inner}\\\\end{aligned}$$ The nested formulation also arises in gradient-based hyperparameter optimization [@maclaurin2015gradient; @pedregosa2016hyperparameter; @franceschi2018bilevel], which is related in a sense that the architecture $\\\\alpha$ could be viewed as a special type of hyperparameter, although its dimension is substantially higher than scalar-valued hyperparameters such as the learning rate, and it is harder to optimize.\\n\\nCreate a mixed operation $\\\\bar{o}^{(i,j)}$ parametrized by $\\\\alpha^{(i,j)}$ for each edge $(i,j)$ Derive the final architecture based on the learned $\\\\alpha$.\\n\\nApproximate Architecture Gradient {#sec:sec:approximation}\\n---------------------------------\\n\\nEvaluating the architecture gradient exactly can be prohibitive due to the expensive inner optimization. We therefore propose a simple approximation scheme as follows: $$\\\\begin{aligned}\\n&\\\\nabla_\\\\alpha \\\\mathcal{L}_{val}(w^*(\\\\alpha), \\\\alpha) \\\\\\\\\\n\\\\approx &\\\\nabla_\\\\alpha \\\\mathcal{L}_{val}(w - \\\\xi \\\\nabla_{w} \\\\mathcal{L}_{train}(w, \\\\alpha), \\\\alpha) \\\\label{eq:single}\\\\end{aligned}$$ where $w$ denotes the current weights maintained by the algorithm, and $\\\\xi$ is the learning rate for a step of inner optimization. The idea is to *approximate $w^*(\\\\alpha)$ by adapting $w$ using only a single training step*, without solving the inner optimization (equation\\xa0[\\\\[eq:inner\\\\]](#eq:inner){reference-type=\"ref\" reference=\"eq:inner\"}) completely by training until convergence. Related techniques have been used in meta-learning for model transfer\\xa0[@finn2017model], gradient-based hyperparameter tuning [@luketina2016scalable] and unrolled generative adversarial networks [@metz2016unrolled]. Note equation\\xa0[\\\\[eq:single\\\\]](#eq:single){reference-type=\"ref\" reference=\"eq:single\"} will reduce to $\\\\nabla_\\\\alpha \\\\mathcal{L}_{val}(w, \\\\alpha)$ if $w$ is already a local optimum for the inner optimization and thus $\\\\nabla_w \\\\mathcal{L}_{train}(w, \\\\alpha) = 0$.\\n\\nThe iterative procedure is outlined in Alg.\\xa0[\\\\[algo:pseudocode\\\\]](#algo:pseudocode){reference-type=\"ref\" reference=\"algo:pseudocode\"}. While we are not currently aware of the convergence guarantees for our optimization algorithm, in practice it is able to reach a fixed point with a suitable choice of $\\\\xi$[^2]. We also note that when momentum is enabled for weight optimisation, the one-step unrolled learning objective in\\xa0equation\\xa0[\\\\[eq:single\\\\]](#eq:single){reference-type=\"ref\" reference=\"eq:single\"} is modified accordingly and all of our analysis still applies.\\n\\nApplying chain rule to the approximate architecture gradient (equation\\xa0[\\\\[eq:single\\\\]](#eq:single){reference-type=\"ref\" reference=\"eq:single\"}) yields $$\\\\nabla_\\\\alpha \\\\mathcal{L}_{val}(w\\', \\\\alpha) - \\\\xi \\\\nabla^2_{\\\\alpha, w} \\\\mathcal{L}_{train}(w, \\\\alpha) \\\\nabla_{w\\'} \\\\mathcal{L}_{val}(w\\', \\\\alpha) \\n        \\\\label{eq:arch-grad}$$ where $w\\' = w - \\\\xi \\\\nabla_w \\\\mathcal{L}_{train}(w, \\\\alpha)$ denotes the weights for a one-step forward model. The expression above contains an expensive matrix-vector product in its second term. Fortunately, the complexity can be substantially reduced using the finite difference approximation. Let $\\\\epsilon$ be a small scalar[^3] and $w^\\\\pm = w \\\\pm \\\\epsilon \\\\nabla_{w\\'} \\\\mathcal{L}_{val}(w\\', \\\\alpha)$. Then: $$\\\\nabla^2_{\\\\alpha, w} \\\\mathcal{L}_{train}(w, \\\\alpha) \\\\nabla_{w\\'} \\\\mathcal{L}_{val}(w\\', \\\\alpha) \\n    \\\\approx\\n    \\\\frac{\\\\nabla_\\\\alpha \\\\mathcal{L}_{train}(w^+, \\\\alpha) - \\\\nabla_\\\\alpha \\\\mathcal{L}_{train}(w^-, \\\\alpha)}{2 \\\\epsilon}$$ Evaluating the finite difference requires only two forward passes for the weights and two backward passes for $\\\\alpha$, and the complexity is reduced from $O(|\\\\alpha||w|)$ to $O(|\\\\alpha|+|w|)$.\\n\\n#### First-order Approximation\\n\\nWhen $\\\\xi = 0$, the second-order derivative in equation\\xa0[\\\\[eq:arch-grad\\\\]](#eq:arch-grad){reference-type=\"ref\" reference=\"eq:arch-grad\"} will disappear. In this case, the architecture gradient is given by $\\\\nabla_\\\\alpha \\\\mathcal{L}_{val}(w, \\\\alpha)$, corresponding to the simple heuristic of optimizing the validation loss by assuming the current $w$ is the same as $w^*(\\\\alpha)$. This leads to some speed-up but empirically worse performance, according to our experimental results in Table\\xa0[\\\\[tab:cifar-results\\\\]](#tab:cifar-results){reference-type=\"ref\" reference=\"tab:cifar-results\"} and Table\\xa0[\\\\[tab:ptb-results\\\\]](#tab:ptb-results){reference-type=\"ref\" reference=\"tab:ptb-results\"}. In the following, we refer to the case of $\\\\xi = 0$ as the first-order approximation, and refer to the gradient formulation with $\\\\xi > 0$ as the second-order approximation.\\n\\nDeriving Discrete Architectures\\n-------------------------------\\n\\nTo form each node in the discrete architecture, we retain the top-$k$ strongest operations (from distinct nodes) among all non-zero candidate operations collected from all the previous nodes. The strength of an operation is defined as $\\\\frac{\\\\exp(\\\\alpha_o^{(i,j)})}{\\\\sum_{o\\' \\\\in \\\\mathcal{O}} \\\\exp(\\\\alpha_{o\\'}^{(i,j)})}$. To make our derived architecture comparable with those in the existing works, we use $k = 2$ for convolutional cells [@zoph2017learning; @liu2017progressive; @real2018regularized] and $k=1$ for recurrent cells [@pham2018efficient].\\n\\nThe zero operations are excluded in the above for two reasons. First, we need exactly $k$ non-zero incoming edges per node for fair comparison with the existing models. Second, the strength of the zero operations is underdetermined, as increasing the logits of zero operations only affects the scale of the resulting node representations, and does not affect the final classification outcome due to the presence of batch normalization [@ioffe2015batch].\\n\\n![Learning dynamics of our iterative algorithm when $\\\\mathcal{L}_{val}(w, \\\\alpha) = \\\\alpha w - 2\\\\alpha + 1$ and $\\\\mathcal{L}_{train}(w, \\\\alpha) = w^2 - 2\\\\alpha w + \\\\alpha^2$, starting from $(\\\\alpha^{(0)}, w^{(0)}) = (2, -2)$. The analytical solution for the corresponding bilevel optimization problem is $(\\\\alpha^*, w^*) = (1, 1)$, which is highlighted in the red circle. The dashed red line indicates the feasible set where constraint equation\\xa0[\\\\[eq:inner\\\\]](#eq:inner){reference-type=\"ref\" reference=\"eq:inner\"} is satisfied exactly (namely, weights in $w$ are optimal for the given architecture $\\\\alpha$). The example shows that a suitable choice of $\\\\xi$ helps to converge to a better local optimum.](convergence.pdf){width=\"1.0\\\\\\\\linewidth\"}\\n\\nExperiments and Results {#sec:experiments}\\n=======================\\n\\nOur experiments on CIFAR-10 and PTB consist of two stages, architecture search (Sect.\\xa0[3.1](#sec:sec:experiment-search){reference-type=\"ref\" reference=\"sec:sec:experiment-search\"}) and architecture evaluation (Sect.\\xa0[3.2](#sec:sec:experiment-eval){reference-type=\"ref\" reference=\"sec:sec:experiment-eval\"}). In the first stage, we search for the cell architectures using DARTS, and determine the best cells based on their validation performance. In the second stage, we use these cells to construct *larger* architectures, which we train from scratch and report their performance on the test set. We also investigate the transferability of the best cells learned on CIFAR-10 and PTB by evaluating them on ImageNet and WikiText-2 (WT2) respectively.\\n\\nArchitecture Search {#sec:sec:experiment-search}\\n-------------------\\n\\n### Searching for Convolutional Cells on CIFAR-10\\n\\nWe include the following operations in $\\\\mathcal{O}$: $3\\\\times3$ and $5\\\\times5$ separable convolutions, $3\\\\times3$ and $5\\\\times5$ dilated separable convolutions, $3\\\\times3$ max pooling, $3\\\\times3$ average pooling, identity, and $zero$. All operations are of stride one (if applicable) and the convolved feature maps are padded to preserve their spatial resolution. We use the ReLU-Conv-BN order for convolutional operations, and each separable convolution is always applied twice [@zoph2017learning; @real2018regularized; @liu2017progressive].\\n\\nOur convolutional cell consists of $N=7$ nodes, among which the output node is defined as the depthwise concatenation of all the intermediate nodes (input nodes excluded). The rest of the setup follows\\xa0[@zoph2017learning; @liu2017progressive; @real2018regularized], where a network is then formed by stacking multiple cells together. The first and second nodes of cell $k$ are set equal to the outputs of cell $k-2$ and cell $k-1$, respectively, and $1 \\\\times 1$ convolutions are inserted as necessary. Cells located at the $1/3$ and $2/3$ of the total depth of the network are reduction cells, in which all the operations adjacent to the input nodes are of stride two. The architecture encoding therefore is $(\\\\alpha_{normal}, \\\\alpha_{reduce})$, where $\\\\alpha_{normal}$ is shared by all the normal cells and $\\\\alpha_{reduce}$ is shared by all the reduction cells.\\n\\nDetailed experimental setup for this section can be found in Sect.\\xa0[5.1.1](#sec:search-cifar10){reference-type=\"ref\" reference=\"sec:search-cifar10\"}.\\n\\n### Searching for Recurrent Cells on Penn Treebank\\n\\nOur set of available operations includes linear transformations followed by one of $\\\\tanh$, $\\\\mathrm{relu}$, $\\\\mathrm{sigmoid}$ activations, as well as the identity mapping and the *zero* operation. The choice of these candidate operations follows [@zoph2016neural; @pham2018efficient].\\n\\nOur recurrent cell consists of $N=12$ nodes. The very first intermediate node is obtained by linearly transforming the two input nodes, adding up the results and then passing through a $\\\\tanh$ activation function, as done in the ENAS cell [@pham2018efficient]. The rest of the cell is learned. Other settings are similar to ENAS, where each operation is enhanced with a highway bypass [@zilly2016recurrent] and the cell output is defined as the average of all the intermediate nodes. As in ENAS, we enable batch normalization in each node to prevent gradient explosion during architecture search, and disable it during architecture evaluation. Our recurrent network consists of only a single cell, i.e.\\xa0we do not assume any repetitive patterns within the recurrent architecture.\\n\\nDetailed experimental setup for this section can be found in Sect.\\xa0[5.1.2](#sec:search-ptb){reference-type=\"ref\" reference=\"sec:search-ptb\"}.\\n\\n![Search progress of DARTS for convolutional cells on CIFAR-10 and recurrent cells on Penn Treebank. We keep track of the most recent architectures over time. Each architecture snapshot is re-trained from scratch using the training set (for 100 epochs on CIFAR-10 and for 300 epochs on PTB) and then evaluated on the validation set. For each task, we repeat the experiments for 4 times with different random seeds, and report the median and the best (per run) validation performance of the architectures over time. As references, we also report the results (under the same evaluation setup; with comparable number of parameters) of the best existing cells discovered using RL or evolution, including NASNet-A [@zoph2017learning] (2000 GPU days), AmoebaNet-A (3150 GPU days) [@real2018regularized] and ENAS (0.5 GPU day) [@pham2018efficient]. ](progress-cifar-crop.pdf \"fig:\"){#fig:progress width=\"0.4875\\\\\\\\linewidth\"} ![Search progress of DARTS for convolutional cells on CIFAR-10 and recurrent cells on Penn Treebank. We keep track of the most recent architectures over time. Each architecture snapshot is re-trained from scratch using the training set (for 100 epochs on CIFAR-10 and for 300 epochs on PTB) and then evaluated on the validation set. For each task, we repeat the experiments for 4 times with different random seeds, and report the median and the best (per run) validation performance of the architectures over time. As references, we also report the results (under the same evaluation setup; with comparable number of parameters) of the best existing cells discovered using RL or evolution, including NASNet-A [@zoph2017learning] (2000 GPU days), AmoebaNet-A (3150 GPU days) [@real2018regularized] and ENAS (0.5 GPU day) [@pham2018efficient]. ](progress-ptb-crop.pdf \"fig:\"){#fig:progress width=\"0.4875\\\\\\\\linewidth\"}\\n\\n![Recurrent cell learned on PTB.](visualize-normal-crop.pdf){#fig:visualization-recurrent width=\"95%\"}\\n\\n![Recurrent cell learned on PTB.](visualize-reduction-crop.pdf){#fig:visualization-recurrent width=\"95%\"}\\n\\n![Recurrent cell learned on PTB.](visualize-recurrent-crop.pdf){#fig:visualization-recurrent width=\"60%\"}\\n\\nArchitecture Evaluation {#sec:sec:experiment-eval}\\n-----------------------\\n\\nTo determine the architecture for final evaluation, we run DARTS four times with different random seeds and pick the best cell based on its validation performance obtained by training from scratch for a short period (100 epochs on CIFAR-10 and 300 epochs on PTB). This is particularly important for recurrent cells, as the optimization outcomes can be initialization-sensitive (Fig.\\xa0[3](#fig:progress){reference-type=\"ref\" reference=\"fig:progress\"}).\\n\\nTo evaluate the selected architecture, we randomly initialize its weights (weights learned during the search process are discarded), train it from scratch, and report its performance on the test set. We note the test set is never used for architecture search or architecture selection.\\n\\nDetailed experimental setup for architecture evaluation on CIFAR-10 and PTB can be found in Sect.\\xa0[5.2.1](#sec:eval-cifar10){reference-type=\"ref\" reference=\"sec:eval-cifar10\"} and Sect.\\xa0[5.2.2](#sec:eval-ptb){reference-type=\"ref\" reference=\"sec:eval-ptb\"}, respectively. Besides CIFAR-10 and PTB, we further investigated the transferability of our best convolutional cell (searched on CIFAR-10) and recurrent cell (searched on PTB) by evaluating them on ImageNet (mobile setting) and WikiText-2, respectively. More details of the transfer learning experiments can be found in Sect.\\xa0[5.2.3](#sec:eval-imagenet){reference-type=\"ref\" reference=\"sec:eval-imagenet\"} and Sect.\\xa0[5.2.4](#sec:eval-wt2){reference-type=\"ref\" reference=\"sec:eval-wt2\"}.\\n\\nObtained by repeating ENAS for 8 times using the code publicly released by the authors. The cell for final\\\\\\nevaluation is chosen according to the same selection protocol as for DARTS.\\n\\nObtained by training the corresponding architectures using our setup.\\n\\nBest architecture among 24 samples according to the validation error after 100 training epochs.\\n\\nObtained using the code [@enascode] publicly released by the authors.\\n\\nObtained by training the corresponding architecture using our setup.\\n\\nBest architecture among 8 samples according to the validation perplexity after 300 training epochs.\\n\\nResults Analysis\\n----------------\\n\\nThe CIFAR-10 results for convolutional architectures are presented in Table\\xa0[\\\\[tab:cifar-results\\\\]](#tab:cifar-results){reference-type=\"ref\" reference=\"tab:cifar-results\"}. Notably, DARTS achieved comparable results with the state of the art [@zoph2017learning; @real2018regularized] while using three orders of magnitude less computation resources (i.e. 1.5 or 4 GPU days vs 2000 GPU days for NASNet and 3150 GPU days for AmoebaNet). Moreover, with slightly longer search time, DARTS outperformed ENAS [@pham2018efficient] by discovering cells with comparable error rates but less parameters. The longer search time is due to the fact that we have repeated the search process four times for cell selection. This practice is less important for convolutional cells however, because the performance of discovered architectures does not strongly depend on initialization (Fig. [3](#fig:progress){reference-type=\"ref\" reference=\"fig:progress\"}).\\n\\n#### Alternative Optimization Strategies\\n\\nTo better understand the necessity of bilevel optimization, we investigated a simplistic search strategy, where $\\\\alpha$ and $w$ are jointly optimized over the union of the training and validation sets using coordinate descent. The resulting best convolutional cell (out of 4 runs) yielded 4.16 $\\\\pm$ 0.16% test error using 3.1M parameters, which is worse than random search. In the second experiment, we optimized $\\\\alpha$ simultaneously with $w$ (without alteration) using SGD, again over all the data available (training + validation). The resulting best cell yielded 3.56 $\\\\pm$ 0.10% test error using 3.0M parameters. We hypothesize that these heuristics would cause $\\\\alpha$ (analogous to hyperparameters) to overfit the training data, leading to poor generalization. Note that $\\\\alpha$ is not directly optimized on the training set in DARTS.\\n\\nTable\\xa0[\\\\[tab:ptb-results\\\\]](#tab:ptb-results){reference-type=\"ref\" reference=\"tab:ptb-results\"} presents the results for recurrent architectures on PTB, where a cell discovered by DARTS achieved the test perplexity of 55.7. This is on par with the state-of-the-art model enhanced by a mixture of softmaxes [@yang2017breaking], and better than all the rest of the architectures that are either manually or automatically discovered. Note that our automatically searched cell outperforms the extensively tuned LSTM [@melis2017state], demonstrating the importance of architecture search in addition to hyperparameter search. In terms of efficiency, the overall cost (4 runs in total) is within 1 GPU day, which is comparable to ENAS and significantly faster than NAS [@zoph2016neural].\\n\\nIt is also interesting to note that random search is competitive for both convolutional and recurrent models, which reflects the importance of the search space design. Nevertheless, with comparable or less search cost, DARTS is able to significantly improve upon random search in both cases (2.76 $\\\\pm$ 0.09 vs 3.29 $\\\\pm$ 0.15 on CIFAR-10; 55.7 vs 59.4 on PTB).\\n\\nResults in Table\\xa0[\\\\[tab:imagenet-results\\\\]](#tab:imagenet-results){reference-type=\"ref\" reference=\"tab:imagenet-results\"} show that the cell learned on CIFAR-10 is indeed transferable to ImageNet. It is worth noticing that DARTS achieves competitive performance with the state-of-the-art RL method [@zoph2017learning] while using three orders of magnitude less computation resources.\\n\\nObtained by training the corresponding architecture using our setup.\\n\\nTable [\\\\[tab:wt2-results\\\\]](#tab:wt2-results){reference-type=\"ref\" reference=\"tab:wt2-results\"} shows that the cell identified by DARTS transfers to WT2 better than ENAS, although the overall results are less strong than those presented in Table [\\\\[tab:ptb-results\\\\]](#tab:ptb-results){reference-type=\"ref\" reference=\"tab:ptb-results\"} for PTB. The weaker transferability between PTB and WT2 (as compared to that between CIFAR-10 and ImageNet) could be explained by the relatively small size of the source dataset (PTB) for architecture search. The issue of transferability could potentially be circumvented by directly optimizing the architecture on the task of interest.\\n\\nConclusion\\n==========\\n\\nWe presented DARTS, a simple yet efficient architecture search algorithm for both convolutional and recurrent networks. By searching in a continuous space, DARTS is able to match or outperform the state-of-the-art non-differentiable architecture search methods on image classification and language modeling tasks with remarkable efficiency improvement by several orders of magnitude.\\n\\nThere are many interesting directions to improve DARTS further. For example, the current method may suffer from discrepancies between the continuous architecture encoding and the derived discrete architecture. This could be alleviated, e.g., by annealing the softmax temperature (with a suitable schedule) to enforce one-hot selection. It would also be interesting to investigate performance-aware architecture derivation schemes based on the shared parameters learned during the search process.\\n\\nAcknowledgements {#acknowledgements .unnumbered}\\n================\\n\\nThe authors would like to thank Zihang Dai, Hieu Pham and Zico Kolter for useful discussions.\\n\\nExperimental Details\\n====================\\n\\nArchitecture Search {#architecture-search}\\n-------------------\\n\\n### CIFAR-10 {#sec:search-cifar10}\\n\\nSince the architecture will be varying throughout the search process, we always use batch-specific statistics for batch normalization rather than the global moving average. Learnable affine parameters in all batch normalizations are disabled during the search process to avoid rescaling the outputs of the candidate operations.\\n\\nTo carry out architecture search, we hold out half of the CIFAR-10 training data as the validation set. A small network of 8 cells is trained using DARTS for 50 epochs, with batch size $64$ (for both the training and validation sets) and the initial number of channels $16$. The numbers were chosen to ensure the network can fit into a single GPU. We use momentum SGD to optimize the weights $w$, with initial learning rate $\\\\eta_w = 0.025$ (annealed down to zero following a cosine schedule without restart [@loshchilov2016sgdr]), momentum $0.9$, and weight decay $3 \\\\times 10^{-4}$. We use zero initialization for architecture variables (the $\\\\alpha$\\'s in both the normal and reduction cells), which implies equal amount of attention (after taking the softmax) over all possible ops. At the early stage this ensures weights in every candidate op to receive sufficient learning signal (more exploration). We use Adam [@kingma2014adam] as the optimizer for $\\\\alpha$, with initial learning rate $\\\\eta_\\\\alpha = 3\\\\times 10^{-4}$, momentum $\\\\beta = (0.5, 0.999)$ and weight decay $10^{-3}$. The search takes one day on a single GPU[^4].\\n\\n### PTB {#sec:search-ptb}\\n\\nFor architecture search, both the embedding and the hidden sizes are set to 300. The linear transformation parameters across all incoming operations connected to the same node are shared (their shapes are all 300 $\\\\times$ 300), as the algorithm always has the option to focus on one of the predecessors and mask away the others. Tying the weights leads to memory savings and faster computation, allowing us to train the continuous architecture using a single GPU. Learnable affine parameters in batch normalizations are disabled, as we did for convolutional cells. The network is then trained for 50 epochs using SGD without momentum, with learning rate $\\\\eta_w=20$, batch size 256, BPTT length 35, and weight decay $5 \\\\times 10^{-7}$. We apply variational dropout [@gal2016theoretically] of $0.2$ to word embeddings, $0.75$ to the cell input, and $0.25$ to all the hidden nodes. A dropout of $0.75$ is also applied to the output layer. Other training settings are identical to those in [@merity2017regularizing; @yang2017breaking]. Similarly to the convolutional architectures, we use Adam for the optimization of $\\\\alpha$ (initialized as zeros), with initial learning rate $\\\\eta_\\\\alpha = 3\\\\times 10^{-3}$, momentum $\\\\beta = (0.9, 0.999)$ and weight decay $10^{-3}$. The search takes 6 hours on a single GPU.\\n\\nArchitecture Evaluation {#architecture-evaluation}\\n-----------------------\\n\\n### CIFAR-10 {#sec:eval-cifar10}\\n\\nA large network of 20 cells is trained for 600 epochs with batch size 96. The initial number of channels is increased from 16 to 36 to ensure our model size is comparable with other baselines in the literature (around 3M). Other hyperparameters remain the same as the ones used for architecture search. Following existing works [@pham2018efficient; @zoph2017learning; @liu2017progressive; @real2018regularized], additional enhancements include cutout [@devries2017improved], path dropout of probability $0.2$ and auxiliary towers with weight $0.4$. The training takes 1.5 days on a single GPU with our implementation in PyTorch [@paszke2017automatic]. Since the CIFAR results are subject to high variance even with exactly the same setup [@liu2017hierarchical], we report the mean and standard deviation of 10 independent runs for our full model.\\n\\nTo avoid any discrepancy between different implementations or training settings (e.g. the batch sizes), we incorporated the NASNet-A cell [@zoph2017learning] and the AmoebaNet-A cell [@real2018regularized] into our training framework and reported their results under the same settings as our cells.\\n\\n### PTB {#sec:eval-ptb}\\n\\nA single-layer recurrent network with the discovered cell is trained until convergence with batch size 64 using averaged SGD [@polyak1992acceleration] (ASGD), with learning rate $\\\\eta_w=20$ and weight decay $8\\\\times 10^{-7}$. To speedup, we start with SGD and trigger ASGD using the same protocol as in [@yang2017breaking; @merity2017regularizing]. Both the embedding and the hidden sizes are set to 850 to ensure our model size is comparable with other baselines. The token-wise dropout on the embedding layer is set to 0.1. Other hyperparameters remain exactly the same as those for architecture search. For fair comparison, we do not finetune our model at the end of the optimization, nor do we use any additional enhancements such as dynamic evaluation [@krause2017dynamic] or continuous cache [@grave2016improving]. The training takes 3 days on a single 1080Ti GPU with our PyTorch implementation. To account for implementation discrepancies, we also incorporated the ENAS cell [@pham2018efficient] into our codebase and trained their network under the same setup as our discovered cells.\\n\\n### ImageNet {#sec:eval-imagenet}\\n\\nWe consider the *mobile* setting where the input image size is 224$\\\\times$224 and the number of multiply-add operations in the model is restricted to be less than 600M.\\n\\nA network of 14 cells is trained for 250 epochs with batch size 128, weight decay $3 \\\\times 10^{-5}$ and initial SGD learning rate 0.1 (decayed by a factor of 0.97 after each epoch). Other hyperparameters follow [@zoph2017learning; @real2018regularized; @liu2017progressive][^5]. The training takes 12 days on a single GPU.\\n\\n### WikiText-2 {#sec:eval-wt2}\\n\\nWe use embedding and hidden sizes 700, weight decay $5 \\\\times 10^{-7}$, and hidden-node variational dropout 0.15. Other hyperparameters remain the same as in our PTB experiments.\\n\\nSearch with Increased Depth\\n===========================\\n\\nTo better understand the effect of depth for architecture search, we conducted architecture search on CIFAR-10 by increasing the number of cells in the stack from 8 to 20. The initial number of channels is reduced from 16 to 6 due to memory budget of a single GPU. All the other hyperparameters remain the same. The search cost doubles and the resulting cell achieves 2.88 $\\\\pm$ 0.09% test error, which is slightly worse than 2.76 $\\\\pm$ 0.09% obtained using a shallower network. This particular setup may have suffered from the enlarged discrepancy of the number of channels between architecture search and final evaluation. Moreover, searching with a deeper model might require different hyperparameters due to the increased number of layers to back-prop through.\\n\\nComplexity Analysis\\n===================\\n\\nIn this section, we analyze the complexity of our search space for convolutional cells.\\n\\nEach of our discretized cell allows $\\\\prod_{k=1}^4 \\\\frac{(k+1)k}{2}\\\\times (7^2) \\\\approx 10^9$ possible DAGs without considering graph isomorphism (recall we have 7 non-zero ops, 2 input nodes, 4 intermediate nodes with 2 predecessors each). Since we are jointly learning both normal and reduction cells, the total number of architectures is approximately $(10^9)^2 = 10^{18}$. This is greater than the \\xa0$5.6\\\\times10^{14}$ of PNAS [@liu2017progressive] which learns only a single type of cell.\\n\\nAlso note that we retained the top-2 predecessors per node only at the very end, and our continuous search space before this final discretization step is even larger. Specifically, each relaxed cell (a fully connected graph) contains $2+3+4+5 = 14$ learnable edges, allowing $(7+1)^{14} \\\\approx 4\\\\times 10^{12}$ possible configurations ($+1$ to include the *zero* op indicating a lack of connection). Again, since we are learning both normal and reduction cells, the total number of architectures covered by the continuous space before discretization is $(4\\\\times 10^{12})^2 \\\\approx 10^{25}$.\\n\\n[^1]: Current affiliation: Google Brain.\\n\\n[^2]: A simple working strategy is to set $\\\\xi$ equal to the learning rate for $w$\\'s optimizer.\\n\\n[^3]: We found $\\\\epsilon = 0.01/\\\\|\\\\nabla_{w\\'} \\\\mathcal{L}_{val}(w\\', \\\\alpha)\\\\|_2$ to be sufficiently accurate in all of our experiments.\\n\\n[^4]: All of our experiments were performed using NVIDIA GTX 1080Ti GPUs.\\n\\n[^5]: We did not conduct extensive hyperparameter tuning.\\n',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{46}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Ahmed \\\\& Torresani(2017)Ahmed and Torresani]{ahmed2017connectivity}\\nKarim Ahmed and Lorenzo Torresani.\\n\\\\newblock Connectivity learning in multi-branch networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1709.09582}, 2017.\\n\\n\\\\bibitem[Anandalingam \\\\& Friesz(1992)Anandalingam and\\n  Friesz]{anandalingam1992hierarchical}\\nG~Anandalingam and TL~Friesz.\\n\\\\newblock Hierarchical optimization: An introduction.\\n\\\\newblock \\\\emph{Annals of Operations Research}, 34\\\\penalty0 (1):\\\\penalty0\\n  1--11, 1992.\\n\\n\\\\bibitem[Baker et~al.(2017)Baker, Gupta, Naik, and Raskar]{baker2016designing}\\nBowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar.\\n\\\\newblock Designing neural network architectures using reinforcement learning.\\n\\\\newblock \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Baker et~al.(2018)Baker, Gupta, Raskar, and\\n  Naik]{baker2018accelerating}\\nBowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik.\\n\\\\newblock Accelerating neural architecture search using performance prediction.\\n\\\\newblock \\\\emph{ICLR Workshop}, 2018.\\n\\n\\\\bibitem[Bender et~al.(2018)Bender, Kindermans, Zoph, Vasudevan, and\\n  Le]{bender2018understanding}\\nGabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc\\n  Le.\\n\\\\newblock Understanding and simplifying one-shot architecture search.\\n\\\\newblock In \\\\emph{International Conference on Machine Learning}, pp.\\\\\\n  549--558, 2018.\\n\\n\\\\bibitem[Brock et~al.(2018)Brock, Lim, Ritchie, and Weston]{brock2017smash}\\nAndrew Brock, Theodore Lim, James~M Ritchie, and Nick Weston.\\n\\\\newblock Smash: one-shot model architecture search through hypernetworks.\\n\\\\newblock \\\\emph{ICLR}, 2018.\\n\\n\\\\bibitem[Cai et~al.(2018)Cai, Chen, Zhang, Yu, and Wang]{cai2018efficient}\\nHan Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang.\\n\\\\newblock Efficient architecture search by network transformation.\\n\\\\newblock \\\\emph{AAAI}, 2018.\\n\\n\\\\bibitem[Colson et~al.(2007)Colson, Marcotte, and Savard]{colson2007overview}\\nBeno{\\\\^\\\\i}t Colson, Patrice Marcotte, and Gilles Savard.\\n\\\\newblock An overview of bilevel optimization.\\n\\\\newblock \\\\emph{Annals of operations research}, 153\\\\penalty0 (1):\\\\penalty0\\n  235--256, 2007.\\n\\n\\\\bibitem[DeVries \\\\& Taylor(2017)DeVries and Taylor]{devries2017improved}\\nTerrance DeVries and Graham~W Taylor.\\n\\\\newblock Improved regularization of convolutional neural networks with cutout.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1708.04552}, 2017.\\n\\n\\\\bibitem[Elsken et~al.(2017)Elsken, Metzen, and Hutter]{elsken2017simple}\\nThomas Elsken, Jan-Hendrik Metzen, and Frank Hutter.\\n\\\\newblock Simple and efficient architecture search for convolutional neural\\n  networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1711.04528}, 2017.\\n\\n\\\\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}\\nChelsea Finn, Pieter Abbeel, and Sergey Levine.\\n\\\\newblock Model-agnostic meta-learning for fast adaptation of deep networks.\\n\\\\newblock In \\\\emph{ICML}, pp.\\\\  1126--1135, 2017.\\n\\n\\\\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, and\\n  Pontil]{franceschi2018bilevel}\\nLuca Franceschi, Paolo Frasconi, Saverio Salzo, and Massimilano Pontil.\\n\\\\newblock Bilevel programming for hyperparameter optimization and\\n  meta-learning.\\n\\\\newblock \\\\emph{ICML}, 2018.\\n\\n\\\\bibitem[Gal \\\\& Ghahramani(2016)Gal and Ghahramani]{gal2016theoretically}\\nYarin Gal and Zoubin Ghahramani.\\n\\\\newblock A theoretically grounded application of dropout in recurrent neural\\n  networks.\\n\\\\newblock In \\\\emph{NIPS}, pp.\\\\  1019--1027, 2016.\\n\\n\\\\bibitem[Grave et~al.(2016)Grave, Joulin, and Usunier]{grave2016improving}\\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\\n\\\\newblock Improving neural language models with a continuous cache.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1612.04426}, 2016.\\n\\n\\\\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,\\n  Andreetto, and Adam]{howard2017mobilenets}\\nAndrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,\\n  Tobias Weyand, Marco Andreetto, and Hartwig Adam.\\n\\\\newblock Mobilenets: Efficient convolutional neural networks for mobile vision\\n  applications.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1704.04861}, 2017.\\n\\n\\\\bibitem[Huang et~al.(2017)Huang, Liu, Weinberger, and van~der\\n  Maaten]{huang2017densely}\\nGao Huang, Zhuang Liu, Kilian~Q Weinberger, and Laurens van~der Maaten.\\n\\\\newblock Densely connected convolutional networks.\\n\\\\newblock In \\\\emph{CVPR}, 2017.\\n\\n\\\\bibitem[Inan et~al.(2017)Inan, Khosravi, and Socher]{inan2017tying}\\nHakan Inan, Khashayar Khosravi, and Richard Socher.\\n\\\\newblock Tying word vectors and word classifiers: A loss framework for\\n  language modeling.\\n\\\\newblock \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Ioffe \\\\& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}\\nSergey Ioffe and Christian Szegedy.\\n\\\\newblock Batch normalization: Accelerating deep network training by reducing\\n  internal covariate shift.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1502.03167}, 2015.\\n\\n\\\\bibitem[Kandasamy et~al.(2018)Kandasamy, Neiswanger, Schneider, Poczos, and\\n  Xing]{kandasamy2018neural}\\nKirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and\\n  Eric Xing.\\n\\\\newblock Neural architecture search with bayesian optimisation and optimal\\n  transport.\\n\\\\newblock \\\\emph{NIPS}, 2018.\\n\\n\\\\bibitem[Kingma \\\\& Ba(2014)Kingma and Ba]{kingma2014adam}\\nDiederik~P Kingma and Jimmy Ba.\\n\\\\newblock Adam: A method for stochastic optimization.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1412.6980}, 2014.\\n\\n\\\\bibitem[Krause et~al.(2017)Krause, Kahembwe, Murray, and\\n  Renals]{krause2017dynamic}\\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals.\\n\\\\newblock Dynamic evaluation of neural sequence models.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1709.07432}, 2017.\\n\\n\\\\bibitem[Liu et~al.(2018{\\\\natexlab{a}})Liu, Zoph, Shlens, Hua, Li, Fei-Fei,\\n  Yuille, Huang, and Murphy]{liu2017progressive}\\nChenxi Liu, Barret Zoph, Jonathon Shlens, Wei Hua, Li-Jia Li, Li~Fei-Fei, Alan\\n  Yuille, Jonathan Huang, and Kevin Murphy.\\n\\\\newblock Progressive neural architecture search.\\n\\\\newblock \\\\emph{ECCV}, 2018{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Liu et~al.(2018{\\\\natexlab{b}})Liu, Simonyan, Vinyals, Fernando, and\\n  Kavukcuoglu]{liu2017hierarchical}\\nHanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray\\n  Kavukcuoglu.\\n\\\\newblock Hierarchical representations for efficient architecture search.\\n\\\\newblock \\\\emph{ICLR}, 2018{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Loshchilov \\\\& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}\\nIlya Loshchilov and Frank Hutter.\\n\\\\newblock Sgdr: Stochastic gradient descent with warm restarts.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1608.03983}, 2016.\\n\\n\\\\bibitem[Luketina et~al.(2016)Luketina, Berglund, Greff, and\\n  Raiko]{luketina2016scalable}\\nJelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko.\\n\\\\newblock Scalable gradient-based tuning of continuous regularization\\n  hyperparameters.\\n\\\\newblock In \\\\emph{ICML}, pp.\\\\  2952--2960, 2016.\\n\\n\\\\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and\\n  Adams]{maclaurin2015gradient}\\nDougal Maclaurin, David Duvenaud, and Ryan Adams.\\n\\\\newblock Gradient-based hyperparameter optimization through reversible\\n  learning.\\n\\\\newblock In \\\\emph{ICML}, pp.\\\\  2113--2122, 2015.\\n\\n\\\\bibitem[Melis et~al.(2018)Melis, Dyer, and Blunsom]{melis2017state}\\nG{\\\\\\'a}bor Melis, Chris Dyer, and Phil Blunsom.\\n\\\\newblock On the state of the art of evaluation in neural language models.\\n\\\\newblock \\\\emph{ICLR}, 2018.\\n\\n\\\\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2017regularizing}\\nStephen Merity, Nitish~Shirish Keskar, and Richard Socher.\\n\\\\newblock Regularizing and optimizing lstm language models.\\n\\\\newblock \\\\emph{ICLR}, 2018.\\n\\n\\\\bibitem[Metz et~al.(2017)Metz, Poole, Pfau, and\\n  Sohl-Dickstein]{metz2016unrolled}\\nLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein.\\n\\\\newblock Unrolled generative adversarial networks.\\n\\\\newblock \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Negrinho \\\\& Gordon(2017)Negrinho and\\n  Gordon]{negrinho2017deeparchitect}\\nRenato Negrinho and Geoff Gordon.\\n\\\\newblock Deeparchitect: Automatically designing and training deep\\n  architectures.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1704.08792}, 2017.\\n\\n\\\\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,\\n  Desmaison, Antiga, and Lerer]{paszke2017automatic}\\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary\\n  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.\\n\\\\newblock Automatic differentiation in pytorch.\\n\\\\newblock In \\\\emph{NIPS-W}, 2017.\\n\\n\\\\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}\\nFabian Pedregosa.\\n\\\\newblock Hyperparameter optimization with approximate gradient.\\n\\\\newblock In \\\\emph{ICML}, 2016.\\n\\n\\\\bibitem[Pham et~al.(2018{\\\\natexlab{a}})Pham, Guan, Zoph, Le, and\\n  Dean]{enascode}\\nHieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean.\\n\\\\newblock Authors\\' implementation of {``Efficient Neural Architecture Search\\n  via Parameter Sharing\\'\\'}.\\n\\\\newblock\\n  \\\\url{https://github.com/melodyguan/enas/tree/2734eb2657847f090e1bc5c51c2b9cbf0be51887},\\n  2018{\\\\natexlab{a}}.\\n\\\\newblock Accessed: 2018-04-05.\\n\\n\\\\bibitem[Pham et~al.(2018{\\\\natexlab{b}})Pham, Guan, Zoph, Le, and\\n  Dean]{pham2018efficient}\\nHieu Pham, Melody~Y Guan, Barret Zoph, Quoc~V Le, and Jeff Dean.\\n\\\\newblock Efficient neural architecture search via parameter sharing.\\n\\\\newblock \\\\emph{ICML}, 2018{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Polyak \\\\& Juditsky(1992)Polyak and Juditsky]{polyak1992acceleration}\\nBoris~T Polyak and Anatoli~B Juditsky.\\n\\\\newblock Acceleration of stochastic approximation by averaging.\\n\\\\newblock \\\\emph{SIAM Journal on Control and Optimization}, 30\\\\penalty0\\n  (4):\\\\penalty0 838--855, 1992.\\n\\n\\\\bibitem[Real et~al.(2018)Real, Aggarwal, Huang, and Le]{real2018regularized}\\nEsteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.\\n\\\\newblock Regularized evolution for image classifier architecture search.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1802.01548}, 2018.\\n\\n\\\\bibitem[Saxena \\\\& Verbeek(2016)Saxena and Verbeek]{saxena2016convolutional}\\nShreyas Saxena and Jakob Verbeek.\\n\\\\newblock Convolutional neural fabrics.\\n\\\\newblock In \\\\emph{NIPS}, pp.\\\\  4053--4061, 2016.\\n\\n\\\\bibitem[Shin et~al.(2018)Shin, Packer, and Song]{shin2018differentiable}\\nRichard Shin, Charles Packer, and Dawn Song.\\n\\\\newblock Differentiable neural network architecture search.\\n\\\\newblock In \\\\emph{ICLR Workshop}, 2018.\\n\\n\\\\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,\\n  Erhan, Vanhoucke, Rabinovich, Rick~Chang, et~al.]{szegedy2015going}\\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\\n  Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Jen-Hao\\n  Rick~Chang, et~al.\\n\\\\newblock Going deeper with convolutions.\\n\\\\newblock In \\\\emph{CVPR}, 2015.\\n\\n\\\\bibitem[Veniat \\\\& Denoyer(2017)Veniat and Denoyer]{veniat2017learning}\\nTom Veniat and Ludovic Denoyer.\\n\\\\newblock Learning time/memory-efficient deep architectures with budgeted super\\n  networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1706.00046}, 2017.\\n\\n\\\\bibitem[Yang et~al.(2018)Yang, Dai, Salakhutdinov, and\\n  Cohen]{yang2017breaking}\\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W Cohen.\\n\\\\newblock Breaking the softmax bottleneck: a high-rank rnn language model.\\n\\\\newblock \\\\emph{ICLR}, 2018.\\n\\n\\\\bibitem[Zhang et~al.(2017)Zhang, Zhou, Lin, and Sun]{zhang2017shufflenet}\\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.\\n\\\\newblock Shufflenet: An extremely efficient convolutional neural network for\\n  mobile devices.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1707.01083}, 2017.\\n\\n\\\\bibitem[Zhong et~al.(2018)Zhong, Yan, Wu, Shao, and Liu]{zhong2018practical}\\nZhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu.\\n\\\\newblock Practical block-wise neural network architecture generation.\\n\\\\newblock In \\\\emph{Proceedings of the IEEE Conference on Computer Vision and\\n  Pattern Recognition}, pp.\\\\  2423--2432, 2018.\\n\\n\\\\bibitem[Zilly et~al.(2016)Zilly, Srivastava, Koutn{\\\\\\'\\\\i}k, and\\n  Schmidhuber]{zilly2016recurrent}\\nJulian~Georg Zilly, Rupesh~Kumar Srivastava, Jan Koutn{\\\\\\'\\\\i}k, and J{\\\\\"u}rgen\\n  Schmidhuber.\\n\\\\newblock Recurrent highway networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1607.03474}, 2016.\\n\\n\\\\bibitem[Zoph \\\\& Le(2017)Zoph and Le]{zoph2016neural}\\nBarret Zoph and Quoc~V Le.\\n\\\\newblock Neural architecture search with reinforcement learning.\\n\\\\newblock \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2017learning}\\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.\\n\\\\newblock Learning transferable architectures for scalable image recognition.\\n\\\\newblock \\\\emph{CVPR}, 2018.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1709.09582': True,\n",
       "   '1708.04552': True,\n",
       "   '1711.04528': True,\n",
       "   '1612.04426': True,\n",
       "   '1704.04861': True,\n",
       "   '1502.03167': True,\n",
       "   '1412.6980': True,\n",
       "   '1709.07432': True,\n",
       "   '1608.03983': True,\n",
       "   '1704.08792': True,\n",
       "   '1802.01548': True,\n",
       "   '1706.00046': True,\n",
       "   '1707.01083': True,\n",
       "   '1607.03474': True}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1906.02530v2',\n",
       "  'title': \"Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\",\n",
       "  'authors': ['Yaniv Ovadia',\n",
       "   'Emily Fertig',\n",
       "   'Jie Ren',\n",
       "   'Zachary Nado',\n",
       "   'D Sculley',\n",
       "   'Sebastian Nowozin',\n",
       "   'Joshua V. Dillon',\n",
       "   'Balaji Lakshminarayanan',\n",
       "   'Jasper Snoek'],\n",
       "  'date_published': '2019-06-06 11:42:53+00:00',\n",
       "  'data_last_modified': '2019-12-17 21:30:28+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1906.02530v2',\n",
       "  'abstract': \"Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\\\\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.\",\n",
       "  'author_comment': 'Advances in Neural Information Processing Systems, 2019',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'stat.ML',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': '',\n",
       "  'text': '',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{57}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Alemi et~al.(2018)Alemi, Fischer, and Dillon]{uncertaintyvib}\\nAlemi, A.~A., Fischer, I., and Dillon, J.~V.\\n\\\\newblock Uncertainty in the variational information bottleneck.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1807.00906}, 2018.\\n\\n\\\\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and\\n  Man{\\\\\\'e}]{amodei2016concrete}\\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and\\n  Man{\\\\\\'e}, D.\\n\\\\newblock Concrete problems in {AI} safety.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1606.06565}, 2016.\\n\\n\\\\bibitem[Behrmann et~al.(2018)Behrmann, Duvenaud, and\\n  Jacobsen]{behrmann2018invertible}\\nBehrmann, J., Duvenaud, D., and Jacobsen, J.-H.\\n\\\\newblock Invertible residual networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1811.00995}, 2018.\\n\\n\\\\bibitem[Bishop(1994)]{bishop1994novelty}\\nBishop, C.~M.\\n\\\\newblock {Novelty Detection and Neural Network Validation}.\\n\\\\newblock \\\\emph{IEE Proceedings-Vision, Image and Signal processing},\\n  141\\\\penalty0 (4):\\\\penalty0 217--222, 1994.\\n\\n\\\\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and\\n  Wierstra]{BBB}\\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.\\n\\\\newblock Weight uncertainty in neural networks.\\n\\\\newblock In \\\\emph{ICML}, 2015.\\n\\n\\\\bibitem[Bojarski et~al.(2016)Bojarski, Testa, Dworakowski, Firner, Flepp,\\n  Goyal, Jackel, Monfort, Muller, Zhang, Zhang, Zhao, and Zieba]{bojarski16}\\nBojarski, M., Testa, D.~D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P.,\\n  Jackel, L.~D., Monfort, M., Muller, U., Zhang, J., Zhang, X., Zhao, J., and\\n  Zieba, K.\\n\\\\newblock End to end learning for self-driving cars.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1604.07316}, 2016.\\n\\n\\\\bibitem[Brier(1950)]{brier1950verification}\\nBrier, G.~W.\\n\\\\newblock Verification of forecasts expressed in terms of probability.\\n\\\\newblock \\\\emph{Monthly weather review}, 1950.\\n\\n\\\\bibitem[Br{\\\\\"o}cker(2009)]{brocker2009reliability}\\nBr{\\\\\"o}cker, J.\\n\\\\newblock Reliability, sufficiency, and the decomposition of proper scores.\\n\\\\newblock \\\\emph{Quarterly Journal of the Royal Meteorological Society},\\n  135\\\\penalty0 (643):\\\\penalty0 1512--1519, 2009.\\n\\n\\\\bibitem[Bulatov(2011)]{notmnist}\\nBulatov, Y.\\n\\\\newblock {NotMNIST dataset}, 2011.\\n\\\\newblock URL\\n  \\\\url{http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html}.\\n\\n\\\\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and\\n  Robinson]{chelba2013one}\\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and\\n  Robinson, T.\\n\\\\newblock One billion word benchmark for measuring progress in statistical\\n  language modeling.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1312.3005}, 2013.\\n\\n\\\\bibitem[DeGroot \\\\& Fienberg(1983)DeGroot and Fienberg]{degroot1983comparison}\\nDeGroot, M.~H. and Fienberg, S.~E.\\n\\\\newblock The comparison and evaluation of forecasters.\\n\\\\newblock \\\\emph{The statistician}, 1983.\\n\\n\\\\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and\\n  Fei-Fei]{imagenet_cvpr09}\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.\\n\\\\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.\\n\\\\newblock In \\\\emph{Computer Vision and Pattern Recognition}, 2009.\\n\\n\\\\bibitem[Esteva et~al.(2017)Esteva, Kuprel, Novoa, Ko, Swetter, Blau, and\\n  Thrun]{esteva2017}\\nEsteva, A., Kuprel, B., Novoa, R.~A., Ko, J., Swetter, S.~M., Blau, H.~M., and\\n  Thrun, S.\\n\\\\newblock Dermatologist-level classification of skin cancer with deep neural\\n  networks.\\n\\\\newblock \\\\emph{Nature}, 542, 1 2017.\\n\\n\\\\bibitem[Gal \\\\& Ghahramani(2016)Gal and Ghahramani]{gal}\\nGal, Y. and Ghahramani, Z.\\n\\\\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty\\n  in deep learning.\\n\\\\newblock In \\\\emph{ICML}, 2016.\\n\\n\\\\bibitem[Geifman \\\\& El-Yaniv(2017)Geifman and El-Yaniv]{geifman2017selective}\\nGeifman, Y. and El-Yaniv, R.\\n\\\\newblock Selective classification for deep neural networks.\\n\\\\newblock In \\\\emph{NeurIPS}, 2017.\\n\\n\\\\bibitem[Gneiting \\\\& Raftery(2007)Gneiting and Raftery]{gneiting2007strictly}\\nGneiting, T. and Raftery, A.~E.\\n\\\\newblock Strictly proper scoring rules, prediction, and estimation.\\n\\\\newblock \\\\emph{Journal of the American Statistical Association}, 102\\\\penalty0\\n  (477):\\\\penalty0 359--378, 2007.\\n\\n\\\\bibitem[Golovin et~al.(2017)Golovin, Solnik, Moitra, Kochanski, Karro, and\\n  Sculley]{vizier}\\nGolovin, D., Solnik, B., Moitra, S., Kochanski, G., Karro, J., and Sculley, D.\\n\\\\newblock Google vizier: A service for black-box optimization.\\n\\\\newblock In \\\\emph{Proceedings of the 23rd ACM SIGKDD International Conference\\n  on Knowledge Discovery and Data Mining}, pp.\\\\  1487--1495. ACM, 2017.\\n\\n\\\\bibitem[Graves(2011)]{graves}\\nGraves, A.\\n\\\\newblock Practical variational inference for neural networks.\\n\\\\newblock In \\\\emph{NeurIPS}, 2011.\\n\\n\\\\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}\\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.\\n\\\\newblock On calibration of modern neural networks.\\n\\\\newblock In \\\\emph{International Conference on Machine Learning}, 2017.\\n\\n\\\\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\n\\\\newblock Deep residual learning for image recognition.\\n\\\\newblock In \\\\emph{Proceedings of the IEEE Conference on Computer Vision and\\n  Pattern Recognition}, pp.\\\\  770--778, 2016.\\n\\n\\\\bibitem[Hendrycks \\\\& Dietterich(2019)Hendrycks and\\n  Dietterich]{hendrycks2018benchmarking}\\nHendrycks, D. and Dietterich, T.\\n\\\\newblock Benchmarking neural network robustness to common corruptions and\\n  perturbations.\\n\\\\newblock In \\\\emph{ICLR}, 2019.\\n\\n\\\\bibitem[Hendrycks \\\\& Gimpel(2017)Hendrycks and Gimpel]{hendrycks2016baseline}\\nHendrycks, D. and Gimpel, K.\\n\\\\newblock {A Baseline for Detecting Misclassified and Out-of-Distribution\\n  Examples in Neural Networks}.\\n\\\\newblock In \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Hensman et~al.(2015)Hensman, Matthews, and\\n  Ghahramani]{hensman2015scalable}\\nHensman, J., Matthews, A., and Ghahramani, Z.\\n\\\\newblock Scalable variational gaussian process classification.\\n\\\\newblock In \\\\emph{International Conference on Artificial Intelligence and\\n  Statistics}. JMLR, 2015.\\n\\n\\\\bibitem[Hern{\\\\\\'a}ndez-Lobato \\\\& Adams(2015)Hern{\\\\\\'a}ndez-Lobato and\\n  Adams]{hernandez2015probabilistic}\\nHern{\\\\\\'a}ndez-Lobato, J.~M. and Adams, R.\\n\\\\newblock {Probabilistic Backpropagation for Scalable Learning of Bayesian\\n  Neural Networks}.\\n\\\\newblock In \\\\emph{ICML}, 2015.\\n\\n\\\\bibitem[Hochreiter \\\\& Schmidhuber(1997)Hochreiter and\\n  Schmidhuber]{hochreiter97}\\nHochreiter, S. and Schmidhuber, J.\\n\\\\newblock Long short-term memory.\\n\\\\newblock \\\\emph{Neural Comput.}, 9\\\\penalty0 (8):\\\\penalty0 1735--1780, November\\n  1997.\\n\\n\\\\bibitem[Kendall \\\\& Gal(2017)Kendall and Gal]{kendall2017uncertainties}\\nKendall, A. and Gal, Y.\\n\\\\newblock {What uncertainties do we need in Bayesian deep learning for computer\\n  vision?}\\n\\\\newblock In \\\\emph{NeurIPS}, 2017.\\n\\n\\\\bibitem[Kingma \\\\& Ba(2014)Kingma and Ba]{kingma2014adam}\\nKingma, D. and Ba, J.\\n\\\\newblock {Adam: A Method for Stochastic Optimization}.\\n\\\\newblock In \\\\emph{ICLR}, 2014.\\n\\n\\\\bibitem[Kingma et~al.(2014)Kingma, Mohamed, Rezende, and\\n  Welling]{kingma2014semi}\\nKingma, D.~P., Mohamed, S., Rezende, D.~J., and Welling, M.\\n\\\\newblock Semi-supervised learning with deep generative models.\\n\\\\newblock In \\\\emph{NeurIPS}, 2014.\\n\\n\\\\bibitem[Kingma et~al.(2015)Kingma, Salimans, and Welling]{Kingma15}\\nKingma, D.~P., Salimans, T., and Welling, M.\\n\\\\newblock Variational dropout and the local reparameterization trick.\\n\\\\newblock In \\\\emph{NeurIPS}, 2015.\\n\\n\\\\bibitem[Klambauer et~al.(2017)Klambauer, Unterthiner, Mayr, and\\n  Hochreiter]{klambauer2017}\\nKlambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S.\\n\\\\newblock Self-normalizing neural networks.\\n\\\\newblock In \\\\emph{NeurIPS}, 2017.\\n\\n\\\\bibitem[Krizhevsky(2009)]{cifar10}\\nKrizhevsky, A.\\n\\\\newblock Learning multiple layers of features from tiny images.\\n\\\\newblock 2009.\\n\\n\\\\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and\\n  Blundell]{deepensembles}\\nLakshminarayanan, B., Pritzel, A., and Blundell, C.\\n\\\\newblock {Simple and Scalable Predictive Uncertainty Estimation Using Deep\\n  Ensembles}.\\n\\\\newblock In \\\\emph{NeurIPS}, 2017.\\n\\n\\\\bibitem[Lang(1995)]{lang1995newsweeder}\\nLang, K.\\n\\\\newblock Newsweeder: Learning to filter netnews.\\n\\\\newblock In \\\\emph{Machine Learning}. 1995.\\n\\n\\\\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun-98}\\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.\\n\\\\newblock Gradient-based learning applied to document recognition.\\n\\\\newblock In \\\\emph{Proceedings of the IEEE}, November 1998.\\n\\n\\\\bibitem[Lee et~al.(2018)Lee, Lee, Lee, and Shin]{lee2018simple}\\nLee, K., Lee, K., Lee, H., and Shin, J.\\n\\\\newblock A simple unified framework for detecting out-of-distribution samples\\n  and adversarial attacks.\\n\\\\newblock In \\\\emph{NeurIPS}, 2018.\\n\\n\\\\bibitem[Liang et~al.(2018)Liang, Li, and Srikant]{odin}\\nLiang, S., Li, Y., and Srikant, R.\\n\\\\newblock {Enhancing the Reliability of Out-of-Distribution Image Detection in\\n  Neural Networks}.\\n\\\\newblock \\\\emph{ICLR}, 2018.\\n\\n\\\\bibitem[Lipton \\\\& Steinhardt(2018)Lipton and Steinhardt]{lipton2018troubling}\\nLipton, Z.~C. and Steinhardt, J.\\n\\\\newblock Troubling trends in machine learning scholarship.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1807.03341}, 2018.\\n\\n\\\\bibitem[Louizos \\\\& Welling(2016)Louizos and Welling]{louizos2016structured}\\nLouizos, C. and Welling, M.\\n\\\\newblock Structured and efficient variational deep learning with matrix\\n  {G}aussian posteriors.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1603.04733}, 2016.\\n\\n\\\\bibitem[Louizos \\\\& Welling(2017)Louizos and\\n  Welling]{louizos2017multiplicative}\\nLouizos, C. and Welling, M.\\n\\\\newblock {Multiplicative Normalizing Flows for Variational Bayesian Neural\\n  Networks}.\\n\\\\newblock In \\\\emph{ICML}, 2017.\\n\\n\\\\bibitem[MacKay(1992)]{mackay1992bayesian}\\nMacKay, D.~J.\\n\\\\newblock \\\\emph{Bayesian methods for adaptive models}.\\n\\\\newblock PhD thesis, California Institute of Technology, 1992.\\n\\n\\\\bibitem[MacKay \\\\& Gibbs(1999)MacKay and Gibbs]{mackay1999density}\\nMacKay, D.~J. and Gibbs, M.~N.\\n\\\\newblock {Density Networks}.\\n\\\\newblock \\\\emph{Statistics and Neural Networks: Advances at the Interface},\\n  1999.\\n\\n\\\\bibitem[Naeini et~al.(2015)Naeini, Cooper, and\\n  Hauskrecht]{naeini2015obtaining}\\nNaeini, M.~P., Cooper, G.~F., and Hauskrecht, M.\\n\\\\newblock {Obtaining Well Calibrated Probabilities Using Bayesian Binning}.\\n\\\\newblock In \\\\emph{AAAI}, pp.\\\\  2901--2907, 2015.\\n\\n\\\\bibitem[Nalisnick et~al.(2019)Nalisnick, Matsukawa, Teh, Gorur, and\\n  Lakshminarayanan]{nalisnickhybrid}\\nNalisnick, E., Matsukawa, A., Teh, Y.~W., Gorur, D., and Lakshminarayanan, B.\\n\\\\newblock Hybrid models with deep and invertible features.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1902.02767}, 2019.\\n\\n\\\\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{svhn}\\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.\\n\\\\newblock {Reading Digits in Natural Images with Unsupervised Feature\\n  Learning}.\\n\\\\newblock In \\\\emph{NeurIPS Workshop on Deep Learning and Unsupervised Feature\\n  Learning}, 2011.\\n\\n\\\\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and\\n  Van~Roy]{bootstrapdqn}\\nOsband, I., Blundell, C., Pritzel, A., and Van~Roy, B.\\n\\\\newblock Deep exploration via bootstrapped {DQN}.\\n\\\\newblock In \\\\emph{NeurIPS}, 2016.\\n\\n\\\\bibitem[Platt(1999)]{platt99}\\nPlatt, J.~C.\\n\\\\newblock Probabilistic outputs for support vector machines and comparisons to\\n  regularized likelihood methods.\\n\\\\newblock In \\\\emph{Advances in Large Margin Classifiers}, pp.\\\\  61--74. MIT\\n  Press, 1999.\\n\\n\\\\bibitem[Quinonero-Candela et~al.(2006)Quinonero-Candela, Rasmussen, Sinz,\\n  Bousquet, and Sch{\\\\\"o}lkopf]{quinonero2006evaluating}\\nQuinonero-Candela, J., Rasmussen, C.~E., Sinz, F., Bousquet, O., and\\n  Sch{\\\\\"o}lkopf, B.\\n\\\\newblock Evaluating predictive uncertainty challenge.\\n\\\\newblock In \\\\emph{Machine Learning Challenges}. Springer, 2006.\\n\\n\\\\bibitem[Rahimi \\\\& Recht(2017)Rahimi and Recht]{rahimi2017addendum}\\nRahimi, A. and Recht, B.\\n\\\\newblock An addendum to alchemy, 2017.\\n\\n\\\\bibitem[Ren et~al.(2019)Ren, Liu, Fertig, Snoek, Poplin, DePristo, Dillon, and\\n  Lakshminarayanan]{ren2019likelihood}\\nRen, J., Liu, P.~J., Fertig, E., Snoek, J., Poplin, R., DePristo, M.~A.,\\n  Dillon, J.~V., and Lakshminarayanan, B.\\n\\\\newblock Likelihood ratios for out-of-distribution detection.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1906.02845}, 2019.\\n\\n\\\\bibitem[Riquelme et~al.(2018)Riquelme, Tucker, and Snoek]{riquelme2018deep}\\nRiquelme, C., Tucker, G., and Snoek, J.\\n\\\\newblock {Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian\\n  Deep Networks for Thompson Sampling}.\\n\\\\newblock In \\\\emph{ICLR}, 2018.\\n\\n\\\\bibitem[Sculley et~al.(2018)Sculley, Snoek, Wiltschko, and\\n  Rahimi]{sculley2018winner}\\nSculley, D., Snoek, J., Wiltschko, A., and Rahimi, A.\\n\\\\newblock Winner\\'s curse? {O}n pace, progress, and empirical rigor.\\n\\\\newblock 2018.\\n\\n\\\\bibitem[Shafaei et~al.(2018)Shafaei, Schmidt, and Little]{shafaei2018does}\\nShafaei, A., Schmidt, M., and Little, J.~J.\\n\\\\newblock {Does Your Model Know the Digit 6 Is Not a Cat? A Less Biased\\n  Evaluation of ``Outlier\" Detectors}.\\n\\\\newblock \\\\emph{ArXiv e-Print arXiv:1809.04729}, 2018.\\n\\n\\\\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and\\n  Schmidhuber]{srivastava2015training}\\nSrivastava, R.~K., Greff, K., and Schmidhuber, J.\\n\\\\newblock {Training Very Deep Networks}.\\n\\\\newblock In \\\\emph{NeurIPS}, 2015.\\n\\n\\\\bibitem[Sugiyama et~al.(2009)Sugiyama, Lawrence, Schwaighofer,\\n  et~al.]{sugiyama2017dataset}\\nSugiyama, M., Lawrence, N.~D., Schwaighofer, A., et~al.\\n\\\\newblock \\\\emph{Dataset shift in machine learning}.\\n\\\\newblock The MIT Press, 2009.\\n\\n\\\\bibitem[Welling \\\\& Teh(2011)Welling and Teh]{Welling2011}\\nWelling, M. and Teh, Y.~W.\\n\\\\newblock {Bayesian Learning via Stochastic Gradient Langevin Dynamics}.\\n\\\\newblock In \\\\emph{ICML}, 2011.\\n\\n\\\\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{flipout}\\nWen, Y., Vicol, P., Ba, J., Tran, D., and Grosse, R.\\n\\\\newblock Flipout: Efficient pseudo-independent weight perturbations on\\n  mini-batches.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1803.04386}, 2018.\\n\\n\\\\bibitem[Wu et~al.(2019)Wu, Nowozin, Meeds, Turner, Hernandez-Lobato, and\\n  Gaunt]{wu2018}\\nWu, A., Nowozin, S., Meeds, E., Turner, R.~E., Hernandez-Lobato, J.~M., and\\n  Gaunt, A.~L.\\n\\\\newblock {Deterministic Variational Inference for Robust Bayesian Neural\\n  Networks}.\\n\\\\newblock In \\\\emph{ICLR}, 2019.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '@incollection{lang1995newsweeder,\\n  title={Newsweeder: Learning to filter netnews},\\n  author={Lang, Ken},\\n  booktitle={Machine Learning},\\n  year={1995},\\n}\\n\\n@article{ovadia2019can,\\n  title={Can You Trust Your Model\\'s Uncertainty? {E}valuating Predictive Uncertainty Under Dataset Shift},\\n  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D and Nowozin, Sebastian and Dillon, Joshua V and Lakshminarayanan, Balaji and Snoek, Jasper},\\n  journal={arXiv preprint arXiv:1906.02530},\\n  year={2019}\\n}\\n\\n@article{chelba2013one,\\n  title={One billion word benchmark for measuring progress in statistical language modeling},\\n  author={Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},\\n  journal={arXiv preprint arXiv:1312.3005},\\n  year={2013}\\n}\\n\\n@article{ren2019likelihood,\\n  title={Likelihood Ratios for Out-of-Distribution Detection},\\n  author={Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and DePristo, Mark A and Dillon, Joshua V and Lakshminarayanan, Balaji},\\n  journal={arXiv preprint arXiv:1906.02845},\\n  year={2019}\\n} \\n\\n@misc{notmnist,\\n  title={{NotMNIST dataset}},\\n  author={Bulatov, Yaroslav},\\n  year={2011},\\n  url={http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html}\\n}\\n\\n@article{behrmann2018invertible,\\n  title={Invertible residual networks},\\n  author={Behrmann, Jens and Duvenaud, David and Jacobsen, J{\\\\\"o}rn-Henrik},\\n  journal={arXiv preprint arXiv:1811.00995},\\n  year={2018}\\n}\\n\\n@article{kuleshov2018accurate,\\n  title={Accurate uncertainties for deep learning using calibrated regression},\\n  author={Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},\\n  journal={arXiv preprint arXiv:1807.00263},\\n  year={2018}\\n}\\n\\n@article{brocker2009reliability,\\n  title={Reliability, sufficiency, and the decomposition of proper scores},\\n  author={Br{\\\\\"o}cker, Jochen},\\n  journal={Quarterly Journal of the Royal Meteorological Society},\\n  volume={135},\\n  number={643},\\n  pages={1512--1519},\\n  year={2009},\\n  publisher={Wiley Online Library}\\n}\\n\\n@book{sugiyama2017dataset,\\n  title={Dataset shift in machine learning},\\n  author={Sugiyama, Masashi and Lawrence, Neil D and Schwaighofer, Anton and others},\\n  year={2009},\\n  publisher={The MIT Press}\\n}\\n\\n@inproceedings{kendall2017uncertainties,\\n  title={{What uncertainties do we need in Bayesian deep learning for computer vision?}},\\n  author={Kendall, Alex and Gal, Yarin},\\n  booktitle={NeurIPS},\\n  year={2017}\\n}\\n\\n@inproceedings{lecun-98,\\n  author={Y. LeCun and L. Bottou and Y. Bengio and P. Haffner},\\n  title={Gradient-based learning applied to document recognition},\\n  booktitle={Proceedings of the IEEE},\\n  year={1998},\\n  month={November}}\\n\\n@inproceedings{lee2018simple,\\n  title={A simple unified framework for detecting out-of-distribution samples and adversarial attacks},\\n  author={Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},\\n  booktitle={NeurIPS},\\n  year={2018}\\n}\\n\\n@article{dillon2017tensorflow,\\n  title={TensorFlow Distributions},\\n  author={Dillon, Joshua V and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A},\\n  journal={arXiv preprint arXiv:1711.10604},\\n  year={2017}\\n}\\n\\n@inproceedings{depeweg2018,\\n   author = {{Depeweg}, S. and {Hern{\\\\\\'a}ndez-Lobato}, J.~M. and {Doshi-Velez}, F. and \\n\\t{Udluft}, S.},\\n    title = \"{Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning}\",\\n  booktitle={ICML},\\n  year={2018}\\n}\\n\\n@inproceedings{bendale2016towards,\\n  title={Towards open set deep networks},\\n  author={Bendale, Abhijit and Boult, Terrance E},\\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\\n  pages={1563--1572},\\n  year={2016}\\n}\\n\\n@inproceedings{\\nhendrycks2018benchmarking,\\ntitle={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},\\nauthor={Dan Hendrycks and Thomas Dietterich},\\nbooktitle={ICLR},\\nyear={2019},\\n}\\n\\n@article{lipton2018detecting,\\n  title={Detecting and Correcting for Label Shift with Black Box Predictors},\\n  author={Lipton, Zachary C and Wang, Yu-Xiang and Smola, Alex},\\n  journal={arXiv preprint arXiv:1802.03916},\\n  year={2018}\\n}\\n\\n@article{malinin2018predictive,\\n  title={Predictive Uncertainty Estimation via Prior Networks},\\n  author={Malinin, Andrey and Gales, Mark},\\n  journal={arXiv preprint arXiv:1802.10501},\\n  year={2018}\\n}\\n\\n@inproceedings{Welling2011,\\n author = {Welling, Max and Teh, Yee Whye},\\n title = {{Bayesian Learning via Stochastic Gradient Langevin Dynamics}},\\n booktitle = {ICML},\\n year = {2011},\\n} \\n\\n@inproceedings{naeini2015obtaining,\\n  title={{Obtaining Well Calibrated Probabilities Using Bayesian Binning}},\\n  author={Naeini, Mahdi Pakdaman and Cooper, Gregory F and Hauskrecht, Milos},\\n  booktitle={AAAI},\\n  pages={2901--2907},\\n  year={2015}\\n}\\n\\n@inproceedings{sener2018multi,\\n  title={Multi-task learning as multi-objective optimization},\\n  author={Sener, Ozan and Koltun, Vladlen},\\n  booktitle={NeurIPS},\\n  pages={525--536},\\n  year={2018}\\n}\\n\\n@inproceedings{geifman2017selective,\\n  title={Selective classification for deep neural networks},\\n  author={Geifman, Yonatan and El-Yaniv, Ran},\\n  booktitle={NeurIPS},\\n  year={2017}\\n}\\n\\n\\n@article{flipout,\\n  title={Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches},\\n  author={Wen, Yeming and Vicol, Paul and Ba, Jimmy and Tran, Dustin and Grosse, Roger},\\n  journal={arXiv preprint arXiv:1803.04386},\\n  year={2018}\\n}\\n\\n@inproceedings{maaten2013learning,\\n  title={Learning with marginalized corrupted features},\\n  author={Maaten, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian},\\n  booktitle={ICML},\\n  year={2013}\\n}\\n\\n\\n@article{noisynetworks,\\n  title={Noisy networks for exploration},\\n  author={Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and others},\\n  journal={arXiv preprint arXiv:1706.10295},\\n  year={2017}\\n}\\n\\n\\n@article{nalisnickhybrid,\\n  title={Hybrid Models with Deep and Invertible Features},\\n  author={Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},\\n  journal={arXiv preprint arXiv:1902.02767},\\n  year={2019}\\n}\\n\\n@article{genomicsood,\\n  title={Detecting out-of-distribution species in genomic sequences},\\n  author={Ren, Jie and others},\\n  journal={Internal draft},\\n  year={2019}\\n}\\n\\n\\n\\n@inproceedings{nalisnick2019ood,\\n   author = {{Nalisnick}, Eric and {Matsukawa}, Akihiro and {Whye Teh}, Yee and {Gorur}, Dilan and \\n\\t{Lakshminarayanan}, Balaji},\\n    title = {{Do Deep Generative Models Know What They Don\\'t Know?}},\\n  booktitle={ICLR},\\n  year={2019}\\n}\\n\\n@inproceedings{\\nhendrycks2018deep,\\ntitle={Deep Anomaly Detection with Outlier Exposure},\\nauthor={Dan Hendrycks and Mantas Mazeika and Thomas Dietterich},\\nbooktitle={ICLR},\\nyear={2019},\\n}\\n\\n@article{esteva2017,\\nauthor={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},\\ntitle = {Dermatologist-level classification of skin cancer with deep neural networks},\\njournal= {Nature},\\nyear={2017},\\nmonth={1},\\nday={25},\\nvolume={542}}\\n\\n@article{bojarski16,\\n  author = {Bojarski, Mariusz and Testa, Davide Del and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},\\n  ee = {http://arxiv.org/abs/1604.07316},\\n  journal = {arXiv preprint arXiv:1604.07316},\\n  title = {End to End Learning for Self-Driving Cars.},\\n  year = 2016\\n}\\n\\n@inproceedings{celeba,\\n author = {Ziwei Liu and Ping Luo and Xiaogang Wang and Xiaoou Tang},\\n title = {Deep Learning Face Attributes in the Wild},\\n booktitle = {ICCV},\\n year = {2015}\\n}\\n\\n@article{fashionmnist,\\n  title={{Fashion-MNIST}: a novel image dataset for benchmarking machine learning algorithms},\\n  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},\\n  journal={arXiv preprint arXiv:1708.07747},\\n  year={2017}\\n}\\n\\n\\n@inproceedings{grandvalet2005semi,\\n  title={{Semi-Supervised Learning by Entropy Minimization}},\\n  author={Grandvalet, Yves and Bengio, Yoshua},\\n  booktitle={NeurIPS},\\n  pages={529--536},\\n  year={2005}\\n}\\n\\n\\n\\n@article{hafner2018reliable,\\n  title={Reliable uncertainty estimates in deep neural networks using noise contrastive priors},\\n  author={Hafner, Danijar and Tran, Dustin and Irpan, Alex and Lillicrap, Timothy and Davidson, James},\\n  journal={arXiv preprint arXiv:1807.09289},\\n  year={2018}\\n}\\n\\n@inproceedings{svigp,\\n  title={Gaussian processes for big data},\\n  author={Hensman, James and Fusi, Nicolo and Lawrence, Neil D},\\n  fbooktitle={Conference on Uncertainty in Artificial Intelligence},\\n   booktitle={Conf. Uncertainty Artificial Intelligence (UAI)},\\n  year={2013}\\n}\\n\\n@article{darlow2018cinic,\\n  title={CINIC-10 is not ImageNet or CIFAR-10},\\n  author={Darlow, Luke N and Crowley, Elliot J and Antoniou, Antreas and Storkey, Amos J},\\n  journal={arXiv preprint arXiv:1810.03505},\\n  year={2018}\\n}\\n\\n\\n@inproceedings{riquelme2018deep,\\n  title={{Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling}},\\n  author={Riquelme, Carlos and Tucker, George and Snoek, Jasper},\\n  booktitle={ICLR},\\n  year={2018}\\n}\\n\\n\\n@inproceedings{lasserre2006principled,\\n  title={Principled hybrids of generative and discriminative models},\\n  author={Lasserre, Julia A and Bishop, Christopher M and Minka, Thomas P},\\n  booktitle={Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on},\\n  volume={1},\\n  pages={87--94},\\n  year={2006},\\n  organization={IEEE}\\n}\\n\\n@inproceedings{kingma2014semi,\\n  title={Semi-supervised learning with deep generative models},\\n  author={Kingma, Diederik P and Mohamed, Shakir and Rezende, Danilo Jimenez and Welling, Max},\\n  booktitle={NeurIPS},\\n  year={2014}\\n}\\n\\n@article{choi2018generative,\\n  title={{Generative Ensembles for Robust Anomaly Detection}},\\n  author={ Choi, Hyunsun and Jang, Eric },\\n  journal={ArXiv e-Print arXiv:1810.01392},\\n  year={2018}\\n}\\n\\n\\n@article{shafaei2018does,\\n  title={{Does Your Model Know the Digit 6 Is Not a Cat? A Less Biased Evaluation of ``Outlier\" Detectors}},\\n  author={Shafaei, Alireza and Schmidt, Mark and Little, James J},\\n  journal={ArXiv e-Print arXiv:1809.04729},\\n  year={2018}\\n}\\n\\n@article{vib,\\n  title={Deep variational information bottleneck},\\n  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin},\\n  journal={arXiv preprint arXiv:1612.00410},\\n  year={2016}\\n}\\n\\n@article{uncertaintyvib,\\n  title={Uncertainty in the variational information bottleneck},\\n  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V},\\n  journal={arXiv preprint arXiv:1807.00906},\\n  year={2018}\\n}\\n\\n@book{nelder1972generalized,\\n  title={{Generalized Linear Models}},\\n  author={Nelder, John Ashworth and Baker, R Jacob},\\n  year={1972},\\n  publisher={Wiley Online Library}\\n}\\n\\n@article{uesato2018adversarial,\\n  title={Adversarial risk and the dangers of evaluating against weak attacks},\\n  author={Uesato, Jonathan and O\\'Donoghue, Brendan and Oord, A{\\\\\"a}ron van den and Kohli, Pushmeet},\\n  journal={arXiv preprint arXiv:1802.05666},\\n  year={2018}\\n}\\n\\n@inproceedings{dinh2016density,\\n  title={{Density Estimation Using Real NVP}},\\n  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},\\n  booktitle={ICLR},\\n  year={2017}\\n}\\n\\n\\n\\n@article{ghosh2009default,\\n  title={Default prior distributions and efficient posterior computation in {B}ayesian factor analysis},\\n  author={Ghosh, Joyee and Dunson, David B},\\n  journal={Journal of Computational and Graphical Statistics},\\n  volume={18},\\n  number={2},\\n  pages={306--320},\\n  year={2009},\\n  publisher={Taylor \\\\& Francis}\\n}\\n\\n\\n@inproceedings{noteevaluation,\\n  title={{A Note on the Evaluation of Generative Models}},\\n  author={Theis, Lucas and van den Oord, A{\\\\\"a}ron and Bethge, Matthias},\\n  booktitle={ICLR},\\n  year={2016}\\n}\\n\\n\\n\\n@inproceedings{rezende2015variational,\\n  title={{Variational Inference with Normalizing Flows}},\\n  author={Rezende, Danilo and Mohamed, Shakir},\\n  booktitle={ICML},\\n  year={2015}\\n}\\n\\n@inproceedings{burda2015importance,\\n  title={{Importance Weighted Autoencoders}},\\n  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},\\n  journal={ICLR},\\n  year={2016}\\n}\\n\\n@article{vae,\\n  title={{Auto-Encoding Variational Bayes}},\\n  author={Kingma, Diederik P and Welling, Max},\\n  journal={ICLR},\\n  year={2014}\\n}\\n\\n\\n\\n@inproceedings{pixelcnn,\\n  title={Conditional Image Generation with Pixel {CNN} Decoders},\\n  author={van den Oord, A{\\\\\"a}ron and Kalchbrenner, Nal and Espeholt, Lasse and Vinyals, Oriol and Graves, Alex and others},\\n  booktitle={NeurIPS},\\n  year={2016}\\n}\\n@article{wavenet,\\n  title={{Wavenet: A Generative Model for Raw Audio}},\\n  author={van den Oord, A{\\\\\"a}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},\\n  journal={ArXiv e-Print},\\n  year={2016}\\n}\\n\\n\\n\\n@article{herbei2006classification,\\n  title={{Classification with Reject Option}},\\n  author={Herbei, Radu and Wegkamp, Marten H},\\n  journal={Canadian Journal of Statistics},\\n  volume={34},\\n  number={4},\\n  pages={709--721},\\n  year={2006},\\n  publisher={Wiley Online Library}\\n}\\n\\n@inproceedings{louizos2017multiplicative,\\n  title={{Multiplicative Normalizing Flows for Variational Bayesian Neural Networks}},\\n  author={Louizos, Christos and Welling, Max},\\n  booktitle={ICML},\\n  year={2017}\\n}\\n\\n\\n\\n@inproceedings{abadi2016tensorflow,\\n  title={Tensorflow: a system for large-scale machine learning.},\\n  author={Abadi, Mart{\\\\\\'\\\\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},\\n  booktitle={OSDI},\\n  volume={16},\\n  pages={265--283},\\n  year={2016}\\n}\\n\\n@inproceedings{deepensembles,\\n  title={{Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles}},\\n  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},\\n  booktitle={NeurIPS},\\n  year={2017}\\n}\\n\\n@inproceedings{gan,\\n  title={{Generative Adversarial Nets}},\\n  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, A{\\\\\"a}ron and Bengio, Yoshua},\\n    booktitle={NeurIPS},\\n  year={2014}\\n}\\n\\n@article{rosca2018distribution,\\n  title={Distribution Matching in Variational Inference},\\n  author={Rosca, Mihaela and Lakshminarayanan, Balaji and Mohamed, Shakir},\\n  journal={arXiv preprint arXiv:1802.06847},\\n  year={2018}\\n}\\n\\n\\n@inproceedings{nair2008analysis,\\n  title={Analysis-by-synthesis by learning to invert generative black boxes},\\n  author={Nair, Vinod and Susskind, Josh and Hinton, Geoffrey E},\\n  booktitle={International Conference on Artificial Neural Networks},\\n  pages={971--981},\\n  year={2008},\\n  organization={Springer}\\n}\\n\\n@inproceedings{liao2018defense,\\n  title={Defense against adversarial attacks using high-level representation guided denoiser},\\n  author={Liao, Fangzhou and Liang, Ming and Dong, Yinpeng and Pang, Tianyu and Zhu, Jun and Hu, Xiaolin},\\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\\n  pages={1778--1787},\\n  year={2018}\\n}\\n\\n@inproceedings{meng2017magnet,\\n  title={Magnet: a two-pronged defense against adversarial examples},\\n  author={Meng, Dongyu and Chen, Hao},\\n  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},\\n  pages={135--147},\\n  year={2017},\\n  organization={ACM}\\n}\\n\\n\\n@inproceedings{hendrycks2016baseline,\\n  title={{A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}},\\n  author={Hendrycks, Dan and Gimpel, Kevin},\\n  booktitle={ICLR},\\n  year={2017}\\n}\\n\\n\\n@article{schott2018robust,\\n  title={Robust Perception through Analysis by Synthesis},\\n  author={Schott, Lukas and Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},\\n  journal={arXiv preprint arXiv:1805.09190},\\n  year={2018}\\n}\\n\\n\\n\\n@article{bishop1994novelty,\\n  title={{Novelty Detection and Neural Network Validation}},\\n  author={Bishop, Christopher M},\\n  journal={IEE Proceedings-Vision, Image and Signal processing},\\n  volume={141},\\n  number={4},\\n  pages={217--222},\\n  year={1994},\\n  publisher={IET}\\n}\\n\\n\\n@article{odin,\\n  title={{Enhancing the Reliability of Out-of-Distribution Image Detection in Neural Networks}},\\n  author={Liang, Shiyu and Li, Yixuan and Srikant, R},\\n  journal={ICLR},\\n  year={2018}\\n}\\n\\n@inproceedings{iaf,\\n  title={Improved variational inference with inverse autoregressive flow},\\n  author={Kingma, Diederik P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},\\n  booktitle={NeurIPS},\\n  year={2016}\\n}\\n\\n@inproceedings{kingma2018glow,\\n  title={{Glow: Generative Flow with Invertible 1x1 Convolutions}},\\n  author={Kingma, Diederik P. and Dhariwal, Prafulla},\\n  booktitle={NeurIPS},\\n  year={2018}\\n}\\n\\n\\n@article{szegedy2013intriguing,\\n  title={{Intriguing Properties of Neural Networks}},\\n  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},\\n  journal={ICLR},\\n  year={2014}\\n}\\n\\n@inproceedings{dlgm,\\n  title={{Stochastic Backpropagation and Approximate Inference in Deep Generative Models}},\\n  author={Rezende, Danilo and Mohamed, Shakir and Wierstra, Daan},\\n  booktitle={ICML},\\n  year={2014}\\n}\\n\\n@article{mackay1999density,\\n  title={{Density Networks}},\\n  author={MacKay, David JC and Gibbs, Mark N},\\n  journal={Statistics and Neural Networks: Advances at the Interface},\\n  year={1999}\\n}\\n\\n\\n@techreport{krizhevsky2009learning,\\n  title={{Learning Multiple Layers of Features from Tiny Images}},\\n  author={Krizhevsky, Alex and Hinton, Geoffrey},\\n  year={2009},\\n  institution={University of Toronto}\\n}\\n\\n@inproceedings{svhn,\\n  title={{Reading Digits in Natural Images with Unsupervised Feature Learning}},\\n  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},\\n  booktitle={NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning},\\n  year={2011}\\n}\\n\\n\\n\\n@article{bishop1995training,\\n  title={{Training with Noise is Equivalent to Tikhonov Regularization}},\\n  author={Bishop, Christopher M},\\n  journal={Neural Computation},\\n  volume={7},\\n  number={1},\\n  pages={108--116},\\n  year={1995},\\n  publisher={MIT Press}\\n}\\n\\n@article{houlsby2011bayesian,\\n  title={{Bayesian Active Learning for Classification and Preference Learning}},\\n  author={Houlsby, Neil and Husz{\\\\\\'a}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\\\\\\'a}t{\\\\\\'e}},\\n  journal={ArXiv e-prints},\\n  year={2011}\\n}\\n\\n@article{efron1975efficiency,\\n  title={{The Efficiency of Logistic Regression Compared to Normal Discriminant Analysis}},\\n  author={Efron, Bradley},\\n  journal={Journal of the American Statistical Association},\\n  volume={70},\\n  number={352},\\n  pages={892--898},\\n  year={1975},\\n  publisher={Taylor \\\\& Francis}\\n}\\n\\n@article{grathwohl2018ffjord,\\n  title={Ffjord: Free-form continuous dynamics for scalable reversible generative models},\\n  author={Grathwohl, Will and Chen, Ricky TQ and Betterncourt, Jesse and Sutskever, Ilya and Duvenaud, David},\\n  journal={arXiv preprint arXiv:1810.01367},\\n  year={2018}\\n}\\n\\n\\n\\n\\n@inproceedings{hernandez2015probabilistic,\\n  title={{Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks}},\\n  author={Hern{\\\\\\'a}ndez-Lobato, Jos{\\\\\\'e} Miguel and Adams, Ryan},\\n  booktitle={ICML},\\n  year={2015}\\n}\\n\\n\\n\\n\\n@inproceedings{jaakkola1997variational,\\n  title={{A Variational Approach to Bayesian Logistic Regression Models and their Extensions}},\\n  author={Jaakkola, Tommi and Jordan, Michael},\\n  booktitle={Sixth International Workshop on Artificial Intelligence and Statistics},\\n  volume={82},\\n  pages={4},\\n  year={1997}\\n}\\n\\n@article{rasmussen2006gaussian,\\n  title={{Gaussian Processes for Machine Learning}},\\n  author={Rasmussen, Carl Edward and Williams, Christopher KI},\\n  year={2006},\\n  publisher={MIT Press}\\n}\\n\\n@article{jordan1999introduction,\\n  title={{An Introduction to Variational Methods for Graphical Models}},\\n  author={Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},\\n  journal={Machine Learning},\\n  volume={37},\\n  number={2},\\n  pages={183--233},\\n  year={1999},\\n  publisher={Springer}\\n}\\n\\n@article{cordella1995method,\\n  title={{A Method for Improving Classification Reliability of Multilayer Perceptrons}},\\n  author={Cordella, Luigi Pietro and De Stefano, Claudio and Tortorella, Francesco and Vento, Mario},\\n  journal={IEEE Transactions on Neural Networks},\\n  volume={6},\\n  number={5},\\n  pages={1140--1147},\\n  year={1995},\\n  publisher={IEEE}\\n}\\n\\n@incollection{fumera2002support,\\n  title={{Support Vector Machines with Embedded Reject Option}},\\n  author={Fumera, Giorgio and Roli, Fabio},\\n  booktitle={Pattern Recognition with Support Vector Machines},\\n  pages={68--82},\\n  year={2002},\\n  publisher={Springer}\\n}\\n\\n@INPROCEEDINGS{platt99,\\n    author = {John C. Platt},\\n    title = {Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods},\\n    booktitle = {Advances in Large Margin Classifiers},\\n    year = {1999},\\n    pages = {61--74},\\n    publisher = {MIT Press}\\n}\\n\\n@article{hellman1970nearest,\\n  title={{The Nearest Neighbor Classification Rule with a Reject Option}},\\n  author={Hellman, Martin E},\\n  journal={IEEE Transactions on Systems Science and Cybernetics},\\n  volume={6},\\n  number={3},\\n  pages={179--185},\\n  year={1970},\\n  publisher={IEEE}\\n}\\n\\n\\n@inproceedings{klambauer2017,\\n  author = {Klambauer, G. and Unterthiner, T. and Mayr, A. and Hochreiter, S.},\\n  title = {Self-Normalizing Neural Networks},\\n  booktitle = {NeurIPS},\\n  year = {2017}\\n}\\n\\n@inproceedings{imagenet_cvpr09,\\n    AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},\\n    TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},\\n    BOOKTITLE = {Computer Vision and Pattern Recognition},\\n    YEAR = {2009},\\n}\\n\\n@article{hochreiter97,\\n author = {Hochreiter, Sepp and Schmidhuber, J\\\\\"{u}rgen},\\n title = {Long Short-Term Memory},\\n journal = {Neural Comput.},\\n issue_date = {November 15, 1997},\\n volume = {9},\\n number = {8},\\n month = nov,\\n year = {1997},\\n pages = {1735--1780},\\n} \\n\\n@inproceedings{srivastava2015training,\\n  title={{Training Very Deep Networks}},\\n  author={Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J{\\\\\"u}rgen},\\n  booktitle={NeurIPS},\\n  year={2015}\\n}\\n\\n@inproceedings{kingma2014adam,\\n  title={{Adam: A Method for Stochastic Optimization}},\\n  author={Kingma, Diederik and Ba, Jimmy},\\n  booktitle={ICLR},\\n  year={2014}\\n}\\n\\n@inproceedings{wu2018,\\n  title={{Deterministic Variational Inference for Robust Bayesian Neural Networks}},\\n  author={Anqi Wu and Sebastian Nowozin and Edward Meeds and Richard E. Turner and Jose Miguel Hernandez-Lobato and Alexander L. Gaunt},\\n  booktitle={ICLR},\\n  year={2019},\\n}\\n\\n@article{hora,\\n  title={Probability judgments for continuous quantities: Linear combinations and calibration},\\n  author={Hora, Stephen C},\\n  journal={Management Science},\\n  volume={50},\\n  number={5},\\n  pages={597--604},\\n  year={2004},\\n  publisher={INFORMS}\\n}\\n\\n\\n\\n\\n\\n@article{alipanahi2015predicting,\\n  title={Predicting the sequence specificities of {DNA}-and {RNA}-binding proteins by deep learning},\\n  author={Alipanahi, Babak and Delong, Andrew and Weirauch, Matthew T and Frey, Brendan J},\\n  journal={Nature biotechnology},\\n  volume={33},\\n  number={8},\\n  pages={831--838},\\n  year={2015},\\n  publisher={Nature Research}\\n}\\n\\n@article{zhou2015predicting,\\n  title={Predicting effects of noncoding variants with deep learning-based sequence model},\\n  author={Zhou, Jian and Troyanskaya, Olga G},\\n  journal={Nature methods},\\n  volume={12},\\n  number={10},\\n  pages={931--934},\\n  year={2015},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@inproceedings{guo2017calibration,\\n author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},\\n title = {On Calibration of Modern Neural Networks},\\n booktitle = {International Conference on Machine Learning},\\n year = {2017}\\n} \\n\\n@article{snapshotensembles,\\n  title={SNAPSHOT ENSEMBLES: TRAIN 1, GET {M} FOR FREE},\\n  author={Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger },\\n  journal={ICLR submission},\\n  year={2017}\\n}\\n\\n@article{jacobs1991adaptive,\\n  title={Adaptive mixtures of local experts},\\n  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},\\n  journal={Neural computation},\\n  volume={3},\\n  number={1},\\n  pages={79--87},\\n  year={1991},\\n  publisher={MIT Press}\\n}\\n\\n@article{bellemare2017distributional,\\n  title={A distributional perspective on reinforcement learning},\\n  author={Bellemare, Marc G and Dabney, Will and Munos, R{\\\\\\'e}mi},\\n  journal={arXiv preprint arXiv:1707.06887},\\n  year={2017}\\n}\\n\\n\\n\\n@article{stacking,\\n  title={Stacked generalization},\\n  author={Wolpert, David H},\\n  journal={Neural networks},\\n  volume={5},\\n  number={2},\\n  pages={241--259},\\n  year={1992},\\n  publisher={Elsevier}\\n}\\n\\n@inproceedings{nguyen2015deep,\\n  title={Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},\\n  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},\\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\\n  pages={427--436},\\n  year={2015}\\n}\\n\\n@article{amodei2016concrete,\\n  title={Concrete problems in {AI} safety},\\n  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\\\\\\'e}, Dan},\\n  journal={arXiv preprint arXiv:1606.06565},\\n  year={2016}\\n}\\n\\n\\n@article{clarke2003comparing,\\n  title={Comparing {B}ayes model averaging and stacking when model approximation error cannot be ignored},\\n  author={Clarke, Bertrand},\\n    journal={J. Mach. Learn. Res. (JMLR)},\\n  volume={4},\\n  pages={683--712},\\n  year={2003},\\n  publisher={JMLR. org}\\n}\\n\\n\\n\\n\\n\\n@inproceedings{resnet,\\n  title={Deep residual learning for image recognition},\\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\\n  pages={770--778},\\n  year={2016}\\n}\\n\\n\\n\\n@article{distillation,\\n  title={Distilling the knowledge in a neural network},\\n  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},\\n  journal={arXiv preprint arXiv:1503.02531},\\n  year={2015}\\n}\\n\\n\\n@article{cifar10,\\n  title={Learning Multiple Layers of Features from Tiny Images},\\n  author={Krizhevsky, Alex},\\n  year={2009}\\n}\\n\\n@inproceedings{inception,\\n  title={Rethinking the inception architecture for computer vision},\\n  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},\\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\\n  pages={2818--2826},\\n  year={2016}\\n}\\n\\n@article{imagenet,\\nAuthor = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},\\nTitle = {{ImageNet Large Scale Visual Recognition Challenge}},\\nYear = {2015},\\njournal   = {International Journal of Computer Vision (IJCV)},\\nvolume={115},\\nnumber={3},\\npages={211-252}\\n}\\n\\n@article{popart,\\n  title={Learning functions across many orders of magnitudes},\\n  author={van Hasselt, Hado and Guez, Arthur and Hessel, Matteo and Silver, David},\\n  journal={arXiv preprint arXiv:1602.07714},\\n  year={2016}\\n}\\n\\n@article{oliveira2016known,\\n  title={Known Unknowns: Uncertainty Quality in {B}ayesian Neural Networks},\\n  author={Oliveira, Ramon and Tabacof, Pedro and Valle, Eduardo},\\n  journal={arXiv preprint arXiv:1612.01251},\\n  year={2016}\\n}\\n\\n@article{louizos2016structured,\\n  title={Structured and Efficient Variational Deep Learning with Matrix {G}aussian Posteriors},\\n  author={Louizos, Christos and Welling, Max},\\n  journal={arXiv preprint arXiv:1603.04733},\\n  year={2016}\\n}\\n\\n\\n@inproceedings{bootstrapdqn,\\n  title={Deep Exploration via Bootstrapped {DQN}},\\n  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},\\n  Booktitle = {NeurIPS}, fBooktitle = {Adv. Neural Information Proc. Systems (NeurIPS)},\\n  year={2016}\\n}\\n\\n\\n@article{snep,\\n  title={Distributed {B}ayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server},\\n  author={Hasenclever, Leonard and Webb, Stefan and Lienart, Thibaut and Vollmer, Sebastian and Lakshminarayanan, Balaji and Blundell, Charles and Teh, Yee Whye},\\n  journal={arXiv preprint arXiv:1512.09327},\\n  year={2015}\\n}\\n\\n@inproceedings{nix1994estimating,\\n  title={Estimating the mean and variance of the target probability distribution},\\n  author={Nix, David A and Weigend, Andreas S},\\n  booktitle={IEEE International Conference on Neural Networks},\\n  year={1994}\\n}\\n\\n@article{gneiting2005calibrated,\\n  title={Calibrated probabilistic forecasting using ensemble model output statistics and minimum {CRPS} estimation},\\n  author={Gneiting, Tilmann and Raftery, Adrian E and Westveld III, Anton H and Goldman, Tom},\\n  journal={Monthly Weather Review},\\n  year={2005}\\n}\\n\\n@book{brown74,\\n  author={Thomas A. Brown},\\n  title={Admissible Scoring Systems for Continuous Distributions},\\n  publisher={{RAND} {C}orporation},\\n  year={1974},\\n  address={Santa Monica}\\n}\\n\\n@article{Hersbach00,\\n  author = {Hersbach, Hans},\\n  title = {Decomposition of the Continuous Ranked Probability Score for Ensemble Ensemble Prediction Systems},\\n  journal = {Weather and Forecasting},\\n  volume = {15},\\n  number = {5},\\n  pages = {559-570},\\n  year = {2000},\\n}\\n\\n@article{unger85,\\n  author={Unger, D. A.},\\n  year={1985},\\n  title={A method to estimate the continuous ranked probability score},\\n  booktitle={Probability and Statistics in Atmospheric Sciences},\\n  publisher={Amer. Meteor. Soc.},\\n  pages={206213}\\n}\\n\\n@article{matheson76,\\n  author={Matheson, J. E., and R. L. Winkler},\\n  year={1976},\\n  title={Scoring rules for continuous probability distributions},\\n  journal={Manage Sci},\\n  volume={22},\\n  pages={1087-1095}\\n}\\n\\n@inproceedings{carney1999confidence,\\n  title={Confidence and prediction intervals for neural network ensembles},\\n  author={Carney, John G and Cunningham, P{\\\\\\'a}draig and Bhagwan, Umesh},\\n  booktitle={IJCNN},\\n  year={1999},\\n\\n}\\n\\n@inproceedings{SEP,\\n  title={Stochastic expectation propagation},\\n  author={Li, Yingzhen and Hern{\\\\\\'a}ndez-Lobato, Jos{\\\\\\'e} Miguel and Turner, Richard E},\\n  Booktitle = {NeurIPS}, fBooktitle = {Adv. Neural Information Proc. Systems (NeurIPS)},\\n  year={2015}\\n}\\n\\n@article{mnih2015human,\\n  title={Human-level control through deep reinforcement learning},\\n  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},\\n  journal={Nature},\\n  volume={518},\\n  number={7540},\\n  pages={529--533},\\n  year={2015},\\n  publisher={Nature Publishing Group}\\n}\\n\\n\\n\\n@inproceedings{adam,\\n  title={Adam: A method for stochastic optimization},\\n  author={Kingma, Diederik P and Ba, Jimmy},\\nBooktitle = {ICLR}, fBooktitle = {Proc. Int. Conf. Learning Representations (ICLR)},\\n  year={2015}\\n}\\n\\n@article{rmsprop,\\n  title={{Lecture 6.5-RMSprop: Divide the gradient by a running average of its recent magnitude}},\\n  author={Tieleman, Tijmen and Hinton, Geoffrey},\\n  journal={Coursera: Neural Networks for Machine Learning},\\n  year={2012}\\n}\\n\\n@inproceedings{adversarial,\\n  title={Explaining and harnessing adversarial examples},\\n  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},\\n   Booktitle = {ICLR}, fBooktitle = {Proc. Int. Conf. Learning Representations (ICLR)},\\n  year={2015}\\n}\\n\\n\\n@book{bernardo2009bayesian,\\n  title={Bayesian Theory},\\n  author={Bernardo, Jos{\\\\\\'e} M and Smith, Adrian FM},\\n  volume={405},\\n  year={2009},\\n  publisher={John Wiley \\\\& Sons}\\n}\\n\\n@article{zhang2015universum,\\n  title={Universum Prescription: Regularization using Unlabeled Data},\\n  author={Zhang, Xiang and LeCun, Yann},\\n  journal={arXiv preprint arXiv:1511.03719},\\n  year={2015}\\n}\\n\\n\\n@article{bishop1994mixture,\\n  title={Mixture density networks},\\n  author={Bishop, Christopher M},\\n  year={1994},\\n  publisher={Aston University}\\n}\\n\\n@TECHREPORT{Bishop94mixturedensity,\\n    author = {Christopher M. Bishop},\\n    title = {Mixture density networks},\\n    institution = {},\\n    year = {1994}\\n}\\n\\n\\n\\n\\n@article{ghahramani2015probabilistic,\\n  title={Probabilistic machine learning and artificial intelligence},\\n  author={Ghahramani, Zoubin},\\n  journal={Nature},\\n  volume={521},\\n  number={7553},\\n  pages={452--459},\\n  year={2015},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@inproceedings{bayesiandark,\\n  title={Bayesian dark knowledge},\\n  author={Korattikara, Anoop and Rathod, Vivek and Murphy, Kevin and Welling, Max},\\n  Booktitle = {NeurIPS}, fBooktitle = {Adv. Neural Information Proc. Systems (NeurIPS)},\\n  year={2015}\\n}\\n\\n@inproceedings{variationaldropout,\\n  title={Variational dropout and the local reparameterization trick},\\n  author={Kingma, Diederik P and Salimans, Tim and Welling, Max},\\n  Booktitle = {NeurIPS}, fBooktitle = {Adv. Neural Information Proc. Systems (NeurIPS)},\\n  year={2015}\\n}\\n\\n@article{maeda2014,\\n  title={A {B}ayesian encourages dropout},\\n  author={Maeda, Shin-ichi},\\n  journal={arXiv preprint arXiv:1412.7003},\\n  year={2014}\\n}\\n\\n@inproceedings{gal,\\n  title={Dropout as a {B}ayesian approximation: Representing model uncertainty in deep learning},\\n  author={Gal, Yarin and Ghahramani, Zoubin},\\nBooktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2016}\\n}\\n\\n@article{galold,\\n  title={Dropout as a {B}ayesian approximation: Representing model uncertainty in deep learning},\\n  author={Gal, Yarin and Ghahramani, Zoubin},\\n  journal={arXiv preprint arXiv:1506.02142},\\n  year={2015}\\n}\\n\\n@article{dropout,\\n  title={Dropout: A simple way to prevent neural networks from overfitting},\\n  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},\\n  journal={JMLR},\\n  year={2014},\\n  publisher={JMLR. org}\\n  }\\n  \\n  @article{mr1763essay,\\n  title={{An Essay towards solving a Problem in the Doctrine of Chances. By the late Rev. Mr. {B}ayes, FRS communicated by Mr. Price, in a letter to John Canton, AMFRS}},\\n  author={Mr. Bayes and Price, Mr},\\n  journal={Philosophical Transactions (1683-1775)},\\n  pages={370--418},\\n  year={1763},\\n  publisher={JSTOR}\\n}\\n\\n@article{szegedy2015rethinking,\\n  title={Rethinking the Inception Architecture for Computer Vision},\\n  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},\\n  journal={arXiv preprint arXiv:1512.00567},\\n  year={2015}\\n}\\n\\n@inproceedings{intriguing,\\n  title={Intriguing properties of neural networks},\\n  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},\\nyear  = 2014,\\nBooktitle = {ICLR}, fBooktitle = {Proc. Int. Conf. Learning Representations (ICLR)},\\n}\\n\\n  @article{dawid1982well,\\n  title={The well-calibrated {B}ayesian},\\n  author={Dawid, A Philip},\\n  journal={Journal of the American Statistical Association},\\n  year={1982},\\n}\\n\\n@inproceedings{khan18,\\n  title={Fast and Scalable Bayesian Deep Learning by  Weight-Perturbation in Adam},\\n  booktitle={ICML},\\n  year={2018},\\n  author={Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin Gal, Akash Srivastava}\\n}\\n  \\n@inproceedings{Kingma15,\\n  title = {Variational Dropout and the Local Reparameterization Trick},\\n  author = {Kingma, Durk P and Salimans, Tim and Welling, Max},\\n  booktitle = {NeurIPS},\\n  year = {2015},\\n}\\n \\n@inproceedings{Welling11,\\n  title={{B}ayesian learning via stochastic gradient {L}angevin dynamics},\\n  author={Welling, Max and Teh, Yee Whye},\\nBooktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2011}\\n}\\n\\n@article{hanley1982meaning,\\n  title={{The meaning and use of the area under a receiver operating characteristic (ROC) curve}},\\n  author={Hanley, James A and McNeil, Barbara J},\\n  journal={Radiology},\\n  year={1982}\\n}\\n  \\n  @inproceedings{rasmussen2005healing,\\n  title={Healing the relevance vector machine through augmentation},\\n  author={Rasmussen, Carl Edward and Quinonero-Candela, Joaquin},\\nBooktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2005}\\n}\\n\\n  @article{brier1950verification,\\n  title={Verification of forecasts expressed in terms of probability},\\n  author={Brier, Glenn W},\\n  journal={Monthly weather review},\\n  year={1950}\\n}\\n\\n  @inproceedings{graves,\\n  title={Practical variational inference for neural networks},\\n  author={Graves, Alex},\\n  Booktitle = {NeurIPS}, fBooktitle = {Adv. Neural Information Proc. Systems (NeurIPS)},\\n  year={2011}\\n}\\n\\n@article{degroot1983comparison,\\n  title={The comparison and evaluation of forecasters},\\n  author={DeGroot, Morris H and Fienberg, Stephen E},\\n  journal={The statistician},\\n  year={1983},\\n  publisher={JSTOR}\\n}\\n\\n@inproceedings{caruana2004data,\\n  title={Data mining in metric space: an empirical analysis of supervised learning performance criteria},\\n  author={Caruana, Rich and Niculescu-Mizil, Alexandru},\\n  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},\\n  pages={69--78},\\n  year={2004},\\n  organization={ACM}\\n}\\n\\n@inproceedings{niculescu2005predicting,\\n  title={Predicting good probabilities with supervised learning},\\n  author={Niculescu-Mizil, Alexandru and Caruana, Rich},\\nBooktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2005}\\n}\\n\\n@incollection{quinonero2006evaluating,\\n  title={Evaluating predictive uncertainty challenge},\\n  author={Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\\\\\"o}lkopf, Bernhard},\\n  booktitle={Machine Learning Challenges},\\n  year={2006},\\n  publisher={Springer}\\n}\\n\\n@article{RF,\\n\\tAuthor = {Breiman, L.},\\n        journal={Machine learning},\\n        fjournal={Machine learning},\\n\\tNumber = {1},\\n\\tPages = {5--32},\\n\\tPublisher = {Springer},\\n\\tTitle = {Random forests},\\n\\tVolume = {45},\\n\\tYear = {2001}}\\n\\n@article{ERT,\\n  title={Extremely randomized trees},\\n  author={Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},\\n  fjournal={Machine learning},\\n  journal={Machine learning},\\n  volume={63},\\n  number={1},\\n  pages={3--42},\\n  year={2006},\\n  publisher={Springer}\\n}\\n\\n@article{lecun2015deep,\\n  title={Deep learning},\\n  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},\\n  journal={Nature},\\n  volume={521},\\n  number={7553},\\n  pages={436--444},\\n  year={2015},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{bagging,\\n  title={Bagging predictors},\\n  author={Breiman, Leo},\\n  volume={24},\\n  fjournal={Machine learning},\\n  journal={Machine learning},\\n  number={2},\\n  pages={123--140},\\n  year={1996},\\n  publisher={Springer}\\n}\\n\\n@inproceedings{PB,\\n  title={Probabilistic Backpropagation for Scalable Learning of {B}ayesian Neural Networks},\\n  author={Hern{\\\\\\'a}ndez-Lobato, Jos{\\\\\\'e} Miguel and Adams, Ryan P},\\n  Booktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2015}\\n}\\n\\n@inproceedings{relu,\\n  title={Rectified linear units improve restricted {B}oltzmann machines},\\n  author={Nair, Vinod and Hinton, Geoffrey E},\\n  Booktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2010}\\n}\\n\\n@inproceedings{BBB,\\n  title={Weight Uncertainty in Neural Networks},\\n  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},\\n  Booktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2015}\\n}\\n\\n@inproceedings{mf,\\n  title={Mondrian forests: Efficient online random forests},\\n  author={Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},\\n  Booktitle = {NeurIPS}, fBooktitle = {Adv. Neural Information Proc. Systems (NeurIPS)},\\n  year={2014}\\n}\\n\\n@inproceedings{mfr,\\n  title={Mondrian forests for large scale regression when uncertainty matters},\\n  author={Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},\\n  fbooktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},\\n    booktitle={Int. Conf. Artificial Intelligence Stat. (AISTATS)},\\n     year={2016}\\n}\\n\\n@inproceedings{dgp,\\n  title={Distributed {G}aussian Processes},\\n  author={Deisenroth, Marc Peter and Ng, Jun Wei},\\n   Booktitle = {ICML},     fBooktitle = {Proc. Int. Conf. Mach. Learn. (ICML)},\\n  year={2015}\\n}\\n\\n@book{Neal96,\\n    author = {Neal, Radford M.},\\n    publisher = {Springer-Verlag New York, Inc.},\\n    title = {{Bayesian Learning for Neural Networks}},\\n    year = {1996}\\n}\\n\\n\\n@inproceedings{svigp,\\n  title={Gaussian processes for big data},\\n  author={Hensman, James and Fusi, Nicolo and Lawrence, Neil D},\\n  booktitle={Conference on Uncertainty in Artificial Intelligence},\\n  pages={282--290},\\n  year={2013}\\n}\\n\\n\\n@incollection{dietterich2000ensemble,\\n  title={Ensemble methods in machine learning},\\n  author={Dietterich, Thomas G},\\n  booktitle={Multiple classifier systems},\\n  year={2000},\\n}\\n\\n\\n@phdthesis{mackay1992bayesian,\\n  title={Bayesian methods for adaptive models},\\n  author={MacKay, David JC},\\n  year={1992},\\n  school={California Institute of Technology}\\n}\\n\\n@article{bui2016deep,\\n      title={Deep {G}aussian Processes for Regression using Approximate Expectation Propagation},\\n        author={Bui, Thang D and Hern{\\\\\\'a}ndez-Lobato, Daniel and Li, Yingzhen and Hern{\\\\\\'a}ndez-Lobato, Jos{\\\\\\'e} Miguel and Turner, Richard E},\\n          journal={arXiv preprint arXiv:1602.04133},\\n            year={2016}\\n}\\n\\n\\n@article{minka2000bayesian,\\n      title={Bayesian model averaging is not model combination},\\n        author={Minka, Thomas P},\\n              year={2000}\\n}\\n\\n@article{gneiting2007strictly,\\n  title={Strictly proper scoring rules, prediction, and estimation},\\n  author={Gneiting, Tilmann and Raftery, Adrian E},\\n  journal={Journal of the American Statistical Association},\\n  volume={102},\\n  number={477},\\n  pages={359--378},\\n  year={2007},\\n  publisher={Taylor \\\\& Francis}\\n}\\n\\n\\n@article{hinton2012deep,\\n      title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},\\n        author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},\\n          journal={Signal Processing Magazine, IEEE},\\n            volume={29},\\n              number={6},\\n                pages={82--97},\\n                  year={2012},\\n                    publisher={IEEE}\\n}\\n\\n\\n@inproceedings{krizhevsky2012imagenet,\\n      title={Imagenet classification with deep convolutional neural networks},\\n        author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},\\n          Booktitle = {NeurIPS}, fBooktitle = {Adv. Neural Information Proc. Systems (NeurIPS)},\\n              year={2012}\\n}\\n\\n@article{mikolov2013efficient,\\n      title={Efficient estimation of word representations in vector space},\\n        author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},\\n          journal={arXiv preprint arXiv:1301.3781},\\n            year={2013}\\n}\\n\\n@inproceedings{vizier,\\n  title={Google vizier: A service for black-box optimization},\\n  author={Golovin, Daniel and Solnik, Benjamin and Moitra, Subhodeep and Kochanski, Greg and Karro, John and Sculley, D},\\n  booktitle={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\\n  pages={1487--1495},\\n  year={2017},\\n  organization={ACM}\\n}\\n\\n@inproceedings{hensman2015scalable,\\n  title={Scalable variational Gaussian process classification},\\n  booktitle={International Conference on Artificial Intelligence and Statistics},\\n  author={Hensman, James and Matthews, Alexander and Ghahramani, Zoubin},\\n  year={2015},\\n  publisher={JMLR}\\n}\\n\\n\\n@article{sculley2018winner,\\n  title={Winner\\'s curse? {O}n pace, progress, and empirical rigor},\\n  author={Sculley, D and Snoek, Jasper and Wiltschko, Alex and Rahimi, Ali},\\n  booktitle={{ICLR} Workshops},\\n  year={2018}\\n}\\n\\n@article{lipton2018troubling,\\n  title={Troubling trends in machine learning scholarship},\\n  author={Lipton, Zachary C and Steinhardt, Jacob},\\n  journal={arXiv preprint arXiv:1807.03341},\\n  year={2018}\\n}\\n\\n@misc{rahimi2017addendum,\\n  title={An addendum to alchemy},\\n  author={Rahimi, A and Recht, B},\\n  year={2017}\\n}\\n',\n",
       "  'arxiv_citations': {'1807.00906': True,\n",
       "   '1606.06565': True,\n",
       "   '1811.00995': True,\n",
       "   '1604.07316': True,\n",
       "   '1312.3005': True,\n",
       "   '1807.03341': True,\n",
       "   '1603.04733': True,\n",
       "   '1902.02767': True,\n",
       "   '1906.02845': True,\n",
       "   '1809.04729': True,\n",
       "   '1803.04386': True,\n",
       "   '1906.02530': True,\n",
       "   '1807.00263': True,\n",
       "   '1711.10604': True,\n",
       "   '1802.03916': True,\n",
       "   '1802.10501': True,\n",
       "   '1706.10295': True,\n",
       "   '1708.07747': True,\n",
       "   '1807.09289': True,\n",
       "   '1810.03505': True,\n",
       "   '1810.01392': True,\n",
       "   '1612.00410': True,\n",
       "   '1802.05666': True,\n",
       "   '1802.06847': True,\n",
       "   '1805.09190': True,\n",
       "   '1810.01367': True,\n",
       "   '1707.06887': True,\n",
       "   '1503.02531': True,\n",
       "   '1602.07714': True,\n",
       "   '1612.01251': True,\n",
       "   '1512.09327': True,\n",
       "   '1511.03719': True,\n",
       "   '1412.7003': True,\n",
       "   '1506.02142': True,\n",
       "   '1512.00567': True,\n",
       "   '1602.04133': True,\n",
       "   '1301.3781': True}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1806.03820v1',\n",
       "  'title': 'An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning',\n",
       "  'authors': ['Dhruv Malik',\n",
       "   'Malayandi Palaniappan',\n",
       "   'Jaime F. Fisac',\n",
       "   'Dylan Hadfield-Menell',\n",
       "   'Stuart Russell',\n",
       "   'Anca D. Dragan'],\n",
       "  'date_published': '2018-06-11 06:06:43+00:00',\n",
       "  'data_last_modified': '2018-06-11 06:06:43+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1806.03820v1',\n",
       "  'abstract': \"Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.\",\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'unlabeled',\n",
       "  'confidence_score': 0.9643339434,\n",
       "  'main_tex_filename': './example_paper.tex',\n",
       "  'text': '---\\nabstract: |\\n  Our goal is for AI systems to correctly identify and act according to their human user\\'s objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this *value alignment* problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL\\'s assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.\\nbibliography:\\n- example_paper.bib\\n---\\n\\nIntroduction {#Intro}\\n============\\n\\nAs AI agents improve in their ability to optimize for a given objective, it becomes increasingly important that these agents pursue the *right* objective. The *value alignment* problem [@hadfield2016cooperative; @Bostrom:2014:SPD:2678074] is that of ensuring that robots optimize for what people want---that robot objectives are aligned with their end-users\\' objectives. (We henceforth use robot to refer generically to an AI agent.)\\n\\nA highly-capable autonomous agent working towards the wrong goal can cause undesired effects, the magnitude of which will tend to increase with the capabilities of the agent. Unfortunately, we humans have a hard time specifying what it is that we actually want. For example, customers may give mistaken instructions to an AI system and system designers may select simple, but potentially incorrect, reward functions to optimize\\xa0[@faulty]. Optimizing for the wrong objective can lead to unintended and negative consequences\\xa0[@DBLP:journals/corr/AmodeiOSCSM16].\\n\\n![A CIRL game. The human **H** and the robot **R** need to work together to prepare a meal. **R** starts off unaware of which meal **H** wants, but both **H** and **R** get rewarded only if they prepare **H**\\'s desired meal. Solving such a CIRL game has thus far been intractable. In Section [3](#bellman){reference-type=\"ref\" reference=\"bellman\"}, we derive a modified Bellman update for computing optimal solutions to CIRL games that achieves an exponential reduction in running time and relaxes CIRL\\'s assumption of human rationality. ](v_alignment.png){#value_alignment width=\"\\\\\\\\columnwidth\"}\\n\\nRather than optimize a pre-specified reward function, a robot may instead attempt to infer what people *internally* want but cannot perfectly explicate. The robot can use a person\\'s actions to learn about the reward function over time. The most common approach for this is Inverse Reinforcement Learning (IRL) [@ng2000algorithms]. IRL makes two implicit assumptions: 1) that the robot is a passive observer, watching the human, and 2) that the human acts as an expert in isolation, ignoring that the robot needs to learn.\\n\\nCooperative Inverse Reinforcement Learning (CIRL) [@hadfield2016cooperative] relaxes these two assumptions. It proposes a formulation in which the human $\\\\mathbf{H}$ and the robot $\\\\mathbf{R}$ are on the same team and collaborate to achieve the same goal. CIRL is a two-player game between $\\\\mathbf{H}$ and $\\\\mathbf{R}$, in which *both take actions*, and *both get rewarded according to the same reward function*. The key to CIRL is that only $\\\\mathbf{H}$ knows the parameters of this reward function.\\n\\nTake for instance the domain from Figure\\xa0[1](#value_alignment){reference-type=\"ref\" reference=\"value_alignment\"}. $\\\\mathbf{H}$ and $\\\\mathbf{R}$ work to prepare a meal using three ingredient types: bread, meat, and tomatoes. $\\\\mathbf{H}$ wants to prepare either a sandwich (2 bread, 1 meat, 0 tomatoes), or tomato soup (1 bread, 1 meat, 2 tomatoes). $\\\\mathbf{R}$ does not know *a priori* which meal $\\\\mathbf{H}$ wants, and, to emulate the difficulty that people have in specifying what they want, we assume $\\\\mathbf{H}$ cannot explicate this information directly to $\\\\mathbf{R}$. At every time step, $\\\\mathbf{R}$ and $\\\\mathbf{H}$ each prepare a single unit of any ingredient, or no ingredient at all. They both receive reward of 1 if they succeed in preparing the right recipe, and 0 otherwise.\\n\\nIn this domain, CIRL captures that the human has an incentive for the robot to infer the correct recipe; and that the robot can take actions in response to the human\\'s, as opposed to passively waiting until it knows which recipe is right. Crucially, the robot shares the reward function and has an incentive to maximize the human\\'s internal reward. This creates an incentive to mitigate and avoid unintended consequences from misspecified rewards.\\n\\nSolving a CIRL game, however, amounts to solving a Dec-POMDP. Previous work has shown that a CIRL game can be reduced to a POMDP. However, the action space in this POMDP is exponential in the size of the reward parameter space. Since POMDP algorithms scale poorly with the size of the action space, non-trivial CIRL games remain difficult to solve with this approach. Additionally, solutions to CIRL are only optimal under the assumption that the human is optimal. This is a strong assumption: it is a well-established fact in cognitive science that humans are often sub-optimal in decision making [@tversky1975judgment; @simon1957models]. Our contributions in this paper are three-fold:\\n\\n**1. A Modified Bellman Update:** We exploit the fact that the human is a full information agent in CIRL to derive an optimality-preserving modification of the standard Bellman update. This reduces the complexity of the problem by an exponential factor. We show how to apply this modification to existing POMDP solvers (both exact and approximate).\\n\\nWe further show that our modified Bellman update allows us to relax CIRL\\'s assumption of human rationality. We instead only require that the human\\'s policy be parameterized by her Q-values. This allows us to solve more realistic CIRL games where the human is modelled as sub-optimal.\\n\\n**2. Empirical Comparison:** We show empirically that our method helps scale POMDP solvers to CIRL games with larger reward parameter and action spaces. We find a speed-up of several orders of magnitude for exact methods, and substantial improvements in value for approximate methods.\\n\\n**3. Implications:** With the ability to solve more complex CIRL problems, we analyze the solutions that emerge. In contrast to IRL, we see solutions that exhibit implicit communication. The human takes explicitly suboptimal actions that are better signals for the right reward, and the robot attains higher value for the human because it can take advantage of these signals to learn faster. The coordination that emerges is a consequence of the human and robot being on the same team and reasoning about helping each other.\\n\\nBackground\\n==========\\n\\nPOMDPs\\n------\\n\\nPOMDPs provide a rich model for planning under uncertainty [@sondik71; @Kaelbling:1998:PAP:1643275.1643301]. Formally, a POMDP is a tuple $\\\\langle X, A, Z, T, O, r, \\\\gamma\\\\rangle$ where $X$ is the set of states; $A$ is the set of the agent\\'s actions; $Z$ is the set of observations; $T(x_t, a_t, x_{t+1})$ is the transition distribution; $O(x_{t+1},a_t,z_{t+1})$ is the observation distribution; $r$ is the reward function; and $\\\\gamma$ is the discount factor.\\n\\nConsider a simplified instance of the cooking task from Figure\\xa0[1](#value_alignment){reference-type=\"ref\" reference=\"value_alignment\"} where $\\\\mathbf{H}$ picks her actions according to only her desired recipe and the quantity of each ingredient prepared so far, i.e., she does not consider $\\\\mathbf{R}$\\'s past or future behavior when picking her actions. The simplified cooking task is now a POMDP: $\\\\mathbf{R}$ is the agent and $\\\\mathbf{H}$ is a part of the environment. The state specifies $\\\\mathbf{H}$\\'s desired recipe and the quantity of each ingredient already prepared. Thus, $\\\\mathbf{H}$ picks her actions as a function of only the state.\\n\\nIn a POMDP, the agent cannot observe the state; instead, it maintains a belief $b\\\\in \\\\Delta X$, where $b(x)$ is the probability that the agent is in state $x$. At each time step, the agent receives an observation that helps inform its decisions. The agent in our cooking task, $\\\\mathbf{R}$, does not know $\\\\mathbf{H}$\\'s desired recipe---a component of the state. $\\\\mathbf{R}$ observes $\\\\mathbf{H}$\\'s actions and tries to infer the desired recipe from $\\\\mathbf{H}$\\'s behavior.\\n\\nThe behavior of the agent is specified by a conditional plan $\\\\sigma = (a, v)$; $a$ denotes the agent\\'s action and $v$ is a mapping from observations to future conditional plans for the agent to follow. An example conditional plan for $\\\\mathbf{R}$ is: prepare meat now and if $\\\\mathbf{H}$ responds by preparing bread, prepare a second slice of bread; if $\\\\mathbf{H}$ prepares tomatoes, prepare another batch of tomatoes; or if $\\\\mathbf{H}$ prepares meat, do not prepare any ingredient.\\n\\nThe $\\\\alpha$-vector of a conditional plan contains the value of following the plan at any given state: $$\\\\begin{aligned}\\n    \\\\alpha_{\\\\sigma}(x)  &=R(x) + \\\\gamma\\\\sum_{x\\'\\\\in X}\\\\sum_{z\\\\in Z}P(x\\', z\\\\mid x, a)\\\\alpha_{v(z)}(x\\')\\\\label{alpha_basic}\\\\end{aligned}$$ The value of a plan at a belief $b$ is the expected value of the plan across the states i.e. $V_{\\\\sigma}(b) = b \\\\cdot \\\\alpha_{\\\\sigma} = \\\\sum_{x\\\\in X} b(x)\\\\alpha_{\\\\sigma}(x)$. The goal of an agent in a POMDP is to find the plan with maximal value from her current belief.\\n\\nValue iteration [@sondik71] can be used to compute the optimal conditional plan. This algorithm starts at the horizon and works backwards. It generates new conditional plans at each time-step and evaluates them according to Eq. 1. It constructs all potentially optimal plans of length $T$ and selects the one with maximal value at the initial belief.\\n\\nCooperative Inverse Reinforcement Learning\\n------------------------------------------\\n\\nNow, consider an instance of the cooking task where $\\\\mathbf{H}$ is a second agent in the game and no longer behaves independently of $\\\\mathbf{R}$. There is now a strong interdependence between $\\\\mathbf{H}$\\'s and $\\\\mathbf{R}$\\'s behavior: $\\\\mathbf{H}$\\'s actions both depend on and influence $\\\\mathbf{R}$\\'s belief. This problem is now no longer a POMDP; it is a CIRL game.\\n\\nA CIRL game is an asymmetric-information two-player game between a human $\\\\mathbf{H}$ and a robot $\\\\mathbf{R}$ \\xa0[@hadfield2016cooperative]. $\\\\mathbf{H}$ knows the true reward function and $\\\\mathbf{R}$ does not initially. Formally, a CIRL game is a tuple: $M = \\\\langle X, \\\\{\\\\mathcal{A}^H, \\\\mathcal{A}^R\\\\}, T, \\\\{\\\\Theta, r\\\\}, \\\\gamma\\\\rangle$. $X$ is the set of observable world-states; $\\\\mathcal{A}^H$ and $\\\\mathcal{A}^R$ are the actions available to $\\\\mathbf{H}$ and $\\\\mathbf{R}$ respectively; $T(x_t, a^H_t, a^R_t, x_{t+1})$ is the transition distribution; $\\\\Theta$ is the set of reward parameters; $r$ is the parameterized reward function shared by both agents; $\\\\gamma$ is the discount factor. A solution to a CIRL game is a pair of policies---one for $\\\\mathbf{H}$ and $\\\\mathbf{R}$ each---that maximizes the expected reward obtained by $\\\\mathbf{H}$ and $\\\\mathbf{R}$.\\n\\nIn our cooking task, $\\\\Theta$ is the set of possible recipes. $\\\\mathbf{R}$ does not know $\\\\mathbf{H}$\\'s desired recipe, $\\\\theta\\\\in \\\\Theta$. The reward function $r$ is parameterized by $\\\\Theta$: both agents receive a reward of 1 if they succeed in preparing $\\\\mathbf{H}$\\'s desired recipe.\\n\\n**Reducing a CIRL game to a POMDP** A CIRL game is a Dec-POMDP [@BGIZmor02] but it can be reduced to a POMDP where the optimal policy corresponds to optimal CIRL policy pairs [@hadfield2016cooperative]. The states in this POMDP are tuples of world-state and reward parameter: $S = X\\\\times \\\\Theta$; the actions are tuples ($\\\\delta^H$, $a^R$) specifying a decision rule $\\\\delta^H: \\\\Theta \\\\rightarrow \\\\mathcal{A}^H$ for $\\\\mathbf{H}$, which maps reward parameters to human actions, and an action $a^R$ for $\\\\mathbf{R}$; the observations are $\\\\mathbf{H}$\\'s action at the last time step.\\n\\nAn example action in the reduced POMDP of the cooking task is a tuple, where the first entry specifies that $\\\\mathbf{H}$ prepares bread if she prefers a sandwich and prepares tomatoes if she prefers soup, and the second entry specifies that $\\\\mathbf{R}$ prepares bread (regardless of its belief).\\n\\nThis reduction enables us to solve a CIRL game using POMDP algorithms. However, the size of the action space in this POMDP is $|\\\\mathcal{A}^H|^{|\\\\Theta|}|\\\\mathcal{A}^R|$, as shown in Figure\\xa0[2](#cirl_tree){reference-type=\"ref\" reference=\"cirl_tree\"}. (There are $|\\\\mathcal{A}^H|^{|\\\\Theta|}$ possible decision rules for $\\\\mathbf{H}$ and $|\\\\mathcal{A}^R|$ actions for $\\\\mathbf{R}$.) In other words, the action space in this POMDP grows exponentially with the size of the reward parameter space. Exact POMDP algorithms are exponential in the size of the action space, so this approach can only be applied to very small CIRL problems.\\n\\nAdditionally, the policy for $\\\\mathbf{R}$ that is output by the reduced POMDP is optimal only if $\\\\mathbf{H}$ is perfectly rational, i.e., if $\\\\mathbf{H}$ is guaranteed to always pick the optimal action. This is an unrealistic assumption: humans are not idealized rational agents [@tversky1975judgment; @simon1957models].\\n\\nA Modified Bellman Update for CIRL {#bellman}\\n==================================\\n\\n![A node in the search tree from the POMDP reduction of our example CIRL game. Actions are tuples that contain an action for $\\\\mathbf{R}$ and a decision rule for $\\\\mathbf{H}$ -- a mapping from her desired recipe to an action. This leads to a branching factor of $|\\\\mathcal{A}^H|^{|\\\\Theta|}|\\\\mathcal{A}^R|$ and makes application of POMDP methods inefficient. In Section [3.2](#derivation){reference-type=\"ref\" reference=\"derivation\"}, we derive a modified Bellman update that prunes away all of H\\'s decision rules but the optimal response. (In the diagram, the gray branches are pruned away by our modified Bellman update.)](fig2_v2.pdf){#cirl_tree width=\"\\\\\\\\columnwidth\"}\\n\\nIf $\\\\mathbf{H}$ were following a fixed policy based on the state $s~=~(x,~\\\\theta)$, we could encode $\\\\mathbf{H}$ as a part of the environment. However, in the interactive setting of a CIRL game, $\\\\mathbf{H}$ may plan for changes in $\\\\mathbf{R}$\\'s belief. If we encode $\\\\mathbf{H}$ in the environment, the dynamics change in response to $\\\\mathbf{R}$\\'s intended plan and the problem is no longer a POMDP. Our main contribution is to derive a modified Bellman update for POMDP algorithms to solve this problem.\\n\\nOur key idea is as follows. During planning, we know $\\\\mathbf{R}$\\'s intended future response to each of $\\\\mathbf{H}$\\'s actions. $\\\\mathbf{H}$ has full state information, so the $\\\\alpha$-vectors in value iteration allow us to directly compute $\\\\mathbf{H}$\\'s Q-values. We can therefore also compute her optimal action based on $\\\\mathbf{R}$\\'s intended future response. This means we do not have to reason over the set of decision rules for $\\\\mathbf{H}$: we can solve a CIRL game by instead solving a POMDP with time-varying dynamics and, importantly, where the action space has size $|\\\\mathcal{A}^R|$. This is exponentially smaller than the action space of size $|\\\\mathcal{A}^H|^{|\\\\Theta|}|\\\\mathcal{A}^R|$ in the reduced POMDP. This amounts to a modified Bellman update.\\n\\nThe Transition Dynamics\\n-----------------------\\n\\nIf $\\\\mathbf{H}$ follows a policy that depends only on the state $s = (x, \\\\theta)$, the dynamics of the game can be computed as: $$\\\\begin{aligned}\\n    &P(s\\', a^H\\\\mid s, a^R) = P((x\\', \\\\theta\\'), a^H\\\\mid (x, \\\\theta), a^R)\\\\\\\\ \\n    &= P((x\\', \\\\theta\\')\\\\mid (x, \\\\theta), a^H, a^R)\\\\cdot P(a^H\\\\mid (x, \\\\theta), a^R)\\\\\\\\\\n    &= T(x, a^H, a^R, x\\')\\\\cdot \\\\mathds{1}(\\\\theta = \\\\theta\\') \\\\cdot P(a^H\\\\mid x, a^R, \\\\theta)\\\\\\\\\\n    &\\\\stackrel{a.}= T(s, a^H, a^R, s\\')\\\\cdot P(a^H\\\\mid x, a^R, \\\\theta)\\\\end{aligned}$$ [^1]\\n\\nHowever, in the CIRL formulation, $\\\\mathbf{H}$ does not behave according to a fixed policy. $\\\\mathbf{H}$, who is assumed to be rational, behaves according to her Q-values and picks the action that maximizes her expected value. Due to the interdependence between $\\\\mathbf{H}$\\'s and $\\\\mathbf{R}$\\'s behavior, these Q-values depend on $\\\\mathbf{R}$\\'s conditional plan. The dynamics then are:\\n\\n$$\\\\begin{aligned}\\n   &P(s\\', a^H\\\\mid s, \\\\sigma) = P((x\\', \\\\theta\\'), a^H\\\\mid (x, \\\\theta), (a^R, v))\\\\nonumber\\\\\\\\\\n    &= T(x, a^H, a^R, x\\')  \\\\cdot \\\\mathds{1}(\\\\theta\\' = \\\\theta)\\\\cdot P(a^H\\\\mid x, a^R, v, \\\\theta)\\\\nonumber\\\\\\\\\\n    &= T(s, a^H, a^R, s\\') \\\\cdot \\\\mathds{1}(a^H = \\\\arg\\\\max_{a^H}Q_{H}(s, a^H, \\n    \\\\sigma))\\\\label{alpha_vi}\\\\end{aligned}$$ These dynamics change over time since they depend on the robot\\'s future behavior. However, $\\\\mathbf{R}$\\'s behavior depends on these dynamics, so, we cannot pre-compute them as part of a POMDP reduction. However, we do have access to $\\\\mathbf{R}$\\'s future conditional plan *during* planning. This means we can compute $\\\\mathbf{H}$\\'s Q-values, and, consequently, the transition probabilities, with a modification to the Bellman update.\\n\\nAdapting POMDP Value Iteration {#derivation}\\n------------------------------\\n\\nPOMDP value iteration rolls back the values of the game from the horizon, storing them as $\\\\alpha$-vectors. If $\\\\mathbf{R}$ follows a plan $\\\\sigma = (a^R, v)$, then we can compute $\\\\mathbf{H}$\\'s Q-values as $$Q_{H}(s, a^H, \\\\sigma) = \\\\sum_{s\\'} T(s, a^H, a^R, s\\') \\\\cdot \\\\alpha_{v(a^H)}(s\\').$$ $\\\\mathbf{H}$\\'s optimal action maximizes this expression. To leverage this, we adapt the Bellman update in Eq. 1 to replace the dynamics of the game with $P(s\\', a^H\\\\mid s, \\\\sigma)$ from Eq. 2. The modified Bellman update is then: $$\\\\begin{aligned}\\n    \\\\alpha_{\\\\sigma}(s)  &=R(s) + \\\\gamma\\\\cdot \\\\max_{a^H}\\\\sum_{s\\'\\\\in S}T(s,a^H, a^R, s\\')\\\\cdot \\\\alpha_{v(a^H)}(s\\').\\\\label{bellman_update}\\\\end{aligned}$$ We can then use value iteration with this modified Bellman update to compute the $\\\\mathbf{R}$\\'s optimal policy. The following theorem establishes that the modified Bellman update, Eq. 3, is optimality-preserving.\\n\\n::: {#optimal .thm}\\n**Theorem 1**. *For any CIRL game, the policy computed by value iteration with the modified Bellman update is optimal.*\\n:::\\n\\nAll theorem proofs are presented in Appendix A.\\n\\nThis modification to the Bellman update allows us to solve a CIRL game without having to include the set of $\\\\mathbf{H}$\\'s decision rules in the action space. As depicted in Figure\\xa0[2](#cirl_tree){reference-type=\"ref\" reference=\"cirl_tree\"}, the modified Bellman update computes $\\\\mathbf{H}$\\'s optimal action given the current state and the robot\\'s plan; all of $\\\\mathbf{H}$\\'s other actions are pruned away in the search tree. The size of the action space is then $|\\\\mathcal{A}^R|$ instead of $|\\\\mathcal{A}^H|^{|\\\\Theta|}|\\\\mathcal{A}^R|$. POMDP algorithms are exponential in the size of the action space; this modification therefore allows us to solve CIRL games much more efficiently. The following theorem establishes the complexity gains made by algorithm.\\n\\n::: {#exact .thm}\\n**Theorem 2**. *The modification to the Bellman update presented above reduces the time and space complexity of a single step of value iteration by a factor of $\\\\mathcal{O}\\\\left(|\\\\mathcal{A}^H|^{|\\\\Theta|}\\\\right)$.*\\n:::\\n\\nRelaxing CIRL\\'s Assumption of Rationality\\n-----------------------------------------\\n\\nTo achieve value alignment, we can now efficiently solve a CIRL game to find an optimal policy for $\\\\mathbf{R}$. However, this policy is optimal only if $\\\\mathbf{H}$ is perfectly rational: a strong assumption. This is rarely true in reality; we thus want to find an optimal policy for $\\\\mathbf{R}$ even when $\\\\mathbf{H}$ is sub-optimal.\\n\\nIn addition to improving efficiency, our modified Bellman update allows us to do exactly that and relax CIRL\\'s assumption of rationality. The dynamics of our modified Bellman update, presented above as Eq. 2, do not require that $\\\\mathbf{H}$ is perfectly rational. These dynamics will remain well-defined so long as we know the distribution over $\\\\mathbf{H}$\\'s actions, $\\\\pi_H$, and can compute the probability that she picks any action from her current state. To avoid compromising the interactive nature of CIRL, we require that $\\\\pi_H$ must be a function of $\\\\mathbf{H}$\\'s Q-values, which account for the robot\\'s future behavior. The dynamics of the game are then given by:$$P(s\\', a^H\\\\mid s, \\\\sigma) = T(s, a^H, a^R, s\\')\\\\cdot \\\\pi_H(a^H\\\\mid Q_H(s, a^H, \\\\sigma)).$$ The modified Bellman update is then:$$\\\\begin{aligned}\\n    &\\\\alpha_{\\\\sigma}(s)  =R(s) +\\\\gamma\\\\cdot \\\\sum_{a^H}\\\\pi_H(a^H\\\\mid Q_H(s, a^H, \\\\sigma))\\\\cdot\\\\nonumber\\\\\\\\\\n    &\\\\ \\\\ \\\\ \\\\ \\\\sum_{s\\'\\\\in S}T(s,a^H, a^R, s\\')\\\\cdot  \\\\alpha_{v(a^H)}(s\\').\\\\label{bellman_advanced_update}\\\\end{aligned}$$ With this modified Bellman update, we may now use value iteration to solve CIRL games without assuming that the human is perfectly rational. We instead only require that the human selects her actions with respect to her Q-values. This restriction is rather broad and includes a variety of models of human decision making from cognitive science. A popular example of such a model is Boltzmann-rationality, where the human picks her actions according to a Boltzmann distribution over her Q-values, i.e., $$\\\\pi_H(a^H\\\\mid Q_H(s, a^H, \\\\sigma)) \\\\propto \\\\exp(\\\\beta\\\\cdot Q_H(s, a^H, \\\\sigma))$$where $\\\\beta$ is a parameter which controls how rational the human is. (A higher $\\\\beta$ corresponds to a more rational human.)\\n\\n$\\\\Gamma_{t} \\\\gets$ Set of trivial plans $\\\\Gamma_{t+1} \\\\gets \\\\Gamma_{t}$ $Q_{H}(s, a^H, \\\\sigma) = \\\\sum_{s\\'} T(s, a^H, a^R, s\\') \\\\cdot$ $\\\\ \\\\ \\\\ \\\\ \\\\alpha_{v(a^H)}(s\\')$ $\\\\alpha_{\\\\sigma}(s) = R(s) + \\\\gamma\\\\cdot \\\\sum_{a^H}$ $\\\\ \\\\ \\\\ \\\\ \\\\pi_H(a^H\\\\mid~Q_H(s, a^H, \\\\sigma)) \\\\cdot Q_{H}(s, a^H, \\\\sigma)$ $\\\\Gamma_{t} \\\\gets$ Prune($\\\\Gamma_{t}$) $a^R_* = argmax_{\\\\sigma\\\\in\\\\Gamma_0} \\\\alpha_{\\\\sigma}\\\\cdot b_0$ **Return** $a^R_*$\\n\\nThe time and space complexity of value iteration with this Bellman update is identical to that with the modified Bellman update presented in Section 3.2, and analyzed in Theorem 2. The pseudocode for our adapted algorithm is presented as Algorithm [\\\\[exactvi\\\\]](#exactvi){reference-type=\"ref\" reference=\"exactvi\"} below.\\n\\nAdapting Approximate Algorithms {#approx_alg}\\n===============================\\n\\nApproximate algorithms for POMDPs often rely on variants of the Bellman update. This lets us use our modified Bellman update to improve approximate algorithms for CIRL.\\n\\nPBVI\\n----\\n\\n**Background** Point Based Value Iteration (PBVI) is an approximate algorithm used to solve POMDPs [@pineau2003point]. The algorithm maintains a representative set of points in belief space and an $\\\\alpha$-vector at each of these belief points. It performs approximate value backups at each of these belief points using this set of $\\\\alpha$-vectors. Let $\\\\Gamma_{t+1}$ denote the set of $\\\\alpha$-vectors for plans that begin at time $t+1$. The value at time $t$ at a belief $b$ is approximated as:$$\\\\begin{aligned}\\nV(b) &= \\\\max_{a\\\\in A}\\\\Bigg[\\\\sum_{s\\\\in S}R(s)b(s)\\\\\\\\&+ \\\\gamma \\\\sum_{o\\\\in O}\\\\max_{\\\\alpha\\\\in \\\\Gamma_{t+1}}\\\\sum_{s\\\\in S}\\\\left(\\\\sum_{s\\'\\\\in S}P(s\\',o\\\\mid s,a)\\\\alpha(s)\\\\right)b(s)\\\\Bigg].\\\\end{aligned}$$ The algorithm trades off computation time and solution quality by expanding the set of belief points over time: it randomly simulates forward trajectories in the POMDP to produce new, reachable beliefs.\\n\\n**Our Adaptation** If $\\\\mathbf{R}$ takes action $a^R$ and follows a conditional plan $\\\\sigma$, then $\\\\mathbf{H}$\\'s Q-values are $Q_{\\\\mathbf{H}}(x, a^H, a^R, \\\\alpha) = \\\\sum_{s\\'} T(s, a^H, a^R, s\\')\\\\cdot \\\\alpha_{\\\\sigma}(s\\')$. Notice that we can compute these Q-values at each step of PBVI. This lets us use the modified Bellman update and to adapt PBVI to solve CIRL games specifically. We replace the transition-observation distribution in the PBVI backup rule with $$\\\\begin{aligned}\\n   P(s\\', a^H\\\\mid s, a^R, \\\\alpha) &= T(s, a^H, a^R, s\\') \\\\cdot \\\\\\\\&\\\\pi_{\\\\mathbf{H}}(Q_{\\\\mathbf{H}}(x, a^H, a^R, \\\\alpha)).\\\\end{aligned}$$ The modified backup rule for PBVI is thus given by$$\\\\begin{aligned}\\nV(b) &= \\\\max_{a^R\\\\in \\\\mathcal{A}^R}\\\\Bigg[\\\\sum_{s\\\\in S}R(s)b(s)\\\\\\\\+ \\\\gamma &\\\\sum_{a^H} \\\\max_{\\\\alpha\\\\in \\\\Gamma_{t+1}}\\\\sum_{s\\\\in S}\\\\left(\\\\sum_{s\\'\\\\in S}P(s\\',a^H\\\\mid s,a^R, \\\\alpha)\\\\alpha(s)\\\\right)b(s)\\\\Bigg]. \\\\end{aligned}$$\\n\\nWe now show that the approximate value function in PBVI converges to the true value function. Let $\\\\epsilon_B$ denote the density of the set of belief points $B$ in PBVI. Formally, $\\\\epsilon_B = \\\\max_{b\\\\in \\\\Delta}\\\\min_{b\\'\\\\in B} ||b-b\\'||_1$ is the maximum distance from any reachable, legal belief to the set $B$.\\n\\n::: {#pbvi_proof .thm}\\n**Theorem 3**. *For any belief set $B$ and horizon $n$, the error of our adapted PBVI algorithm $\\\\eta = ||V_n - V_n^*||_{\\\\infty}$ is bounded as $$\\\\eta \\\\leq \\\\frac{(R_{\\\\max} - R_{\\\\min})\\\\epsilon_B}{(1-\\\\gamma)^2}.$$*\\n:::\\n\\nPOMCP\\n-----\\n\\n**Background** POMCP is a Monte Carlo tree-search (MCTS) based approximate algorithm for solving large POMDPs [@silver2010monte]. The algorithm constructs a search tree of action-observation histories and uses Monte Carlo simulations to estimate the value of each node in the tree. During search, actions within the tree are selected by UCB1. This maintains a balance between exploiting actions known to have good return and exploring actions not yet taken [@Kocsis:2006:BBM:2091602.2091633]. At leaf nodes, a rollout policy accrues reward which is then backed up through the tree. The algorithm estimates the belief at each node by keeping track of the hidden state from previous rollouts.\\n\\nPOMCP scales well with the size of the state space, but not with the size of the action space, which determines the branching factor in the search tree. POMCP is thus ill-suited to solving the reduced POMDP of CIRL games since the size of the action space is $|\\\\mathcal{A}^H|^{|\\\\Theta|}|\\\\mathcal{A}^R|$.\\n\\n**Our Adaptation** Using the idea behind our modified Bellman update, we adapt POMCP to solve CIRL games more efficiently. We approximate $\\\\mathbf{H}$\\'s policy while running the algorithm (much like we exactly compute $\\\\mathbf{H}$\\'s policy in exact value iteration). We maintain a live estimate of the sampled Q-values for $\\\\mathbf{H}$ at each node. With enough exploration of the search tree (for instance, if actions are selected using UCB1), the estimated Q-values converge to the true values (in the limit). This guarantees that $\\\\mathbf{H}$\\'s policy converges to her true policy. The following result establishes convergence of our algorithm.\\n\\n::: {#pomcp_proof .thm}\\n**Theorem 4**. *With suitable exploration, the value function constructed by our adapted POMCP algorithm converges in probability to the optimal value function, ${V(h) \\\\rightarrow V^*(h)}$. As the number of visits $N(h)$ approaches infinity, the bias of the value function $\\\\mathbb{E}[V(h) - V^*(h)]$ is $O(log(N(h))/N(h))$.*\\n:::\\n\\nThe pseudocode for our adapted PBVI and our adapted POMCP algorithm is presented as Algorithm 1 and 2 respectively in Appendix B.\\n\\nRelated Work\\n============\\n\\n**POMDP Algorithms** We chose to explicate our modified Bellman update in the context of PBVI and POMCP because they are the seminal point-based and MCTS algorithms respectively, for solving POMDPs. For example, SARSOP [@Kurniawati08sarsop:efficient] and DESPOT [@YeSom17], two state-of-the-art algorithm for POMDPs, are variants of PBVI and POMCP respectively. The principles we outlined in Sections [3](#bellman){reference-type=\"ref\" reference=\"bellman\"} and [4](#approx_alg){reference-type=\"ref\" reference=\"approx_alg\"} can be easily adapted to a large variety of point-based and MCTS algorithms, including any which may be developed in the future, to derive even more efficient algorithms for solving CIRL games.\\n\\n**MOMDP Algorithms** The POMDP representation of CIRL is also a mixed-observability Markov decision process (MOMDP) since the state space can be factored into a fully- and a partially-observable component. This structure allows for more efficient solution methods; @ong2010planning leverage the factored nature of the state space to create a lower dimensional representation of belief space. This core idea is orthogonal to ours, which exploits CIRL\\'s information asymmetry instead. The two can be leveraged together.\\n\\n**Dec-POMDP Algorithms** Dec-POMDP algorithms can be used to solve CIRL directly, without using the POMDP reduction. These solution methods are generally intractable, but recent work has made progress on this front. Such Dec-POMDP algorithms which attempt to prune away unreasonable strategies resemble our approach. @conf/aips/AmatoDZ09 use reachability analysis to identify reachable states, then consider all policies which are useful at such states. @Hansen04dynamicprogramming model other agents\\' possible strategies as part of a player\\'s belief, and prune away weakly dominated strategies at each step. While such approaches use heuristics to prune away some suboptimal strategies, we leverage the information structure of CIRL to compute the optimal strategy for $\\\\mathbf{H}$ and prune away *all* other strategies. This guarantees an exponential reduction in complexity while preserving optimality; this is not true for the other methods.\\n\\n**Value Alignment** Recent work has explored relaxing the rationality requirement of CIRL [@fisac2017pragmatic]. Our work improves on their relaxation in several ways: (1) Their Bellman update assumes that the human acts Boltzmann-rationally. Our modification can model a large variety of human behaviors (including this). (2) Their discretized belief value iteration algorithm has neither the guarantee of optimality of our adapted VI algorithm nor the scalability of our adapted POMCP/PBVI algorithms.\\n\\nExperiments\\n===========\\n\\nWe now verify that our modified Bellman update allows POMDP algorithms to solve CIRL games more efficiently than the standard update. We ran three experiments: one for exact value iteration (VI), PBVI, and POMCP each. The results of the PBVI experiment are presented in Appendix C.1 due to space constraints. To verify the results of our experiments, we ran further experiments on a second domain. The details and results of these experiments are presented in Appendix C.2.\\n\\nExperimental Design\\n-------------------\\n\\n**Domain** Our experimental domain is based on our running example from Section [1](#Intro){reference-type=\"ref\" reference=\"Intro\"}. Assume there are $m$ recipes and $n$ ingredients. The state space is an $n$-tuple representing the quantity of each ingredient prepared thus far. At each time step, each agent can prepare any of the $n$ ingredients or none at all. Each of the $m$ recipes corresponds to a different $\\\\theta$ (i.e. reward parameter) value. Both agents receive a reward of $1$ if $\\\\mathbf{H}$\\'s desired recipe is prepared correctly and a reward of $0$ otherwise. The robot $\\\\mathbf{R}$ begins the game entirely uncertain about $\\\\mathbf{H}$\\'s desired recipe i.e. $\\\\mathbf{R}$ has a uniform belief over $\\\\Theta$.\\n\\nWe want to stress that this experimental domain is not trivial: one of the domains we managed to solve in our experiments had $\\\\sim 10^{10}$ states.\\n\\n::: {#vi-table}\\n  $\\\\#$ Recipes         Exact VI               Ours            \\n  -------------- --------------------- ------------------- -- --\\n                   4.448 $\\\\pm$ 0.057    0.071 $\\\\pm$ 0.013     \\n  3               394.546 $\\\\pm$ 6.396   0.111 $\\\\pm$ 0.013     \\n  4                       NA            0.158 $\\\\pm$ 0.003     \\n  5                       NA            0.219 $\\\\pm$ 0.007     \\n                          NA            0.307 $\\\\pm$ 0.005     \\n\\n  : Time taken (s) to find the optimal robot policy using exact VI and our adaptation of it for various numbers of possible recipes. NA denotes that the algorithm failed to solve the problem.\\n:::\\n\\n![image](pomcp_plots.png){width=\"\\\\\\\\textwidth\"}\\n\\n**Manipulated Variables** Our primary variable is the type of Bellman update used: modified vs. standard. We also varied the number of recipes, i.e., size of the reward parameter space, and the number of ingredients, i.e., size of $\\\\mathbf{H}$\\'s and $\\\\mathbf{R}$\\'s action space.\\n\\n**Dependent Measures** In our first experiment (exact VI), we measured the time taken by the algorithms to solve the problem. In our second experiment (PBVI), we measured the value attained by the algorithms in a fixed time. In our third experiment (POMCP), we measured the value attained by the algorithms in a fixed number of samples.\\n\\n**Hypothesis** *POMDP algorithms are more efficient at solving CIRL games with the modified Bellman update than with the standard Bellman update.*\\n\\nAnalysis\\n--------\\n\\n**Exact VI** In our first experiment, we compared the time taken by exact VI and by our adaptation of it with the modified Bellman update. We first fixed the number of ingredients at two and varied the number of recipes in the domain. Table [1](#vi-table){reference-type=\"ref\" reference=\"vi-table\"} compares the results. For the simpler problems, where the number of recipes was 2 or 3, our adapted algorithm solved the problem up to $\\\\sim$3500$\\\\times$ faster than exact VI. On more complex problems where the number of recipes is greater than 3, exact VI failed to solve the problem after depleting our system\\'s 16GB memory; in contrast, our adapted algorithm solved each of these more complex problems in less than 0.5 seconds. We next fixed the number of recipes and compared the performance of both these algorithms for various numbers of ingredients. Both the exact methods, but especially the one using the standard Bellman update, scaled much worse with the number of ingredients than with the number of recipes. With even three ingredients, exact VI timed out and failed to solve the problem within two hours; our algorithm however solved the problem in five seconds.\\n\\n**POMCP** We compared the value attained in 30,000 samples by POMCP and by our adaptation with the modified Bellman update. We additionally compared these algorithms with FV-POMCP, a state-of-the-art MCTS method for solving MPOMDPs, a type of Dec-POMDP in which all agents can observe each others\\' behavior (as in CIRL).\\n\\nWe first fixed the number of recipes at 2 and varied the number of ingredients. Our adapted algorithm outperformed the other two algorithms across the board, especially for large numbers of ingredients. The results of this comparison are presented in Figure\\xa0[\\\\[pomcp\\\\]](#pomcp){reference-type=\"ref\" reference=\"pomcp\"} (left). POMCP did poorly on games with more than 4 ingredients. Although FV-POMCP scaled better to more complex games than POMCP, its values had high variance. For the largest games, with 6 and 7 ingredients, our adapted algorithm was the only one capable of solving the problem in 30,000 iterations. We also compared the value attained by each algorithm across 500,000 samples on the 6 ingredient game. The results of this comparison are depicted in Figure\\xa0[\\\\[pomcp\\\\]](#pomcp){reference-type=\"ref\" reference=\"pomcp\"} (right). Our algorithm converged to the true value faster than either of the other algorithms.\\n\\nWe next fixed the number of ingredients at 4 and varied the number of recipes. We found that the results of this experiment broadly matched the results of our previous experiment where we varied the number of ingredients. For example, with 4 recipes, our method achieves an average value of $0.631 \\\\pm 0.221$ in 30,000 iterations while POMCP gets $0.429 \\\\pm 0.183$ and FV-POMCP gets $0.511 \\\\pm 0.124$.\\n\\nThese results together demonstrate that POMDP algorithms with our modified Bellman update scales much better to more complex CIRL games than with the standard Bellman update. This offers strong evidence for our hypothesis.\\n\\nDiscussion\\n==========\\n\\nThe previous section showed that we can now solve larger, non-trivial CIRL games. While we are still far from addressing value alignment in the high dimensional and continuous real world, our work allows us to analyze CIRL solutions to non-trivial problems and understand their implications for value alignment.\\n\\nCIRL vs IRL\\n-----------\\n\\nIn the absence of CIRL solutions, a standard approach to learning the human\\'s internal reward is Inverse Reinforcement Learning (IRL) [@ng2000algorithms]. We thus compare what advantages CIRL has compared to IRL. On a collaborative task, IRL is equivalent to assuming that $\\\\mathbf{H}$ chooses her actions in isolation, and $\\\\mathbf{R}$ uses observations of her behavior to infer her preferences. Specifically, $\\\\mathbf{H}$ solves a single-agent, fully-observable, variant of the problem, and $\\\\mathbf{R}$ responds by solving the POMDP described in Section\\xa0[2](#background){reference-type=\"ref\" reference=\"background\"}.\\n\\nWe fix the number of ingredients at 3 and vary the number of recipes. Figure\\xa0[3](#irlcomp){reference-type=\"ref\" reference=\"irlcomp\"} shows the results. In each experiment the optimal CIRL solution prepares the correct recipe while the IRL solution fails to do so up to 50% of the time. To understand the nature of this difference in performance, we analyze the CIRL and IRL solutions. Consider a case of our running example with two recipes. The state is a tuple $(\\\\#meat, \\\\#bread, \\\\#tomatoes)$ and $\\\\Theta = \\\\{sandwich = (1,2,0), soup =  (1,1,2)\\\\}$. For both approaches, $\\\\mathbf{R}$ initially prepares meat. In the baseline IRL solution, $\\\\mathbf{H}$ can initially make any ingredient if she wants soup and can make meat or bread if she wants a sandwich. In each case, she chooses uniformly at random between allowed ingredients. This conveys some information about her desired recipe, but is not enough to uniquely identify it. Since the same ingredient is optimal for multiple recipes, $\\\\mathbf{R}$ is still confused after one turn. This means $\\\\mathbf{R}$ will sometimes fail to complete the desired recipe, reducing average utility.\\n\\n![Value attained by CIRL and standard IRL on the cooking domain with various numbers of possible recipes. Unlike IRL, CIRL produces solutions where $\\\\mathbf{H}$ picks her actions pedagogically and $\\\\mathbf{R}$ reasons about $\\\\mathbf{H}$ accordingly.](cirl_irl.png){#irlcomp width=\"\\\\\\\\columnwidth\"}\\n\\nThe CIRL solution, in contrast, relies on the implicit communication between the human and the robot. Here, if $\\\\mathbf{H}$ wants soup, she prepares tomatoes, as opposed to any ingredients that are common with the sandwich. Even more interestingly, she waits (i.e. does nothing) if she wants a sandwich. This is pure signaling behavior---*waiting is suboptimal in isolation*, but picking an ingredient is more likely to confuse the robot. In turn $\\\\mathbf{R}$ knows that $\\\\mathbf{H}$ would have picked tomatoes if she wanted soup, and responds appropriately.\\n\\nIn other words, $\\\\mathbf{H}$ teaches the robot about her preferences with her action selection. This works because $\\\\mathbf{H}$ knows that $\\\\mathbf{R}$ will interpret her behavior pragmatically, i.e., $\\\\mathbf{R}$ expects to be taught by $\\\\mathbf{H}$. This is reflected in the experiment: the optimal CIRL solution prepares the correct recipe each time.\\n\\nThe value alignment problem is necessarily cooperative: without the robot, the human is unable to complete her desired task, and without explicit signaling from the human, the robot learns inefficiently, is less valuable and is more likely to make a mistake. Pedagogic behavior from $\\\\mathbf{H}$ naturally falls out of the CIRL solution. In response, $\\\\mathbf{R}$ interprets $\\\\mathbf{H}$\\'s actions pragmatically. These instructive and communicative behaviors allow for faster learning and create an opportunity to generate higher value for the human.\\n\\nCIRL with Suboptimal Humans\\n---------------------------\\n\\nTo further investigate the performance of CIRL in realistic settings (e.g., where $\\\\mathbf{H}$ may not be rational), we ran another experiment. We varied whether $\\\\mathbf{H}$ behaved according to CIRL or IRL, $\\\\mathbf{R}$\\'s model of $\\\\mathbf{H}$ in training (rational or Boltzmann-rational), and the actual model of $\\\\mathbf{H}$ (same as previous). We measured the proportion of times they prepared the correct recipe in each setting, fixing the number of ingredients at 3 and recipes at 4. Figure\\xa0[5](#rational_comp){reference-type=\"ref\" reference=\"rational_comp\"} in Appendix D shows the results. (We also conducted a more comprehensive experiment with 20 human behaviors instead of 2. Results are presented in Appendix E.)\\n\\nAveraged across different models of $\\\\mathbf{H}$ used to train $\\\\mathbf{R}$, when $\\\\mathbf{H}$ behaved according to CIRL, $\\\\mathbf{H}$ and $\\\\mathbf{R}$ succeeded in preparing the correct recipe $>90\\\\%$ of the time. This was also true when $\\\\mathbf{H}$ behaved Boltzmann-rationally. This suggests that the pedagogic behavior that arises from CIRL makes it more robust to any sub-optimality from $\\\\mathbf{H}$. In contrast, when $\\\\mathbf{H}$ behaved as in IRL (i.e., not pedagogically), they only prepared the correct recipe $\\\\sim$70$\\\\%$ of the time when $\\\\mathbf{H}$ was rational, and $\\\\sim$40$\\\\%$ of the time when $\\\\mathbf{H}$ was not. So, the importance of pedagogic behavior from $\\\\mathbf{H}$ to achieve value alignment is clear.\\n\\nDo People Behave Pedagogically?\\n-------------------------------\\n\\nA question arises at this point as to whether *real* people will adopt the pedagogic behavior predicted by CIRL solutions. To start testing this, we ran a (very preliminary) pilot study to start investigating whether CIRL improves interactions with real people. The details of this experiment are presented in Appendix F. We found some encouraging evidence that suggests people do indeed behave pedagogically when collaborating with a robot; and that a CIRL-trained robot is can be better at exploiting this pedagogic behavior to achieve fluid human-robot collaboration than an IRL-trained robot.\\n\\nIn future work, we plan to conduct a more extensive human subjects study to validate these preliminary findings. We additionally plan to explore techniques to make the robot better elicit pedagogic behavior from the human in their interaction and to make CIRL robust to variations in human behavior.\\n\\nAppendix {#appendix .unnumbered}\\n========\\n\\nProofs\\n======\\n\\nIn this section, we present the proofs for the propositions and theorems in the main paper.\\n\\n**Theorem 1.** For any CIRL game, the policy computed by value iteration with the modified Bellman update is optimal.\\n\\n::: {.proof}\\n*Proof.* During POMDP value iteration, values are propagated from the horizon in the form of $\\\\alpha$ vectors, which store the expected values of executing a conditonal plan from each state. In CIRL games, $\\\\mathbf{H}$ is a full information actor, which means she has knowledge of her true reward parameter $\\\\theta$. So, given $\\\\mathbf{R}$\\'s conditional plan, she can directly compute her Q values from the $\\\\alpha$ vectors. The presence of the inner $\\\\max$ in our modified update rule ensures that $\\\\mathbf{H}$ always picks the action corresponding to her maximum Q value. This implies that at every backup step, our modified Bellman update never prunes away the optimal action. Hence, the output policy, which is the best policy among those not pruned away, must be optimal.\\xa0\\n:::\\n\\n**Theorem 2.** The modification to the Bellman update presented above reduces the time and space complexity of a single step of value iteration by a factor of $\\\\mathcal{O}\\\\left(|\\\\mathcal{A}^H|^{|\\\\Theta|}\\\\right)$.\\n\\n::: {.proof}\\n*Proof.* The complexity of one step of POMDP value iteration is linear in the size of the action space, $|A|$ [@russell1995modern]. Since the structure of our algorithm is identical to that of exact value iteration, this is also true for our algorithm.\\n\\nThe action space in the POMDP reduction of CIRL has size $|\\\\mathcal{A}^H|^{|\\\\Theta|}|\\\\mathcal{A}^R|$. Our modification to the Bellman update reduces the size of the action space to simply $|\\\\mathcal{A}^R|$. Therefore, our algorithm reduces the time taken to run, and number of plans generated at, each time step by a factor of $|\\\\mathcal{A}^H|^{|\\\\Theta|}$.\\xa0\\n:::\\n\\n**Theorem 3.** For any belief set $B$ and horizon $n$, the error of our adapted PBVI algorithm $\\\\eta = ||V_n - V_n^*||_{\\\\infty}$ is bounded as $$\\\\eta \\\\leq \\\\frac{(R_{\\\\max} - R_{\\\\min})\\\\epsilon_B}{(1-\\\\gamma)^2}.$$\\n\\n::: {.proof}\\n*Proof.* Since the dynamics of our problem are now time-varying instead of static, the backup operator applied at every time-step changes. Let $H_t$ denote the backup operator applied to compute the value of $V_t$. It will suffice to show that each such backup operator $H_t$ is a contraction mapping. The result then follows by following the proof of Theorem 1 in [@pineau2003point] exactly. We will prove the result by showing that each $H_t$ is the backup operator for a specific POMDP and thus, for this POMDP\\'s corresponding belief MDP; it must therefore be a contraction mapping.\\n\\nTake $H_t$ for some $1 \\\\leq t\\\\leq n$. We will now construct a new POMDP for which $H_t$ is the backup operator. Let $\\\\hat{S} = S \\\\times \\\\Gamma_{t+1}$, where $\\\\Gamma_{t+1}$ denotes the set of $\\\\alpha$-vectors from our original problem at time $t+1$. Let the $\\\\alpha$-vector component of the state be static i.e. $P((s\\', \\\\alpha\\')\\\\mid (s,\\\\alpha), a^R) = 0$ if $\\\\alpha \\\\neq \\\\alpha\\'$. The action and observation spaces remain as they are in our original problem. The transition-observation distribution, given in Eqn. (3), is now time-invariant: we do not need to look forward in the search tree to compute the Q-values since the $\\\\alpha$-vectors are available in the state space. Hence, this POMDP is well-defined.\\n\\nThe dynamics of the POMDP are static and identical to the dynamics of our problem at time $t$. Thus, the backup operator $H_t$ is the backup operator for this POMDP and also for this POMDP\\'s corresponding belief MDP. Therefore, the backup operator $H_t$ must be a contraction mapping.\\xa0\\n:::\\n\\n**Theorem 4.** With suitable exploration, the value function constructed by our adapted POMCP algorithm converges in probability to the optimal value function, $V(h) \\\\rightarrow V^*(h)$. As the number of visits $N(h)$ approaches infinity, the bias of the value function $\\\\mathbb{E}[V(h) - V^*(h)]$ is $O(log(N(h))/N(h))$.\\n\\n::: {.proof}\\n*Proof.* We will show that with enough exploration, in the limit of infinite samples, we have a well defined POMDP. The result then follows from Theorem 1 in [@silver2010monte].\\n\\nThe human action nodes in the search tree maintain an array of values, which store the values of picking that action for different $\\\\theta$. At any point in the search tree, the human actions (like the robot actions) are selected by picking the one that has the maximum augmented value (current estimated value plus exploration bonus). So, in the limit of infinite samples, each human action node is visited infinitely many times and the value estimates at the nodes converge to the true Q values. Having the correct human Q values gives us a POMDP, with well defined transition-observation dynamics. The result then follows from applying the analysis given in Theorem 1 of [@silver2010monte] to this POMDP.\\xa0\\n:::\\n\\nPseudocode\\n==========\\n\\nThe pseudocode for adapted PBVI is presented below. The algorithm follows a similar structure to the standard PBVI algorithm [@pineau2003point]. The main difference between our adapted algorithm and the standard PBVI algorithm is that ours uses the modified Bellman update instead of the standard one. See lines 15-16.\\n\\nThe pseudocode for adapted POMCP is also presented below, similar in style to that presented in [@silver2010monte]. The key difference is that we maintain a live estimate of the sampled Q-values for $\\\\mathbf{H}$ at each node. We maintain arrays which store the estimated values of taking each human action for different $\\\\theta$. The optimal human action is selected with regard to these estimates. Like the robot actions, the human actions are selected while balancing exploration and exploitation. Rollouts use random action selection.\\n\\n$B \\\\gets \\\\{b_0\\\\}$ $V \\\\gets$ Set of $\\\\alpha$-vectors belonging to trivial plans $V \\\\gets$ Backup($B, V)$ $B \\\\gets$ Expand($B, V$) **Return** $V$\\\\\\n\\xa0\\\\\\n$V \\\\gets \\\\{\\\\}$ $Q_{H}(s, a^H) = \\\\sum_{s\\'} T(s, a^H, a^R, s\\') \\\\cdot$ $\\\\ \\\\ \\\\ \\\\ \\\\alpha_{v(a^H)}(s\\')$ $\\\\Gamma^{a^R} \\\\gets \\\\alpha_i(s) = r(s) + \\\\gamma\\\\cdot$ $\\\\ \\\\ \\\\ \\\\sum_{a^H}\\\\pi_{H}(a^H\\\\mid Q_{H}(s, a^H))\\\\cdot$ $\\\\ \\\\ \\\\ \\\\ \\\\sum_{s\\'} P(s\\', a^H\\\\mid s, a^R, \\\\alpha_i\\') \\\\cdot \\\\alpha_i(s\\')$ $V_b \\\\gets \\\\{\\\\}$ $V_b\\\\gets argmax_{\\\\alpha\\\\in\\\\Gamma^{a^R}} \\\\alpha\\\\cdot b$ $V \\\\gets argmax_{\\\\alpha\\\\in \\\\Gamma_b} \\\\alpha\\\\cdot b$ **Return** $V$\\\\\\n\\xa0\\\\\\n$B \\\\gets B\\'$ $B_b \\\\gets \\\\{\\\\}$ $s \\\\sim b(s)$ $a^H \\\\sim P(a^H \\\\mid s,a^R, \\\\alpha)$ $b\\'(s\\') = \\\\eta \\\\sum_{s} P(s\\', a^H\\\\mid s, a^R, \\\\alpha) b(s)$ \\xa0\\xa0\\xa0\\xa0where $\\\\eta$ is the normalizing constant $B_b \\\\gets B_b \\\\cup b\\'$ $B \\\\gets B \\\\cup argmax ||b-b\\'||_1, \\\\forall b \\\\in B_b, b\\' \\\\in B\\'$ **Return** $B$\\n\\n$\\\\textsc{Simulate}(s,h,0)$ **Return** $argmax_{a^R}V(ha^R)$\\\\\\n\\xa0\\\\\\n**Return** 0\\\\\\n\\xa0\\\\\\n**Return** 0 **Return** [Rollout]{.smallcaps}$(s,h,depth)$ $a^R \\\\gets argmax_{a^R}V(ha^R) + c\\\\sqrt{\\\\frac{\\\\log N(h)}{N(ha^R)}}$ $\\\\theta \\\\gets s_{\\\\theta}$ $B(h) \\\\gets B(h) \\\\cup \\\\{s\\\\}$ $N(h) \\\\gets N(h) + 1$ $N(ha^R) \\\\gets N(ha^R) + 1$ $N(ha^Ra^H) \\\\gets N(ha^Ra^H) + 1$ $V(ha^R) \\\\gets V(ha^R) + \\\\frac{R-V(ha^R)}{N(ha^R)}$ $V_{\\\\theta}(ha^Ra^H) \\\\gets V_{\\\\theta}(ha^Ra^H) + \\\\frac{R-V_{\\\\theta}(ha^Ra^H)}{N_{\\\\theta}(ha^Ra^H)}$ **Return** $R$\\\\\\n\\xa0\\\\\\n$a^H \\\\sim \\\\pi_H\\\\Big (a^H\\\\mid V_{\\\\theta}(ha^Ra^H) + c\\\\sqrt{\\\\frac{\\\\log N_{\\\\theta}(ha^R)}{N_{\\\\theta}(ha^Ra^H)}}\\\\Big )$ **Return** $a^H$\\n\\nAdditional Experiments Omitted in Section 5\\n===========================================\\n\\nPBVI Experiment on Cooking Domain\\n---------------------------------\\n\\n![Value attained by PBVI and our approximate algorithm for various numbers of ingredients. (For the first 3 data points, the values attained by both methods were the same -- we jittered the plot slightly for visibility).](pbvi_ing_2.png){#pbvi_ing width=\"\\\\\\\\columnwidth\"}\\n\\nIn our second of the three experiments conducted in Section 5, we compared the values attained by PBVI and our adaptation, with one hour of computation time. We first fixed the number of recipes and varied the number of ingredients. The results of this experiment are presented in Figure\\xa0[4](#pbvi_ing){reference-type=\"ref\" reference=\"pbvi_ing\"}. We found that both these algorithms, but especially our adapted algorithm, scaled much better with the number of ingredients than their exact VI counterparts. For simpler games, with 3 and 4 ingredients, both algorithms attained the maximal value of 0.9025. However, with 5 ingredients, PBVI found a value of 0 in an hour. In contrast, our algorithm easily solved the game with 5, 6, and 7 ingredients, attaining the maximal value of 0.9025. We next fixed the number of ingredients and varied the number of recipes. Again, our adapted algorithm outperformed PBVI. For example, with 4 recipes, our adapted method attains a value of 0.67875 while the standard PBVI method attains a value of 0.45125.\\n\\nThese results suggest that our modified Bellman update allows PBVI to scale to larger CIRL games, especially in terms of the size of $\\\\mathbf{H}$\\'s and $\\\\mathbf{R}$\\'s action space. This offers further support to our hypothesis.\\n\\nExperiment on *RockSample* Domain\\n---------------------------------\\n\\n### Domain\\n\\nThe second benchmark CIRL domain we present is an extension of the POMDP benchmark domain *RockSample*, that models Mars-rover exploration [@smith2004heuristic]. Consider a collaborative task where a human $\\\\mathbf{H}$ wants to take samples of some rocks from Mars, with the help of a rover $\\\\mathbf{R}$ deployed on Mars. There are some number of hours during the day (working hours) when $\\\\mathbf{H}$ can control $\\\\mathbf{R}$ herself but for the rest of the day, $\\\\mathbf{R}$ has to behave autonomously. Not all types of rocks are of equal scientific value; $\\\\mathbf{H}$ knows the value of each of these rocks but $\\\\mathbf{R}$ does not. (Once again, we assume that $\\\\mathbf{H}$ cannot communicate these values to $\\\\mathbf{R}$ directly.)\\n\\nFormally, consider an instance of *RockSample* on a $m\\\\times m$ grid, with $n$ rocks, each of which belong to one of $k$ different types. The state space is a cross-product of the x- and y-coordinate of $\\\\mathbf{R}$ with $n$ binary features $IsSampled_i = \\\\{Yes, No\\\\}$, which indicate which of the rocks have already been sampled. (Each rock can only be sampled once.)\\n\\n*RockSample* is a turn-based game: $\\\\mathbf{R}$ may first take $l_{\\\\mathbf{R}}$ steps in any of the four cardinal direction (during those hours when it is running autonomously) after which $\\\\mathbf{H}$ may similarly take $l_{\\\\mathbf{H}}$ steps (during the remaining hours). Thus, the set of actions available to $\\\\mathbf{H}$ is the set of all trajectories of length exactly $l_{\\\\mathbf{H}}$ while that available to $\\\\mathbf{R}$ is the set of all trajectories with length at most $l_{\\\\mathbf{R}}$. ($\\\\mathbf{R}$ may wait on any specific step if it requires more information while $\\\\mathbf{H}$ may not wait since she has all the information required.)\\n\\nThe set of all reward parameters $\\\\Theta$ is composed of a collection of $k$-dimensional vectors, where the $i^{th}$ entry represents the reward received for sampling rock $i$. Both agents receive the reward specified by the true reward parameter $\\\\theta$ when they sample a rock and receive no reward for any other action.\\n\\n### Details of Experiment\\n\\nWe repeat the experiment from Section 5.1 of the main paper on this new domain, with a $5 \\\\times 5$ grid ($m=5$), $3$ types of rocks ($k=4$), and $4$ rocks total ($n=4$).\\n\\nThis domain is much more complex than the cooking domain. For example, note that for even the simplest iteration of this domain, with $l_{\\\\mathbf{H}} = l_{\\\\mathbf{R}} = 1$, $|\\\\mathcal{A}^H| = 4$ and $|\\\\mathcal{A}^R= 5|$ (since $\\\\mathbf{R}$ can also choose to wait); if we raise $l_{\\\\mathbf{R}}$ slightly to 2, we have $|\\\\mathcal{A}^R= 13|$. Hence, we only ran our experiments with POMCP, which scales the best of the three types of the algorithms.\\n\\nWe found similar results to that of Section 5.2 of the main paper. For any value of $l_{\\\\mathbf{R}}$ beyond $1$, FV-POMCP and POMCP fail to solve the problem; the branching factor of the search tree they both construct is too large and thus, both methods deplete the 16GB of memory in our system almost immediately. Our method however manages to scale to larger values of $l_{\\\\mathbf{H}}$ and $l_{\\\\mathbf{R}}$ with relative ease; our method successfully computed the optimal policy for this domain with values of $l_{\\\\mathbf{H}} = l_{\\\\mathbf{R}} = 2$ within two hours of computation.\\n\\nAdditional Figure from Section 6.2\\n==================================\\n\\nFigure\\xa0[5](#rational_comp){reference-type=\"ref\" reference=\"rational_comp\"} describes the results from the experiment in Section 6.2 of the main paper. In this experiment, we analyzed the performance of CIRL and IRL on our collaborative domain when the human does not behave rationally (and when the robot may or may not be aware of this fact).\\n\\n![The proportion of times that $\\\\mathbf{H}$ and $\\\\mathbf{R}$ prepared the correct recipe on the cooking domain when $\\\\mathbf{R}$ is trained with, and $\\\\mathbf{H}$ actually behaves according to, a variety of different behaviors. They were significantly more successful at preparing the correct recipe when $\\\\mathbf{H}$ behaved pedagogically according to CIRL. ](cirl_irl_table.png){#rational_comp width=\"\\\\\\\\columnwidth\"}\\n\\nResults of Follow-up Experiment to Section 6.2\\n==============================================\\n\\nThe manipulated variables and dependent measures are exactly as in Section 6.2, with the sole exception being that we considered 10 possible human policies instead of 2. The 10 policies were chosen from a $5\\\\times 2$ factorial of the human\\'s behavior and presence of bias. The 5 possible behaviors were rational, Boltzmann-rational with $\\\\beta = 1$, Boltzmann-ration with $\\\\beta = 5$, $\\\\epsilon$-greedy with $\\\\epsilon = 0.1$, $\\\\epsilon$-greedy with $\\\\epsilon = 0.01$. The 2 possible levels for presence of bias were \\\\\"No Bias\\\\\" and \\\\\"Bias\\\\\", where \\\\\"Bias\\\\\" denoted that $\\\\mathbf{H}$ had a systematic preference for choosing the \\\\\"Wait\\\\\" action. (In our setting, $\\\\mathbf{H}$ received a reward of 0.25 every time she chose the \\\\\"Wait\\\\\" action.)\\n\\nThe results of our experiment are presented below in Figure\\xa0[\\\\[heat_map\\\\]](#heat_map){reference-type=\"ref\" reference=\"heat_map\"} as a heat map. Much like the simpler experiment in Section 7.2 of the main paper, we find that $\\\\mathbf{H}$ and $\\\\mathbf{R}$ are much more successful when $\\\\mathbf{H}$ behaves pedagogically and that in the presence of pedagogic behavior, the team\\'s performance is more robust to any sub-optimality from $\\\\mathbf{H}$.\\n\\n![image](heat_map.png){width=\"\\\\\\\\textwidth\"}\\n\\nPreliminary Human Subjects Study\\n================================\\n\\nPrevious work observed that CIRL has the potential to outperform standard IRL, and achieve value alignment by allowing a robot to exploit pedagogic behavior from humans [@hadfield2016cooperative]. However, CIRL was only empirically shown to improve upon the performance of IRL in theory or, as in our main paper, in simulation. This does not guarantee that we will observe a similar result in the real world, where the robot interacts with imperfect humans.\\n\\nOur goal is to investigate whether the benefits of using CIRL over IRL in human-robot collaboration tasks carry over to practice. Here, we conduct a *very preliminary* investigation into whether humans behave pedagogically in practice, and whether a robot trained with CIRL achieves value alignment more successfully than one trained with IRL.\\n\\nHypotheses\\n----------\\n\\nWe anticipate that humans will objectively succeed at a collaborative task more frequently when collaborating with a CIRL robot as opposed to an IRL robot, especially when the task is complex. We also expect that humans will subjectively prefer to work with a CIRL robot instead of an IRL robot.\\n\\n**H1 - Objective Performance.** *The type of algorithm used will positively affect the collaboration objectively across a range of problem difficulties, with CIRL being better than IRL.*\\n\\n**H2 - Objective Performance in Complex Problems.** *On more complex problems, the type of algorithm used will positively affect the collaboration objectively, with CIRL being better than IRL.*\\n\\n**H3 - Perceptions of the Collaboration.** *The type of algorithm used will positively affect the participants\\' perception of the collaboration, with CIRL being better than IRL.*\\n\\n![We conduct a *very preliminary* investigation into whether humans do indeed behave pedagogically in practice and whether, as a result, CIRL is more effective than IRL for practically achieving value alignment. Participants collaborated with two robots, one trained with CIRL and another with IRL, to prepare a specified recipe, selected from a larger set. Both the participant and robot were allowed to make a single ingredient at each step but were only given two steps to complete the recipe; so, the human could not succeed without the robot\\'s help. The robot did not know which recipe the participant was instructed to prepare. Participants therefore had to simultaneously teach the robot about their preferred recipe and make progress toward successfully preparing the recipe.](diagram.png){#front_fig width=\"\\\\\\\\columnwidth\"}\\n\\nExperimental Design\\n-------------------\\n\\nTo explore the effect of the type of robot on human-robot collaboration, we conducted a counterbalanced within subjects study.\\n\\n### Experimental Domain\\n\\nParticipants collaborated with a virtual robot on the cooking task illustrated briefly in Figure [6](#front_fig){reference-type=\"ref\" reference=\"front_fig\"} and described extensively in section 5.1 of the main paper. For this experiment, we kept the number of ingredients in the domain fixed at 3, and the length of the horizon fixed at 2.\\n\\nThe robot moved first in this domain. The human was allowed to observe the robot\\'s move before selecting her own move.\\n\\n### Manipulated Variables\\n\\nWe manipulated two variables in our experiment. The first was the *type of robot* used; the two levels were CIRL and IRL. We henceforth refer to a robot trained with CIRL as a CIRL-robot and similarly, to a robot trained with IRL as an IRL-robot.\\n\\nWe additionally wanted to investigate how both robots behaved across a variety of problem difficulties. We suspected that both robots would behave similarly on simple problems due to the straightforward nature of the tasks but that they would behave differently on more complex tasks where they could achieve the goal in a variety of ways. Hence, we additionally manipulated the *number of recipes* used in the task from 2 to 5.\\n\\nWe initially attempted to also vary the length of the horizon on the collaborative tasks. However, we could only solve the longer horizon methods with POMCP, an approximate solver. Hence, there was no guarantee that the solution computed for these problems would be optimal or would be of similar quality across various runs. To avoid confounding the results, we chose to not vary the length of the horizon, and kept it fixed at 2.\\n\\nWe ran a full (2 by 4) factorial experiment with these two manipulated variables, leading to a total of 8 conditions.\\n\\n### Procedure\\n\\nParticipants entered the lab and were administered a pre-study questionnaire. Next, the experimenter explained the collaborative task and informed participants that they would be working with two different robots during the course of the experiment. The experiment was administered virtually -- the participants did not interact with physical robots.\\n\\nThey performed the task four times (one for each possible number of recipes) with one robot chosen at random, and then were administered a questionnaire and asked to describe the robot they had just worked with. They then performed the task four more times with the other robot and were similarly administered a questionnaire. They were finally administered a post-study questionnaire.\\n\\n### Participant Assignment Method\\n\\nA total of 12 participants (10 males, 2 females, aged 18-25) were recruited from the local community. Ten of the participants reported having a technical background.\\n\\nThe experiment used a within-subjects design because it enables participants to compare the two robots. They were informed that one of the robots was a \\\\\"student\\\\\" robot that expected to be taught, and that the other was an \\\\\"observer\\\\\" robot that did not expect to be taught but would learn by watching the human perform the task as best as they could. They were made aware of which robot was the \\\\\"student\\\\\" and which was the \\\\\"observer\\\\\", so that they may behave accordingly and maximize their chance of succeeding at the task.\\n\\nThe order of the robot was counterbalanced to control for order effects. The recipe that participants were instructed to prepare in each condition was randomly chosen from the set of possible recipes to eliminate any systematic or familiarity bias.\\n\\n::: {#list_questions}\\n  ------------------------------------------------------------\\n  **Fluency**\\n  1\\\\. The human-robot team worked fluently together.\\n  2\\\\. The robot contributed to the fluency of the team\\n  interaction.\\n  **Robot Contribution \\\\[shortened\\\\]**\\n  1\\\\. I had to carry the weight to make the human-robot\\n  team better.\\n  2\\\\. The robot contributed equally to the team\\n  performance.\\n  3\\\\. The robot\\'s performance was an important\\n  contribution to the success of the team.\\n  **Trust \\\\[shortened\\\\]**\\n  1\\\\. I trusted the robot to do the right thing at the right\\n  time.\\n  2\\\\. The robot was trustworthy.\\n  **Capability**\\n  1\\\\. I am confident in the robot\\'s ability to help me.\\n  2\\\\. The robot is intelligent.\\n  **Predictability \\\\[rephrased for clarity\\\\]**\\n  1\\\\. The robot\\'s ingredient selection matched what I\\n  would have expected.\\n  2\\\\. The robot\\'s ingredient selection was surprising.\\n  **Forced-Choice Questions**\\n  1\\\\. Which robot was the easiest to work with?\\n  2\\\\. Which robot do you prefer?\\n  ------------------------------------------------------------\\n\\n  : Subjective Measures\\n:::\\n\\n### Dependent Measures\\n\\n![image](objective.png){width=\"90%\"}\\n\\nThe measures capture the success of a collaboration in both objective and subjective ways, and are based on Hoffman\\'s metrics for fluency in human-robot collaborations [@hoffman2013evaluating].\\n\\nThe objective measure was *success at preparing the desired recipe*. Participants were assigned a score of one when they succeeded and a score of zero when they failed.\\n\\nTable 1 shows the six subjective scales that were used, together with a few forced-choice questions. We did not include the questions on Safety/Comfort since the participants did not interact with physical robots. The scales on Robot Contribution and Trust were shortened to avoid asking participants too many questions. The scale on Predictability was rephrased to more appropriately describe the setup of the experiment.\\n\\nAdditionally, participants answered forced-choice questions at the end, about which robot was easier to work with and which robot they preferred.\\n\\nAnalysis\\n--------\\n\\n### H1 - Objective Performance\\n\\nA repeated measures ANOVA on success at preparing the desired recipe showed that CIRL was only marginally better than IRL, when measured across all numbers of recipes ($F$(1,11) = 3.667, $p$ = 0.08). This offers some evidence in support of **H1**.\\n\\nThis is in line with the left plot in Figure [\\\\[av_success\\\\]](#av_success){reference-type=\"ref\" reference=\"av_success\"}, which show the results of the experiment. We see that the CIRL-robot outperforms the IRL-robot when averaged across all number of recipes but the improvement is marginal; the error bars for both algorithms have significant overlap.\\n\\n### H2 - Objective Performance in Complex Problems\\n\\nA repeated measures ANOVA on success at preparing the desired recipe showed that there was a statistically significant interaction effect between the algorithm used and the number of recipes ($F$(1,11) = 16.18, $p$ = 0.002). A post-hoc analysis with Tukey HSD revealed that on complex problems with 5 recipes, CIRL significantly outperformed IRL, but on simple problem, there was no difference in performance between the two algorithms. This offers strong evidence for **H2**.\\n\\nThe right plot on Figure [\\\\[av_success\\\\]](#av_success){reference-type=\"ref\" reference=\"av_success\"} echoes these findings. On problems with 2, 3, or 4 recipes, the proportion of trials on which the CIRL-robot and IRL-robot prepared the correct recipe is very similar. However, on problems with 5 recipes, the CIRL-robot was much more successful that the IRL-robot in the collaboration task.\\n\\n### H3 - Perceptions of the Collaboration\\n\\n::: {#results_sub}\\n         Scale          Cronbach\\'s $\\\\alpha$   $F$(1,11)     p-value\\n  -------------------- --------------------- ----------- --------------\\n        Fluency                0.84             23.14     **$<$0.001**\\n   Robot Contribution          0.69             17.73      **0.001**\\n         Trust                 0.89             16.95      **0.002**\\n       Capability              0.81             20.07     **$<$0.001**\\n     Predictability            0.86             5.189       **0.04**\\n     Forced-Choice             0.87             6.494       **0.03**\\n\\n  : Results of ANOVA on subjective metrics collected from a 7-point Likert-scale survey.\\n:::\\n\\n![image](subjective.png){width=\"90%\"}\\n\\nTable [3](#results_sub){reference-type=\"ref\" reference=\"results_sub\"} shows the results of the experiment. The internal consistency of each scale is reported via Cronbach\\'s $\\\\alpha$. All scales except one had \\\\\"good\\\\\" consistency, the exception being robot contribution, whose consistency was \\\\\"questionable\\\\\" with a Cronbach $\\\\alpha$ of 0.69. Scale items were combined into a score and analyzed with repeated-measures ANOVAs. Figure [\\\\[subjective_meas\\\\]](#subjective_meas){reference-type=\"ref\" reference=\"subjective_meas\"} plots the results.\\n\\nThe score produced by the overall forced-choice questions was significantly affected by the type of robot. The CIRL-robot had a significantly higher score than the IRL-robot ($p$ = 0.03); it was rated as being easier to work with by 9 of the 12 participants, and was preferred by 10 of the 12 participants. One participant rated the IRL robot as being easier to work with but preferred to work with the CIRL robot, remarking that he felt more \\\\\"understood\\\\\" by the CIRL robot.\\n\\nAll the Likert ratings showed a significant effect for type of robot as well; the CIRL-robot was rated significantly higher than the IRL-robot in *every case* (with $p<$ 0.01 in all but one case -- predictability). The biggest difference between the two types of robot were in *fluency* and *capability* (both $p<$ 0.001). Several participants described the IRL robot as \\\\\"not intelligent\\\\\", with one remarking that she felt \\\\\"the only reason we succeeded as much as we did was because some of the problems were so simple.\\\\\"\\n\\nThese results offer strong evidence in favor of **H3.**\\n\\nDiscussion\\n----------\\n\\nWe have empirically provided strong evidence that suggests that, in practice, CIRL is a more effective framework than IRL for value alignment. In our experiments, participants were objectively more successful at performing the specified human-robot collaboration task when working with a CIRL-robot than with an IRL-robot. Our results further suggest that CIRL leads to more fluent interaction between human and robot; our participants broadly preferred working with the CIRL robot than the IRL robot.\\n\\nInterestingly, when asked to describe their behavior, many participants described behaving similarly with both robots. One participant remarked that regardless of the robot she interacted with, she was \\\\\"picking her ingredients to eliminate the wrong recipes as quickly as possible.\\\\\"\\n\\nThese remarks agree with the notion that humans tend to behave pedagogically when working with a learner in practice. It is then perhaps no surprise that CIRL significantly outperformed IRL -- by exploiting the pedagogic nature of humans, the CIRL-robot was able to infer more information more quickly than the IRL-robot was.\\n\\n### Limitations and Future Work\\n\\nDue to the computational challenges of CIRL (outlined in the main paper), our experimental domain was still relatively straightforward. The short horizon nature of our task may made it easier for participants to behave pedagogically and questions remain as to whether people will behave similarly on more complex problems.\\n\\nAdditionally, the demographics of the participants of our survey were rather skewed toward males from technical backgrounds. It is entirely possible that people from technical backgrounds would be more informed about the behavior of the two robots and therefore able to more successfully collaborate with the robots than a non-technical person would.\\n\\nIn future work, we will explore how people behave in collaborative tasks over a long horizon, where their desire or ability to behave pedagogically may be impeded. Furthermore, we intend to deploy our algorithms on real robots and investigate how humans behave in collaborative tasks with actual robots as opposed to virtual ones on computer screens. To do so, we intend to develop a better online solution method for CIRL with stronger theoretical guarantees on the quality of the solution, thereby allowing us to solve larger problems in practice with real individuals.\\n\\nAcknowledgements {#acknowledgements .unnumbered}\\n================\\n\\nThis work was supported in part by grants from the NSF NRI, and the Open Philantrophy Project.\\n\\n[^1]: a\\\\. We let $T(s, a^H, a^R, s\\') = T(x, a^H, a^R, x\\')\\\\cdot \\\\mathds{1}(\\\\theta = \\\\theta\\')$.\\n',\n",
       "  'bibliography_bbl': \"\\\\begin{thebibliography}{22}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Amato et~al.(2009)Amato, Dibangoye, and\\n  Zilberstein]{conf/aips/AmatoDZ09}\\nAmato, C., Dibangoye, J.~S., and Zilberstein, S.\\n\\\\newblock {Incremental Policy Generation for Finite-Horizon DEC-{POMDP}s.}\\n\\\\newblock In Gerevini, A., Howe, A.~E., Cesta, A., and Refanidis, I. (eds.),\\n  \\\\emph{ICAPS}. AAAI, 2009.\\n\\\\newblock ISBN 978-1-57735-406-2.\\n\\\\newblock URL\\n  \\\\url{http://dblp.uni-trier.de/db/conf/aips/icaps2009.html#AmatoDZ09}.\\n\\n\\\\bibitem[Amodei \\\\& Clark(2016)Amodei and Clark]{faulty}\\nAmodei, D. and Clark, J.\\n\\\\newblock {Faulty Reward Functions in the Wild}.\\n\\\\newblock \\\\url{https://blog.openai.com/faulty-reward-functions/}, 2016.\\n\\n\\\\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and\\n  Man{\\\\'{e}}]{DBLP:journals/corr/AmodeiOSCSM16}\\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and\\n  Man{\\\\'{e}}, D.\\n\\\\newblock {Concrete Problems in {AI} Safety}.\\n\\\\newblock \\\\emph{CoRR}, abs/1606.06565, 2016.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1606.06565}.\\n\\n\\\\bibitem[Bernstein et~al.(2002)Bernstein, Givan, Immerman, and\\n  Zilberstein]{BGIZmor02}\\nBernstein, D.~S., Givan, R., Immerman, N., and Zilberstein, S.\\n\\\\newblock {The Complexity of Decentralized Control of {M}arkov Decision\\n  Processes}.\\n\\\\newblock \\\\emph{Mathematics of Operations Research}, 27\\\\penalty0 (4):\\\\penalty0\\n  819--840, 2002.\\n\\n\\\\bibitem[Bostrom(2014)]{Bostrom:2014:SPD:2678074}\\nBostrom, N.\\n\\\\newblock \\\\emph{{Superintelligence: Paths, Dangers, Strategies}}.\\n\\\\newblock Oxford University Press, Oxford, UK, 1st edition, 2014.\\n\\\\newblock ISBN 0199678111, 9780199678112.\\n\\n\\\\bibitem[Fisac et~al.(2017)Fisac, Gates, Hamrick, Liu, Hadfield-Menell,\\n  Palaniappan, Malik, Sastry, Griffiths, and Dragan]{fisac2017pragmatic}\\nFisac, J.~F., Gates, M.~A., Hamrick, J.~B., Liu, C., Hadfield-Menell, D.,\\n  Palaniappan, M., Malik, D., Sastry, S.~S., Griffiths, T.~L., and Dragan,\\n  A.~D.\\n\\\\newblock Pragmatic-pedagogic value alignment.\\n\\\\newblock \\\\emph{International Symposium on Robotics Research}, 2017.\\n\\n\\\\bibitem[Hadfield-Menell et~al.(2016)Hadfield-Menell, Russell, Abbeel, and\\n  Dragan]{hadfield2016cooperative}\\nHadfield-Menell, D., Russell, S.~J., Abbeel, P., and Dragan, A.\\n\\\\newblock {Cooperative Inverse Reinforcement Learning}.\\n\\\\newblock In \\\\emph{Advances in neural information processing systems}, pp.\\\\\\n  3909--3917, 2016.\\n\\n\\\\bibitem[Hansen(2004)]{Hansen04dynamicprogramming}\\nHansen, E.~A.\\n\\\\newblock {Dynamic Programming for Partially Observable Stochastic Games}.\\n\\\\newblock In \\\\emph{In Proceedings Of The Nineteenth National Conference On\\n  Artificial Intelligence}, pp.\\\\  709--715, 2004.\\n\\n\\\\bibitem[Hoffman(2013)]{hoffman2013evaluating}\\nHoffman, G.\\n\\\\newblock Evaluating fluency in human-robot collaboration.\\n\\\\newblock In \\\\emph{International conference on human-robot interaction (HRI),\\n  workshop on human robot collaboration}, volume 381, pp.\\\\  1--8, 2013.\\n\\n\\\\bibitem[Kaelbling et~al.(1998)Kaelbling, Littman, and\\n  Cassandra]{Kaelbling:1998:PAP:1643275.1643301}\\nKaelbling, L.~P., Littman, M.~L., and Cassandra, A.~R.\\n\\\\newblock {Planning and Acting in Partially Observable Stochastic Domains}.\\n\\\\newblock \\\\emph{Artif. Intell.}, 101\\\\penalty0 (1-2):\\\\penalty0 99--134, May\\n  1998.\\n\\\\newblock ISSN 0004-3702.\\n\\\\newblock \\\\doi{10.1016/S0004-3702(98)00023-X}.\\n\\\\newblock URL \\\\url{http://dx.doi.org/10.1016/S0004-3702(98)00023-X}.\\n\\n\\\\bibitem[Kocsis \\\\& Szepesv\\\\'{a}ri(2006)Kocsis and\\n  Szepesv\\\\'{a}ri]{Kocsis:2006:BBM:2091602.2091633}\\nKocsis, L. and Szepesv\\\\'{a}ri, C.\\n\\\\newblock {Bandit Based Monte-Carlo Planning}.\\n\\\\newblock In \\\\emph{Proceedings of the 17th European Conference on Machine\\n  Learning}, ECML'06, pp.\\\\  282--293, Berlin, Heidelberg, 2006.\\n  Springer-Verlag.\\n\\\\newblock ISBN 3-540-45375-X, 978-3-540-45375-8.\\n\\\\newblock \\\\doi{10.1007/11871842_29}.\\n\\\\newblock URL \\\\url{http://dx.doi.org/10.1007/11871842_29}.\\n\\n\\\\bibitem[Kurniawati et~al.(2008)Kurniawati, Hsu, and\\n  Lee]{Kurniawati08sarsop:efficient}\\nKurniawati, H., Hsu, D., and Lee, W.~S.\\n\\\\newblock {{SARSOP}: Efficient Point-Based {POMDP} Planning by Approximating\\n  Optimally Reachable Belief Spaces}.\\n\\\\newblock In \\\\emph{In Proc. Robotics: Science and Systems}, 2008.\\n\\n\\\\bibitem[Ng \\\\& Russell(2000)Ng and Russell]{ng2000algorithms}\\nNg, A.~Y. and Russell, S.\\n\\\\newblock {Algorithms for Inverse Reinforcement Learning}.\\n\\\\newblock In \\\\emph{in Proc. 17th International Conf. on Machine Learning}.\\n  Citeseer, 2000.\\n\\n\\\\bibitem[Ong et~al.(2010)Ong, Png, Hsu, and Lee]{ong2010planning}\\nOng, S.~C., Png, S.~W., Hsu, D., and Lee, W.~S.\\n\\\\newblock Planning under uncertainty for robotic tasks with mixed\\n  observability.\\n\\\\newblock \\\\emph{The International Journal of Robotics Research}, 29\\\\penalty0\\n  (8):\\\\penalty0 1053--1068, 2010.\\n\\n\\\\bibitem[Pineau et~al.(2003)Pineau, Gordon, and Thrun]{pineau2003point}\\nPineau, J., Gordon, G., and Thrun, S.\\n\\\\newblock {Point-Based Value Iteration: An Anytime Algorithm for {POMDP}s}.\\n\\\\newblock In \\\\emph{IJCAI}, volume~3, pp.\\\\  1025--1032, 2003.\\n\\n\\\\bibitem[Russell \\\\& Norvig(1995)Russell and Norvig]{russell1995modern}\\nRussell, S. and Norvig, P.\\n\\\\newblock {A Modern Approach}.\\n\\\\newblock \\\\emph{Artificial Intelligence. Prentice-Hall, Egnlewood Cliffs},\\n  25:\\\\penalty0 27, 1995.\\n\\n\\\\bibitem[Silver \\\\& Veness(2010)Silver and Veness]{silver2010monte}\\nSilver, D. and Veness, J.\\n\\\\newblock {Monte Carlo Planning in Large POMDPs}.\\n\\\\newblock In \\\\emph{Advances in neural information processing systems}, pp.\\\\\\n  2164--2172, 2010.\\n\\n\\\\bibitem[Simon(1957)]{simon1957models}\\nSimon, H.~A.\\n\\\\newblock Models of man; social and rational.\\n\\\\newblock 1957.\\n\\n\\\\bibitem[Smith \\\\& Simmons(2004)Smith and Simmons]{smith2004heuristic}\\nSmith, T. and Simmons, R.\\n\\\\newblock Heuristic search value iteration for pomdps.\\n\\\\newblock In \\\\emph{Proceedings of the 20th conference on Uncertainty in\\n  artificial intelligence}, pp.\\\\  520--527. AUAI Press, 2004.\\n\\n\\\\bibitem[Sondik(1971)]{sondik71}\\nSondik, E.~J.\\n\\\\newblock \\\\emph{{The Optimal Control of Partially Observable {M}arkov\\n  Processes}}.\\n\\\\newblock PhD thesis, Stanford University, 1971.\\n\\n\\\\bibitem[Tversky \\\\& Kahneman(1975)Tversky and Kahneman]{tversky1975judgment}\\nTversky, A. and Kahneman, D.\\n\\\\newblock Judgment under uncertainty: Heuristics and biases.\\n\\\\newblock In \\\\emph{Utility, probability, and human decision making}, pp.\\\\\\n  141--162. Springer, 1975.\\n\\n\\\\bibitem[Ye et~al.(2017)Ye, Somani, Hsu, and Lee]{YeSom17}\\nYe, N., Somani, A., Hsu, D., and Lee, W.\\n\\\\newblock {{DESPOT}: Online {POMDP} Planning with Regularization}.\\n\\\\newblock 58:\\\\penalty0 231--266, 2017.\\n\\n\\\\end{thebibliography}\\n\",\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1606.06565': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Learning human intent',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #11',\n",
       "   'newsletter_url': 'https://mailchi.mp/7ad36e25be22/alignment-newsletter-11',\n",
       "   'summarizer': 'Rohin',\n",
       "   'summary': \"Previously, Cooperative Inverse Reinforcement Learning (CIRL) games were solved by reducing them to a POMDP with an exponentially-sized action space, and then solving with POMDP algorithms that are exponential in the size of the action space, leading to a doubly-exponential algorithm. This paper leverages the fact that the human has perfect information to create a modified Bellman update that still computes the optimal policy, but no longer requires an exponential action space. The modified Bellman update works with the human's policy, and so we can now swap in more accurate models of the human, including eg. noisy rationality (whereas previously the human had to be exactly optimal). They show huge speedups in experiments, and discuss some interesting qualitative behavior that arises out of CIRL games -- for example, sometimes the human _waits_ instead of making progress on the task, because it is a good signal to the robot of what the human wants.\",\n",
       "   'opinion': \"I'm excited by this improvement, since now we can actually solve non-trivial CIRL games -- one of the games they solve has around 10 billion states. With this we can run experiments with real humans, which seems really important, and the paper does mention a very preliminary pilot study run with real humans.\",\n",
       "   'prerequisites': 'Cooperative Inverse Reinforcement Learning',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '1806.03820v1',\n",
       "   'arxiv_id': '1806.03820',\n",
       "   'title': 'An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning',\n",
       "   'authors': ['Dhruv Malik',\n",
       "    'Malayandi Palaniappan',\n",
       "    'Jaime F. Fisac',\n",
       "    'Dylan Hadfield-Menell',\n",
       "    'Stuart Russell',\n",
       "    'Anca D. Dragan'],\n",
       "   'date_published': '2018-06-11 06:06:43+00:00',\n",
       "   'data_last_modified': '2018-06-11 06:06:43+00:00',\n",
       "   'url': 'http://arxiv.org/abs/1806.03820v1',\n",
       "   'abstract': \"Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.\",\n",
       "   'author_comment': 'None',\n",
       "   'journal_ref': 'None',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'cs.AI',\n",
       "   'categories': \"['cs.AI']\",\n",
       "   'individual_summary': \"Title: An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning\\nAuthors: Dhruv Malik, Malayandi Palaniappan, Jaime F. Fisac, Dylan Hadfield-Menell, Stuart Russell, Anca D. Dragan\\nPaper abstract: Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL---the human is a full information agent---to derive an optimality-preserving modification to the standard Bellman update; this reduces the complexity of the problem by an exponential factor and allows us to relax CIRL's assumption of human rationality. We apply this update to a variety of POMDP solvers and find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogic (teaching) behavior, while the robot interprets it as such and attains higher value for the human.\\nSummary: Previously, Cooperative Inverse Reinforcement Learning (CIRL) games were solved by reducing them to a POMDP with an exponentially-sized action space, and then solving with POMDP algorithms that are exponential in the size of the action space, leading to a doubly-exponential algorithm. This paper leverages the fact that the human has perfect information to create a modified Bellman update that still computes the optimal policy, but no longer requires an exponential action space. The modified Bellman update works with the human's policy, and so we can now swap in more accurate models of the human, including eg. noisy rationality (whereas previously the human had to be exactly optimal). They show huge speedups in experiments, and discuss some interesting qualitative behavior that arises out of CIRL games -- for example, sometimes the human _waits_ instead of making progress on the task, because it is a good signal to the robot of what the human wants.\\nMy opinion: I'm excited by this improvement, since now we can actually solve non-trivial CIRL games -- one of the games they solve has around 10 billion states. With this we can run experiments with real humans, which seems really important, and the paper does mention a very preliminary pilot study run with real humans.\",\n",
       "   'paper_text': '',\n",
       "   'text': 'Highlights\\n**[Learning to Follow Language Instructions with Adversarial Reward Induction](https://arxiv.org/abs/1806.01946)**\\xa0*(Dzmitry Bahdanau et al)*: Adversarial Goal-Induced Learning from Examples (AGILE) is a way of training an agent to follow instructions. The authors consider a 5x5 gridworld environment with colored shapes that the agent can manipulate. The agent is given an instruction in a structured domain-specific language. Each instruction can correspond to many goal states -- for example, the instruction corresponding to \"red square south of the blue circle\" has many different goal states, since only the relative orientation of the shapes matters, not their absolute positions.\\nThe key idea is to learn two things simultaneously -- an encoding of\\xa0*what*\\xa0the agent needs to do, and a policy that encodes\\xa0*how*\\xa0to do it, and to use these two modules to train each other. The \"what\" is encoded by a discriminator that can classify (state, instruction) pairs as either being a correct goal state or not, and the \"how\" is encoded by a policy. They assume they have some human-annotated goal states for instructions. The discriminator is then trained with supervised learning, where the positive examples are the human-annotated goal states, and the negative examples are states that the policy achieves during training (which are usually failures). The policy is trained using A3C with a reward function that is 1 if the discriminator says the state is more likely than not to be a goal state, and 0 otherwise. Of course, if the policy actually achieves the goal state, there is no way of knowing this apart from the discriminator -- so by default\\xa0*all*\\xa0of the states that the policy achieves (including goal states) are treated as negative examples for the dsicriminator. This leads to the discriminator getting slightly worse over time as the policy becomes better, since it is incorrectly told that certain states are not goal states. To fix this issue, the authors drop the top 25% of states achieved by the policy that have the highest probability of being a goal state (according to the discriminator).\\nThe authors compare AGILE against A3C with the true reward function (i.e. the reward function implied by a perfect discriminator) and found that AGILE actually performed\\xa0*better*, implying that the inaccuracy of the discriminator actually\\xa0*helped*\\xa0with learning. The authors hypothesize that this is because when the discriminator incorrectly rewards non-goal states, it is actually providing useful reward shaping that rewards progress towards the goal, leading to faster learning. Note though that A3C with an auxiliary reward prediction objective performed best. They have several other experiments that look at individual parts of the system.\\n**My opinion:**\\xa0I like the idea of separating \"what to do\" from \"how to do it\", since the \"what to do\" is more likely to generalize to new circumstances. Of course, this can also be achieved by learning a reward function, which is one way to encode \"what to do\". I\\'m also happy to see progress on the front of learning what humans want where we can take advantage of adversarial training that leads to a natural curriculum -- this has been key in many systems, most notably AlphaZero.\\nI\\'m somewhat surprised that dropping the top 25% of states ranked highly by the discriminator works. I would have guessed that states that are \"near\" the goal states might be misclassified by the discriminator, and the mistake will never be fixed because those states will always be in the top 25% and so will never show up as negative examples. I don\\'t know whether I should expect this problem to show up in other environments, or whether there\\'s a good reason to expect it won\\'t happen.\\nI\\'m also surprised at the results from one of their experiments. In this experiment, they trained the agent in the normal environment, but then made red squares immovable in the test environment. This only changes the dynamics, and so the discriminator should work just as well (about 99.5% accuracy). The policy performance tanks (from 98% to 52%), as you\\'d expect when changing dynamics, but if you then finetune the policy, it only gets back to 69% success. Given that the discriminator should be just as accurate, you\\'d expect the policy to get back to 98% accuracy. Partly the discrepancy is that some tasks become unsolvable when red squares are immovable, but they say that this is a small effect. My hypothesis is before finetuning, the policy is very certain of what to do, and so doesn\\'t explore enough during finetuning, and can\\'t learn new behaviors effectively. This would mean that if they instead retrained the policy starting from a random initialization, they\\'d achieve better performance (though likely requiring many more samples).\\n**[A general model of safety-oriented AI development](https://www.lesswrong.com/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development)**\\xa0*(Wei Dai)*: A general model for developing safe powerful AI systems is to have a team of humans and AIs, which continually develops and adds more AIs to the team, while inductively preserving alignment.\\n**My opinion:**\\xa0I\\'m glad this was finally written down -- I\\'ve been calling this the \"induction hammer\" and have used it a lot in my own thinking. Thinking about this sort of a model, and in particular what kinds of properties we could best preserve inductively, has been quite helpful for me.\\n**[AGI Strategy - List of Resources](https://docs.google.com/spreadsheets/d/1ojSJFrDsBpLj0_snavF3AQjDImTElYLq33WUiB5x5lg/edit#gid=763026677)**: Exactly what it sounds like.\\n\\xa0\\n\\nTechnical AI alignment\\n\\xa0\\n\\nAgent foundations\\n[Counterfactual Mugging Poker Game](https://www.lesswrong.com/posts/g3PwPgcdcWiP33pYn/counterfactual-mugging-poker-game)\\xa0*(Scott Garrabrant)*: This is a variant of counterfactual mugging, in which an agent doesn\\'t take the action that is locally optimal, because that would provide information in the counterfactual world where one aspect of the environment was different that would lead to a large loss in that setting.\\n**My opinion:**\\xa0This example is very understandable and very short -- I haven\\'t summarized it because I don\\'t think I can make it any shorter.\\n[Weak arguments against the universal prior being malign](https://www.lesswrong.com/posts/Ecxevhvx85Y4eyFcu/weak-arguments-against-the-universal-prior-being-malign)\\xa0*(X4vier)*: In an\\xa0[earlier post](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/), Paul Christiano has argued that if you run Solomonoff induction and use its predictions for important decisions, most of your probability mass will be placed on universes with intelligent agents that make the right predictions so that their predictions will influence your decisions, and then use that influence to manipulate you into doing things that they value. This post makes a few arguments that this wouldn\\'t actually happen, and Paul responds to the arguments in the comments.\\n**My opinion:**\\xa0I still have only a fuzzy understanding of what\\'s going on here, so I\\'m going to abstain from an opinion on this one.\\n**Prerequisities:**\\xa0[What does the universal prior actually look like?](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/)\\n\\xa0\\n\\nLearning human intent\\n**[Learning to Follow Language Instructions with Adversarial Reward Induction](https://arxiv.org/abs/1806.01946)**\\xa0*(Dzmitry Bahdanau et al)*: Summarized in the highlights!\\n[An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning](http://arxiv.org/abs/1806.03820)\\xa0*(Dhruv Malik, Malayandi Palaniappan et al)*: Previously, Cooperative Inverse Reinforcement Learning (CIRL) games were solved by reducing them to a POMDP with an exponentially-sized action space, and then solving with POMDP algorithms that are exponential in the size of the action space, leading to a doubly-exponential algorithm. This paper leverages the fact that the human has perfect information to create a modified Bellman update that still computes the optimal policy, but no longer requires an exponential action space. The modified Bellman update works with the human\\'s policy, and so we can now swap in more accurate models of the human, including eg. noisy rationality (whereas previously the human had to be exactly optimal). They show huge speedups in experiments, and discuss some interesting qualitative behavior that arises out of CIRL games -- for example, sometimes the human\\xa0*waits*\\xa0instead of making progress on the task, because it is a good signal to the robot of what the human wants.\\n**My opinion:**\\xa0I\\'m excited by this improvement, since now we can actually solve non-trivial CIRL games -- one of the games they solve has around 10 billion states. With this we can run experiments with real humans, which seems really important, and the paper does mention a very preliminary pilot study run with real humans.\\n**Prerequisities:**\\xa0[Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137)\\n[Learning a Prior over Intent via Meta-Inverse Reinforcement Learning](http://arxiv.org/abs/1805.12573)\\xa0*(Kelvin Xu et al)*: For complex rewards, such as reward functions defined on pixels, standard IRL methods require a large number of demonstrations. However, many tasks are very related, and so we should be able to leverage demonstrations from one task to learn rewards for other tasks. This naturally suggests that we use meta learning. The authors adapt\\xa0[MAML](https://arxiv.org/abs/1703.03400)\\xa0to work with maximum entropy IRL (which requires differentiating through the MaxEnt IRL gradient). They evaluate their approach, called MandRIL, on a navigation task whose underlying structure is a gridworld, but the state is represented as an image so that the reward function is nonlinear and requires a convnet.\\n**My opinion:**\\xa0In one of the experiments, the baseline of running IRL from scratch performed second best, beating out two other methods of meta-learning. I\\'d guess that this is because both MandRIL and standard IRL benefit from assuming the maxent IRL distribution over trajectories (which I believe is how the demonstrations were synthetically generated), whereas the other two meta learning baselines do not have any such assumption, and must learn this relationship.\\n[Imitating Latent Policies from Observation](https://arxiv.org/abs/1805.07914)\\xa0*(Ashley D. Edwards et al)*: Typically in imitation learning, we assume that we have access to demonstrations that include the actions that the expert took. However, in many realistic settings we only have access to state observations (eg. driving videos). In this setting, we could still infer a reward function and then use reinforcement learning (RL) to imitate the behavior, but this would require a lot of interaction with the environment to learn the dynamics of the environment. Intuitively, even demonstrations with only states and no actions should give us a lot of information about the dynamics -- if we can extract this information, then we would need much less environment interaction during RL. (For example, if you watch a friend play a video game, you only see states, not actions; yet you can infer a lot about the game rules and gameplay.) The key idea is that each action probably causes similar effects on different states. So, they create a model with hidden action nodes z, and use the state observations to learn a policy P(z | s) and dynamics s\\' = g(s, z) (they assume deterministic dynamics). This is done end-to-end with neural nets, but essentially the net is looking at the sequence of states and figuring out how to assign actions z to each s (this is P(z | s)), such that we can learn a function g(s, z) that outputs the next observed state s\\'. Once this is trained, intuitively g(s, z) will already have captured most of the dynamics, and so now we only require a small number of environment actions to figure out how the true actions a correspond to the hidden actions z -- concretely, we train a model P(a | s, z). Then, in any state s, we first choose the most likely hidden action z*according to P(z | s), and then the most likely action a*\\xa0according to P(a | s, z\\\\*).\\n**My opinion:**\\xa0The intuition behind this method makes a lot of sense to me, but I wish the experiments were clearer in showing how the method compares to other methods. They show that, on Cartpole and Acrobat, they can match the results of behavioral cloning with 50,000 state-action pairs using 50,000 state observations and 200 environment interactions, but I don\\'t know if behavioral cloning actually needed that many state-action pairs. Similarly, I\\'m not sure how much environment interaction would be needed if you inferred a reward function but not the dynamics, since they don\\'t compare against such a method. I\\'m also unclear on how hard it is to assign transitions to latent actions -- they only test on MDPs with at most 3 actions, it\\'s plausible to me that with more actions it becomes much harder to figure out which hidden action a state transition should correspond to.\\n\\xa0\\n\\nPreventing bad behavior\\n[Worrying about the Vase: Whitelisting](https://www.lesswrong.com/posts/H7KB44oKoSjSCkpzL/worrying-about-the-vase-whitelisting)\\xa0*(TurnTrout)*: It\\'s really hard to avoid negative side effects because explicitly listing out all possible side effects the agent should avoid would be far too expensive. The issue is that we\\'re trying to build a blacklist of things that can\\'t be done, and that list will never be complete, and so some bad things will still happen. Instead, we should use whitelists, because if we forget to add something to the whitelist, that only limits the agent, it doesn\\'t lead to catastrophe. In this proposal, we assume that we have access to the agent\\'s ontology (in current systems, this might be the output of an object detection system), and we operationalize an \"effect\" as the transformation of one object into another (i.e. previously the AI believed an object was most likely an A, and now it believes it is most likely a B). We then whitelist allowed transformations -- for example, it is allowed to transform a carrot into carrot slices. If the agent causes any transformations not on the whitelist (such as \"transforming\" a vase into a broken vase), it incurs a negative reward. We also don\\'t have to explicitly write down the whitelist -- we can provide demonstrations of acceptable behavior, and any transitions in these demonstrations can be added to the whitelist. The post and paper have a long list of considerations on how this would play out in a superintelligent AI system.\\n**My opinion:**\\xa0Whitelisting seems like a good thing to do, since it is safe by default. (Computer security has a similar principle of preferring to whitelist instead of blacklist.) I was initially worried that we\\'d have the problems of symbolic approaches to AI, where we\\'d have to enumerate far too many transitions for the whitelist in order to be able to do anything realistic, but since whitelisting could work on learned embedding spaces, and the whitelist itself can be learned from demonstrations, this could be a scalable method. I\\'m worried that it presents generalization challenges -- if you are distinguishing between different colors of tiles, to encode \"you can paint any tile\" you\\'d have to whitelist transitions (redTile -> blueTile), (blueTile -> redTile), (redTile -> yellowTile) etc. Those won\\'t all be in the demonstrations. If you are going to generalize there, how do you\\xa0*not*\\xa0generalize (redLight -> greenLight) to (greenLight -> redLight) for an AI that controls traffic lights? On another note, I personally don\\'t want to assume that we can point to a part of the architecture as the AI\\'s ontology. I hope to see future work address these challenges!\\n\\xa0\\n\\nHandling groups of agents\\n[Adaptive Mechanism Design: Learning to Promote Cooperation](http://arxiv.org/abs/1806.04067)\\xa0*(Tobias Baumann et al)*\\n[Multi-Agent Deep Reinforcement Learning with Human Strategies](http://arxiv.org/abs/1806.04562)\\xa0*(Thanh Nguyen et al)*\\n\\xa0\\n\\nInterpretability\\n[Neural Stethoscopes: Unifying Analytic, Auxiliary and Adversarial Network Probing](http://arxiv.org/abs/1806.05502)\\xa0*(Fabian B. Fuchs et al)*\\n[Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning](http://arxiv.org/abs/1806.00069)\\xa0*(Leilani H. Gilpin et al)*\\n\\xa0\\n\\nMiscellaneous (Alignment)\\n**[A general model of safety-oriented AI development](https://www.lesswrong.com/posts/idb5Ppp9zghcichJ5/a-general-model-of-safety-oriented-ai-development)**\\xa0*(Wei Dai)*: Summarized in the highlights!\\n[To Trust Or Not To Trust A Classifier](http://arxiv.org/abs/1805.11783)\\xa0*(Heinrich Jiang, Been Kim et al)*: The confidence scores given by a classifier (be it logistic regression, SVMs, or neural nets) are typically badly calibrated, and so it is hard to tell whether or not we should trust our classifier\\'s prediction. The authors propose that we compute a\\xa0*trust score*\\xa0to tell us how much to trust the classifier\\'s prediction, computed from a training set of labeled datapoints. For every class, they filter out some proportion of the data points, which removes outliers. Then, the trust score for a particular test point is the ratio of (distance to nearest non-predicted class) to (distance to predicted class). They have theoretical results showing that a high trust score means that the classifier likely agrees with the Bayes-optimal classifier, as well as empirical results showing that this method does better than several baselines for determining when to trust a classifier. One cool thing about this method is that it can be done with any representation of the input data points -- they find that working with the activations of deeper layers of a neural net improves the results.\\n**My opinion:**\\xa0I\\'m a big fan of trying to understand when our AI systems work well, and when they don\\'t. However, I\\'m a little confused by this -- ultimately the trust score is just comparing the given classifier with a nearest neighbor classifier. Why not just use the nearest neighbor classifier in that case? This paper is a bit further out of my expertise than I\\'d like to admit, so perhaps there\\'s an obvious answer I\\'m not seeing.\\n[Podcast: Astronomical Future Suffering and Superintelligence with Kaj Sotala](https://futureoflife.org/2018/06/14/podcast-astronomical-future-suffering-and-superintelligence-with-kaj-sotala/?cn-reloaded=1)\\xa0*(Lucas Perry)*\\n\\xa0\\n\\nNear-term concerns\\n\\xa0\\n\\nAdversarial examples\\n[Defense Against the Dark Arts: An overview of adversarial example security research and future research directions](http://arxiv.org/abs/1806.04169)\\xa0*(Ian Goodfellow)*\\n\\xa0\\n\\nAI strategy and policy\\n**[AGI Strategy - List of Resources](https://docs.google.com/spreadsheets/d/1ojSJFrDsBpLj0_snavF3AQjDImTElYLq33WUiB5x5lg/edit#gid=763026677)**: Summarized in the highlights!\\n[Accounting for the Neglected Dimensions of AI Progress](https://www.cser.ac.uk/resources/dimensions-ai-progress/)\\xa0*(Fernando Martinez-Plumed et al)*\\n[Artificial Intelligence and International Affairs: Disruption Anticipated](https://www.chathamhouse.org/publication/artificial-intelligence-and-international-affairs)\\xa0*(Chatham House)*\\n[India\\'s National Strategy for Artificial Intelligence](http://www.niti.gov.in/writereaddata/files/document_publication/NationalStrategy-for-AI-Discussion-Paper.pdf)\\n\\xa0\\n\\nAI capabilities\\n\\xa0\\n\\nReinforcement learning\\n[Self-Imitation Learning](http://arxiv.org/abs/1806.05635)\\xa0*(Junhyuk Oh et al)*\\n\\xa0\\n\\nDeep learning\\n[Neural scene representation and rendering](https://deepmind.com/blog/neural-scene-representation-and-rendering/)\\xa0*(S. M. Ali Eslami, Danilo J. Rezende et al)*\\n[Improving Language Understanding with Unsupervised Learning](https://blog.openai.com/language-unsupervised/)\\xa0*(Alec Radford et al)*\\n\\xa0\\n\\nMeta learning\\n[Unsupervised Meta-Learning for Reinforcement Learning](http://arxiv.org/abs/1806.04640)\\xa0*(Abhishek Gupta et al)*\\n[Bayesian Model-Agnostic Meta-Learning](http://arxiv.org/abs/1806.03836)\\xa0*(Taesup Kim et al)*\\n\\xa0\\n\\nNews\\n[Research Scholars Programme](https://www.fhi.ox.ac.uk/rsp/): From the website: \"The Future of Humanity Institute is launching a Research Scholars Programme, likely to start in October 2018. It is a selective, two-year research programme, with lots of latitude for exploration as well as significant training and support elements. We will offer around six salaried positions to early-career researchers who aim to answer questions that shed light on the big-picture questions critical to humanitys wellbeing. We are collecting formal applications to the programme from now until 11 July, 2018.\"\\n[Announcing the second AI Safety Camp](http://effective-altruism.com/ea/1px/announcing_the_second_ai_safety_camp/)\\xa0*(Anne Wissemann)*: I forgot to mention last week that the second AI safety camp will be held Oct 4-14 in Prague.\\n[Human-aligned AI Summer School](http://humanaligned.ai/): The first Human-aligned AI Summer School will be held in Prague from 2nd to 5th August, with a focus on learning from humans (in particular, IRL and models of bounded rationality). Applications are open till July 14, but may close sooner if spots are filled up. |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| [If you got forwarded this email, you can sign up here!](http://eepurl.com/dqMSZj \"If you got forwarded this email, you can sign up here!\") |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n |\\n\\n\\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n |\\n\\n\\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n |\\n\\n\\n |\\n\\n |\\n\\n |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2018 Rohin Shah, All rights reserved.*\\n\\n\\n'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1902.08265v1',\n",
       "  'title': 'Quantifying Perceptual Distortion of Adversarial Examples',\n",
       "  'authors': ['Matt Jordan',\n",
       "   'Naren Manoj',\n",
       "   'Surbhi Goel',\n",
       "   'Alexandros G. Dimakis'],\n",
       "  'date_published': '2019-02-21 21:02:58+00:00',\n",
       "  'data_last_modified': '2019-02-21 21:02:58+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1902.08265v1',\n",
       "  'abstract': 'Recent work has shown that additive threat models, which only permit the addition of bounded noise to the pixels of an image, are insufficient for fully capturing the space of imperceivable adversarial examples. For example, small rotations and spatial transformations can fool classifiers, remain imperceivable to humans, but have large additive distance from the original images. In this work, we leverage quantitative perceptual metrics like LPIPS and SSIM to define a novel threat model for adversarial attacks.   To demonstrate the value of quantifying the perceptual distortion of adversarial examples, we present and employ a unifying framework fusing different attack styles. We first prove that our framework results in images that are unattainable by attack styles in isolation. We then perform adversarial training using attacks generated by our framework to demonstrate that networks are only robust to classes of adversarial perturbations they have been trained against, and combination attacks are stronger than any of their individual components. Finally, we experimentally demonstrate that our combined attacks retain the same perceptual distortion but induce far higher misclassification rates when compared against individual attacks.',\n",
       "  'author_comment': '18 pages, codebase/framework available at\\n  https://github.com/revbucket/mister_ed',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'stat.ML',\n",
       "  'categories': ['stat.ML', 'cs.LG'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': '',\n",
       "  'text': '',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{27}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Athalye et~al.(2017)Athalye, Engstrom, Ilyas, and\\n  Kwok]{Athalye2017-aa}\\nAthalye, A., Engstrom, L., Ilyas, A., and Kwok, K.\\n\\\\newblock Synthesizing robust adversarial examples.\\n\\\\newblock \\\\emph{CoRR}, abs/1707.07397, 2017.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1707.07397}.\\n\\n\\\\bibitem[Biggio \\\\& Roli(2017)Biggio and Roli]{biggio2017wild}\\nBiggio, B. and Roli, F.\\n\\\\newblock Wild patterns: Ten years after the rise of adversarial machine\\n  learning.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1712.03141}, 2017.\\n\\n\\\\bibitem[Cadene(2018)]{nasnetmobile}\\nCadene, R.\\n\\\\newblock pretrained-models.pytorch.\\n\\\\newblock \\\\emph{GitHub repository}, 2018.\\n\\n\\\\bibitem[Carlini \\\\& Wagner(2017{\\\\natexlab{a}})Carlini and\\n  Wagner]{Carlini2017-qm}\\nCarlini, N. and Wagner, D.\\n\\\\newblock Towards evaluating the robustness of neural networks.\\n\\\\newblock In \\\\emph{2017 {IEEE} Symposium on Security and Privacy ({SP})}, pp.\\\\\\n  39--57, May 2017{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Carlini \\\\& Wagner(2017{\\\\natexlab{b}})Carlini and\\n  Wagner]{carlini2017adversarial}\\nCarlini, N. and Wagner, D.\\n\\\\newblock Adversarial examples are not easily detected: Bypassing ten detection\\n  methods.\\n\\\\newblock In \\\\emph{Proceedings of the 10th ACM Workshop on Artificial\\n  Intelligence and Security}, AISec \\'17, pp.\\\\  3--14, New York, NY, USA,\\n  2017{\\\\natexlab{b}}. ACM.\\n\\\\newblock ISBN 978-1-4503-5202-4.\\n\\\\newblock \\\\doi{10.1145/3128572.3140444}.\\n\\\\newblock URL \\\\url{http://doi.acm.org/10.1145/3128572.3140444}.\\n\\n\\\\bibitem[Dalvi et~al.(2004)Dalvi, Domingos, Sanghai, Verma,\\n  et~al.]{dalvi2004adversarial}\\nDalvi, N., Domingos, P., Sanghai, S., Verma, D., et~al.\\n\\\\newblock Adversarial classification.\\n\\\\newblock In \\\\emph{Proceedings of the tenth ACM SIGKDD international conference\\n  on Knowledge discovery and data mining}, pp.\\\\  99--108. ACM, 2004.\\n\\n\\\\bibitem[Donahue et~al.(2016)Donahue, Kr{\\\\\"a}henb{\\\\\"u}hl, and\\n  Darrell]{Donahue2016-ip}\\nDonahue, J., Kr{\\\\\"a}henb{\\\\\"u}hl, P., and Darrell, T.\\n\\\\newblock Adversarial feature learning.\\n\\\\newblock In \\\\emph{International Conference on Learning Representations}, May\\n  2016.\\n\\\\newblock URL \\\\url{https://openreview.net/pdf?id=BJtNZAFgg}.\\n\\n\\\\bibitem[Engstrom et~al.(2017)Engstrom, Tsipras, Schmidt, and\\n  Madry]{Engstrom2017-ib}\\nEngstrom, L., Tsipras, D., Schmidt, L., and Madry, A.\\n\\\\newblock A rotation and a translation suffice: Fooling cnns with simple\\n  transformations.\\n\\\\newblock \\\\emph{CoRR}, abs/1712.02779, 2017.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1712.02779}.\\n\\n\\\\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and\\n  Szegedy]{Goodfellow2014-vh}\\nGoodfellow, I.~J., Shlens, J., and Szegedy, C.\\n\\\\newblock Explaining and harnessing adversarial examples.\\n\\\\newblock \\\\emph{CoRR}, abs/1412.6572, 2014.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1412.6572}.\\n\\n\\\\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\n\\\\newblock Deep residual learning for image recognition.\\n\\\\newblock In \\\\emph{2016 IEEE Conference on Computer Vision and Pattern\\n  Recognition (CVPR)}, pp.\\\\  770--778, June 2016.\\n\\\\newblock \\\\doi{10.1109/CVPR.2016.90}.\\n\\n\\\\bibitem[Idelbayev(2018)]{cifar2018pretrained}\\nIdelbayev, Y.\\n\\\\newblock pytorch\\\\_resnet\\\\_cifar10.\\n\\\\newblock \\\\emph{GitHub repository}, 2018.\\n\\n\\\\bibitem[Jaderberg et~al.(2015)Jaderberg, Simonyan, Zisserman, and\\n  Kavukcuoglu]{Jaderberg2015-xm}\\nJaderberg, M., Simonyan, K., Zisserman, A., and Kavukcuoglu, K.\\n\\\\newblock Spatial transformer networks.\\n\\\\newblock In Cortes, C., Lawrence, N.~D., Lee, D.~D., Sugiyama, M., and\\n  Garnett, R. (eds.), \\\\emph{Advances in Neural Information Processing Systems\\n  28}, pp.\\\\  2017--2025. Curran Associates, Inc., 2015.\\n\\n\\\\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}\\nKrizhevsky, A.\\n\\\\newblock Learning multiple layers of features from tiny images.\\n\\\\newblock Technical report, Citeseer, 2009.\\n\\n\\\\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and Bengio]{Kurakin2016-qg}\\nKurakin, A., Goodfellow, I.~J., and Bengio, S.\\n\\\\newblock Adversarial examples in the physical world.\\n\\\\newblock \\\\emph{CoRR}, abs/1607.02533, 2016.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1607.02533}.\\n\\n\\\\bibitem[Lowd \\\\& Meek(2005)Lowd and Meek]{lowd2005adversarial}\\nLowd, D. and Meek, C.\\n\\\\newblock Adversarial learning.\\n\\\\newblock In \\\\emph{Proceedings of the eleventh ACM SIGKDD international\\n  conference on Knowledge discovery in data mining}, pp.\\\\  641--647. ACM, 2005.\\n\\n\\\\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and\\n  Vladu]{Madry2017-ia}\\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.\\n\\\\newblock Towards deep learning models resistant to adversarial attacks.\\n\\\\newblock In \\\\emph{International Conference on Learning Representations}, 2018.\\n\\\\newblock URL \\\\url{https://openreview.net/forum?id=rJzIBfZAb}.\\n\\n\\\\bibitem[Papernot et~al.(2018)Papernot, Faghri, Carlini, Goodfellow, Feinman,\\n  Kurakin, Xie, Sharma, Brown, Roy, Matyasko, Behzadan, Hambardzumyan, Zhang,\\n  Juang, Li, Sheatsley, Garg, Uesato, Gierke, Dong, Berthelot, Hendricks,\\n  Rauber, and Long]{papernot2018cleverhans}\\nPapernot, N., Faghri, F., Carlini, N., Goodfellow, I., Feinman, R., Kurakin,\\n  A., Xie, C., Sharma, Y., Brown, T., Roy, A., Matyasko, A., Behzadan, V.,\\n  Hambardzumyan, K., Zhang, Z., Juang, Y.-L., Li, Z., Sheatsley, R., Garg, A.,\\n  Uesato, J., Gierke, W., Dong, Y., Berthelot, D., Hendricks, P., Rauber, J.,\\n  and Long, R.\\n\\\\newblock Technical report on the cleverhans v2.1.0 adversarial examples\\n  library.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1610.00768}, 2018.\\n\\n\\\\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,\\n  Desmaison, Antiga, and Lerer]{paszke2017automatic}\\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\\n  Desmaison, A., Antiga, L., and Lerer, A.\\n\\\\newblock Automatic differentiation in pytorch.\\n\\\\newblock In \\\\emph{NIPS-W}, 2017.\\n\\n\\\\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,\\n  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}\\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\\n  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.\\n\\\\newblock {ImageNet Large Scale Visual Recognition Challenge}.\\n\\\\newblock \\\\emph{International Journal of Computer Vision (IJCV)}, 115\\\\penalty0\\n  (3):\\\\penalty0 211--252, 2015.\\n\\\\newblock \\\\doi{10.1007/s11263-015-0816-y}.\\n\\n\\\\bibitem[Schott et~al.(2018)Schott, Rauber, Brendel, and Bethge]{overfitt}\\nSchott, L., Rauber, J., Brendel, W., and Bethge, M.\\n\\\\newblock Robust perception through analysis by synthesis.\\n\\\\newblock \\\\emph{CoRR}, abs/1805.09190, 2018.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1805.09190}.\\n\\n\\\\bibitem[Sharif et~al.(2018)Sharif, Bauer, and Reiter]{Sharif2018-fd}\\nSharif, M., Bauer, L., and Reiter, M.~K.\\n\\\\newblock On the suitability of lp-norms for creating and preventing\\n  adversarial examples.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1802. 09653}, 2018.\\n\\n\\\\bibitem[Su et~al.(2017)Su, Vargas, and Kouichi]{Su2017-op}\\nSu, J., Vargas, D.~V., and Kouichi, S.\\n\\\\newblock One pixel attack for fooling deep neural networks.\\n\\\\newblock \\\\emph{CoRR}, abs/1710.08864, 2017.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1710.08864}.\\n\\n\\\\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,\\n  Goodfellow, and Fergus]{Szegedy2013}\\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,\\n  and Fergus, R.\\n\\\\newblock Intriguing properties of neural networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1312.6199}, 2013.\\n\\n\\\\bibitem[Wang et~al.(2004)Wang, Bovik, Sheikh, and Simoncelli]{Wang2004-ky}\\nWang, Z., Bovik, A.~C., Sheikh, H.~R., and Simoncelli, E.~P.\\n\\\\newblock Image quality assessment: from error visibility to structural\\n  similarity.\\n\\\\newblock \\\\emph{IEEE Trans. Image Process.}, 13\\\\penalty0 (4):\\\\penalty0\\n  600--612, April 2004.\\n\\n\\\\bibitem[Xiao et~al.(2018)Xiao, Zhu, Li, He, Liu, and Song]{xiao2018spatially}\\nXiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song, D.\\n\\\\newblock Spatially transformed adversarial examples.\\n\\\\newblock \\\\emph{ICLR}, 2018.\\n\\n\\\\bibitem[Zhang et~al.(2018)Zhang, Isola, Efros, Shechtman, and\\n  Wang]{Zhang2018-mw}\\nZhang, R., Isola, P., Efros, A.~A., Shechtman, E., and Wang, O.\\n\\\\newblock The unreasonable effectiveness of deep features as a perceptual\\n  metric.\\n\\\\newblock \\\\emph{IEEE Conference on Computer Vision and Pattern Recognition\\n  (CVPR)}, January 2018.\\n\\n\\\\bibitem[Zoph et~al.(2017)Zoph, Vasudevan, Shlens, and Le]{zoph2017learning}\\nZoph, B., Vasudevan, V., Shlens, J., and Le, Q.~V.\\n\\\\newblock Learning transferable architectures for scalable image recognition.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1707.07012}, 2017.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1707.07397': True,\n",
       "   '1712.03141': True,\n",
       "   '1712.02779': True,\n",
       "   '1412.6572': True,\n",
       "   '1607.02533': True,\n",
       "   '1610.00768': True,\n",
       "   '1805.09190': True,\n",
       "   '1710.08864': True,\n",
       "   '1312.6199': True,\n",
       "   '1707.07012': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Adversarial examples',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #48',\n",
       "   'newsletter_url': 'https://mailchi.mp/3091c6e9405c/alignment-newsletter-48',\n",
       "   'summarizer': 'Dan H',\n",
       "   'summary': 'This paper takes a step toward more general adversarial threat models by combining adversarial additive perturbations small in an l_p sense with [spatially transformed adversarial examples](https://arxiv.org/abs/1801.02612), among other other attacks. In this more general setting, they measure the size of perturbations by computing the [SSIM](https://ece.uwaterloo.ca/~z70wang/research/ssim/#MAD) between clean and perturbed samples, which has limitations but is on the whole better than the l_2 distance. This work shows, along with other concurrent works, that perturbation robustness under some threat models does not yield robustness under other threat models. Therefore the view that l_p perturbation robustness must be achieved before considering other threat models is made more questionable. The paper also contributes a large code library for testing adversarial perturbation robustness.',\n",
       "   'opinion': 'nan',\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '1902.08265v1',\n",
       "   'arxiv_id': '1902.08265',\n",
       "   'title': 'Quantifying Perceptual Distortion of Adversarial Examples',\n",
       "   'authors': ['Matt Jordan',\n",
       "    'Naren Manoj',\n",
       "    'Surbhi Goel',\n",
       "    'Alexandros G. Dimakis'],\n",
       "   'date_published': '2019-02-21 21:02:58+00:00',\n",
       "   'data_last_modified': '2019-02-21 21:02:58+00:00',\n",
       "   'url': 'http://arxiv.org/abs/1902.08265v1',\n",
       "   'abstract': 'Recent work has shown that additive threat models, which only permit the addition of bounded noise to the pixels of an image, are insufficient for fully capturing the space of imperceivable adversarial examples. For example, small rotations and spatial transformations can fool classifiers, remain imperceivable to humans, but have large additive distance from the original images. In this work, we leverage quantitative perceptual metrics like LPIPS and SSIM to define a novel threat model for adversarial attacks.   To demonstrate the value of quantifying the perceptual distortion of adversarial examples, we present and employ a unifying framework fusing different attack styles. We first prove that our framework results in images that are unattainable by attack styles in isolation. We then perform adversarial training using attacks generated by our framework to demonstrate that networks are only robust to classes of adversarial perturbations they have been trained against, and combination attacks are stronger than any of their individual components. Finally, we experimentally demonstrate that our combined attacks retain the same perceptual distortion but induce far higher misclassification rates when compared against individual attacks.',\n",
       "   'author_comment': '18 pages, codebase/framework available at\\n  https://github.com/revbucket/mister_ed',\n",
       "   'journal_ref': 'None',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'stat.ML',\n",
       "   'categories': \"['stat.ML', 'cs.LG']\",\n",
       "   'individual_summary': 'Title: Quantifying Perceptual Distortion of Adversarial Examples\\nAuthors: Matt Jordan, Naren Manoj, Surbhi Goel, Alexandros G. Dimakis\\nPaper abstract: Recent work has shown that additive threat models, which only permit the addition of bounded noise to the pixels of an image, are insufficient for fully capturing the space of imperceivable adversarial examples. For example, small rotations and spatial transformations can fool classifiers, remain imperceivable to humans, but have large additive distance from the original images. In this work, we leverage quantitative perceptual metrics like LPIPS and SSIM to define a novel threat model for adversarial attacks.   To demonstrate the value of quantifying the perceptual distortion of adversarial examples, we present and employ a unifying framework fusing different attack styles. We first prove that our framework results in images that are unattainable by attack styles in isolation. We then perform adversarial training using attacks generated by our framework to demonstrate that networks are only robust to classes of adversarial perturbations they have been trained against, and combination attacks are stronger than any of their individual components. Finally, we experimentally demonstrate that our combined attacks retain the same perceptual distortion but induce far higher misclassification rates when compared against individual attacks.\\nSummary: This paper takes a step toward more general adversarial threat models by combining adversarial additive perturbations small in an l_p sense with [spatially transformed adversarial examples](https://arxiv.org/abs/1801.02612), among other other attacks. In this more general setting, they measure the size of perturbations by computing the [SSIM](https://ece.uwaterloo.ca/~z70wang/research/ssim/#MAD) between clean and perturbed samples, which has limitations but is on the whole better than the l_2 distance. This work shows, along with other concurrent works, that perturbation robustness under some threat models does not yield robustness under other threat models. Therefore the view that l_p perturbation robustness must be achieved before considering other threat models is made more questionable. The paper also contributes a large code library for testing adversarial perturbation robustness.\\nMy opinion: nan',\n",
       "   'paper_text': '',\n",
       "   'text': 'Highlights\\n**[Quantilizers: A Safer Alternative to Maximizers for Limited Optimization](https://intelligence.org/files/QuantilizersSaferAlternative.pdf)\\xa0and\\xa0[When to use quantilization](https://www.alignmentforum.org/posts/Rs6vZCrnQFWQ4p37P/when-to-use-quantilization)**\\xa0*(Jessica Taylor and Ryan Carey)*: A key worry with AI alignment is that if we maximize expected utility for some utility function chosen by hand, we will likely get unintended side effects that score highly by the utility function but are nevertheless not what we intended. We might hope to leverage human feedback to solve this: in particular, an AI system that simply mimics human actions would often be desirable. However, mimicry can only achieve human performance, and cannot improve upon it. The first link is a 2015 paper that introduces quantilization, which interpolates between these two extremes to improve upon human performance while bounding the potential (expected) loss from unintended side effects.\\nIn particular, let\\'s suppose that humans have some policy  (i.e. probability distribution over actions). We evaluate utility or performance using a utility function U, but we\\xa0*do not assume*\\xa0it is well-specified -- U can be any function, including one we would not want to maximize. Our goal is to design a policy  that gets higher expected U than  (reflecting our hope that U measures utility well) without doing too much worse than  in the worst case when U was as badly designed as possible. We\\'ll consider a one-shot case:  is used to select an action once, and then the game is over.\\nThe core idea behind quantilization is simple: if our policy only does things that the human might have done, any expected loss it incurs corresponds to some loss that the human could incur. So, let\\'s take our human policy , keep only the top q-fraction of  (as evaluated by U), and then sample an action from there. This defines our policy \\\\_q, also called a q-quantilizer. For example, suppose the human would choose A with probability 0.25, B with probability 0.5, and C with probability 0.25, and U(A) > U(B) > U(C). Then a (1/4)-quantilizer would choose A with certainty, a (1/2)-quantilizer would choose randomly between A and B, and a (3/8)-quantilizer would choose A twice as often as B.\\nNote that even if \\\\_q places all of its probability on the worst possible actions (because U was badly designed), by construction the human had probability q of doing the same thing as \\\\_q, and so the expected cost of \\\\_q can be at most (1/q) times larger than the expected cost of the human policy . In fact, if we have no other knowledge, a q-quantilizer maximizes expected utility U subject to the constraint of never doing worse than (1/q) times as bad as .\\nAs soon as you move to the setting with multiple actions, if you choose each action using quantilization, then your worst case bound is exponential in the number of actions. If you assume the cost for each action is independent, you recover the guarantees, but this is not a realistic assumption (as the authors note). Long-term plans are very good or very bad\\xa0*because*\\xa0all of the actions build on each other to achieve some goal, so the costs are not going to be independent.\\nThe second link is a recent post that further analyzes quantilization. It points out that the theoretical analysis in the paper assumes that deviating from the human policy can only lead to costs. If the deviation could also forgo benefits, then the theorems no longer apply. Perhaps the bottom (1-q) of actions actually have strong positive benefits, that we failed to model in U. Then by eliminating those actions, we may have lost arbitrary amounts of expected value. In these situations, the only way to bound the expected regret is by exact mimicry. It also points out that if you are aiming to simultaneously do well both on U and the worst-case bound, then typically imitation will be better since adding any optimization can drastically weaken the worst-case bound and usually will not make U better by the same amount. Quantilization makes sense when there is a \"sweet-spot of actions that are pretty common but substantially outperform imitation\".\\n**Rohin\\'s opinion:**\\xa0The exponential blowup in potential loss with multiple actions would make this prohibitive, but of course you could instead view the full sequence of actions (i.e. trajectory) as a mega-action, and quantilize over this mega-action. In this case, a one-millionth-quantilizer could choose from among the million best plans that a human would make (assuming a well-specified U), and any unintended consequences (that were intentionally chosen by the quantilizer) would have to be ones that a human had a one-in-a-million chance of causing to occur, which quite plausibly excludes really bad outcomes.\\nPhrased this way, quantilization feels like an amplification of a human policy. Unlike the amplification in iterated amplification, it does\\xa0*not*\\xa0try to preserve alignment, it simply tries to bound how far away from alignment the resulting policy can diverge. As a result, you can\\'t iterate quantilization to get arbitrarily good capabilities. You might hope that humans could learn from powerful AI systems, grow more capable themselves (while remaining as safe as they were before), and then the next quantilizers would be more powerful.\\nIt\\'s worth noting that the theorem in the paper shows that, to the extent that you think quantilization is insufficient for AI alignment, you need to make some other assumption, or find some other source of information, in order to do better, since quantilization is optimal for its particular setup. For example, you could try to assume that U is at least somewhat reasonable and not pathologically bad; or you could assume an interactive setting where the human can notice and correct for any issues with the U-maximizing plan before it is executed; or you could not have U at all and exceed human performance through some other technique.\\nI\\'m not very worried about the issue that quantilization could forgo benefits that the human policy had. It seems that even if this happens, we could notice this, turn off the quantilizer, and fix the utility function U so that it no longer ignores those benefits. (We wouldn\\'t be able to prevent the quantilizer from forgoing benefits of our policy that we didn\\'t know about, but that seems okay to me.)\\nTechnical AI alignment\\n\\xa0\\n\\nIterated amplification\\n[Can HCH epistemically dominate Ramanujan?](https://www.alignmentforum.org/posts/4qY9zEHLa2su4PkQ4/can-hch-epistemically-dominate-ramanujan)\\xa0*(Alex Zhu)*: Iterated amplification rests on the hope that we can achieve arbitrarily high capabilities with (potentially very large) trees of explicit verbal breakdowns of problems. This is often formalized as a question about\\xa0[HCH](https://www.alignmentforum.org/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch)\\xa0([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)). This post considers the example of Srinivasa Ramanujan, who is \"famously known for solving math problems with sudden and inexplicable flashes of insight\". It is not clear how HCH would be able to replicate this sort of reasoning.\\nLearning human intent\\n[Unsupervised Visuomotor Control through Distributional Planning Networks](https://arxiv.org/abs/1902.05542)\\xa0*(Tianhe Yu et al)*\\n[Syntax vs semantics: alarm better example than thermostat](https://www.alignmentforum.org/posts/bbw6c9as5STvWXAgB/syntax-vs-semantics-alarm-better-example-than-thermostat)\\xa0*(Stuart Armstrong)*: This post gives a new example that more clearly illustrates the points made in a\\xa0[previous post](https://www.alignmentforum.org/posts/EEPdbtvW8ei9Yi2e8/bridging-syntax-and-semantics-empirically)\\xa0([AN #26](https://mailchi.mp/1ecd1b775703/alignment-newsletter-26)).\\n**Prerequisities:**\\xa0[Bridging syntax and semantics, empirically](https://www.alignmentforum.org/posts/EEPdbtvW8ei9Yi2e8/bridging-syntax-and-semantics-empirically)\\nInterpretability\\n[Synthesizing the preferred inputs for neurons in neural networks via deep generator networks](https://arxiv.org/abs/1605.09304)\\xa0*(Anh Nguyen et al)*\\nAdversarial examples\\n[Quantifying Perceptual Distortion of Adversarial Examples](https://arxiv.org/abs/1902.08265)\\xa0*(Matt Jordan et al)*\\xa0(summarized by Dan H): This paper takes a step toward more general adversarial threat models by combining adversarial additive perturbations small in an l\\\\_p sense with\\xa0[spatially transformed adversarial examples](https://arxiv.org/abs/1801.02612), among other other attacks. In this more general setting, they measure the size of perturbations by computing the\\xa0[SSIM](https://ece.uwaterloo.ca/~z70wang/research/ssim/#MAD)\\xa0between clean and perturbed samples, which has limitations but is on the whole better than the l\\\\_2 distance. This work shows, along with other concurrent works, that perturbation robustness under some threat models does not yield robustness under other threat models. Therefore the view that l\\\\_p perturbation robustness must be achieved before considering other threat models is made more questionable. The paper also contributes a large code library for testing adversarial perturbation robustness.\\n[On the Sensitivity of Adversarial Robustness to Input Data Distributions](https://arxiv.org/abs/1902.08336)\\xa0*(Gavin Weiguang Ding et al)*\\nForecasting\\n[Primates vs birds: Is one brain architecture better than the other?](https://aiimpacts.org/primates-vs-birds-is-one-brain-architecture-better-than-the-other/)\\xa0*(Tegan McCaslin)*: Progress in AI can be driven by both larger models as well as architectural improvements (given sufficient data and compute), but which of these is more important? One source of evidence comes from animals: different species that are closely related will have similar neural architectures, but potentially quite different brain sizes. This post compares intelligence across birds and primates: while primates (and mammals more generally) have a neocortex (often used to explain human intelligence), birds have a different, independently-evolved type of cortex. Using a survey over non-expert participants about how intelligent different bird and primate behavior is, it finds that there is not much difference in intelligence ratings between birds and primates, but that species with larger brains are rated as more intelligent than those with smaller brains. This only suggests that there are at least two neural architectures that work -- it could still be a hard problem to find them in the vast space of possible architectures. Still, it is some evidence that at least in the case of evolution, you get more intelligence through more neurons, and architectural improvements are relatively less important.\\n**Rohin\\'s opinion:**\\xa0Upon reading the experimental setup I didn\\'t really know which way the answer was going to turn out, so I\\'m quite happy about now having another data point with which to understand learning dynamics. Of course, it\\'s not clear how data about evolution will generalize to AI systems. For example, architectural improvements probably require some hard-to-find insight which make them hard to find via random search (imagine how hard it would be to invent CNNs by randomly trying stuff), while scaling up model size is easy, and so we might expect AI researchers to be differentially better at finding architectural improvements relative to scaling up model size (as compared to evolution).\\n**Read more:**\\xa0[Investigation into the relationship between neuron count and intelligence across differing cortical architectures](https://aiimpacts.org/investigation-into-the-relationship-between-neuron-count-and-intelligence-across-differing-cortical-architectures/)\\nMiscellaneous (Alignment)\\n**[Quantilizers: A Safer Alternative to Maximizers for Limited Optimization](https://intelligence.org/files/QuantilizersSaferAlternative.pdf)\\xa0and\\xa0[When to use quantilization](https://www.alignmentforum.org/posts/Rs6vZCrnQFWQ4p37P/when-to-use-quantilization)**\\xa0*(Jessica Taylor and Ryan Carey)*: Summarized in the highlights!\\n[Human-Centered Artificial Intelligence and Machine Learning](http://arxiv.org/abs/1901.11184)\\xa0*(Mark O. Riedl)*\\nAI strategy and policy\\n[Stable Agreements in Turbulent Times](https://forum.effectivealtruism.org/out?url=https%3A%2F%2Fwww.fhi.ox.ac.uk%2Fwp-content%2Fuploads%2FStable-Agreements.pdf)\\xa0*(Cullen OKeefe)*: On the one hand we would like actors to be able to cooperate before the development of AGI by entering into binding agreements, but on the other hand such agreements are often unpalatable and hard to write because there is a lot of uncertainty, indeterminacy and unfamiliarity with the consequences of developing powerful AI systems. This makes it very hard to be confident that any given agreement is actually net positive for a given actor. The key point of this report is that we can strike a balance between these two extremes by agreeing pre-AGI to be bound by decisions that are made post-AGI with the benefit of increased knowledge. It examines five tools for this purpose: options, impossibility doctrines, contractual standards, renegotiation, and third-party resolution.\\n[Advice to UN High-level Panel on Digital Cooperation](https://www.cser.ac.uk/news/advice-un-high-level-panel-digital-cooperation/)\\xa0*(Luke Kemp et al)*\\nOther progress in AI\\n\\xa0\\n\\nReinforcement learning\\n[Neural MMO](https://blog.openai.com/neural-mmo/)\\xa0*(OpenAI)*\\xa0(summarized by Richard): Neural MMO is \"a massively multiagent game environment for reinforcement learning agents\". It was designed to be persistent (with concurrent learning and no environment resets), large-scale, efficient and expandable. Agents need to traverse an environment to obtain food and water in order to survive for longer (the metric for which they are rewarded), and are also able to engage in combat with other agents. Agents trained within a larger population explore more and consistently outperform those trained in smaller populations (when evaluated together). The authors note that multiagent training is a curriculum magnifier, not a curriculum in itself, and that the environment must facilitate adaptive pressures by allowing a sufficient range of interactions.\\n[Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research](https://arxiv.org/abs/1903.00742)\\xa0*(Joel Z. Leibo, Edward Hughes, Marc Lanctot, Thore Graepel)*\\xa0(summarized by Richard): The authors argue that the best solution to the problem of task generation is creating multi-agent systems where each agent must adapt to the others. These agents do so first by learning how to implement a high-level strategy, and then by adapting it based on the strategies of others. (The authors use the term \"adaptive unit\" rather than \"agent\" to emphasise that change can occur at many different hierarchical levels, and either by evolution or learning). This adaptation may be exogenous (driven by the need to respond to a changing environment) or endogenous (driven by a unit\\'s need to improve its own functionality). An example of the latter is a society implementing institutions which enforce cooperation between individuals. Since individuals will try to exploit these institutions, the process of gradually robustifying them can be considered an automatically-generated curriculum (aka autocurriuclum).\\n**Richard\\'s opinion:**\\xa0My guess is that multiagent learning will become very popular fairly soon. In addition to this paper and the Neural MMO paper, it was also a key part of the AlphaStar training process. The implications of this research direction for safety are still unclear, and it seems valuable to explore them further. One which comes to mind: the sort of deceptive behaviour required for treacherous turns seems more likely to emerge from multiagent training than from single-agent training.\\n[Long-Range Robotic Navigation via Automated Reinforcement Learning](https://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html)\\xa0*(Aleksandra Faust and Anthony Francis)*: How can we get robots that successfully navigate in the real world? One approach is to use a high-level route planner that uses a learned control policy over very short distances (10-15 meters). The control policy is learned using deep reinforcement learning, where the network architecture and reward shaping is also learned via neural architecture search (or at least something very similar). The simulations have enough noise that the learned control policy transfers well to new environments. Given this policy as well as a floorplan of the environment we want the robot to navigate in, we can build a graph of points on the floorplan, where there is an edge between two points if the robot can safely navigate between the two points using the learned controller (which I\\xa0*think*\\xa0is checked in simulation). At execution time, we can find a path to the goal in this graph, and move along the edges using the learned policy. They were able to build a graph for the four buildings at the Google main campus using 300 workers over 4 days. They find that the robots are very robust in the real world. See also\\xa0[Import AI](https://jack-clark.net/2019/03/04/import-ai-136-what-machine-learning-power-infrastructure-means-for-humanity-new-gca-benchmarkdataset-challenges-image-captioning-systems-and-google-uses-frankenrl-to-create-more-mobile-robot/).\\n**Rohin\\'s opinion:**\\xa0This is a great example of a pattern that seems quite common: once we automate tasks using end-to-end training that previously required more structured approaches, new more complex tasks will arise that will use the end-to-end trained systems as building blocks in a bigger structured approach. In this case, we can now train robots to navigate over short distances using end-to-end training, and this has been used in a structured approach involving graphs and waypoints to create robots that can traverse larger distances.\\nIt\\'s also an example of what you can do when you have a ton of compute: for the learned controller, they learned both the network architecture and the reward shaping. About the only thing that had to be explicity specified was the sparse true reward. (Although I\\'m sure in practice it took a lot of effort to get everything to actually work.)\\n[Competitive Experience Replay](http://arxiv.org/abs/1902.00528)\\xa0*(Hao Liu et al)*\\nNews\\n[Q&A with Jason Matheny, Founding Director of CSET](https://www.georgetown.edu/news/q-and-a-with-cset-founding-director-jason-matheny)\\xa0*(Jason Matheny)*: The\\xa0[Center for Security and Emerging Technology](https://cset.georgetown.edu/)\\xa0has been announced, with a\\xa0[$55 million grant from the Open Philanthropy Project](https://www.openphilanthropy.org/giving/grants/georgetown-university-center-security-and-emerging-technology), and is\\xa0[hiring](https://cset.georgetown.edu/careers/). While the center will work on emerging technologies generally, it will initially focus on AI, since demand for AI policy analysis has far outpaced supply.\\nOne area of focus is the implications of AI on national and international security. Current AI systems are brittle and can easily be fooled, implying several safety and security challenges. What are these challenges, and how important are they? How can we make systems that are more robust and mitigate these problems?\\nAnother area is how to enable effective competition on AI in a global environment, while also cooperating on issues of safety, security and ethics? This will likely require measurement of investment flows, publications, data and hardware across countries, as well as management of talent and knowledge workflows.\\nSee also\\xa0[Import AI](https://jack-clark.net/2019/03/04/import-ai-136-what-machine-learning-power-infrastructure-means-for-humanity-new-gca-benchmarkdataset-challenges-image-captioning-systems-and-google-uses-frankenrl-to-create-more-mobile-robot/).\\n**Rohin\\'s opinion:**\\xa0It\\'s great to see a center for AI policy that\\'s run by a person who has wanted to consume AI policy analysis in the past (Jason Matheny was previously the director of IARPA). It\\'s interesting to see the areas he focuses on in this Q&A -- it\\'s not what I would have expected given my very little knowledge of AI policy. |\\n\\n\\n |\\n\\n |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n |\\n\\n\\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n |\\n\\n\\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n |\\n\\n\\n |\\n\\n |\\n\\n |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2019 Rohin Shah, All rights reserved.*\\n\\n\\n'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2006.13258v6',\n",
       "  'title': 'Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization',\n",
       "  'authors': ['Paul Barde',\n",
       "   'Julien Roy',\n",
       "   'Wonseok Jeon',\n",
       "   'Joelle Pineau',\n",
       "   'Christopher Pal',\n",
       "   'Derek Nowrouzezahrai'],\n",
       "  'date_published': '2020-06-23 18:29:13+00:00',\n",
       "  'data_last_modified': '2021-04-16 10:09:13+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2006.13258v6',\n",
       "  'abstract': \"Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.\",\n",
       "  'author_comment': None,\n",
       "  'journal_ref': 'Advances in Neural Information Processing Systems 33 (2020)',\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 0.9940390821,\n",
       "  'main_tex_filename': 'main.tex',\n",
       "  'text': '---\\nabstract: |\\n  Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert\\'s demonstrations from generated ones -- and a generator\\'s policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator\\'s iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator\\'s policy. Consequently, our discriminator\\'s update solves the generator\\'s optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.\\nauthor:\\n- |\\n  Paul Barde[^1] $^{\\\\,\\\\dagger}$\\\\\\n  Qubec AI institute (Mila)\\\\\\n  McGill University\\\\\\n  `bardepau@mila.quebec`\\\\\\n  Julien Roy$^{\\\\ast \\\\dagger}$\\\\\\n  Qubec AI institute (Mila)\\\\\\n  Polytechnique Montral\\\\\\n  `julien.roy@mila.quebec`\\\\\\n  Wonseok Jeon$^{\\\\ast}$\\\\\\n  Qubec AI institute (Mila)\\\\\\n  McGill University\\\\\\n  `jeonwons@mila.quebec`\\\\\\n  Joelle Pineau$^\\\\ddag$\\\\\\n  Qubec AI institute (Mila)\\\\\\n  McGill University\\\\\\n  Facebook AI Research\\\\\\n  Christopher Pal$^\\\\ddag$\\\\\\n  Qubec AI institute (Mila)\\\\\\n  Polytechnique Montral\\\\\\n  Element AI\\\\\\n  Derek Nowrouzezahrai\\\\\\n  Qubec AI institute (Mila)\\\\\\n  McGill University\\\\\\nbibliography:\\n- sources.bib\\ntitle: |\\n  Adversarial Soft Advantage Fitting:\\\\\\n  Imitation Learning without Policy Optimization\\n---\\n\\nIntroduction\\n============\\n\\nImitation Learning (IL) treats the task of learning a policy from a set of expert demonstrations. IL is effective on control problems that are challenging for traditional Reinforcement Learning (RL) methods, either due to reward function design challenges or the inherent difficulty of the task itself [@abbeel2004apprenticeship; @ross2011reduction].\\n\\nMost IL work can be divided into two branches: Behavioral Cloning and Inverse Reinforcement Learning. Behavioral Cloning casts IL as a supervised learning objective and seeks to imitate the expert\\'s actions using the provided demonstrations as a fixed dataset [@pomerleau1991efficient]. Thus, Behavioral Cloning usually requires a lot of expert data and results in agents that struggle to generalize. As an agent deviates from the demonstrated behaviors -- straying outside the state distribution on which it was trained -- the risks of making additional errors increase, a problem known as compounding error [@ross2011reduction].\\n\\nInverse Reinforcement Learning aims to reduce compounding error by learning a reward function under which the expert policy is optimal [@abbeel2004apprenticeship]. Once learned, an agent can be trained (with any RL algorithm) to learn how to act at any given state of the environment. Early methods were prohibitively expensive on large environments because they required training the RL agent to convergence at each learning step of the reward function [@ziebart2008maximum; @abbeel2004apprenticeship]. Recent approaches instead apply an adversarial formulation (Adversarial Imitation Learning, AIL) in which a discriminator learns to distinguish between expert and agent behaviors to learn the reward optimized by the expert. AIL methods allow for the use of function approximators and can in practice be used with only a few policy improvement steps for each discriminator update [@ho2016generative; @fu2017learning; @finn2016connection].\\n\\nWhile these advances have allowed Imitation Learning to tackle bigger and more complex environments [@kuefler2017imitating; @ding2019goal], they have also significantly complexified the implementation and learning dynamics of Imitation Learning algorithms. It is worth asking how much of this complexity is actually mandated. For example, in recent work, @reddy2019sqil have shown that competitive performance can be obtained by hard-coding a very simple reward function to incentivize expert-like behaviors and manage to imitate it through off-policy direct RL. @reddy2019sqil therefore remove the reward learning component of AIL and focus on the RL loop, yielding a regularized version of Behavioral Cloning. Motivated by these results, we also seek to simplify the AIL framework but following the opposite direction: keeping the reward learning module and removing the policy improvement loop.\\n\\nWe propose a simpler yet competitive AIL framework. Motivated by @finn2016connection who use the optimal discriminator form, we propose a structured discriminator that estimates the probability of demonstrated and generated behavior using a single parameterized maximum entropy policy. Discriminator learning and policy learning therefore occur simultaneously, rendering seamless generator updates: once the discriminator has been trained for a few epochs, we simply use its policy model to generate new rollouts. We call this approach Adversarial Soft Advantage Fitting (ASAF).\\n\\nWe make the following contributions:\\n\\n-   **Algorithmic**: we present a novel algorithm (ASAF) designed to imitate expert demonstrations without any Reinforcement Learning step.\\n\\n-   **Theoretical**: we show that our method retrieves the expert policy when trained to optimality.\\n\\n-   **Empirical**: we show that ASAF outperforms prevalent IL algorithms on a variety of discrete and continuous control tasks. We also show that, in practice, ASAF can be easily modified to account for different trajectory lengths (from full length to transition-wise).\\n\\nBackground\\n==========\\n\\n#### Markov Decision Processes (MDPs)\\n\\nWe use @hazan2018provably\\'s notation and consider the classic $T$-horizon $\\\\gamma$-discounted MDP $\\\\mathcal{M}=\\\\langle \\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{P}, \\\\mathcal{P}_0, \\\\gamma, r, T \\\\rangle$. For simplicity, we assume that $\\\\mathcal{S}$ and $\\\\mathcal{A}$ are finite. Successor states are given by the transition distribution $\\\\mathcal{P}(s\\'|s,a)\\\\in [0,1]$, and the initial state $s_0$ is drawn from $\\\\mathcal{P}_0(s)\\\\in[0,1]$. Transitions are rewarded with $r(s,a)\\\\in\\\\mathbb{R}$ with $r$ being bounded. The discount factor and the episode horizon are $\\\\gamma \\\\in [0,1]$ and $T\\\\in \\\\mathbb{N}\\\\cup \\\\{\\\\infty\\\\}$, where $T<\\\\infty$ for $\\\\gamma=1$. Finally, we consider stationary stochastic policies $\\\\pi \\\\in \\\\Pi : \\\\mathcal{S}\\\\times\\\\mathcal{A} \\\\rightarrow ]0,1[$ that produce trajectories $\\\\tau = (s_0, a_0, s_1, a_1, ..., s_{T-1}, a_{T-1}, s_T)$ when executed on $\\\\mathcal{M}$.\\n\\nThe probability of trajectory $\\\\tau$ under policy $\\\\pi$ is $P_\\\\pi(\\\\tau)\\n    \\\\triangleq \\n    \\\\mathcal{P}_0(s_0)\\\\prod_{t=0}^{T-1}\\\\pi(a_t|s_t)\\\\mathcal{P}(s_{t+1}|s_t, a_t)$ and the corresponding marginals are defined as $d_{t, \\\\pi}(s)\\n    \\\\triangleq \\n    \\\\sum_{\\\\tau:s_t=s}P_\\\\pi(\\\\tau)$ and $d_{t, \\\\pi}(s, a)\\n    \\\\triangleq \\n    \\\\sum_{\\\\tau:s_t=s, a_t=a}P_\\\\pi(\\\\tau)\\n    =\\n    d_{t, \\\\pi}(s)\\\\pi(a|s)$, respectively. With these marginals, we define the normalized discounted state and state-action occupancy measures as $d_{\\\\pi}(s)\\n    \\\\triangleq\\n    \\\\frac{1}{Z(\\\\gamma, T)}\\\\sum_{t=0}^{T-1}\\\\gamma^td_{t, \\\\pi}(s)$ and $d_{\\\\pi}(s, a)\\n    \\\\triangleq\\n    \\\\frac{1}{Z(\\\\gamma, T)}\\\\sum_{t=0}^{T-1}\\\\gamma^td_{t, \\\\pi}(s, a)\\n    =\\n    d_{\\\\pi}(s)\\\\pi(a|s)$ where the partition function $Z(\\\\gamma, T)$ is equal to $\\\\sum_{t=0}^{T-1}\\\\gamma^t$. Intuitively, the state (or state-action) occupancy measure can be interpreted as the discounted visitation distribution of the states (or state-action pairs) that the agent encounters when navigating with policy $\\\\pi$. The expected sum of discounted rewards can be expressed in term of the occupancy measures as follows: $$\\\\begin{aligned}\\n    J_{\\\\pi}[r(s, a)] \\n    \\\\triangleq\\n    \\\\mathbb{E}_{\\\\tau\\\\sim P_\\\\pi}\\\\left[\\\\textstyle\\\\sum_{t=0}^{T-1}\\\\gamma^t \\\\, r(s_t, a_t)\\\\right]\\n    =\\n    Z(\\\\gamma, T) \\\\, \\\\mathbb{E}_{(s,a)\\\\sim d_{\\\\pi}}[r(s, a)].\\n\\\\end{aligned}$$ In the entropy-regularized Reinforcement Learning framework\\xa0[@haarnoja2018soft], the optimal policy maximizes its entropy at each visited state in addition to the standard RL objective: $$\\\\pi^*\\n    \\\\triangleq\\n    \\\\mathop{\\\\mathrm{arg\\\\,max}}_\\\\pi J_\\\\pi[r(s,a)+\\\\alpha \\\\mathcal{H}(\\\\pi(\\\\cdot|s))]\\\\,, \\\\quad \\\\mathcal{H}(\\\\pi(\\\\cdot|s)) = \\\\mathbb{E}_{a\\\\sim \\\\pi(\\\\cdot|s)}[-\\\\log(\\\\pi(a|s))].$$ As shown in [@ziebart2010modeling; @haarnoja2017reinforcement] the corresponding optimal policy is $$\\\\begin{aligned}\\n    \\\\label{eq:max_ent_policy}\\n    \\\\pi_{\\\\mathrm{soft}}^*(a|s) =\\\\exp \\\\left({\\\\alpha}^{-1} \\\\, A^*_{\\\\mathrm{soft}}(s,a)\\\\right) \\\\quad \\\\text{with} \\\\quad\\n    &A^*_{\\\\mathrm{soft}}(s,a)\\n    \\\\triangleq\\n    Q^*_{\\\\mathrm{soft}}(s,a)- V^*_{\\\\mathrm{soft}}(s),\\n    \\\\\\\\\\n    V^*_{\\\\mathrm{soft}}(s)\\n    =\\n    \\\\alpha \\\\log\\\\sum_{a\\\\in\\\\mathcal{A}}\\\\exp \\\\left({\\\\alpha}^{-1} \\\\, Q^*_{\\\\mathrm{soft}}(s,a)\\\\right), \\\\,\\\\,\\n    &Q^*_{\\\\mathrm{soft}}(s,a)\\n    =\\n    r(s,a) + \\\\gamma\\\\mathbb{E}_{s\\'\\\\sim\\\\mathcal{P}(\\\\cdot|s, a)}\\\\left[ V^*_{\\\\mathrm{soft}}(s\\')\\\\right]\\\\end{aligned}$$\\n\\n#### Maximum Causal Entropy Inverse Reinforcement Learning\\n\\nIn the problem of Inverse Reinforcement Learning (IRL), it is assumed that the MDP\\'s reward function is unknown but that demonstrations from using expert\\'s policy $\\\\pi_{\\\\!_E}$ are provided. Maximum causal entropy IRL [@ziebart2008maximum] proposes to fit a reward function $r$ from a set $\\\\mathcal{R}$ of reward functions and retrieve the corresponding optimal policy by solving the optimization problem $$\\\\label{eq:irl_problem}\\n    \\\\min_{r\\\\in\\\\mathcal{R}}\\\\left(\\\\max_{\\\\pi} J_\\\\pi[r(s, a) + \\\\mathcal{H}(\\\\pi(\\\\cdot|s))]  \\\\right)-J_{\\\\pi_{\\\\!_E}}[r(s, a)].$$ In brief, the problem reduces to finding a reward function $r$ for which the expert policy is optimal. In order to do so, the optimization procedure searches high entropy policies that are optimal with respect to $r$ and minimizes the difference between their returns and the return of the expert policy, eventually reaching a policy $\\\\pi$ that approaches $\\\\pi_{\\\\!_E}$. Most of the proposed solutions [@abbeel2004apprenticeship; @ziebart2010modeling; @ho2016generative] transpose IRL to the problem of distribution matching; @abbeel2004apprenticeship and @ziebart2008maximum used linear function approximation and proposed to match the feature expectation; @ho2016generative proposed to cast Eq.\\xa0([\\\\[eq:irl_problem\\\\]](#eq:irl_problem){reference-type=\"ref\" reference=\"eq:irl_problem\"}) with a convex reward function regularizer into the problem of minimizing the Jensen-Shannon divergence between the state-action occupancy measures: $$\\\\label{eq:irl_feature_matching}\\n    \\\\min_{\\\\pi} D_{\\\\text{JS}}(d_\\\\pi, d_{\\\\pi_{\\\\!_E}}) - J_\\\\pi[\\\\mathcal{H}(\\\\pi(\\\\cdot|s))]$$\\n\\n#### Connections between Generative Adversarial Networks (GANs) and IRL\\n\\nFor the data distribution $p_{\\\\!_E}$ and the generator distribution $p_{\\\\!_G}$ defined on the domain $\\\\mathcal{X}$, the GAN objective\\xa0[@goodfellow2014generative] is $$\\\\begin{aligned}\\n    \\\\label{eq:GAN_obj}\\n    \\\\min_{p_{\\\\!_G}} \\\\max_D L(D, p_{\\\\!_G}) \\\\, , \\\\quad L(D, p_{\\\\!_G})\\n    \\\\triangleq\\n    \\\\mathbb{E}_{x\\\\sim p_{\\\\!_E}}[\\\\log D(x)]\\n    +\\n    \\\\mathbb{E}_{x\\\\sim p_{\\\\!_G}}[\\\\log(1-D(x))].\\\\end{aligned}$$\\n\\nIn\\xa0@goodfellow2014generative, the maximizer of the inner problem in Eq.\\xa0([\\\\[eq:GAN_obj\\\\]](#eq:GAN_obj){reference-type=\"ref\" reference=\"eq:GAN_obj\"}) is shown to be $$\\\\begin{aligned}\\n\\\\label{eq:optimal_D}\\n    D_{p_{\\\\!_G}}^*\\n    \\\\triangleq \\\\mathop{\\\\mathrm{arg\\\\,max}}_D L(D, p_{\\\\!_G})\\n    =\\\\frac{p_{\\\\!_E}}{p_{\\\\!_E}+ p_{\\\\!_G}},\\\\end{aligned}$$ and the optimizer for Eq.\\xa0([\\\\[eq:GAN_obj\\\\]](#eq:GAN_obj){reference-type=\"ref\" reference=\"eq:GAN_obj\"}) is $\\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}}\\n     \\\\max_D L(D, p_{\\\\!_G})\\n     = \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}}L(D_{p_{\\\\!_G}}^*, p_{\\\\!_G})=p_{\\\\!_E}$. Later, @finn2016connection and @ho2016generative concurrently proposed connections between GANs and IRL. The Generative Adversarial Imitation Learning (GAIL) formulation in @ho2016generative is based on matching state-action occupancy measures, while @finn2016connection considered matching trajectory distributions. Our work is inspired by the discriminator proposed and used by @finn2016connection, $$\\\\begin{aligned}\\n    \\\\label{eq:irl_D}\\n    D_{\\\\theta}(\\\\tau)\\n    \\\\triangleq\\n    \\\\frac{\\n        p_\\\\theta(\\\\tau)\\n    }{\\n        p_\\\\theta(\\\\tau)+q(\\\\tau)\\n    },\\\\end{aligned}$$\\n\\nwhere $p_\\\\theta(\\\\tau)\\\\propto \\\\exp r_\\\\theta(\\\\tau)$ with reward approximator $r_\\\\theta$ motivated by maximum causal entropy IRL. Note that Eq.\\xa0([\\\\[eq:irl_D\\\\]](#eq:irl_D){reference-type=\"ref\" reference=\"eq:irl_D\"}) matches the form of the optimal discriminator in Eq.\\xa0([\\\\[eq:optimal_D\\\\]](#eq:optimal_D){reference-type=\"ref\" reference=\"eq:optimal_D\"}). Although @finn2016connection do not empirically support the effectiveness of their method, the Adversarial IRL approach of @fu2017learning (AIRL) successfully used a similar discriminator for state-action occupancy measure matching.\\n\\nImitation Learning without Policy Optimization\\n==============================================\\n\\nIn this section, we derive Adversarial Soft Advantage Fitting (ASAF), our novel Adversarial Imitation Learning approach. Specifically, in Section\\xa0[3.1](#sec:asaf_theory){reference-type=\"ref\" reference=\"sec:asaf_theory\"}, we present the theoretical foundations for ASAF to perform Imitation Learning on full-length trajectories. Intuitively, our method is based on the use of such *structured discriminators* -- that match the optimal discriminator form -- to fit the trajectory distribution induced by the expert policy. This approach requires being able to evaluate and sample from the learned policy and allows us to learn that policy and train the discriminator simultaneously, thus drastically simplifying the training procedure. We present in Section\\xa0[3.2](#sec:policy_class){reference-type=\"ref\" reference=\"sec:policy_class\"} parametrization options that satisfy these requirements. Finally, in Section\\xa0[3.3](#sec:asaf_practical_algorithm){reference-type=\"ref\" reference=\"sec:asaf_practical_algorithm\"}, we explain how to implement a practical algorithm that can be used for arbitrary trajectory-lengths, including the transition-wise case.\\n\\nAdversarial Soft Advantage Fitting -- Theoretical setting {#sec:asaf_theory}\\n---------------------------------------------------------\\n\\nBefore introducing our method, we derive GAN training with a structured discriminator.\\n\\n#### GAN with structured discriminator\\n\\nSuppose that we have a generator distribution $p_{\\\\!_G}$ and some arbitrary distribution $\\\\tilde{p}$ and that both can be evaluated efficiently, e.g., categorical distribution or probability density with normalizing flows\\xa0[@rezende2015variational]. We call a *structured discriminator* a function $D_{\\\\tilde{p}, p_{\\\\!_G}}:\\\\mathcal{X}\\\\rightarrow [0,1]$ of the form $D_{\\\\tilde{p}, p_{\\\\!_G}}(x)\\n    =\\n    {\\\\tilde{p}(x)}\\\\big/({\\\\tilde{p}(x)+p_{\\\\!_G}(x)})$ which matches the optimal discriminator form for\\xa0Eq.\\xa0([\\\\[eq:optimal_D\\\\]](#eq:optimal_D){reference-type=\"ref\" reference=\"eq:optimal_D\"}). Considering our new GAN objective, we get: $$\\\\begin{aligned}\\n    \\\\label{eq:structured_GAN_obj}\\n    \\\\min_{p_{\\\\!_G}}\\\\max_{\\\\tilde{p}}\\n    L(\\\\tilde{p}, p_{\\\\!_G})\\\\, , \\\\quad \\n    L(\\\\tilde{p}, p_{\\\\!_G})\\n    \\\\triangleq\\n    \\\\mathbb{E}_{x\\\\sim p_{\\\\!_E}}\\n    [\\\\log D_{\\\\tilde{p}, p_{\\\\!_G}}(x)]\\n    +\\n    \\\\mathbb{E}_{x\\\\sim p_{\\\\!_G}}\\n    [\\\\log (1-D_{\\\\tilde{p}, p_{\\\\!_G}}(x))].\\\\end{aligned}$$\\n\\nWhile the unstructured discriminator $D$ from Eq.\\xa0([\\\\[eq:GAN_obj\\\\]](#eq:GAN_obj){reference-type=\"ref\" reference=\"eq:GAN_obj\"}) learns a mapping from $x$ to a Bernoulli distribution, we now learn a mapping from $x$ to an arbitrary distribution $\\\\tilde{p}$ from which we can analytically compute $D_{\\\\tilde{p}, p_{\\\\!_G}}(x)$. One can therefore say that $D_{\\\\tilde{p}, p_{\\\\!_G}}$ is *parameterized* by $\\\\tilde{p}$. For the optimization problem of Eq.\\xa0([\\\\[eq:structured_GAN_obj\\\\]](#eq:structured_GAN_obj){reference-type=\"ref\" reference=\"eq:structured_GAN_obj\"}), we have the following optima:\\n\\n::: {#lem:optim_D_gan .lem}\\n**Lemma 1**. *The optimal discriminator parameter for any generator $p_{\\\\!_G}$ in Eq.\\xa0([\\\\[eq:structured_GAN_obj\\\\]](#eq:structured_GAN_obj){reference-type=\"ref\" reference=\"eq:structured_GAN_obj\"}) is equal to the expert\\'s distribution, $\\\\tilde{p}^* \\n    \\\\triangleq \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{p}} L(\\\\tilde{p}, p_{\\\\!_G}) \\n    = p_{\\\\!_E}$ , and the optimal discriminator parameter is also the optimal generator, i.e., $$p_{\\\\!_G}^* \\n    \\\\triangleq \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} \\\\max_{\\\\tilde{p}} L(\\\\tilde{p}, p_{\\\\!_G})\\n    = \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} L(p_{\\\\!_E}, p_{\\\\!_G})\\n    = p_{\\\\!_E}= \\\\tilde{p}^*.$$*\\n:::\\n\\nIntuitively, Lemma\\xa0[Lemma\\xa01](#lem:optim_D_gan){reference-type=\"ref\" reference=\"lem:optim_D_gan\"} shows that the optimal discriminator parameter is also the target data distribution of our optimization problem (i.e., the optimal generator). In other words, solving the inner optimization yields the solution of the outer optimization. In practice, we update $\\\\tilde{p}$ to minimize the discriminator objective and use it directly as $p_{\\\\!_G}$ to sample new data.\\n\\n#### Matching trajectory distributions with structured discriminator {#subsec:asaf_discriminator_theory}\\n\\nMotivated by the GAN with structured discriminator, we consider the trajectory distribution matching problem in IL. Here, we optimise Eq.\\xa0([\\\\[eq:structured_GAN_obj\\\\]](#eq:structured_GAN_obj){reference-type=\"ref\" reference=\"eq:structured_GAN_obj\"}) with $x=\\\\tau, \\n\\\\mathcal{X}={\\\\mathcal{T}}, \\np_{\\\\!_E}=P_{\\\\pi_{\\\\!_E}},\\np_{\\\\!_G}=P_{\\\\pi_{\\\\!_G}},$ which yields the following objective: $$\\\\begin{aligned}\\n    \\\\label{eq:TASAF_obj}\\n    \\\\min_{\\\\pi_{\\\\!_G}}\\\\max_{\\\\tilde{\\\\pi}}\\n    L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})\\n    \\\\,,\\\\quad L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})\\n    \\\\triangleq\\n    \\\\mathbb{E}_{\\\\tau\\\\sim P_{\\\\pi_{\\\\!_E}}}\\n    [\\\\log D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau)]\\n    +\\n    \\\\mathbb{E}_{\\\\tau\\\\sim P_{\\\\pi_{\\\\!_G}}}\\n    [\\\\log (1-D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau))],\\\\end{aligned}$$ with the structured discriminator: $$\\\\begin{aligned}\\n    % \\\\label{eq:trajectory_optimal_discriminator}\\n    \\\\label{eq:simplified_trajectory_optimal_discriminator}\\n    D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau)\\n    = \\\\frac{P_{\\\\tilde{\\\\pi}}(\\\\tau)}{P_{\\\\tilde{\\\\pi}}(\\\\tau)+P_{\\\\pi_{\\\\!_G}}(\\\\tau)}\\n    = \\\\frac{q_{\\\\tilde{\\\\pi}}(\\\\tau)}{q_{\\\\tilde{\\\\pi}}(\\\\tau)+q_{\\\\pi_{\\\\!_G}}(\\\\tau)}.\\\\end{aligned}$$ Here we used the fact that $P_{\\\\pi}(\\\\tau)$ decomposes into two distinct products: $q_\\\\pi(\\\\tau)\\\\triangleq\\\\prod_{t=0}^{T-1}\\\\pi(a_t|s_t)$ which depends on the stationary policy $\\\\pi$ and $\\\\xi(\\\\tau)\\\\triangleq\\\\mathcal{P}_0(s_0)\\\\prod_{t=0}^{T-1}\\\\mathcal{P}(s_{t+1}|s_t, a_t)$ which accounts for the environment dynamics. Crucially, $\\\\xi(\\\\tau)$ cancels out in the numerator and denominator leaving $\\\\tilde{\\\\pi}$ as the sole parameter of this structured discriminator. In this way, $D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau)$ can evaluate the probability of a trajectory being generated by the expert policy simply by evaluating products of stationary policy distributions $\\\\tilde{\\\\pi}$ and $\\\\pi_{\\\\!_G}$. With this form, we can get the following result:\\n\\n::: {#thm:traj_GAN .thm}\\n**Theorem 1**. *The optimal discriminator parameter for any generator policy $\\\\pi_{\\\\!_G}$ in Eq.\\xa0([\\\\[eq:TASAF_obj\\\\]](#eq:TASAF_obj){reference-type=\"ref\" reference=\"eq:TASAF_obj\"}) $\\\\tilde{\\\\pi}^*\\n\\\\triangleq\\n\\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{\\\\pi}}L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})$ is such that $q_{\\\\tilde{\\\\pi}^*}\\n=\\nq_{\\\\pi_{\\\\!_E}}$, and using generator policy $\\\\tilde{\\\\pi}^*$ minimizes $L(\\\\tilde{\\\\pi}^*, \\\\pi_{\\\\!_G})$, i.e., $$\\\\tilde{\\\\pi}^*\\n\\\\in\\n\\\\mathop{\\\\mathrm{arg\\\\,min}}_{\\\\pi_{\\\\!_G}}\\\\max_{\\\\tilde{\\\\pi}}L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})\\n=\\n\\\\mathop{\\\\mathrm{arg\\\\,min}}_{\\\\pi_{\\\\!_G}}L(\\\\tilde{\\\\pi}^*, \\\\pi_{\\\\!_G}).$$*\\n:::\\n\\nTheorem\\xa0[Theorem\\xa01](#thm:traj_GAN){reference-type=\"ref\" reference=\"thm:traj_GAN\"}\\'s benefits are similar to the ones from Lemma\\xa0[Lemma\\xa01](#lem:optim_D_gan){reference-type=\"ref\" reference=\"lem:optim_D_gan\"}: we can use a discriminator of the form of Eq.\\xa0([\\\\[eq:simplified_trajectory_optimal_discriminator\\\\]](#eq:simplified_trajectory_optimal_discriminator){reference-type=\"ref\" reference=\"eq:simplified_trajectory_optimal_discriminator\"}) to fit to the expert demonstrations a policy $\\\\tilde{\\\\pi}^*$ that simultaneously yields the optimal generator\\'s policy and produces the same trajectory distribution as the expert policy.\\n\\nA Specific Policy Class {#sec:policy_class}\\n-----------------------\\n\\nThe derivations of Section\\xa0[3.1](#sec:asaf_theory){reference-type=\"ref\" reference=\"sec:asaf_theory\"} rely on the use of a learnable policy that can both be evaluated and sampled from in order to fit the expert policy. A number of parameterization options that satisfy these conditions are available.\\n\\nFirst of all, we observe that since $\\\\pi_{\\\\!_E}$ is independent of $r$ and $\\\\pi$, we can add the entropy of the expert policy $\\\\mathcal{H}(\\\\pi_{\\\\!_E}(\\\\cdot|s))$ to the MaxEnt IRL objective of Eq.\\xa0([\\\\[eq:irl_problem\\\\]](#eq:irl_problem){reference-type=\"ref\" reference=\"eq:irl_problem\"}) without modifying the solution to the optimization problem: $$\\\\min_{r\\\\in\\\\mathcal{R}}\\n    \\\\left(\\\\max_{\\\\pi \\\\in \\\\Pi} J_{\\\\pi}[r(s,a)+\\\\mathcal{H}(\\\\pi(\\\\cdot|s))]\\n    \\\\right)\\n    - J_{\\\\pi_{\\\\!_E}}[r(s,a)+\\\\mathcal{H}(\\\\pi_{\\\\!_E}(\\\\cdot|s))]$$ The max over policies implies that when optimising $r$, $\\\\pi$ has already been made optimal with respect to the causal entropy augmented reward function $r\\'(s,a| \\\\pi) = r(s,a) + \\\\mathcal{H}(\\\\pi(\\\\cdot|s))$ and therefore it must be of the form presented in Eq.\\xa0([\\\\[eq:max_ent_policy\\\\]](#eq:max_ent_policy){reference-type=\"ref\" reference=\"eq:max_ent_policy\"}). Moreover, since $\\\\pi$ is optimal w.r.t. $r\\'$ the difference in performance $J_{\\\\pi}[r\\'(s,a| \\\\pi)]-J_{\\\\pi_{\\\\!_E}}[r\\'(s,a|\\\\pi_{\\\\!_E})]$ is always non-negative and its minimum of 0 is only reached when $\\\\pi_{\\\\!_E}$ is also optimal w.r.t. $r\\'$, in which case $\\\\pi_{\\\\!_E}$ must also be of the form of Eq.\\xa0([\\\\[eq:max_ent_policy\\\\]](#eq:max_ent_policy){reference-type=\"ref\" reference=\"eq:max_ent_policy\"}).\\n\\nWith discrete action spaces we propose to parameterize the MaxEnt policy defined in Eq.\\xa0([\\\\[eq:max_ent_policy\\\\]](#eq:max_ent_policy){reference-type=\"ref\" reference=\"eq:max_ent_policy\"}) with the following categorical distribution $\\\\tilde{\\\\pi}(a|s) = \\\\exp\\\\left(Q_\\\\theta(s,a) - \\\\log\\\\sum_{a\\'}\\\\exp Q_\\\\theta(s,a\\') \\\\right)$, where $Q_\\\\theta$ is a model parameterized by $\\\\theta$ that approximates $\\\\frac{1}{\\\\alpha} Q^*_{\\\\text{soft}}$.\\n\\nWith continuous action spaces, the soft value function involves an intractable integral over the action domain. Therefore, we approximate the MaxEnt distribution with a Normal distribution with diagonal covariance matrix like it is commonly done in the literature [@haarnoja2018soft; @nachum2018trustpcl]. By parameterizing the mean and variance we get a learnable density function that can be easily evaluated and sampled from.\\n\\nAdversarial Soft Advantage Fitting (ASAF) -- practical algorithm {#sec:asaf_practical_algorithm}\\n----------------------------------------------------------------\\n\\nSection\\xa0[3.1.0.2](#subsec:asaf_discriminator_theory){reference-type=\"ref\" reference=\"subsec:asaf_discriminator_theory\"} shows that assuming $\\\\tilde{\\\\pi}$ can be evaluated and sampled from, we can use the structured discriminator of Eq.\\xa0([\\\\[eq:simplified_trajectory_optimal_discriminator\\\\]](#eq:simplified_trajectory_optimal_discriminator){reference-type=\"ref\" reference=\"eq:simplified_trajectory_optimal_discriminator\"}) to learn a policy $\\\\tilde{\\\\pi}$ that matches the expert\\'s trajectory distribution. Section\\xa0[3.2](#sec:policy_class){reference-type=\"ref\" reference=\"sec:policy_class\"} proposes parameterizations for discrete and continuous action spaces that satisfy those assumptions.\\n\\nIn practice, as with GANs\\xa0[@goodfellow2014generative], we do not train the discriminator to convergence as gradient-based optimisation cannot be expected to find the global optimum of non-convex problems. Instead, Adversarial Soft Advantage Fitting (ASAF) alternates between two simple steps: (1) training $D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}$ by minimizing the binary cross-entropy loss, $$\\\\label{eq:BCE_loss}\\n\\\\begin{aligned}\\n    &\\\\mathcal{L}_{BCE}(\\\\mathcal{D}_E, \\\\mathcal{D}_G, \\\\tilde{\\\\pi}) \\\\approx -\\\\frac{1}{n_E} \\\\sum_{i=1}^{n_E} \\\\log D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau_i^{(E)}) - \\\\frac{1}{n_G} \\\\sum_{i=1}^{n_G} \\\\log \\\\left(1 - D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau_i^{(G)})\\\\right) \\\\\\\\\\n    &\\\\text{where  }\\\\quad \\\\tau_i^{(E)}\\\\sim \\\\mathcal{D}_E \\\\text{ ,  } \\\\tau_i^{(G)}\\\\sim \\\\mathcal{D}_G \\\\, \\\\text{ and  } \\\\, D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau) = \\\\frac{\\\\prod_{t=0}^{T-1}\\\\tilde{\\\\pi}(a_t|s_t)}{\\\\prod_{t=0}^{T-1}\\\\tilde{\\\\pi}(a_t|s_t)+\\\\prod_{t=0}^{T-1}\\\\pi_{\\\\!_G}(a_t|s_t)}\\n\\\\end{aligned}$$ with minibatch sizes $n_E = n_G$, and (2) updating the generator\\'s policy as $\\\\pi_{\\\\!_G}\\\\leftarrow \\\\tilde{\\\\pi}$ to minimize Eq.\\xa0([\\\\[eq:TASAF_obj\\\\]](#eq:TASAF_obj){reference-type=\"ref\" reference=\"eq:TASAF_obj\"}) (see Algorithm\\xa0[\\\\[alg:asaf\\\\]](#alg:asaf){reference-type=\"ref\" reference=\"alg:asaf\"}).\\n\\nWe derived ASAF considering full trajectories, yet it might be preferable in practice to split full trajectories into smaller chunks. This is particularly true in environments where trajectory length varies a lot or tends to infinity.\\n\\nTo investigate whether the practical benefits of using partial trajectories hurt ASAF\\'s performance, we also consider a variation, ASAF-*w*, where we treat trajectory-windows of size *w* as if they were full trajectories. Note that considering windows as full trajectories results in approximating that the initial state of these sub-trajectories have equal probability under the expert\\'s and the generator\\'s policy (this is easily seen when deriving Eq.\\xa0([\\\\[eq:simplified_trajectory_optimal_discriminator\\\\]](#eq:simplified_trajectory_optimal_discriminator){reference-type=\"ref\" reference=\"eq:simplified_trajectory_optimal_discriminator\"})).\\n\\n  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  In the limit, ASAF-1 (window-size of 1) becomes a transition-wise algorithm which can be desirable if one wants to collect rollouts asynchronously or has only access to unsequential expert data. While ASAF-1 may work well in practice it essentially assumes that the expert\\'s and the generator\\'s policies have the same state occupancy measure, which is incorrect until actually recovering the true expert policy.   expert trajectories $\\\\mathcal{D}_E = \\\\{\\\\tau_i\\\\}_{i=1}^{N_E}$ Randomly initialize $\\\\tilde{\\\\pi}$ and set $\\\\pi_{\\\\!_G}\\\\leftarrow \\\\tilde{\\\\pi}$ Collect trajectories $\\\\mathcal{D}_G = \\\\{\\\\tau_i\\\\}_{i=1}^{N_G}$ using $\\\\pi_{\\\\!_G}$ Update $\\\\tilde{\\\\pi}$ by minimizing Eq.\\xa0([\\\\[eq:BCE_loss\\\\]](#eq:BCE_loss){reference-type=\"ref\" reference=\"eq:BCE_loss\"}) Set $\\\\pi_{\\\\!_G}\\\\leftarrow \\\\tilde{\\\\pi}$\\n  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nFinally, to offer a complete family of algorithms based on the structured discriminator approach, we show in Appendix\\xa0[8](#app:ASQF){reference-type=\"ref\" reference=\"app:ASQF\"} that this assumption is not mandatory and derive a transition-wise algorithm based on Soft Q-function Fitting (rather than soft advantages) that also gets rid of the RL loop. We call this algorithm ASQF. While theoretically sound, we found that in practice, ASQF is outperformed by ASAF-1 in more complex environments (see Section\\xa0[5.1](#sec:results_and_discussion){reference-type=\"ref\" reference=\"sec:results_and_discussion\"}).\\n\\nRelated works\\n=============\\n\\n@ziebart2008maximum first proposed MaxEnt IRL, the foundation of modern IL. @ziebart2010modeling further elaborated MaxEnt IRL as well as deriving the optimal form of the MaxEnt policy at the core of our methods. @finn2016connection proposed a GAN formulation to IRL that leveraged the energy based models of @ziebart2010modeling. @finn2016guided\\'s implementation of this method, however, relied on processing full trajectories with Linear Quadratic Regulator and on optimizing with guided policy search, to manage the high variance of trajectory costs. To retrieve robust rewards, @fu2017learning proposed a straightforward transposition of [@finn2016connection] to state-action transitions. In doing so, they had to however do away with a GAN objective during policy optimization, consequently minimizing the Kullback--Leibler divergence from the expert occupancy measure to the policy occupancy measure (instead of the Jensen-Shannon divergence)\\xa0[@ghasemipour2019divergence].\\n\\nLater works [@sasaki2018sample; @Kostrikov2020Imitation] move away from the Generative Adversarial formulation. To do so, @sasaki2018sample directly express the expectation of the Jensen-Shannon divergence between the occupancy measures in term of the agent\\'s Q-function, which can then be used to optimize the agent\\'s policy with off-policy Actor-Critic [@degris2012off]. Similarly, @Kostrikov2020Imitation use Dual Stationary Distribution Correction Estimation [@nachum2019dualdice] to approximate the Q-function on the expert\\'s demonstrations before optimizing the agent\\'s policy under the initial state distribution using the reparametrization trick [@haarnoja2018soft]. While [@sasaki2018sample; @Kostrikov2020Imitation] are related to our methods in their interests in learning directly the value function, they differ in their goal and thus in the resulting algorithmic complexity. Indeed, they aim at improving the sample efficiency in terms of environment interaction and therefore move away from the algorithmically simple Generative Adversarial formulation towards more complicated divergence minimization methods. In doing so, they further complicate the Imitation Learning methods while still requiring to explicitly learn a policy. Yet, simply using the Generative Adversarial formulation with an Experience Replay Buffer can significantly improve the sample efficiency [@kostrikov2018discriminatoractorcritic]. For these reasons, and since our aim is to propose efficient yet simple methods, we focus on the Generative Adversarial formulation.\\n\\nWhile @reddy2019sqil share our interest for simpler IL methods, they pursue an opposite approach to ours. They propose to eliminate the reward learning steps of IRL by simply hard-coding a reward of 1 for expert\\'s transitions and of 0 for agent\\'s transitions. They then use Soft Q-learning [@haarnoja2017reinforcement] to learn a value function by sampling transitions in equal proportion from the expert\\'s and agent\\'s buffers. Unfortunately, once the learner accurately mimics the expert, it collects expert-like transitions that are labeled with a reward of 0 since they are generated and not coming from the demonstrations. This effectively causes the reward of expert-like behavior to decay as the agent improves and can severely destabilize learning to a point where early-stopping becomes required [@reddy2019sqil].\\n\\nOur work builds on [@finn2016connection], yet its novelty is to explicitly express the probability of a trajectory in terms of the policy in order to directly learn this latter when training the discriminator. In contrast, [@fu2017learning] considers a transition-wise discriminator with un-normalized probabilities which makes it closer to ASQF (Appendix\\xa0[8](#app:ASQF){reference-type=\"ref\" reference=\"app:ASQF\"}) than to ASAF-1. Additionally, AIRL [@fu2017learning] minimizes the Kullback-Leiber Divergence [@ghasemipour2019divergence] between occupancy measures whereas ASAF minimizes the Jensen-Shanon Divergence between trajectory distributions.\\n\\nFinally, Behavioral Cloning uses the loss function from supervised learning (classification or regression) to match expert\\'s actions given expert\\'s states and suffers from compounding error due to co-variate shift [@ross2010efficient] since its data is limited to the demonstrated state-action pairs without environment interaction. Contrarily, ASAF-1 uses the binary cross entropy loss in Eq.\\xa0([\\\\[eq:BCE_loss\\\\]](#eq:BCE_loss){reference-type=\"ref\" reference=\"eq:BCE_loss\"}) and does not suffer from compounding error as it learns on both generated and expert\\'s trajectories.\\n\\nResults and discussion\\n======================\\n\\nWe evaluate our methods on a variety of discrete and continuous control tasks. Our results show that, in addition to drastically simplifying the adversarial IRL framework, our methods perform on par or better than previous approaches on all but one environment. When trajectory length is really long or drastically varies across episodes (see MuJoCo experiments Section\\xa0[5.3](#sec:mujoco_results){reference-type=\"ref\" reference=\"sec:mujoco_results\"}), we find that using sub-trajectories with fixed window-size (ASAF-*w* or ASAF-1) significantly outperforms its full trajectory counterpart ASAF.\\n\\nExperimental setup {#sec:results_and_discussion}\\n------------------\\n\\nWe compare our algorithms ASAF, ASAF-*w* and ASAF-1 against GAIL\\xa0[@ho2016generative], the predominant Adversarial Imitation Learning algorithm in the litterature, and AIRL\\xa0[@fu2017learning], one of its variations that also leverages the access to the generator\\'s policy distribution. Additionally, we compare against SQIL\\xa0[@reddy2019sqil], a recent Reinforcement Learning-only approach to Imitation Learning that proved successful on high-dimensional tasks. Our implementations of GAIL and AIRL use PPO [@schulman2017proximal] instead of TRPO [@schulman2015trust] as it has been shown to improve performance [@kostrikov2018discriminatoractorcritic]. Finally, to be consistent with [@ho2016generative], we do not use causal entropy regularization.\\n\\nFor all tasks except MuJoCo, we selected the best performing hyperparameters through a random search of equal budget for each algorithm-environment pair (see Appendix\\xa0[10](#app:hyperparameters){reference-type=\"ref\" reference=\"app:hyperparameters\"}) and the best configuration is retrained on ten random seeds. For the MuJoCo experiments, GAIL required extensive tuning (through random searches) of both its RL and IRL components to achieve satisfactory performances. Our methods, ASAF-*w* and ASAF-1, on the other hand showed much more stable and robust to hyperparameterization, which is likely due to their simplicity. SQIL used the same SAC[@haarnoja2018soft] implementation and hyperparameters that were used to generate the expert demonstrations.\\n\\nFinally for each task, all algorithms use the same neural network architectures for their policy and/or discriminator (see full description in Appendix\\xa0[10](#app:hyperparameters){reference-type=\"ref\" reference=\"app:hyperparameters\"}). Expert demonstrations are either generated by hand (mountaincar), using open-source bots (Pommerman) or from our implementations of SAC and PPO (all remaining). More details are given in Appendix\\xa0[11](#app:environments){reference-type=\"ref\" reference=\"app:environments\"}.\\n\\nExperiments on classic control and Box2D tasks (discrete and continuous) {#sec:classic_control_results}\\n------------------------------------------------------------------------\\n\\nFigure\\xa0[1](#fig:results_toy){reference-type=\"ref\" reference=\"fig:results_toy\"} shows that ASAF and its approximate variations ASAF-1 and ASAF-*w* quickly converge to expert\\'s performance (here *w* was tuned to values between 32 to 200, see Appendix\\xa0[10](#app:hyperparameters){reference-type=\"ref\" reference=\"app:hyperparameters\"} for selected window-sizes). This indicates that the practical benefits of using shorter trajectories or even just transitions does not hinder performance on these simple tasks. Note that for Box2D and classic control environments, we retrain the best configuration of each algorithm for twice as long than was done in the hyperparameter search, which allows to uncover unstable learning behaviors. Figure\\xa0[1](#fig:results_toy){reference-type=\"ref\" reference=\"fig:results_toy\"} shows that our methods display much more stable learning: their performance rises until they match the expert\\'s and does not decrease once it is reached. This is a highly desirable property for an Imitation Learning algorithm since in practice one does not have access to a reward function and thus cannot monitor the performance of the learning algorithm to trigger early-stopping. The baselines on the other hand experience occasional performance drops. For GAIL and AIRL, this is likely due to the concurrent RL and IRL loops, whereas for SQIL, it has been noted that an effective reward decay can occur when accurately mimicking the expert [@reddy2019sqil]. This instability is particularly severe in the continuous control case. In practice, all three baselines use early stopping to avoid performance decay [@reddy2019sqil].\\n\\n![Results on classic control and Box2D tasks for 10 expert demonstrations. First row contains discrete actions environments, second row corresponds to continuous control.](figures/merged_toy_results.pdf){#fig:results_toy width=\"1.\\\\\\\\textwidth\"}\\n\\nExperiments on MuJoCo (continuous control) {#sec:mujoco_results}\\n------------------------------------------\\n\\nTo scale up our evaluations in continuous control we use the popular MuJoCo benchmarks. In this domain, the trajectory length is either fixed at a large value (1000 steps on HalfCheetah) or varies a lot across episodes due to termination when the character falls down (Hopper, Walker2d and Ant). Figure\\xa0[2](#fig:mujoco_results){reference-type=\"ref\" reference=\"fig:mujoco_results\"} shows that these trajectory characteristics hinder ASAF\\'s learning as ASAF requires collecting multiple episodes for every update, while ASAF-1 and ASAF-*w* perform well and are more sample-efficient than ASAF in these scenarios. We focus on GAIL since [@fu2017learning] claim that AIRL performs on par with it on MuJoCo environments. In Figure\\xa0[7](#fig:gail_gradient_penalty){reference-type=\"ref\" reference=\"fig:gail_gradient_penalty\"} in Appendix\\xa0[9](#app:additional_experiments){reference-type=\"ref\" reference=\"app:additional_experiments\"} we evaluate GAIL both with and without gradient penalty (GP) on discriminator updates\\xa0[@gulrajani2017improved; @kostrikov2018discriminatoractorcritic] and while GAIL was originally proposed without GP\\xa0[@ho2016generative], we empirically found that GP prevents the discriminator to overfit and enables RL to exploit dense rewards, which highly improves its sample efficiency. Despite these ameliorations, GAIL proved to be quite inconsistent across environments despite substantial efforts on hyperparameter tuning. On the other hand, ASAF-1 performs well across all environments. Finally, we see that SQIL\\'s instability is exacerbated on MuJoCo.\\n\\n![Results on MuJoCo tasks for 25 expert demonstrations. ](figures/benchmark_learning_mujoco_NeurIPSrebuttal.pdf){#fig:mujoco_results width=\"1.\\\\\\\\textwidth\"}\\n\\nExperiments on Pommerman (discrete control) {#sec:pommerman_results}\\n-------------------------------------------\\n\\nFinally, to scale up our evaluations in discrete control environments, we consider the domain of Pommerman\\xa0[@resnick2018pommerman], a challenging and very dynamic discrete control environment that uses rich and high-dimensional observation spaces (see Appendix\\xa0[11](#app:environments){reference-type=\"ref\" reference=\"app:environments\"}). We perform evaluations of all of our methods and baselines on a 1 vs 1 task where a learning agent plays against a random agent, the opponent. The goal for the learning agent is to navigate to the opponent and eliminate it using expert demonstrations provided by the champion algorithm of the FFA 2018 competition\\xa0[@zhou2018hybrid]. We removed the ability of the opponent to lay bombs so that it doesn\\'t accidentally eliminate itself. Since it can still move around, it is however surprisingly tricky to eliminate: the expert has to navigate across the whole map, lay a bomb next to the opponent and retreat to avoid eliminating itself. This entire routine has then to be repeated several times until finally succeeding since the opponent will often avoid the hit by chance. We refer to this task as *Pommerman Random-Tag*. Note that since we measure success of the imitation task with the win-tie-lose outcome (sparse performance metric), a learning agent has to truly reproduce the expert behavior until the very end of trajectories to achieve higher scores. Figure\\xa0[5](#fig:pommerman_results_randomTag){reference-type=\"ref\" reference=\"fig:pommerman_results_randomTag\"} shows that all three variations of ASAF as well as Behavioral Cloning (BC) outperform the baselines.\\n\\n  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n   ![Results on Pommerman Random-Tag: (Left) Snapshot of the environment. (Center) Learning measured as evaluation return over episodes for 150 expert trajectories (Right) Average return on last 20% of training for decreasing number of expert trajectories \\\\[300, 150, 75, 15, 5, 1\\\\].](figures/pommerman_screenshot.png){#fig:pommerman_results_randomTag width=\".2\\\\\\\\textwidth\"}   ![Results on Pommerman Random-Tag: (Left) Snapshot of the environment. (Center) Learning measured as evaluation return over episodes for 150 expert trajectories (Right) Average return on last 20% of training for decreasing number of expert trajectories \\\\[300, 150, 75, 15, 5, 1\\\\].](figures/benchmark_learning_pommerman_randomTag.pdf){#fig:pommerman_results_randomTag width=\".45\\\\\\\\textwidth\"}   ![Results on Pommerman Random-Tag: (Left) Snapshot of the environment. (Center) Learning measured as evaluation return over episodes for 150 expert trajectories (Right) Average return on last 20% of training for decreasing number of expert trajectories \\\\[300, 150, 75, 15, 5, 1\\\\].](figures/fig_pommerman_nDemos.pdf){#fig:pommerman_results_randomTag width=\".26\\\\\\\\textwidth\"}\\n  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nConclusion\\n==========\\n\\nWe propose an important simplification to the Adversarial Imitation Learning framework by removing the Reinforcement Learning optimisation loop altogether. We show that, by using a particular form for the discriminator, our method recovers a policy that matches the expert\\'s trajectory distribution. We evaluate our approach against prior works on many different benchmarking tasks and show that our method (ASAF) compares favorably to the predominant Imitation Learning algorithms. The approximate versions, ASAF-*w* and ASAF-1, that use sub-trajectories yield a flexible algorithms that work well both on short and long time horizons. Finally, our approach still involves a reward learning module through its discriminator, and it would be interesting in future work to explore how ASAF can be used to learn robust rewards, along the lines of\\xa0@fu2017learning.\\n\\nBroader Impact {#broader-impact .unnumbered}\\n==============\\n\\nOur contributions are mainly theoretical and aim at simplifying current Imitation Learning methods. We do not propose new applications nor use sensitive data or simulator. Yet our method can ease and promote the use, design and development of Imitation Learning algorithms and may eventually lead to applications outside of simple and controlled simulators. We do not pretend to discuss the ethical implications of the general use of autonomous agents but we rather try to investigate what are some of the differences in using Imitation Learning rather than reward oriented methods in the design of such agents.\\n\\nUsing only a scalar reward function to specify the desired behavior of an autonomous agent is a challenging task as one must weight different desiderata and account for unsuspected behaviors and situations. Indeed, it is well known in practice that Reinforcement Learning agents tend to find bizarre ways of exploiting the reward signal without solving the desired task. The fact that it is difficult to specify and control the behavior or an RL agents is a major flaw that prevent current methods to be applied to risk sensitive situations. On the other hand, Imitation Learning proposes a more natural way of specifying nuanced preferences by demonstrating desirable ways of solving a task. Yet, IL also has its drawbacks. First of all one needs to be able to demonstrate the desired behavior and current methods tend to be only as good as the demonstrator. Second, it is a challenging problem to ensure that the agent will be able to adapt to new situations that do not resemble the demonstrations. For these reasons, it is clear for us that additional safeguards are required in order to apply Imitation Learning (and Reinforcement Learning) methods to any application that could effectively have a real world impact.\\n\\nWe thank Eloi Alonso, Olivier Delalleau, Flix G. Harvey, Maxim Peter and the entire research team at Ubisoft Montreal\\'s La Forge R&D laboratory. Their feedback and comments contributed significantly to this work. Christopher Pal and Derek Nowrouzezahrai acknowledge funding from the Fonds de Recherche Nature et Technologies (FRQNT), Ubisoft Montreal and Mitacs\\' Accelerate Program in support of our work, as well as Compute Canada for providing computing resources. Derek and Paul also acknowledge support from the NSERC Industrial Research Chair program.\\n\\nAppendix {#appendix .unnumbered}\\n========\\n\\nProofs\\n======\\n\\nProof of Lemma\\xa0[Lemma\\xa01](#lem:optim_D_gan){reference-type=\"ref\" reference=\"lem:optim_D_gan\"} {#app:proof_of_optim_D_gan}\\n--------------------------------------------------------------------------------------------\\n\\n::: {.proof}\\n*Proof.* Lemma\\xa0[Lemma\\xa01](#lem:optim_D_gan){reference-type=\"ref\" reference=\"lem:optim_D_gan\"} states that given $L(\\\\tilde{p}, p_{\\\\!_G})$ defined in\\xa0Eq.\\xa0([\\\\[eq:structured_GAN_obj\\\\]](#eq:structured_GAN_obj){reference-type=\"ref\" reference=\"eq:structured_GAN_obj\"}):\\n\\n1.  $\\\\displaystyle \\\\tilde{p}^* \\\\triangleq \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{p}} L(\\\\tilde{p}, p_{\\\\!_G}) = p_{\\\\!_E}$\\n\\n2.  $\\\\displaystyle \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} L(p_{\\\\!_E}, p_{\\\\!_G}) = p_{\\\\!_E}$\\n\\nStarting with (a), we have: $$\\\\begin{aligned}\\n    \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{p}} L(\\\\tilde{p}, p_{\\\\!_G})\\n    &= \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{p}}\\n    \\\\sum_{x_i} p_{\\\\!_E}(x_i)\\\\log D_{\\\\tilde{p}, p_{\\\\!_G}}(x_i) + p_{\\\\!_G}(x_i)\\\\log (1 - D_{\\\\tilde{p}, p_{\\\\!_G}}(x_i))\\\\\\\\\\n    &\\\\triangleq \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{p}} \\\\sum_{x_i} L_i\\\\end{aligned}$$ Assuming infinite discriminator\\'s capacity, $L_i$ can be made independent for all $x_i \\\\in \\\\mathcal{X}$ and we can construct our optimal discriminator $D_{\\\\tilde{p}, p_{\\\\!_G}}^*$ as a look-up table $D_{\\\\tilde{p}, p_{\\\\!_G}}^*:\\\\mathcal{X} \\\\rightarrow \\\\, ]0,1[ \\\\, ; \\\\, x_i \\\\mapsto D^*_i$ with $D^*_i$ the optimal discriminator for each $x_i$ defined as: $$D^*_i = \\\\mathop{\\\\mathrm{arg\\\\,max}}_{D_i}L_i = \\\\mathop{\\\\mathrm{arg\\\\,max}}_{D_i} p_{\\\\!_{E}, i}\\n    \\\\log D_i + p_{\\\\!_{G}, i}\\\\log (1 - D_i),$$ with $p_{\\\\!_{G}, i}\\\\triangleq p_{\\\\!_G}(x_i)$, $p_{\\\\!_{E}, i}\\\\triangleq p_{\\\\!_E}(x_i)$ and $D_i\\\\triangleq D(x_i)$.\\n\\nRecall that $D_i \\\\in \\\\, ]0,1[$ and that $p_{\\\\!_{G}, i}\\\\in \\\\, ]0,1[$. Therefore the function $\\\\tilde{p}_i \\\\mapsto D_i = \\\\dfrac{\\\\tilde{p}_i}{\\\\tilde{p}_i + p_{\\\\!_{G}, i}}$ is defined for $\\\\tilde{p}_i \\\\in ]0, +\\\\infty[$. Since it is strictly monotonic over that domain we have that: $$\\\\begin{aligned}\\nD_i^*=\\\\mathop{\\\\mathrm{arg\\\\,max}}_{D_i} L_i \\\\, \\\\Leftrightarrow \\\\, \\\\tilde{p}^*_i =\\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{p}_i} L_i\\\\end{aligned}$$ Taking the derivative and setting to zero, we get: $$\\\\begin{aligned}\\n    \\\\left.\\\\frac{d L_i}{d \\\\tilde{p}_i}\\\\right|_{\\\\tilde{p}_i} = 0 \\\\, \\\\Leftrightarrow& \\\\,\\n    \\\\tilde{p}_i = p_{\\\\!_{E}, i}\\\\end{aligned}$$ The second derivative test confirms that we have a maximum, i.e. $\\\\left.\\\\dfrac{d^2 L_i}{d \\\\tilde{p}_i^2}\\\\right|_{\\\\tilde{p}_i^*} < 0$. The values of $L_i$ at the boundaries of the domain of definition of $\\\\tilde{p}_i$ tend to $-\\\\infty$, therefore $L_i(\\\\tilde{p}_i^*=p_{\\\\!_{E}, i})$ is the global maximum of $L_i$ w.r.t. $\\\\tilde{p}_i$. Finally, the optimal global discriminator is given by: $$\\\\label{eq:D_optim_GAN_proof}\\n    D_{\\\\tilde{p}, p_{\\\\!_G}}^*(x) = \\\\frac{p_{\\\\!_E}(x)}{p_{\\\\!_E}(x)+p_{\\\\!_G}(x)} \\\\quad \\\\forall x \\\\in \\\\mathcal{X}$$ This concludes the proof for (a).\\n\\nThe proof for (b) can be found in the work of @goodfellow2014generative. We reproduce it here for completion. Since from (a) we know that $\\\\tilde{p}^*(x) = p_{\\\\!_E}(x) \\\\, \\\\forall x \\\\in \\\\mathcal{X}$, we can write the GAN objective for the optimal discriminator as: $$\\\\begin{aligned}\\n    \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} L(\\\\tilde{p}^*, p_{\\\\!_G})\\n    &= \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} L(p_{\\\\!_E}, p_{\\\\!_G}) \\\\\\\\\\n    &= \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} \\\\mathbb{E}_{x\\\\sim p_{\\\\!_E}}\\\\left[\\\\log \\\\frac{p_{\\\\!_E}(x)}{p_{\\\\!_E}(x)+p_{\\\\!_G}(x)}\\\\right] + \\\\mathbb{E}_{x\\\\sim p_{\\\\!_G}}\\\\left[\\\\log \\\\frac{p_{\\\\!_G}(x)}{p_{\\\\!_E}(x)+p_{\\\\!_G}(x)}\\\\right]\\\\label{eq:L_Dopt}\\\\end{aligned}$$ Note that: $$\\\\label{eq:minuslog4}\\n    \\\\log4 = \\\\mathbb{E}_{x\\\\sim p_{\\\\!_E}}\\\\left[\\\\log 2\\\\right] + \\\\mathbb{E}_{x\\\\sim p_{\\\\!_G}}\\\\left[\\\\log 2\\\\right]$$ Adding Eq.\\xa0([\\\\[eq:minuslog4\\\\]](#eq:minuslog4){reference-type=\"ref\" reference=\"eq:minuslog4\"}) to Eq.\\xa0([\\\\[eq:L_Dopt\\\\]](#eq:L_Dopt){reference-type=\"ref\" reference=\"eq:L_Dopt\"}) and subtracting $\\\\log4$ on both sides: $$\\\\begin{aligned}\\n    \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} L(p_{\\\\!_E}, p_{\\\\!_G}) &= -\\\\log4 + \\\\mathbb{E}_{x\\\\sim p_{\\\\!_E}}\\\\left[\\\\log \\\\frac{2p_{\\\\!_E}(x)}{p_{\\\\!_E}(x)+p_{\\\\!_G}(x)}\\\\right] + \\\\mathbb{E}_{x \\\\sim p_{\\\\!_G}}\\\\left[\\\\log \\\\frac{2p_{\\\\!_G}(x)}{p_{\\\\!_E}(x)+p_{\\\\!_G}(x)}\\\\right]\\\\\\\\\\n    &= -\\\\log4 + D_{\\\\mathrm{KL}}\\\\left(p_{\\\\!_E}\\\\left\\\\|\\\\frac{p_{\\\\!_E}+ p_{\\\\!_G}}{2}\\\\right.\\\\right)+ D_{\\\\mathrm{KL}}\\\\left(p_{\\\\!_E}\\\\left\\\\|\\\\frac{p_{\\\\!_E}+ p_{\\\\!_G}}{2}\\\\right.\\\\right)\\\\\\\\\\n    &= -\\\\log4 + 2D_{\\\\mathrm{JS}}\\\\left(p_{\\\\!_E}\\\\left\\\\|p_{\\\\!_G}\\\\right.\\\\right)\\\\end{aligned}$$ Where $D_{\\\\mathrm{KL}}$ and $D_{\\\\mathrm{JS}}$ are respectively the Kullback-Leibler and the Jensen-Shannon divergences. Since the Jensen-Shannon divergence between two distributions is always non-negative and zero if and only if the two distributions are equal, we have that $\\\\displaystyle \\\\mathop{\\\\mathrm{arg\\\\,min}}_{p_{\\\\!_G}} L(p_{\\\\!_E}, p_{\\\\!_G}) = p_{\\\\!_E}$.\\n\\nThis concludes the proof for (b).\\xa0\\n:::\\n\\nProof of Theorem\\xa0[Theorem\\xa01](#thm:traj_GAN){reference-type=\"ref\" reference=\"thm:traj_GAN\"} {#app:proof_of_thm_traj_GAN}\\n------------------------------------------------------------------------------------------\\n\\n::: {.proof}\\n*Proof.* Theorem\\xa0[Theorem\\xa01](#thm:traj_GAN){reference-type=\"ref\" reference=\"thm:traj_GAN\"} states that given $L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})$ defined in\\xa0Eq.\\xa0([\\\\[eq:TASAF_obj\\\\]](#eq:TASAF_obj){reference-type=\"ref\" reference=\"eq:TASAF_obj\"}):\\n\\n1.  $\\\\displaystyle \\\\tilde{\\\\pi}^* \\\\triangleq \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{\\\\pi}} L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}) \\\\text{ satisfies } q_{\\\\tilde{\\\\pi}^*} = q_{\\\\pi_{\\\\!_E}}$\\n\\n2.  $\\\\displaystyle \\\\pi_{\\\\!_G}^* = \\\\tilde{\\\\pi}^* \\\\in \\\\mathop{\\\\mathrm{arg\\\\,min}}_{\\\\pi_{\\\\!_G}}L(\\\\tilde{\\\\pi}^*, \\\\pi_{\\\\!_G})$\\n\\nThe proof of (a) is very similar to the one from Lemma\\xa0[Lemma\\xa01](#lem:optim_D_gan){reference-type=\"ref\" reference=\"lem:optim_D_gan\"}. Starting from Eq.\\xa0([\\\\[eq:TASAF_obj\\\\]](#eq:TASAF_obj){reference-type=\"ref\" reference=\"eq:TASAF_obj\"}) we have: $$\\\\begin{aligned}\\n    \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{\\\\pi}}\\n    L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})\\n    &=\\n    \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{\\\\pi}} \\\\sum_{\\\\tau_i} P_{\\\\pi_{\\\\!_E}}(\\\\tau_i) \\\\log D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau_i)\\n    +\\n    P_{\\\\pi_{\\\\!_G}}(\\\\tau_i) \\\\log (1-D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau_i)) \\\\\\\\\\n    &=\\n    \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{\\\\pi}} \\\\sum_{\\\\tau_i} \\\\xi(\\\\tau_i) \\\\left( q_{\\\\pi_{\\\\!_E}}(\\\\tau_i) \\\\log D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau_i)\\n    +\\n    q_{\\\\pi_{\\\\!_G}}(\\\\tau_i) \\\\log (1-D_{\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G}}(\\\\tau_i))\\n    \\\\right) \\\\\\\\\\n    &=\\n    \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{\\\\pi}} \\\\sum_{\\\\tau_i} L_i\\\\end{aligned}$$ Like for Lemma\\xa0[Lemma\\xa01](#lem:optim_D_gan){reference-type=\"ref\" reference=\"lem:optim_D_gan\"}, we can optimise for each $L_i$ individually. When doing so, $\\\\xi(\\\\tau_i)$ can be omitted as it is constant w.r.t $\\\\tilde{\\\\pi}$. The rest of the proof is identical to the one of but Lemma\\xa0[Lemma\\xa01](#lem:optim_D_gan){reference-type=\"ref\" reference=\"lem:optim_D_gan\"} with $p_{\\\\!_E}= q_{\\\\pi_{\\\\!_E}}$ and $p_{\\\\!_G}= q_{\\\\pi_{\\\\!_G}}$. It follows that the max of $L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})$ is reached for $q_{\\\\tilde{\\\\pi}}^* = q_{\\\\pi_{\\\\!_E}}$. From that we obtain that the policy $\\\\tilde{\\\\pi}^*$ that makes the discriminator $D_{\\\\tilde{\\\\pi}^*, \\\\pi_{\\\\!_G}}$ optimal w.r.t $L(\\\\tilde{\\\\pi}, \\\\pi_{\\\\!_G})$ is such that $q_{\\\\tilde{\\\\pi}^*} = q_{\\\\tilde{\\\\pi}}^* = q_{\\\\pi_{\\\\!_E}}$ i.e. $\\\\prod_{t=0}^{T-1}\\\\tilde{\\\\pi}^*(a_t|s_t) = \\\\prod_{t=0}^{T-1}\\\\pi_{\\\\!_E}(a_t|s_t) \\\\, \\\\forall \\\\, \\\\tau$.\\n\\nThe proof for (b) stems from the observation that choosing $\\\\pi_{\\\\!_G}= \\\\tilde{\\\\pi}^*$ (the policy recovered by the optimal discriminator $D_{\\\\tilde{\\\\pi}^*, \\\\pi_{\\\\!_G}}$) minimizes $L(\\\\tilde{\\\\pi}^*, \\\\pi_{\\\\!_G})$: $$\\\\begin{aligned}\\n    \\\\pi_{\\\\!_G}(a|s) = \\\\tilde{\\\\pi}^*(a|s) \\\\,\\n    \\\\, \\\\forall \\\\, (s,a) \\\\in {\\\\mathcal{S}}\\\\times{\\\\mathcal{A}}\\\\quad\\n    &\\\\Rightarrow \\\\quad \\\\prod_{t=0}^{T-1}\\\\pi_{\\\\!_G}(a_t|s_t) = \\\\prod_{t=0}^{T-1}\\\\tilde{\\\\pi}^*(a_t|s_t) \\\\,\\\\, \\\\forall \\\\tau \\\\in {\\\\mathcal{T}}\\\\\\\\\\n    &\\\\Rightarrow \\\\quad q_{\\\\pi_{\\\\!_G}}(\\\\tau) = q_{\\\\pi_{\\\\!_E}}(\\\\tau) \\\\,\\\\, \\\\forall \\\\, \\\\tau \\\\in {\\\\mathcal{T}}\\\\\\\\\\n    &\\\\Rightarrow \\\\quad D_{\\\\tilde{\\\\pi}^*, \\\\tilde{\\\\pi}^*} = \\\\frac{1}{2} \\\\,\\\\, \\\\forall \\\\,\\\\tau \\\\in {\\\\mathcal{T}}\\\\\\\\\\n    &\\\\Rightarrow \\\\quad L(\\\\tilde{\\\\pi}^*, \\\\tilde{\\\\pi}^*) = -\\\\log 4\\\\end{aligned}$$ By multiplying the numerator and denominator of $D_{\\\\tilde{\\\\pi}^*, \\\\tilde{\\\\pi}^*}$ by $\\\\xi(\\\\tau)$ it can be shown in exactly the same way as in Appendix\\xa0[7.1](#app:proof_of_optim_D_gan){reference-type=\"ref\" reference=\"app:proof_of_optim_D_gan\"} that $-\\\\log4$ is the global minimum of $L(\\\\tilde{\\\\pi}^*, \\\\pi_{\\\\!_G})$.\\xa0\\n:::\\n\\nAdversarial Soft Q-Fitting: transition-wise Imitation Learning without Policy Optimization {#app:ASQF}\\n==========================================================================================\\n\\nIn this section we present Adversarial Soft Q-Fitting (ASQF), a principled approach to Imitation Learning without Reinforcement Learning that relies exclusively on transitions. Using transitions rather than trajectories presents several practical benefits such as the possibility to deal with asynchronously collected data or non-sequential experts demonstrations. We first present the theoretical setting for ASQF and then test it on a variety of discrete control tasks. We show that while it is theoretically sound, ASQF is often outperformed by ASAF-1, an approximation to ASAF that also allows to rely on transitions instead of trajectories.\\n\\n#### Theoretical Setting\\n\\nWe consider the GAN objective of Eq.\\xa0([\\\\[eq:GAN_obj\\\\]](#eq:GAN_obj){reference-type=\"ref\" reference=\"eq:GAN_obj\"}) with $x=(s,a)$, $\\\\mathcal{X}={\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}$, $p_{\\\\!_E}=d_{\\\\pi_{\\\\!_E}}$, $\\\\quad\\np_{\\\\!_G}=d_{\\\\pi_{\\\\!_G}}$ and a discriminator $D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}}$ of the form of @fu2017learning: $$\\\\begin{aligned}\\n    \\\\label{eq:ASQF_obj}\\n    \\\\min_{\\\\pi_{\\\\!_G}}\\\\max_{\\\\tilde{f}}\\n    L(\\\\tilde{f}, \\\\pi_{\\\\!_G})\\n    \\\\,,\\\\quad L(\\\\tilde{f}, \\\\pi_{\\\\!_G})\\n    &\\\\triangleq\\n    \\\\mathbb{E}_{d_{\\\\pi_{\\\\!_E}}}\\n    [\\\\log D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}}(s,a)]\\n    +\\n    \\\\mathbb{E}_{d_{\\\\pi_{\\\\!_G}}}\\n    [\\\\log (1-D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}}(s,a))],\\\\\\\\\\n    \\\\text{with}\\\\quad D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}} &= \\\\frac{\\\\exp \\\\tilde{f}(s, a)}{\\\\exp \\\\tilde{f}(s, a) + \\\\pi_{\\\\!_G}(a|s)},\\n\\\\end{aligned}$$ for which we present the following theorem.\\n\\n::: {#thm:ASQF_thm .thm}\\n**Theorem 2**. *For any generator policy $\\\\pi_{\\\\!_G}$, the optimal discriminator parameter for Eq.\\xa0([\\\\[eq:ASQF_obj\\\\]](#eq:ASQF_obj){reference-type=\"ref\" reference=\"eq:ASQF_obj\"}) is $$\\\\tilde{f}^* \\\\triangleq \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{f}} L(\\\\tilde{f}, \\\\pi_{\\\\!_G}) = \\\\log\\\\left(\\\\pi_{\\\\!_E}(a|s)\\\\frac{d_{\\\\pi_{\\\\!_E}}(s)}{d_{\\\\pi_{\\\\!_G}}(s)}\\\\right) \\\\, \\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}$$ Using $\\\\tilde{f}^*$, the optimal generator policy $\\\\pi_{\\\\!_G}^*$ is $$\\\\begin{aligned}\\n \\\\mathop{\\\\mathrm{arg\\\\,min}}_{\\\\pi_{\\\\!_G}}\\\\max_{\\\\tilde{f}}L(\\\\tilde{f}, \\\\pi_{\\\\!_G}) = \\\\mathop{\\\\mathrm{arg\\\\,min}}_{\\\\pi_{\\\\!_G}}L(\\\\tilde{f}^*, \\\\pi_{\\\\!_G}) = \\\\pi_{\\\\!_E}(a|s) = \\n \\\\frac{\\\\exp \\\\tilde{f}^*(s,a)}{\\\\sum_{a\\'}\\\\exp \\\\tilde{f}^*(s,a\\')} \\\\,\\\\, \\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}.\\\\end{aligned}$$*\\n:::\\n\\n::: {.proof}\\n*Proof.* The beginning of the proof closely follows the proof of Appendix\\xa0[7.1](#app:proof_of_optim_D_gan){reference-type=\"ref\" reference=\"app:proof_of_optim_D_gan\"}. $$\\\\begin{aligned}\\n    \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{f}} L &(\\\\tilde{f},\\\\pi_{\\\\!_G})= \\\\\\\\& \\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{f}} \\\\sum_{s_i, a_i} d_{\\\\pi_{\\\\!_E}}(s_i,a_i) \\\\log D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}}(s_i,a_i) + d_{\\\\pi_{\\\\!_G}}(s_i,a_i)\\\\log(1-D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}}(s_i,a_i))\\n\\\\end{aligned}$$ We solve for each individual $(s_i,a_i)$ pair and note that $\\\\tilde{f}_{i} \\\\mapsto D_i = \\\\dfrac{\\\\exp \\\\tilde{f}_{i}}{\\\\exp \\\\tilde{f}_{i} + \\\\pi_{\\\\!_{G}, i}}$ is strictly monotonic on $\\\\tilde{f}_{i} \\\\in {\\\\mathbb{R}}\\\\, \\\\forall \\\\, \\\\pi_{\\\\!_{G}, i}\\\\in ]0,1[$ so, $$\\\\begin{aligned}\\n    D^*_i=\\\\mathop{\\\\mathrm{arg\\\\,max}}_{D_i} L_i\\n    \\\\, \\\\Leftrightarrow& \\\\, \\\\tilde{f}^*_i =\\\\mathop{\\\\mathrm{arg\\\\,max}}_{\\\\tilde{f}} L_i\\\\end{aligned}$$ Taking the derivative and setting it to 0, we find that $$\\\\begin{aligned}\\n \\\\left.\\\\frac{d L_i}{d \\\\tilde{f}_i}\\\\right|_{\\\\tilde{f}_i} = 0 \\\\quad \\\\Leftrightarrow \\\\quad\\n    \\\\tilde{f}_i =\\\\log\\\\left(\\\\pi_{\\\\!_{G}, i}\\\\frac{d_{\\\\pi_{\\\\!_E},i}}{d_{\\\\pi_{\\\\!_G},i}}\\\\right)\\\\end{aligned}$$ We confirm that we have a global maximum with the second derivative test and the values at the border of the domain i.e. $\\\\left.\\\\dfrac{d^2 L_i}{d \\\\tilde{f}_i^2}\\\\right|_{\\\\tilde{f}_i^*} < 0$ and $L_i$ goes to $-\\\\infty$ for $\\\\tilde{f}_i \\\\rightarrow +\\\\infty$ and for $\\\\tilde{f}_i \\\\rightarrow -\\\\infty$.\\n\\nIt follows that $$\\\\begin{aligned}\\n    \\\\tilde{f}^*(s,a) &=\\\\log\\\\left(\\\\pi_{\\\\!_G}(a|s)\\\\frac{d_{\\\\pi_{\\\\!_E}}(s,a)}{d_{\\\\pi_{\\\\!_G}}(s,a)}\\\\right) \\\\quad \\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}\\\\\\\\\\n    \\\\implies \\\\tilde{f}^*(s,a) &=\\\\log\\\\left(\\\\cancel{\\\\pi_{\\\\!_G}(a|s)}\\\\frac{d_{\\\\pi_{\\\\!_E}}(s)\\\\pi_{\\\\!_E}(a|s)}{d_{\\\\pi_{\\\\!_G}}(s)\\\\cancel{\\\\pi_{\\\\!_G}(a|s)}}\\\\right) \\\\quad \\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}\\\\\\\\\\n    \\\\label{eq:ASQF_optimal_parameter}\\n    \\\\implies \\\\tilde{f}^*(s,a) &=\\\\log\\\\left(\\\\pi_{\\\\!_E}(a|s)\\\\frac{d_{\\\\pi_{\\\\!_E}}(s)}{d_{\\\\pi_{\\\\!_G}}(s)}\\\\right) \\\\quad \\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}\\\\end{aligned}$$ This proves the first part of Theorem\\xa0[Theorem\\xa02](#thm:ASQF_thm){reference-type=\"ref\" reference=\"thm:ASQF_thm\"}.\\n\\nTo prove the second part notice that $$\\\\begin{aligned}\\n    D_{\\\\tilde{f}^*, \\\\pi_{\\\\!_G}}(s,a) &= \\\\dfrac{\\\\pi_{\\\\!_E}(a|s)\\\\dfrac{d_{\\\\pi_{\\\\!_E}}(s)}{d_{\\\\pi_{\\\\!_G}}(s)}}{\\\\pi_{\\\\!_E}(a|s)\\\\dfrac{d_{\\\\pi_{\\\\!_E}}(s)}{d_{\\\\pi_{\\\\!_G}}(s)} + \\\\pi_{\\\\!_G}(a|s)}\\\\\\\\\\n    &= \\\\frac{\\\\pi_{\\\\!_E}(a|s)d_{\\\\pi_{\\\\!_E}}(s)}{\\\\pi_{\\\\!_E}(a|s)d_{\\\\pi_{\\\\!_E}}(s) + \\\\pi_{\\\\!_G}(a|s)d_{\\\\pi_{\\\\!_G}}(s)}\\\\\\\\\\n    &= \\\\frac{d_{\\\\pi_{\\\\!_E}}(s,a)}{d_{\\\\pi_{\\\\!_E}}(s,a) + d_{\\\\pi_{\\\\!_G}}(s,a)}\\n\\\\end{aligned}$$ This is equal to the optimal discriminator of the GAN objective Eq.\\xa0([\\\\[eq:D_optim_GAN_proof\\\\]](#eq:D_optim_GAN_proof){reference-type=\"ref\" reference=\"eq:D_optim_GAN_proof\"}) when $x=(s,a)$. For this discriminator we showed in Section\\xa0[7.1](#app:proof_of_optim_D_gan){reference-type=\"ref\" reference=\"app:proof_of_optim_D_gan\"} that the optimal generator $\\\\pi_{\\\\!_G}^*$ is such that $d_{\\\\pi_{\\\\!_G}^*}(s,a) = d_{\\\\pi_{\\\\!_E}}(s,a)$ $\\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}$, which is satisfied for $\\\\pi_{\\\\!_G}^*(a|s) = \\\\pi_{\\\\!_E}(a|s)$ $\\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times{\\\\mathcal{A}}$. Using the fact that $$\\\\label{eq:ASQF_partition_function}\\n    \\\\sum_{a\\'}\\\\exp\\\\tilde{f}^*(s,a\\') = \\\\sum_{a\\'}\\\\pi_{\\\\!_E}(a\\'|s)\\\\frac{d_{\\\\pi_{\\\\!_E}}(s)}{d_{\\\\pi_{\\\\!_G}}(s)} = \\\\frac{d_{\\\\pi_{\\\\!_E}}(s)}{d_{\\\\pi_{\\\\!_G}}(s)}\\\\sum_{a\\'}\\\\pi_{\\\\!_E}(a\\'|s) = \\\\frac{d_{\\\\pi_{\\\\!_E}}(s)}{d_{\\\\pi_{\\\\!_G}}(s)}.$$ we can combine Eq.\\xa0([\\\\[eq:ASQF_optimal_parameter\\\\]](#eq:ASQF_optimal_parameter){reference-type=\"ref\" reference=\"eq:ASQF_optimal_parameter\"}) and Eq.\\xa0([\\\\[eq:ASQF_partition_function\\\\]](#eq:ASQF_partition_function){reference-type=\"ref\" reference=\"eq:ASQF_partition_function\"}) to write the expert\\'s policy $\\\\pi_{\\\\!_E}$ as a function of the optimal discriminator parameter $\\\\tilde{f}^*$: $$\\\\pi_{\\\\!_E}(a|s) = \\n \\\\frac{\\\\exp \\\\tilde{f}^*(s,a)}{\\\\sum_{a\\'}\\\\exp \\\\tilde{f}^*(s,a\\')} \\\\,\\\\, \\\\forall (s,a) \\\\in {\\\\mathcal{S}}\\\\times {\\\\mathcal{A}}.$$ This concludes the second part of the proof.\\xa0\\n:::\\n\\n#### Adversarial Soft-Q Fitting (ASQF) - practical algorithm\\n\\nIn a nutshell, Theorem\\xa0[Theorem\\xa02](#thm:ASQF_thm){reference-type=\"ref\" reference=\"thm:ASQF_thm\"} tells us that training the discriminator in Eq.\\xa0([\\\\[eq:ASQF_obj\\\\]](#eq:ASQF_obj){reference-type=\"ref\" reference=\"eq:ASQF_obj\"}) to distinguish between transitions from the expert and transitions from a generator policy can be seen as retrieving $\\\\tilde{f}^*$ which plays the role of the expert\\'s soft Q-function (i.e. which matches Eq.\\xa0([\\\\[eq:max_ent_policy\\\\]](#eq:max_ent_policy){reference-type=\"ref\" reference=\"eq:max_ent_policy\"}) for $\\\\tilde{f}^*=\\\\frac{1}{\\\\alpha}Q_{\\\\text{soft},E}^*$): $$\\\\label{eq:ASQF_pi}\\n    \\\\pi_{\\\\!_E}(a|s) = \\\\frac{\\\\exp \\\\tilde{f}^*(s,a)}{\\\\sum_{a\\'}\\\\exp \\\\tilde{f}^*(s,a\\')} = \\\\exp\\\\left(\\\\tilde{f}^*(s,a) - \\\\log\\\\sum_{a\\'}\\\\exp \\\\tilde{f}^*(s,a\\') \\\\right),$$ Therefore, by training the discriminator, one simultaneously retrieves the optimal generator policy.\\n\\nThere is one caveat though: the summation over actions that is required in Eq.\\xa0([\\\\[eq:ASQF_pi\\\\]](#eq:ASQF_pi){reference-type=\"ref\" reference=\"eq:ASQF_pi\"}) to go from $\\\\tilde{f}^*$ to the policy is intractable in continuous action spaces and would require an additional step such as a projection to a proper distribution (@haarnoja2018soft use a Gaussian) in order to draw samples and evaluate likelihoods. Updating in this way the generator policy to match a softmax over our learned state-action preferences ($\\\\tilde{f}^*$) becomes very similar in requirements and computational load to a policy optimization step, thus defeating the purpose of this work which is to get rid of the policy optimization step. For this reason we only consider ASQF for discrete action spaces.\\n\\nAs explained in Section\\xa0[3.3](#sec:asaf_practical_algorithm){reference-type=\"ref\" reference=\"sec:asaf_practical_algorithm\"}, in practice we optimize $D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}}$ only for a few steps before updating $\\\\pi_{\\\\!_G}$ by normalizing $\\\\exp \\\\tilde{f}(s,a)$ over the action dimension. See Algorithm\\xa0[\\\\[alg:ASQF_alg\\\\]](#alg:ASQF_alg){reference-type=\"ref\" reference=\"alg:ASQF_alg\"} for the pseudo-code.\\n\\n[\\\\[alg:ASQF_alg\\\\]]{#alg:ASQF_alg label=\"alg:ASQF_alg\"}\\n\\nexpert transitions $\\\\mathcal{D}_E = \\\\{(s_i, a_i)\\\\}_{i=1}^{N_E}$ Randomly initialize $\\\\tilde{f}$ and get $\\\\pi_{\\\\!_G}$ from Eq.\\xa0([\\\\[eq:ASQF_pi\\\\]](#eq:ASQF_pi){reference-type=\"ref\" reference=\"eq:ASQF_pi\"}) Collect transitions $\\\\mathcal{D}_G = \\\\{(s_i, a_i)\\\\}_{i=1}^{N_G}$ by executing $\\\\pi_{\\\\!_G}$ Train $D_{\\\\tilde{f}, \\\\pi_{\\\\!_G}}$ using binary cross-entropy on minibatches of transitions from $\\\\mathcal{D}_E$ and $\\\\mathcal{D}_G$ Get $\\\\pi_{\\\\!_G}$ from Eq.\\xa0([\\\\[eq:ASQF_pi\\\\]](#eq:ASQF_pi){reference-type=\"ref\" reference=\"eq:ASQF_pi\"})\\n\\n#### Experimental results\\n\\nFigure\\xa0[6](#fig:toy_asaf1_asqf){reference-type=\"ref\" reference=\"fig:toy_asaf1_asqf\"} shows that ASQF performs well on small scale environments but struggles and eventually fails on more complicated environments. Specifically, it seems that ASQF does not scale well with the observation space size. Indeed mountaincar, cartpole, lunarlander and pommerman have respectively an observation space dimensionality of 2, 4, 8 and 960. This may be due to the fact that the partition function Eq.\\xa0([\\\\[eq:ASQF_partition_function\\\\]](#eq:ASQF_partition_function){reference-type=\"ref\" reference=\"eq:ASQF_partition_function\"}) becomes more difficult to learn. Indeed, for each state, several transitions with different actions are required in order to learn it. Poorly approximating this partition function could lead to assigning too low a probability to expert-like actions and eventually failing to behave appropriately. ASAF on the other hand explicitly learns the probability of an action given the state -- in other word it explicitly learns the partition function -- and is therefore immune to that problem.\\n\\n![Comparison between ASAF-1 and ASQF, our two transition-wise methods, on environments with increasing observation space dimensionality](figures/results_comparison_asqf.pdf){#fig:toy_asaf1_asqf width=\".75\\\\\\\\textwidth\"}\\n\\nAdditional Experiments {#app:additional_experiments}\\n======================\\n\\nGAIL - Importance of Gradient Penalty\\n-------------------------------------\\n\\n![Comparison between original GAIL [@ho2016generative] and GAIL with gradient penalty (GP) [@gulrajani2017improved; @kostrikov2018discriminatoractorcritic]](figures/gail_gradient_penalty.pdf){#fig:gail_gradient_penalty width=\"1.\\\\\\\\textwidth\"}\\n\\nMimicking the expert\\n--------------------\\n\\nTo ensure that our method actually mimics the expert and doesn\\'t just learn a policy that collects high rewards when trained with expert demonstrations, we ran ASAF-1 on the Ant-v2 MuJoCo environment using various sets of 25 demonstrations. These demonstrations were generated from a Soft Actor-Critic agent at various levels of performance during its training. Since at low-levels of performance the variance of episode\\'s return is high, we filtered collected demonstrations to lie in the targeted range of performance (e.g. return in \\\\[800, 1200\\\\] for the 1K set). Results in Figure\\xa0[\\\\[fig:gradual_expert\\\\]](#fig:gradual_expert){reference-type=\"ref\" reference=\"fig:gradual_expert\"} show that our algorithm succeeds at learning a policy that closely emulates various demonstrators (even when non-optimal).\\n\\n![image](figures/demosGradation_learning.pdf){width=\"1.\\\\\\\\textwidth\"} [\\\\[fig:gradual_expert\\\\]]{#fig:gradual_expert label=\"fig:gradual_expert\"}\\n\\nWall Clock Time\\n---------------\\n\\nWe report training times in Figure\\xa0[8](#fig:wall_clock_times){reference-type=\"ref\" reference=\"fig:wall_clock_times\"} and observe that ASAF-1 is always fastest to learn. Note however that reports of performance w.r.t wall-clock time should always be taken with a grain of salt as they are greatly influenced by hyper-parameters and implementation details.\\n\\n![Training times on MuJoCo tasks for 25 expert demonstrations.](figures/WallClockTimeMuJoCo.pdf){#fig:wall_clock_times width=\"1.\\\\\\\\textwidth\"}\\n\\nHyperparameter tuning and best configurations {#app:hyperparameters}\\n=============================================\\n\\nClassic Control\\n---------------\\n\\nFor this first set of experiments, we use the fixed hyperparameters presented in Table\\xa0[1](#table:fixed_hyperparams_classic_control){reference-type=\"ref\" reference=\"table:fixed_hyperparams_classic_control\"}.\\n\\n::: {#table:fixed_hyperparams_classic_control}\\n  **RL component**                                          \\n  -------------------------------------- ------------------ --------------------\\n  Hyper-parameter                         Discrete Control   Continuous Control\\n                                                            \\n  **SAC**                                                   \\n  Batch size (in transitions)                   256                 256\\n  Replay Buffer length $|\\\\mathcal{B}|$        $10^{6}$            $10^{6}$\\n  Warmup (in transitions)                       1280               10240\\n  Initial entropy weight $\\\\alpha$               0.4                 0.4\\n  Gradient norm clipping threshold              0.2                  1\\n  Transitions between update                     40                  1\\n  Target network weight $\\\\tau$                  0.01                0.01\\n                                                            \\n  **PPO**                                                   \\n  Batch size (in transitions)                   256                 256\\n  GAE parameter $\\\\lambda$                       0.95                0.95\\n  Transitions between update                     \\\\-                 2000\\n  Episodes between updates                       10                  \\\\-\\n  Epochs per update                              10                  10\\n  Update clipping parameter                     0.2                 0.2\\n                                                            \\n  **Reward Learning component**                             \\n  Hyper-parameter                         Discrete Control   Continuous Control\\n                                                            \\n  **AIRL, GAIL, ASAF-1**                                    \\n  Batch size (in transitions)                   256                 256\\n  Transitions between update                     \\\\-                 2000\\n  Episodes between updates                       10                  \\\\-\\n  Epochs per update                              50                  50\\n  Gradient value clipping threshold              \\\\-                  1\\n  (ASAF-1)                                                  \\n                                                            \\n  **ASAF, ASAF-*w***                                        \\n  Batch size (in trajectories)                   10                  10\\n  Episodes between updates                       10                  20\\n  Epochs per update                              50                  50\\n  Window size $w$                            (searched)             200\\n  Gradient value clipping threshold              \\\\-                  1\\n\\n  : Fixed Hyperparameters for classic control tasks\\n:::\\n\\nFor the most sensitive hyperparameters, the learning rates for the reinforcement learning and discriminator updates ($\\\\epsilon_{\\\\text{RL}}$ and $\\\\epsilon_{\\\\text{D}}$), we perform a random search over 50 configurations and 3 seeds each (for each algorithm on each task) for 500 episodes. We consider logarithmic ranges, i.e. $\\\\epsilon = 10^{u}$ with $u \\\\sim Uniform(-6, -1)$ for $\\\\epsilon_{\\\\text{D}}$ and $u \\\\sim Uniform(-4, -1)$ for $\\\\epsilon_{\\\\text{RL}}$. We also include in this search the critic learning rate coefficient $\\\\kappa$ for PPO also sampled according to a logarithmic scale with $u \\\\sim Uniform(-2, 2)$ so that the effective learning rate for PPO\\'s critic network is $\\\\kappa \\\\cdot \\\\epsilon_{\\\\text{RL}}$. For discrete action tasks, the window-size *w* for ASAF-*w* is sampled uniformly within $\\\\{32, 64, 128\\\\}$. The best configuration for each algorithm is presented in Tables\\xa0[\\\\[table:hyperparams_cartpole\\\\]](#table:hyperparams_cartpole){reference-type=\"ref\" reference=\"table:hyperparams_cartpole\"}\\xa0to\\xa0[\\\\[table:hyperparams_lunarlander_c\\\\]](#table:hyperparams_lunarlander_c){reference-type=\"ref\" reference=\"table:hyperparams_lunarlander_c\"}. Figure\\xa0[1](#fig:results_toy){reference-type=\"ref\" reference=\"fig:results_toy\"} uses these configurations retrained on 10 seeds and twice as long.\\n\\nFinally for all neural networks (policies and discriminators) for these experiments we use a fully-connected MLP with two hidden layers and ReLU activation (except for the last layer). We used hidden sizes of 64 for the discrete tasks and of 256 for the continuous tasks.\\n\\nMuJoCo\\n------\\n\\nFor MuJoCo experiments (Hopper-v2, Walker2d-v2, HalfCheetah-v2, Ant-v2), the fixed hyperparameters are presented in Table\\xa0[2](#table:fixed_hyperparams_MuJoCo){reference-type=\"ref\" reference=\"table:fixed_hyperparams_MuJoCo\"}. For all exeperiments, fully-connected MLPs with two hidden layers and ReLU activation (except for the last layer) were used, where the number of hidden units is equal to 256.\\n\\n::: {#table:fixed_hyperparams_MuJoCo}\\n  **RL component**                                                            \\n  -------------------------------- ------------------------------------ -- -- --\\n  Hyper-parameter                   Hopper, Walker2d, HalfCheetah, Ant        \\n                                                                              \\n  **PPO (for GAIL)**                                                          \\n  GAE parameter $\\\\lambda$                          0.98                       \\n  Transitions between updates                      2000                       \\n  Epochs per update                                 5                         \\n  Update clipping parameter                        0.2                        \\n  Critic lr coefficient $\\\\kappa$                   0.25                       \\n  Discount factor $\\\\gamma$                         0.99                       \\n  **Reward Learning component**                                               \\n  Hyper-parameter                   Hopper, Walker2d, HalfCheetah, Ant        \\n                                                                              \\n  **GAIL**                                                                    \\n  Transitions between updates                      2000                       \\n                                                                              \\n  **ASAF**                                                                    \\n  Episodes between updates                          25                        \\n                                                                              \\n  **ASAF-1 and ASAF-*w***                                                     \\n  Transitions between updates                      2000                       \\n\\n  : Fixed hyperparameters for MuJoCo environments.\\n:::\\n\\nFor SQIL we used SAC with the same hyperparameters that were used to generate the expert demonstrations. For ASAF, ASAF-1 and ASAF-*w*, we set the learning rate for the discriminator at 0.001 and ran random searches over 25 randomly sampled configurations and 2 seeds for each task to select the other hyperparameters for the discriminator training. These hyperparameters included the discriminator batch size sampled from a uniform distribution over $\\\\{10, 20, 30\\\\}$ for ASAF and ASAF-*w* (in trajectories) and over $\\\\{100, 500, 1000, 2000\\\\}$ for ASAF-1 (in transitions), the number of epochs per update sampled from a uniform distribution over $\\\\{10, 20, 50\\\\}$, the gradient norm clipping threshold sampled form a uniform distribution over $\\\\{1, 10\\\\}$, the window-size (for ASAF-*w*) sampled from a uniform distribution over $\\\\{100, 200, 500, 1000\\\\}$ and the window stride (for ASAF-*w*) sampled from a uniform distribution over $\\\\{1, 50, w\\\\}$. For GAIL, we obtained poor results using the original hyperparameters from [@ho2016generative] for a number of tasks so we ran random searches over 100 randomly sampled configurations for each task and 2 seeds to select for the following hyperparameters: the log learning rate of the RL update and the discriminator update separately sampled from uniform distributions over $[-7, -1]$, the gradient norm clipping for the RL update and the discriminator update separately sampled from uniform distributions over $\\\\{None, 1, 10\\\\}$, the number of epochs per update sampled from a uniform distribution over $\\\\{5, 10, 30, 50\\\\}$, the gradient penalty coefficient sampled from a uniform distribution over $\\\\{1, 10\\\\}$ and the batch size for the RL update and discriminator update separately sampled from uniform distributions over $\\\\{100, 200, 500, 1000, 2000\\\\}$.\\n\\nPommerman\\n---------\\n\\nFor this set of experiments, we use a number of fixed hyperparameters for all algorithms either inspired from their original papers for the baselines or selected through preliminary searches. These fixed hyperparameters are presented in Table\\xa0[3](#table:fixed_hyperparams_pommerman_random_tag){reference-type=\"ref\" reference=\"table:fixed_hyperparams_pommerman_random_tag\"}.\\n\\n::: {#table:fixed_hyperparams_pommerman_random_tag}\\n  **RL component**                                              \\n  -------------------------------------- ---------------------- --\\n  Hyper-parameter                         Pommerman Random-Tag  \\n                                                                \\n  **SAC**                                                       \\n  Batch size (in transitions)                     256           \\n  Replay Buffer length $|\\\\mathcal{B}|$          $10^{5}$        \\n  Warmup (in transitions)                         1280          \\n  Initial entropy weight $\\\\alpha$                 0.4           \\n  Gradient norm clipping threshold                0.2           \\n  Transitions between update                       10           \\n  Target network weight $\\\\tau$                    0.05          \\n                                                                \\n  **PPO**                                                       \\n  Batch size (in transitions)                     256           \\n  GAE parameter $\\\\lambda$                         0.95          \\n  Episodes between updates                         10           \\n  Epochs per update                                10           \\n  Update clipping parameter                       0.2           \\n  Critic lr coefficient $\\\\kappa$                  0.5           \\n                                                                \\n  **Reward Learning component**                                 \\n  Hyper-parameter                         Pommerman Random-Tag  \\n                                                                \\n  **AIRL, GAIL, ASAF-1**                                        \\n  Batch size (in transitions)                     256           \\n  Episodes between updates                         10           \\n  Epochs per update                                10           \\n                                                                \\n  **ASAF, ASAF-*w***                                            \\n  Batch size (in trajectories)                     5            \\n  Episodes between updates                         10           \\n  Epochs per update                                10           \\n\\n  : Fixed Hyperparameters for Pommerman Random-Tag environment.\\n:::\\n\\nFor the most sensitive hyperparameters, the learning rates for the reinforcement learning and discriminator updates ($\\\\epsilon_{\\\\text{RL}}$ and $\\\\epsilon_{\\\\text{D}}$), we perform a random search over 25 configurations and 2 seeds each for all algorithms. We consider logarithmic ranges, i.e. $\\\\epsilon = 10^{u}$ with $u \\\\sim Uniform(-7, -3)$ for $\\\\epsilon_{\\\\text{D}}$ and $u \\\\sim Uniform(-4, -1)$ for $\\\\epsilon_{\\\\text{RL}}$. We also include in this search the window-size *w* for ASAF-*w*, sampled uniformly within $\\\\{32, 64, 128\\\\}$. The best configuration for each algorithm is presented in Table\\xa0[\\\\[table:hyperparams_pommerman\\\\]](#table:hyperparams_pommerman){reference-type=\"ref\" reference=\"table:hyperparams_pommerman\"}. Figure\\xa0[5](#fig:pommerman_results_randomTag){reference-type=\"ref\" reference=\"fig:pommerman_results_randomTag\"} uses these configurations retrained on 10 seeds.\\n\\nFinally for all neural networks (policies and discriminators) we use the same architecture. Specifically, we first process the feature maps (see Section\\xa0[11.3](#app:env_pommerman){reference-type=\"ref\" reference=\"app:env_pommerman\"}) using a 3-layers convolutional network with number of hidden feature maps of 16, 32 and 64 respectivelly. Each one of these layers use a kernel size of 3x3 with stride of 1, no padding and a ReLU activation. This module ends with a fully connected layer of hidden size 64 followed by a ReLU activation. The output vector is then concatenated to the unprocessed additional information vector (see Section\\xa0[11.3](#app:env_pommerman){reference-type=\"ref\" reference=\"app:env_pommerman\"}) and passed through a final MLP with two hidden layers of size 64 and ReLU activations (except for the last layer).\\n\\nEnvironments and expert data {#app:environments}\\n============================\\n\\nClassic Control\\n---------------\\n\\nThe environments used here are the reference Gym implementations for classic control[^2] and for Box2D[^3]. We generated the expert trajectories for mountaincar (both discrete and continuous version) by hand using keyboard inputs. For the other tasks, we trained our SAC implementation to get experts on the discrete action tasks and our PPO implementation to get experts on the continuous action tasks.\\n\\nMuJoCo\\n------\\n\\nThe experts were trained using our implementation of SAC\\xa0[@haarnoja2018soft] the state-of-the-art RL algorithm in MuJoCo continuous control tasks. Our implementation basically refactors the SAC implementation from Rlpyt[^4]. We trained SAC agent for 1,000,000 steps for Hopper-v2 and 3,000,000 steps for Walker2d-v2 and HalfCheetah-v2 and Ant-v2. We used the default hyper-parameters from Rlpyt.\\n\\nPommerman {#app:env_pommerman}\\n---------\\n\\nThe observation space that we use for Pommerman domain\\xa0[@resnick2018pommerman] is composed of a set of 15 feature maps as well as an additional information vector. The feature maps whose dimensions are given by the size of the board (8x8 in the case of 1vs1 tasks) are one-hot across the third dimension and represent which element is present at which location. Specifically, these feature maps identify whether a given location is the current player, an ally, an ennemy, a passage, a wall, a wood, a bomb, a flame, fog, a power-up. Other feature maps contain integers indicating bomb blast stength, bomb life, bomb moving direction and flame life for each location. Finally, the additional information vecor contains the time-step, number of ammunition, whether the player can kick and blast strengh for the current player. The agent has an action space composed of six actions: do-nothing, up, down, left, right and lay bomb.\\n\\nFor these experiments, we generate the expert demonstrations using Agent47Agent, the open-source champion algorithm of the FFA 2018 competition\\xa0[@zhou2018hybrid] which uses hardcoded heuristics and Monte-Carlo Tree-Search[^5]. While this agent occasionally eliminates itself during a match, we only select trajectories leading to a win as being expert demonstrations.\\n\\nDemonstrations summary\\n----------------------\\n\\nTable\\xa0[\\\\[table:demo_summary\\\\]](#table:demo_summary){reference-type=\"ref\" reference=\"table:demo_summary\"} provides a summary of the expert data used.\\n\\n[^1]: Equal contribution. $\\\\text{   }\\\\quad^{\\\\dagger}$Work conducted while interning at Ubisoft Montreal\\'s La Forge R&D laboratory. $\\\\text{   }\\\\quad^\\\\ddag$Canada CIFAR AI Chair.\\n\\n[^2]: See: <http://gym.openai.com/envs/#classic_control>\\n\\n[^3]: See: <http://gym.openai.com/envs/#box2d>\\n\\n[^4]: See: <https://github.com/astooke/rlpyt>\\n\\n[^5]: See: <https://github.com/YichenGong/Agent47Agent/tree/master/pommerman>\\n',\n",
       "  'bibliography_bbl': \"\\\\begin{thebibliography}{30}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Abbeel and Ng(2004)]{abbeel2004apprenticeship}\\nPieter Abbeel and Andrew~Y Ng.\\n\\\\newblock Apprenticeship learning via inverse reinforcement learning.\\n\\\\newblock In \\\\emph{Proceedings of the 21st International Conference on Machine\\n  Learning (ICML)}, 2004.\\n\\n\\\\bibitem[Degris et~al.(2012)Degris, White, and Sutton]{degris2012off}\\nThomas Degris, Martha White, and Richard~S Sutton.\\n\\\\newblock Off-policy actor-critic.\\n\\\\newblock In \\\\emph{Proceedings of the 29th International Conference on Machine\\n  Learning (ICML)}, pages 179--186, 2012.\\n\\n\\\\bibitem[Ding et~al.(2019)Ding, Florensa, Abbeel, and Phielipp]{ding2019goal}\\nYiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp.\\n\\\\newblock Goal-conditioned imitation learning.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems\\n  (NeurIPS)}, pages 15298--15309, 2019.\\n\\n\\\\bibitem[Finn et~al.(2016{\\\\natexlab{a}})Finn, Christiano, Abbeel, and\\n  Levine]{finn2016connection}\\nChelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine.\\n\\\\newblock A connection between generative adversarial networks, inverse\\n  reinforcement learning, and energy-based models.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1611.03852}, 2016{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Finn et~al.(2016{\\\\natexlab{b}})Finn, Levine, and\\n  Abbeel]{finn2016guided}\\nChelsea Finn, Sergey Levine, and Pieter Abbeel.\\n\\\\newblock Guided cost learning: Deep inverse optimal control via policy\\n  optimization.\\n\\\\newblock In \\\\emph{Proceedings of the 33rd International Conference on Machine\\n  Learning (ICML)}, pages 49--58, 2016{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Fu et~al.(2017)Fu, Luo, and Levine]{fu2017learning}\\nJustin Fu, Katie Luo, and Sergey Levine.\\n\\\\newblock Learning robust rewards with adversarial inverse reinforcement\\n  learning.\\n\\\\newblock In \\\\emph{Proceedings of the 5th International Conference on Learning\\n  Representations (ICLR)}, 2017.\\n\\n\\\\bibitem[Ghasemipour et~al.(2019)Ghasemipour, Zemel, and\\n  Gu]{ghasemipour2019divergence}\\nSeyed Kamyar~Seyed Ghasemipour, Richard Zemel, and Shixiang Gu.\\n\\\\newblock A divergence minimization perspective on imitation learning methods.\\n\\\\newblock In \\\\emph{Proceedings of the 3rd Conference on Robot Learning (CoRL)},\\n  2019.\\n\\n\\\\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,\\n  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\\n  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.\\n\\\\newblock Generative adversarial nets.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems\\n  (NeurIPS)}, pages 2672--2680, 2014.\\n\\n\\\\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and\\n  Courville]{gulrajani2017improved}\\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C\\n  Courville.\\n\\\\newblock Improved training of {W}asserstein {GAN}s.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems\\n  (NeurIPS)}, pages 5767--5777, 2017.\\n\\n\\\\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and\\n  Levine]{haarnoja2017reinforcement}\\nTuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.\\n\\\\newblock Reinforcement learning with deep energy-based policies.\\n\\\\newblock In \\\\emph{Proceedings of the 34th International Conference on Machine\\n  Learning (ICML)}, pages 1352--1361, 2017.\\n\\n\\\\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan,\\n  Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft}\\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,\\n  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et~al.\\n\\\\newblock Soft actor-critic algorithms and applications.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1812.05905}, 2018.\\n\\n\\\\bibitem[Hazan et~al.(2018)Hazan, Kakade, Singh, and\\n  Van~Soest]{hazan2018provably}\\nElad Hazan, Sham~M Kakade, Karan Singh, and Abby Van~Soest.\\n\\\\newblock Provably efficient maximum entropy exploration.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1812.02690}, 2018.\\n\\n\\\\bibitem[Ho and Ermon(2016)]{ho2016generative}\\nJonathan Ho and Stefano Ermon.\\n\\\\newblock Generative adversarial imitation learning.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems\\n  (NeurIPS)}, pages 4565--4573, 2016.\\n\\n\\\\bibitem[Kostrikov et~al.(2019)Kostrikov, Agrawal, Dwibedi, Levine, and\\n  Tompson]{kostrikov2018discriminatoractorcritic}\\nIlya Kostrikov, Kumar~Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and\\n  Jonathan Tompson.\\n\\\\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward\\n  bias in adversarial imitation learning.\\n\\\\newblock In \\\\emph{Proceedings of the 7th International Conference on Learning\\n  Representations (ICLR)}, 2019.\\n\\n\\\\bibitem[Kostrikov et~al.(2020)Kostrikov, Nachum, and\\n  Tompson]{Kostrikov2020Imitation}\\nIlya Kostrikov, Ofir Nachum, and Jonathan Tompson.\\n\\\\newblock Imitation learning via off-policy distribution matching.\\n\\\\newblock In \\\\emph{Proceedings of the 8th International Conference on Learning\\n  Representations (ICLR)}, 2020.\\n\\n\\\\bibitem[Kuefler et~al.(2017)Kuefler, Morton, Wheeler, and\\n  Kochenderfer]{kuefler2017imitating}\\nAlex Kuefler, Jeremy Morton, Tim Wheeler, and Mykel Kochenderfer.\\n\\\\newblock Imitating driver behavior with generative adversarial networks.\\n\\\\newblock In \\\\emph{Proceedings of 2017 IEEE Intelligent Vehicles Symposium\\n  (IV)}, pages 204--211, 2017.\\n\\n\\\\bibitem[Nachum et~al.(2018)Nachum, Norouzi, Xu, and\\n  Schuurmans]{nachum2018trustpcl}\\nOfir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.\\n\\\\newblock Trust-{PCL}: An off-policy trust region method for continuous\\n  control.\\n\\\\newblock In \\\\emph{Proceedings of the 6th International Conference on Learning\\n  Representations (ICLR)}, 2018.\\n\\n\\\\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}\\nOfir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.\\n\\\\newblock Dual{DICE}: Behavior-agnostic estimation of discounted stationary\\n  distribution corrections.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems\\n  (NeurIPS)}, pages 2318--2328, 2019.\\n\\n\\\\bibitem[Pomerleau(1991)]{pomerleau1991efficient}\\nDean~A Pomerleau.\\n\\\\newblock Efficient training of artificial neural networks for autonomous\\n  navigation.\\n\\\\newblock \\\\emph{Neural computation}, 3\\\\penalty0 (1):\\\\penalty0 88--97, 1991.\\n\\n\\\\bibitem[Reddy et~al.(2019)Reddy, Dragan, and Levine]{reddy2019sqil}\\nSiddharth Reddy, Anca~D. Dragan, and Sergey Levine.\\n\\\\newblock {SQIL}: Imitation learning via reinforcement learning with sparse\\n  rewards, 2019.\\n\\n\\\\bibitem[Resnick et~al.(2018)Resnick, Eldridge, Ha, Britz, Foerster, Togelius,\\n  Cho, and Bruna]{resnick2018pommerman}\\nCinjon Resnick, Wes Eldridge, David Ha, Denny Britz, Jakob Foerster, Julian\\n  Togelius, Kyunghyun Cho, and Joan Bruna.\\n\\\\newblock Pommerman: A multi-agent playground.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1809.07124}, 2018.\\n\\n\\\\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}\\nDanilo Rezende and Shakir Mohamed.\\n\\\\newblock Variational inference with normalizing flows.\\n\\\\newblock In \\\\emph{Proceedings of the 32nd International Conference on Machine\\n  Learning (ICML)}, pages 1530--1538, 2015.\\n\\n\\\\bibitem[Ross and Bagnell(2010)]{ross2010efficient}\\nSt{\\\\'e}phane Ross and Drew Bagnell.\\n\\\\newblock Efficient reductions for imitation learning.\\n\\\\newblock In \\\\emph{Proceedings of the 13th International Conference on\\n  Artificial Intelligence and Statistics (AISTATS)}, pages 661--668, 2010.\\n\\n\\\\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}\\nSt{\\\\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.\\n\\\\newblock A reduction of imitation learning and structured prediction to\\n  no-regret online learning.\\n\\\\newblock In \\\\emph{Proceedings of the 14th International Conference on\\n  Artificial Intelligence and Statistics (AISTATS)}, pages 627--635, 2011.\\n\\n\\\\bibitem[Sasaki et~al.(2018)Sasaki, Yohira, and Kawaguchi]{sasaki2018sample}\\nFumihiro Sasaki, Tetsuya Yohira, and Atsuo Kawaguchi.\\n\\\\newblock Sample efficient imitation learning for continuous control.\\n\\\\newblock In \\\\emph{Proceedings of the 6th International Conference on Learning\\n  Representations (ICLR)}, 2018.\\n\\n\\\\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and\\n  Moritz]{schulman2015trust}\\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp\\n  Moritz.\\n\\\\newblock Trust region policy optimization.\\n\\\\newblock In \\\\emph{Proceedings of the 32nd International Conference on Machine\\n  Learning (ICML)}, pages 1889--1897, 2015.\\n\\n\\\\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and\\n  Klimov]{schulman2017proximal}\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.\\n\\\\newblock Proximal policy optimization algorithms.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1707.06347}, 2017.\\n\\n\\\\bibitem[Zhou et~al.(2018)Zhou, Gong, Mugrai, Khalifa, Nealen, and\\n  Togelius]{zhou2018hybrid}\\nHongwei Zhou, Yichen Gong, Luvneesh Mugrai, Ahmed Khalifa, Andy Nealen, and\\n  Julian Togelius.\\n\\\\newblock A hybrid search agent in pommerman.\\n\\\\newblock In \\\\emph{Proceedings of the 13th International Conference on the\\n  Foundations of Digital Games (FDG)}, pages 1--4, 2018.\\n\\n\\\\bibitem[Ziebart(2010)]{ziebart2010modeling}\\nBrian~D Ziebart.\\n\\\\newblock \\\\emph{Modeling purposeful adaptive behavior with the principle of\\n  maximum causal entropy}.\\n\\\\newblock PhD thesis, 2010.\\n\\n\\\\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and\\n  Dey]{ziebart2008maximum}\\nBrian~D Ziebart, Andrew~L Maas, J~Andrew Bagnell, and Anind~K Dey.\\n\\\\newblock Maximum entropy inverse reinforcement learning.\\n\\\\newblock In \\\\emph{Proceedings of the 23rd AAAI Conference on Artificial\\n  Intelligence}, pages 1433--1438, 2008.\\n\\n\\\\end{thebibliography}\\n\",\n",
       "  'bibliography_bib': \"@article{peng2018continual,\\n  title={Continual match based training in Pommerman: Technical report},\\n  author={Peng, Peng and Pang, Liang and Yuan, Yufeng and Gao, Chao},\\n  journal={arXiv preprint arXiv:1812.07297},\\n  year={2018}\\n}\\n@inproceedings{ho2016generative,\\n  title={Generative adversarial imitation learning},\\n  author={Ho, Jonathan and Ermon, Stefano},\\n  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},\\n  pages={4565--4573},\\n  year={2016}\\n}\\n\\n@inproceedings{ziebart2008maximum,\\n  title={Maximum entropy inverse reinforcement learning},\\n  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},\\n  booktitle={Proceedings of the 23rd AAAI Conference on Artificial Intelligence},\\n  pages={1433--1438},\\n  year={2008}\\n}\\n\\n\\n@inproceedings{abbeel2004apprenticeship,\\n  title={Apprenticeship learning via inverse reinforcement learning},\\n  author={Abbeel, Pieter and Ng, Andrew Y},\\n  booktitle={Proceedings of the 21st International Conference on Machine Learning (ICML)},\\n  year={2004}\\n}\\n\\n@phdthesis{ziebart2010modeling,\\n  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},\\n  author={Ziebart, Brian D},\\n  year={2010}\\n}\\n@inproceedings{goodfellow2014generative,\\n  title={Generative adversarial nets},\\n  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},\\n  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},\\n  pages={2672--2680},\\n  year={2014}\\n}\\n\\n@inproceedings{haarnoja2017reinforcement,\\n  title={Reinforcement learning with deep energy-based policies},\\n  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},\\n  booktitle={Proceedings of the 34th International Conference on Machine Learning (ICML)},\\n  pages={1352--1361},\\n  year={2017}\\n}\\n\\n@article{finn2016connection,\\n  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},\\n  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},\\n  journal={arXiv preprint arXiv:1611.03852},\\n  year={2016}\\n}\\n\\n@inproceedings{finn2016guided,\\n  title={Guided cost learning: Deep inverse optimal control via policy optimization},\\n  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},\\n  booktitle={Proceedings of the 33rd International Conference on Machine Learning (ICML)},\\n  pages={49--58},\\n  year={2016}\\n}\\n@inproceedings{peters2006policy,\\n  title={Policy gradient methods for robotics},\\n  author={Peters, Jan and Schaal, Stefan},\\n  booktitle={Proceedings of 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\\n  pages={2219--2225},\\n  year={2006}\\n}\\n\\n@misc{reddy2019sqil,\\n    title={{SQIL}: Imitation learning via reinforcement learning with sparse rewards},\\n    author={Siddharth Reddy and Anca D. Dragan and Sergey Levine},\\n    year={2019},\\n    eprint={1905.11108},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.LG}\\n}\\n\\n@inproceedings{ross2011reduction,\\n  title={A reduction of imitation learning and structured prediction to no-regret online learning},\\n  author={Ross, St{\\\\'e}phane and Gordon, Geoffrey and Bagnell, Drew},\\n  booktitle={Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)},\\n  pages={627--635},\\n  year={2011}\\n}\\n\\n@inproceedings{kuefler2017imitating,\\n  title={Imitating driver behavior with generative adversarial networks},\\n  author={Kuefler, Alex and Morton, Jeremy and Wheeler, Tim and Kochenderfer, Mykel},\\n  booktitle={Proceedings of 2017 IEEE Intelligent Vehicles Symposium (IV)},\\n  pages={204--211},\\n  year={2017}\\n}\\n@inproceedings{ding2019goal,\\n  title={Goal-conditioned imitation learning},\\n  author={Ding, Yiming and Florensa, Carlos and Abbeel, Pieter and Phielipp, Mariano},\\n  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},\\n  pages={15298--15309},\\n  year={2019}\\n}\\n@article{resnick2018pommerman,\\n  title={Pommerman: A multi-agent playground},\\n  author={Resnick, Cinjon and Eldridge, Wes and Ha, David and Britz, Denny and Foerster, Jakob and Togelius, Julian and Cho, Kyunghyun and Bruna, Joan},\\n  journal={arXiv preprint arXiv:1809.07124},\\n  year={2018}\\n}\\n\\n@inproceedings{zhou2018hybrid,\\n  title={A hybrid search agent in pommerman},\\n  author={Zhou, Hongwei and Gong, Yichen and Mugrai, Luvneesh and Khalifa, Ahmed and Nealen, Andy and Togelius, Julian},\\n  booktitle={Proceedings of the 13th International Conference on the Foundations of Digital Games (FDG)},\\n  pages={1--4},\\n  year={2018}\\n}\\n\\n\\n@inproceedings{Kostrikov2020Imitation,\\n    title={Imitation learning via off-Policy distribution matching},\\n    author={Ilya Kostrikov and Ofir Nachum and Jonathan Tompson},\\n    booktitle={Proceedings of the 8th International Conference on Learning Representations (ICLR)},\\n    year={2020}\\n}\\n\\n@inproceedings{fu2017learning,\\n    title={Learning robust rewards with adversarial inverse reinforcement learning},\\n    author={Fu, Justin and Luo, Katie and Levine, Sergey},\\n    booktitle={Proceedings of the 5th International Conference on Learning Representations (ICLR)},\\n    year={2017}\\n}\\n\\n@inproceedings{sasaki2018sample,\\n    title={Sample efficient imitation learning for continuous control},\\n    author={Sasaki, Fumihiro and Yohira, Tetsuya and Kawaguchi, Atsuo},\\n    booktitle={Proceedings of the 6th International Conference on Learning Representations (ICLR)},\\n    year={2018}\\n}\\n\\n@inproceedings{kostrikov2018discriminatoractorcritic,\\n    title={Discriminator-Actor-Critic: Addressing sample inefficiency and reward bias in adversarial imitation learning},\\n    author={Ilya Kostrikov and Kumar Krishna Agrawal and Debidatta Dwibedi and Sergey Levine and Jonathan Tompson},\\n    booktitle={Proceedings of the 7th International Conference on Learning Representations (ICLR)},\\n    year={2019}\\n}\\n\\n@inproceedings{degris2012off,\\n  title={Off-policy actor-critic},\\n  author={Degris, Thomas and White, Martha and Sutton, Richard S},\\n  booktitle={Proceedings of the 29th International Conference on Machine Learning (ICML)},\\n  pages={179--186},\\n  year={2012}\\n}\\n\\n\\n@article{haarnoja2018soft,\\n  title={Soft actor-critic algorithms and applications},\\n  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},\\n  journal={arXiv preprint arXiv:1812.05905},\\n  year={2018}\\n}\\n\\n@inproceedings{rezende2015variational,\\n  title={Variational Inference with Normalizing Flows},\\n  author={Rezende, Danilo and Mohamed, Shakir},\\n  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML)},\\n  pages={1530--1538},\\n  year={2015}\\n}\\n\\n@inproceedings{\\nnachum2018trustpcl,\\ntitle={Trust-{PCL}: An off-policy trust region method for continuous control},\\nauthor={Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},\\nbooktitle={Proceedings of the 6th International Conference on Learning Representations (ICLR)},\\nyear={2018}\\n}\\n\\n@article{hazan2018provably,\\n  title={Provably efficient maximum entropy exploration},\\n  author={Hazan, Elad and Kakade, Sham M and Singh, Karan and Van Soest, Abby},\\n  journal={arXiv preprint arXiv:1812.02690},\\n  year={2018}\\n}\\n\\n@article{pomerleau1991efficient,\\n  title={Efficient training of artificial neural networks for autonomous navigation},\\n  author={Pomerleau, Dean A},\\n  journal={Neural computation},\\n  volume={3},\\n  number={1},\\n  pages={88--97},\\n  year={1991},\\n  publisher={MIT Press}\\n}\\n\\n@inproceedings{gulrajani2017improved,\\n  title={Improved training of {W}asserstein {GAN}s},\\n  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},\\n  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},\\n  pages={5767--5777},\\n  year={2017}\\n}\\n\\n@inproceedings{nachum2019dualdice,\\n  title={Dual{DICE}: Behavior-agnostic estimation of discounted stationary distribution corrections},\\n  author={Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},\\n  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},\\n  pages={2318--2328},\\n  year={2019}\\n}\\n\\n@article{schulman2017proximal,\\n  title={Proximal policy optimization algorithms},\\n  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},\\n  journal={arXiv preprint arXiv:1707.06347},\\n  year={2017}\\n}\\n\\n@inproceedings{schulman2015trust,\\n  title={Trust region policy optimization},\\n  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},\\n  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML)},\\n  pages={1889--1897},\\n  year={2015}\\n}\\n\\n@inproceedings{ghasemipour2019divergence,\\n  title={A divergence minimization perspective on imitation learning methods},\\n  author={Ghasemipour, Seyed Kamyar Seyed and Zemel, Richard and Gu, Shixiang},\\n  booktitle={Proceedings of the 3rd Conference on Robot Learning (CoRL)},\\n  year={2019}\\n}\\n\\n@inproceedings{ross2010efficient,\\n  title={Efficient reductions for imitation learning},\\n  author={Ross, St{\\\\'e}phane and Bagnell, Drew},\\n  booktitle={Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},\\n  pages={661--668},\\n  year={2010}\\n}\\n\\n@article{schulman2015high,\\n  title={High-dimensional continuous control using generalized advantage estimation},\\n  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},\\n  journal={arXiv preprint arXiv:1506.02438},\\n  year={2015}\\n}\",\n",
       "  'arxiv_citations': {'1611.03852': True,\n",
       "   '1812.05905': True,\n",
       "   '1812.02690': True,\n",
       "   '1809.07124': True,\n",
       "   '1707.06347': True,\n",
       "   '1812.07297': True,\n",
       "   '1506.02438': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Learning human intent',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #110',\n",
       "   'newsletter_url': 'https://mailchi.mp/474d6a8a4527/an-110-learning-features-from-human-feedback-to-enable-reward-learning',\n",
       "   'summarizer': 'Sudhanshu',\n",
       "   'summary': \"This work aims to simplify algorithms for adversarial imitation learning by using a _structured_ discriminator, which is parameterised by the current generator and a learned policy. They prove that if so formulated, the policy that yields the optimal discriminator is exactly\\xa0the same as the policy that generated the expert data, which is also precisely what we hope the generator will learn. As long as the\\xa0discriminator's learned policy is parameterised correctly such that it can be sampled and evaluated, this eliminates the need for a reinforcement learning outer loop for policy improvement, as this learned\\xa0policy can be substituted in for the generator's policy in the next training iteration. They empirically show the competitiveness of their method with state-of-the-art algorithms across a small but increasingly complex suite of tasks.\",\n",
       "   'opinion': \"Since their theoretical results are only for optimal values, it's unclear whether starting from random initial policies will necessarily converge to these optimal values -- indeed, they make this point themselves, that they do not train to convergence as gradient descent cannot hope to find the global optimum for GAN-like non-convex loss functions. In light of that, it's not evident *why* their algorithms outperform the competition. Additionally, they do not report computational speed-up or wall-clock comparisons, which to me felt like the broad motivation behind this work. Nonetheless, the work illuminates new territory in adversarial imitation learning, provides positive evidence for a novel technique, and raises interesting questions for future work,\\xa0such as how to learn robust\\xa0reward functions via this method,\\xa0or what kind of convergence properties can be expected.\",\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '2006.13258v6',\n",
       "   'arxiv_id': '2006.13258',\n",
       "   'title': 'Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization',\n",
       "   'authors': ['Paul Barde',\n",
       "    'Julien Roy',\n",
       "    'Wonseok Jeon',\n",
       "    'Joelle Pineau',\n",
       "    'Christopher Pal',\n",
       "    'Derek Nowrouzezahrai'],\n",
       "   'date_published': '2020-06-23 18:29:13+00:00',\n",
       "   'data_last_modified': '2021-04-16 10:09:13+00:00',\n",
       "   'url': 'http://arxiv.org/abs/2006.13258v6',\n",
       "   'abstract': \"Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.\",\n",
       "   'author_comment': 'None',\n",
       "   'journal_ref': 'Advances in Neural Information Processing Systems 33 (2020)',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'cs.LG',\n",
       "   'categories': \"['cs.LG', 'cs.AI', 'stat.ML']\",\n",
       "   'individual_summary': \"Title: Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization\\nAuthors: Paul Barde*, Julien Roy*, Wonseok Jeon*, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai\\nPaper abstract: Adversarial Imitation Learning alternates between learning a discriminator -- which tells apart expert's demonstrations from generated ones -- and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.\\nSummary: This work aims to simplify algorithms for adversarial imitation learning by using a _structured_ discriminator, which is parameterised by the current generator and a learned policy. They prove that if so formulated, the policy that yields the optimal discriminator is exactly\\xa0the same as the policy that generated the expert data, which is also precisely what we hope the generator will learn. As long as the\\xa0discriminator's learned policy is parameterised correctly such that it can be sampled and evaluated, this eliminates the need for a reinforcement learning outer loop for policy improvement, as this learned\\xa0policy can be substituted in for the generator's policy in the next training iteration. They empirically show the competitiveness of their method with state-of-the-art algorithms across a small but increasingly complex suite of tasks.\\nMy opinion: Since their theoretical results are only for optimal values, it's unclear whether starting from random initial policies will necessarily converge to these optimal values -- indeed, they make this point themselves, that they do not train to convergence as gradient descent cannot hope to find the global optimum for GAN-like non-convex loss functions. In light of that, it's not evident *why* their algorithms outperform the competition. Additionally, they do not report computational speed-up or wall-clock comparisons, which to me felt like the broad motivation behind this work. Nonetheless, the work illuminates new territory in adversarial imitation learning, provides positive evidence for a novel technique, and raises interesting questions for future work,\\xa0such as how to learn robust\\xa0reward functions via this method,\\xa0or what kind of convergence properties can be expected.\",\n",
       "   'paper_text': '',\n",
       "   'text': \"HIGHLIGHTS\\n[Feature Expansive Reward Learning: Rethinking Human Input](https://arxiv.org/abs/2006.13208) *(Andreea Bobu, Marius Wiggert et al)* (summarized by Rohin): One goal we might have with our algorithms is that *after* training, when the AI system is deployed with end users, the system would be personalized to those end users. You might hope that we could use deep inverse RL algorithms like [AIRL](https://arxiv.org/abs/1710.11248) ([AN #17](https://mailchi.mp/ad852629e45a/alignment-newsletter-17))), but unfortunately they require a lot of data, which isnt feasible for end users. You could use earlier IRL algorithms like [MCEIRL](http://www.cs.cmu.edu/~bziebart/publications/maximum-causal-entropy.pdf) ([AN #12](https://mailchi.mp/bcb2c6f1d507/alignment-newsletter-12)) that require you to specify what features of the environment you care about, but in practice youll never successfully write down all of these features. Can we somehow get the best of both worlds?[Past work](https://arxiv.org/abs/1810.05157) ([AN #28](https://mailchi.mp/df2e472140b6/alignment-newsletter-28)) made progress on this front, by allowing the agent to at least *detect* when it is missing some feature, by checking whether the human feedback is surprisingly inefficient given the existing features. But what do you do once you detect it? The key insight of this paper is that applying a deep IRL algorithm here would be inefficient because it has to implicitly learn the unknown feature, and we can do much better by explicitly querying the human for the unknown feature.In particular, their method Feature Expansive Reward Learning (FERL) asks the human for a few *feature traces*: demonstrations in which the new features value monotonically decreases. For example, suppose a robot arm carrying a cup of water gets too close to a laptop, but the arm doesnt know the feature close to a laptop. Then a feature trace would start with the arm close to the laptop, and move it successively further away. Given a set of feature traces, we can convert this into a dataset of noisy comparisons, where earlier states are more likely to have higher feature values than later states, and use this to train a neural net to predict the feature value (similarly to the reward model in [Deep RL from Human Preferences](https://deepmind.com/blog/learning-through-human-feedback/)). We can then add this to our set of features, and learn rewards over the new set of features.They evaluate their method with a few human-robot interaction scenarios (though without a user study due to COVID), comparing it against deep MaxEnt IRL, and find that their method does better on a variety of metrics. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** I really liked this paper -- it seems like a far more efficient use of human feedback to figure out what *features* of the environment are important. This doesnt need to be limited to reward learning: I expect that learning the right features to focus on would help with exploration in reinforcement learning, out-of-distribution generalization, etc. It also seems plausible that in more complex environments you could learn a set of features that was useful for all of these tasks, thus being somewhat general (though still specific to the environment).Its worth noting that in this setting you wouldnt really want to use a vanilla deep IRL algorithm -- youd instead want to do something like [meta-IRL](https://arxiv.org/abs/1805.08882). |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n TECHNICAL AI ALIGNMENT\\n\\ufeff\\n\\n ITERATED AMPLIFICATION\\n[Parallels Between AI Safety by Debate and Evidence Law](https://www.alignmentforum.org/posts/SrsH2MyyH8MqH9QSr/parallels-between-ai-safety-by-debate-and-evidence-law) *(Cullen O'Keefe)* (summarized by Rohin): [Debate](https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1) ([AN #86](https://mailchi.mp/598f425b1533/an-86-improving-debate-and-factored-cognition-through-human-experiments)) requires us to provide a structure for a debate as well as rules for how the human judge should decide who wins. This post points out that we have an existing system that has been heavily optimized for this already: evidence law, which governs how court cases are run. A court case is high-stakes and involves two sides presenting opposing opinions; evidence law tells us how to structure these arguments and how to limit the kinds of arguments debaters can use. Evidence is generally admissible by default, but there are many exceptions, often based on the fallibility of fact-finders.As a result, it may be fruitful to look to evidence law for how we might structure debates, and to see what types of arguments we should be looking for. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** This seems eminently sensible to me. Of course, evidence law is going to be specialized to arguments about innocence or guilt of a crime, and may not generalize to what we would like to do with debate, but it still seems like we should be able to learn some generalizable lessons. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Weak HCH accesses EXP](https://www.alignmentforum.org/posts/CtGH3yEoo4mY2taxe/weak-hch-accesses-exp) *(Evan Hubinger)* (summarized by Rohin): This followup to last week's [Alignment proposals and complexity classes](https://www.alignmentforum.org/posts/N64THGX7XNCqRtvPG/alignment-proposals-and-complexity-classes) ([AN #109](https://mailchi.mp/ee62c1c9e331/an-109teaching-neural-nets-to-generalize-the-way-humans-would)) shows that the amplification-based proposals can access EXP.\\ufeff\\n\\n LEARNING HUMAN INTENT\\n[Multi-Principal Assistance Games](http://arxiv.org/abs/2007.09540) *(Arnaud Fickinger et al)* (summarized by Rohin): So far the work in the [assistance games framework](https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem-ebook/dp/B07N5J5FTS) ([AN #69](https://mailchi.mp/59ddebcb3b9a/an-69-stuart-russells-new-book-on-why-we-need-to-replace-the-standard-model-of-ai)) (previously called CIRL) has focused on the case where there is a single human and a single AI assistant. Once we have multiple humans (or *principals*, as the paper calls them), things get much trickier.One problem is that we dont know how to aggregate the values across different principals. Rather than taking a stance on the problem, this paper assumes that we have some mechanism that can combine reward functions in some reasonable way. It instead focuses on a second problem: while previously we could trust the human to report their preferences accurately (as the human and agent were aligned), when there are multiple principals whose preference will be aggregated, the principals have an incentive to misrepresent their preferences (which well call non-straightforward play).Lets consider the case where the principals provide demonstrations, *and get reward for those demonstrations*. For now our agent will assume that the principals are playing straightforwardly, and so the agent simply infers their preferences, aggregates them, and optimizes the results. In this setting, if the agent will act far more often than the principals provide demonstrations (so that the reward of the demonstrations is almost irrelevant), we can apply the Gibbard-Satterthwaite theorem to show that any non-trivial mechanism will be vulnerable to non-straightforward play. In contrast, if the principals provide lots of demonstrations, while the agent only acts for a short period of time, then optimal principals primarily want to ensure their demonstrations are good, and so will be straightforward most of the time (provably). In the middle, the fact that principals get rewarded for demonstrations does help reduce non-straightforward play, but does not eliminate it.Now lets consider the case where the agent can design a mechanism. Here, when the principals are providing demonstrations, the agent can override their action choice with one of its own (a setting considered [previously](https://arxiv.org/abs/1901.08654) ([AN #70](https://mailchi.mp/732eaa192df0/an-70-agents-that-help-humans-who-are-still-learning-about-their-own-preferences))). Roughly speaking, the algorithm only executes a proposed human action if it hasnt executed it before. By doing so, it incentivizes the principals to report second-best actions, and so on, giving the agent more information about the principals' utility functions. The mechanism incentivizes straightforward play, and is approximately efficient (i.e. there is an upper bound on the worst case social welfare achieved). |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** According to me, the main insight of this paper is that it is both necessary and difficult to design mechanisms that incentivize principals to report not just the best thing to do, but a comparison amongst different alternatives. Within the formalism of paper, this is done by overriding a principals action unless it is a novel action, but I expect in practice well do this in some other way (it seems rather unusual to imagine the agent overriding a human, Id be surprised if that was how we ended up building our AI systems). |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization](http://arxiv.org/abs/2006.13258) *(Paul Barde, Julien Roy, Wonseok Jeon et al)* (summarized by Sudhanshu): This work aims to simplify algorithms for adversarial imitation learning by using a *structured* discriminator, which is parameterised by the current generator and a learned policy. They prove that if so formulated, the policy that yields the optimal discriminator is exactly\\xa0the same as the policy that generated the expert data, which is also precisely what we hope the generator will learn. As long as the\\xa0discriminator's learned policy is parameterised correctly such that it can be sampled and evaluated, this eliminates the need for a reinforcement learning outer loop for policy improvement, as this learned\\xa0policy can be substituted in for the generator's policy in the next training iteration. They empirically show the competitiveness of their method with state-of-the-art algorithms across a small but increasingly complex suite of tasks. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Sudhanshu's opinion:** Since their theoretical results are only for optimal values, it's unclear whether starting from random initial policies will necessarily converge to these optimal values -- indeed, they make this point themselves, that they do not train to convergence as gradient descent cannot hope to find the global optimum for GAN-like non-convex loss functions. In light of that, it's not evident *why* their algorithms outperform the competition. Additionally, they do not report computational speed-up or wall-clock comparisons, which to me felt like the broad motivation behind this work. Nonetheless, the work illuminates new territory in adversarial imitation learning, provides positive evidence for a novel technique, and raises interesting questions for future work,\\xa0such as how to learn robust\\xa0reward functions via this method,\\xa0or what kind of convergence properties can be expected. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning](http://arxiv.org/abs/2006.14804) *(Lin Guan, Mudit Verma et al)* (summarized by Rohin): This paper starts from a similar position as the highlighted paper: that we can improve on algorithms by having humans provide different kinds of feedback that help with learning. They ask humans to provide explanations to improve sample efficiency in deep RL, which in this case means asking a human to segment parts of the image observation that are important (similar to a saliency map). They use this to define auxiliary losses that incentivize the agent to be invariant to augmentations of the irrelevant parts of the image. Their empirical evaluation shows improvements in sample efficiency relative to simple good/bad evaluative feedback. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** The idea is cool, but the empirical results are not great. On Taxi, training with the reward signal and binary good/bad evaluative feedback takes 180k environment steps, and adding in explanations for a quarter of the steps brings it down to 130k environment steps. However, this seems like it would increase the human effort required by an order of magnitude or more, which seems way too high for the benefit provided.It does seem to me that saliency explanations could contain a fair amount of information, and so you should be able to do better -- maybe a future algorithm will do so. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n FORECASTING\\n[Alignment As A Bottleneck To Usefulness Of GPT-3](https://www.alignmentforum.org/posts/BnDF5kejzQLqd5cjH/alignment-as-a-bottleneck-to-usefulness-of-gpt-3) *(John S. Wentworth)* (summarized by Rohin): Currently, [many people](https://twitter.com/xuenay/status/1283312640199196673) are trying to figure out how to prompt GPT-3 into doing what they want -- in other words, how to align GPT-3 with their desires. GPT-3 may be capable of the task, but that doesnt mean it will do it ([potential example](https://www.lesswrong.com/posts/H9knnv8BWGKj6dZim/usd1000-bounty-for-openai-to-show-whether-gpt3-was)). This suggests that alignment will soon be a bottleneck on our ability to get value from large language models.Certainly GPT-3 isnt perfectly capable yet. The author thinks that in the immediate future the major bottleneck will still be its capability, but we have a clear story for how to improve its capabilities: just scale up the model and data even more. Alignment on the other hand is much harder: we dont know how to [translate](https://www.alignmentforum.org/posts/42YykiTqtGMyJAjDM/alignment-as-translation) ([AN #94](https://mailchi.mp/0f80d83f9c44/an-94ai-alignment-as-translation-between-humans-and-machines)) the tasks we want into a format that will cause GPT-3 to try to accomplish that task.As a result, in the future we might expect a lot more work to go into prompt design (or whatever becomes the next way to direct language models at specific tasks). In addition, once GPT is better than humans (at least in some domains), alignment in those domains will be particularly difficult, as it is unclear how you would get a system trained to mimic humans [to do better than humans](https://www.alignmentforum.org/posts/h9DesGT3WT9u2k7Hr/the-easy-goal-inference-problem-is-still-hard) ([AN #31](https://mailchi.mp/7d0e3916e3d9/alignment-newsletter-31)). |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** The general point of this post seems clearly correct and worth pointing out. Im looking forward to the work well see in the future figuring out how to apply these broad and general methods to real tasks in a reliable way. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n MISCELLANEOUS (ALIGNMENT)\\n[Generalizing the Power-Seeking Theorems](https://www.alignmentforum.org/posts/nyDnLif4cjeRe9DSv/generalizing-the-power-seeking-theorems) *(Alex Turner)* (summarized by Rohin): [Previously](https://www.alignmentforum.org/posts/6DuJxY8X45Sco4bS2/seeking-power-is-provably-instrumentally-convergent-in-mdps) ([AN #78](https://mailchi.mp/eef1d6c95d7c/an-78formalizing-power-and-instrumental-convergence-and-the-end-of-year-ai-safety-charity-comparison)) weve seen that if we take an MDP, and have a distribution over state-based reward functions, such that the reward for two different states is iid, then farsighted (i.e. no discount) optimal agents tend to seek power. This post relaxes some of these requirements, giving sufficient (but not necessary) criteria for determining instrumental convergence.Some of these use a new kind of argument. Suppose that action A leads you to a part of the MDP modeled by a graph G1, and B leads you to a part of the MDP modeled by a graph G2. If there is a subgraph of G2 that is isomorphic to G1, then we know that whatever kinds of choices the agent would have by taking action A, the agent would also have those choices from action B, and so we know B is at least as likely as A. This matches our intuitive reasoning -- collecting resources is instrumentally convergent because you can do the same things that you could if you didnt collect resources, as well as some additional things enabled by your new resources. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n AI STRATEGY AND POLICY\\n[AI Benefits](https://cullenokeefe.com/ai-benefits-index) *(Cullen O'Keefe)* (summarized by Rohin): This sequence of posts investigates *AI Benefits*: how a benefactor can leverage advanced AI systems to benefit humanity. It focuses on what can be done by a single benefactor, outside of what we might think of as the norm -- in particular, the sequence ignores benefits that would be provided by default market incentives. This is relevant to OpenAI (where the author works) given their focus on ensuring AI is beneficial to humanity.Note that AI Benefits is distinct from AI alignment. Sometimes AI alignment is defined broadly enough to encompass AI Benefits, but often it is not, e.g. if the notion of being aligned depends on an AI system being aligned with some principal, that would not be AI Benefits, since AI Benefits are meant to accrue to all of humanity. While it is about maximizing well-being by default, it should also have secondary goals of equality, autonomy, democratization, and epistemic modesty.The obvious approach to AI Benefits is the *direct* approach: figuring out how to apply advanced AI to directly generate benefits for humanity, e.g. by producing electricity more efficiently to mitigate climate change. However, it is important to also consider the indirect approach of making money using AI, and then donating the surplus to a different organization that can better produce benefits.Given the massive number of potential ways to benefit humanity and our uncertainty about their efficacy, it is important to have a portfolio approach to AI Benefits, rather than scaling up a single intervention. In addition, since any given intervention will probably primarily benefit some subset of humanity, a portfolio approach should help lead to more equal distribution of benefits.There are many outstanding questions on how AI Benefits should be done in practice. Should the benefactor pursue a direct or indirect approach? To what extent should they explore potential approaches for generating benefits, relative to exploiting approaches that we know work? Should they generate benefits now, or invest in the ability to generate benefits later? Should they focus on global (supranational) approaches, or allocate resources to each nation that can be used in a manner specialized to their citizens?There are many questions on the governance side as well. We will presumably want some Benefits Group involving external experts to help distribute benefits optimally. When should such a group get democratic input? How do we evaluate such a group to ensure they are actually benefiting humanity optimally? To what extent will we also need internal governance within the group and benefactor, and how can this be done? |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** AI Benefits is effectively asking how we can answer the question of how to do the most good in the future, and as such many of the considerations also come up in effective altruism, especially at the current high level of abstraction. Nonetheless, there are differences in the situation, which will matter: for example, the effective altruism community does not currently need to plan for the situation where they control a majority of the worlds resources; a sufficiently ambitious and optimistic AI company may need to. Such a situation vastly increases the importance of e.g. democratic input, portfolio approaches, and information value. Im glad that these questions are being tackled now and look forward to seeing more details in the future. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n OTHER PROGRESS IN AI\\n\\ufeff\\n\\n REINFORCEMENT LEARNING\\n[An Optimistic Perspective on Offline Reinforcement Learning](https://arxiv.org/abs/1907.04543) *(Rishabh Agarwal et al)* (summarized by Zach): Off-policy reinforcement learning (RL) that can be done using offline-logged interactions is an important aspect of real-world applications. However, most RL algorithms assume that an agent interacts with an online environment or simulator and learns from its own collected experience. Moreover, the authors show that DQN trained offline on its *own* experience replay buffer has markedly decreased performance on most of the Atari suite. The authors attempt to address this discrepancy by introducing a robust Q-learning algorithm that randomly mixes estimates for particular Q-values. Specifically, by creating convex combinations from an underlying basis of Q-value estimates the authors are able to create a much larger ensemble. This is similar in spirit to dropout in deep learning where connections in the network are randomly turned off. The authors then go on to show that offline DQN is feasible by training this algorithm and other related algorithms on the DQN Replay Dataset and show it has comparable performance to, and occasionally even surpasses, the original RL baselines. The DQN Replay Dataset is released at <https://offline-rl.github.io/>. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Zach's opinion:** What I learned from this paper is that estimating the mean Q-value is not always enough for robustness. By leveraging distributional information, via ensembles or quantiles, these methods can become quite effective at offline DQN. The release of the dataset is also impressive. I think the dataset will have broad applicability to researchers interested in offline RL as well as imitation learning.  |\\n\\n |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI'm always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2020 Alignment Newsletter, All rights reserved.*\\n\\n**\"}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2011.05623v3',\n",
       "  'title': 'Fooling the primate brain with minimal, targeted image manipulation',\n",
       "  'authors': ['Li Yuan',\n",
       "   'Will Xiao',\n",
       "   'Giorgia Dellaferrera',\n",
       "   'Gabriel Kreiman',\n",
       "   'Francis E. H. Tay',\n",
       "   'Jiashi Feng',\n",
       "   'Margaret S. Livingstone'],\n",
       "  'date_published': '2020-11-11 08:30:54+00:00',\n",
       "  'data_last_modified': '2022-03-30 05:36:53+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2011.05623v3',\n",
       "  'abstract': \"Artificial neural networks (ANNs) are considered the current best models of biological vision. ANNs are the best predictors of neural activity in the ventral stream; moreover, recent work has demonstrated that ANN models fitted to neuronal activity can guide the synthesis of images that drive pre-specified response patterns in small neuronal populations. Despite the success in predicting and steering firing activity, these results have not been connected with perceptual or behavioral changes. Here we propose an array of methods for creating minimal, targeted image perturbations that lead to changes in both neuronal activity and perception as reflected in behavior. We generated 'deceptive images' of human faces, monkey faces, and noise patterns so that they are perceived as a different, pre-specified target category, and measured both monkey neuronal responses and human behavior to these images. We found several effective methods for changing primate visual categorization that required much smaller image change compared to untargeted noise. Our work shares the same goal with adversarial attack, namely the manipulation of images with minimal, targeted noise that leads ANN models to misclassify the images. Our results represent a valuable step in quantifying and characterizing the differences in perturbation robustness of biological and artificial vision.\",\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'q-bio.NC',\n",
       "  'categories': ['q-bio.NC', 'cs.CV', 'cs.NE', 'eess.IV'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': '',\n",
       "  'text': '',\n",
       "  'bibliography_bbl': \"\\\\begin{thebibliography}{10}\\n\\\\expandafter\\\\ifx\\\\csname url\\\\endcsname\\\\relax\\n  \\\\def\\\\url#1{\\\\texttt{#1}}\\\\fi\\n\\\\expandafter\\\\ifx\\\\csname urlprefix\\\\endcsname\\\\relax\\\\def\\\\urlprefix{URL }\\\\fi\\n\\\\providecommand{\\\\bibinfo}[2]{#2}\\n\\\\providecommand{\\\\eprint}[2][]{\\\\url{#2}}\\n\\n\\\\bibitem{he2015delving}\\n\\\\bibinfo{author}{He, K.}, \\\\bibinfo{author}{Zhang, X.}, \\\\bibinfo{author}{Ren,\\n  S.} \\\\& \\\\bibinfo{author}{Sun, J.}\\n\\\\newblock \\\\bibinfo{title}{Delving deep into rectifiers: Surpassing human-level\\n  performance on imagenet classification}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the IEEE international\\n  conference on computer vision}}, \\\\bibinfo{pages}{1026--1034}\\n  (\\\\bibinfo{year}{2015}).\\n\\n\\\\bibitem{lecun2015deep}\\n\\\\bibinfo{author}{LeCun, Y.}, \\\\bibinfo{author}{Bengio, Y.} \\\\&\\n  \\\\bibinfo{author}{Hinton, G.}\\n\\\\newblock \\\\bibinfo{title}{Deep learning}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{nature}} \\\\textbf{\\\\bibinfo{volume}{521}},\\n  \\\\bibinfo{pages}{436--444} (\\\\bibinfo{year}{2015}).\\n\\n\\\\bibitem{yamins2014performance}\\n\\\\bibinfo{author}{Yamins, D.~L.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Performance-optimized hierarchical models predict\\n  neural responses in higher visual cortex}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Proceedings of the National Academy of\\n  Sciences}} \\\\textbf{\\\\bibinfo{volume}{111}}, \\\\bibinfo{pages}{8619--8624}\\n  (\\\\bibinfo{year}{2014}).\\n\\n\\\\bibitem{cadena2019deep}\\n\\\\bibinfo{author}{Cadena, S.~A.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Deep convolutional models improve predictions of\\n  macaque v1 responses to natural images}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{PLoS computational biology}}\\n  \\\\textbf{\\\\bibinfo{volume}{15}}, \\\\bibinfo{pages}{e1006897}\\n  (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{szegedy2013intriguing}\\n\\\\bibinfo{author}{Szegedy, C.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Intriguing properties of neural networks}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{arXiv preprint arXiv:1312.6199}}\\n  (\\\\bibinfo{year}{2013}).\\n\\n\\\\bibitem{goodfellow2014explaining}\\n\\\\bibinfo{author}{Goodfellow, I.~J.}, \\\\bibinfo{author}{Shlens, J.} \\\\&\\n  \\\\bibinfo{author}{Szegedy, C.}\\n\\\\newblock \\\\bibinfo{title}{Explaining and harnessing adversarial examples}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{arXiv preprint arXiv:1412.6572}}\\n  (\\\\bibinfo{year}{2014}).\\n\\n\\\\bibitem{elsayed2018adversarial}\\n\\\\bibinfo{author}{Elsayed, G.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Adversarial examples that fool both computer vision\\n  and time-limited humans}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Advances in Neural Information\\n  Processing Systems}}, \\\\bibinfo{pages}{3910--3920} (\\\\bibinfo{year}{2018}).\\n\\n\\\\bibitem{zhou2019humans}\\n\\\\bibinfo{author}{Zhou, Z.} \\\\& \\\\bibinfo{author}{Firestone, C.}\\n\\\\newblock \\\\bibinfo{title}{Humans can decipher adversarial images}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature communications}}\\n  \\\\textbf{\\\\bibinfo{volume}{10}}, \\\\bibinfo{pages}{1--9} (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{berardino2017eigen}\\n\\\\bibinfo{author}{Berardino, A.}, \\\\bibinfo{author}{Laparra, V.},\\n  \\\\bibinfo{author}{Ball{\\\\'e}, J.} \\\\& \\\\bibinfo{author}{Simoncelli, E.}\\n\\\\newblock \\\\bibinfo{title}{Eigen-distortions of hierarchical representations}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Advances in neural information\\n  processing systems}}, \\\\bibinfo{pages}{3530--3539} (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{klindt2017neural}\\n\\\\bibinfo{author}{Klindt, D.}, \\\\bibinfo{author}{Ecker, A.~S.},\\n  \\\\bibinfo{author}{Euler, T.} \\\\& \\\\bibinfo{author}{Bethge, M.}\\n\\\\newblock \\\\bibinfo{title}{Neural system identification for large populations\\n  separating what and where}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Advances in Neural Information\\n  Processing Systems}}, \\\\bibinfo{pages}{3506--3516} (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{he2016deep}\\n\\\\bibinfo{author}{He, K.}, \\\\bibinfo{author}{Zhang, X.}, \\\\bibinfo{author}{Ren,\\n  S.} \\\\& \\\\bibinfo{author}{Sun, J.}\\n\\\\newblock \\\\bibinfo{title}{Deep residual learning for image recognition}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the IEEE conference on\\n  computer vision and pattern recognition}}, \\\\bibinfo{pages}{770--778}\\n  (\\\\bibinfo{year}{2016}).\\n\\n\\\\bibitem{schrimpf2018brain}\\n\\\\bibinfo{author}{Schrimpf, M.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Brain-score: Which artificial neural network for\\n  object recognition is most brain-like?}\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{BioRxiv}} \\\\bibinfo{pages}{407007}\\n  (\\\\bibinfo{year}{2018}).\\n\\n\\\\bibitem{moeller2008patches}\\n\\\\bibinfo{author}{Moeller, S.}, \\\\bibinfo{author}{Freiwald, W.~A.} \\\\&\\n  \\\\bibinfo{author}{Tsao, D.~Y.}\\n\\\\newblock \\\\bibinfo{title}{Patches with links: a unified system for processing\\n  faces in the macaque temporal lobe}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Science}} \\\\textbf{\\\\bibinfo{volume}{320}},\\n  \\\\bibinfo{pages}{1355--1359} (\\\\bibinfo{year}{2008}).\\n\\n\\\\bibitem{deng2009imagenet}\\n\\\\bibinfo{author}{Deng, J.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Imagenet: A large-scale hierarchical image database}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{2009 IEEE conference on computer vision\\n  and pattern recognition}}, \\\\bibinfo{pages}{248--255}\\n  (\\\\bibinfo{organization}{Ieee}, \\\\bibinfo{year}{2009}).\\n\\n\\\\bibitem{konkle2010conceptual}\\n\\\\bibinfo{author}{Konkle, T.}, \\\\bibinfo{author}{Brady, T.~F.},\\n  \\\\bibinfo{author}{Alvarez, G.~A.} \\\\& \\\\bibinfo{author}{Oliva, A.}\\n\\\\newblock \\\\bibinfo{title}{Conceptual distinctiveness supports detailed visual\\n  long-term memory for real-world objects}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Journal of experimental Psychology: general}}\\n  \\\\textbf{\\\\bibinfo{volume}{139}}, \\\\bibinfo{pages}{558} (\\\\bibinfo{year}{2010}).\\n\\n\\\\bibitem{ma2015chicago}\\n\\\\bibinfo{author}{Ma, D.~S.}, \\\\bibinfo{author}{Correll, J.} \\\\&\\n  \\\\bibinfo{author}{Wittenbrink, B.}\\n\\\\newblock \\\\bibinfo{title}{The chicago face database: A free stimulus set of\\n  faces and norming data}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Behavior research methods}}\\n  \\\\textbf{\\\\bibinfo{volume}{47}}, \\\\bibinfo{pages}{1122--1135}\\n  (\\\\bibinfo{year}{2015}).\\n\\n\\\\bibitem{tsao2006cortical}\\n\\\\bibinfo{author}{Tsao, D.~Y.}, \\\\bibinfo{author}{Freiwald, W.~A.},\\n  \\\\bibinfo{author}{Tootell, R.~B.} \\\\& \\\\bibinfo{author}{Livingstone, M.~S.}\\n\\\\newblock \\\\bibinfo{title}{A cortical region consisting entirely of\\n  face-selective cells}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Science}} \\\\textbf{\\\\bibinfo{volume}{311}},\\n  \\\\bibinfo{pages}{670--674} (\\\\bibinfo{year}{2006}).\\n\\n\\\\bibitem{mcinnes2018umap}\\n\\\\bibinfo{author}{McInnes, L.}, \\\\bibinfo{author}{Healy, J.} \\\\&\\n  \\\\bibinfo{author}{Melville, J.}\\n\\\\newblock \\\\bibinfo{title}{{UMAP}: Uniform manifold approximation and projection\\n  for dimension reduction}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{arXiv preprint arXiv:1802.03426}}\\n  (\\\\bibinfo{year}{2018}).\\n\\n\\\\bibitem{zhang2019theoretically}\\n\\\\bibinfo{author}{Zhang, H.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Theoretically principled trade-off between robustness\\n  and accuracy}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{arXiv preprint arXiv:1901.08573}}\\n  (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{tramer2017ensemble}\\n\\\\bibinfo{author}{Tram{\\\\`e}r, F.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Ensemble adversarial training: Attacks and defenses}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{arXiv preprint arXiv:1705.07204}}\\n  (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{yamins2016goal}\\n\\\\bibinfo{author}{Yamins, D.~L.} \\\\& \\\\bibinfo{author}{DiCarlo, J.~J.}\\n\\\\newblock \\\\bibinfo{title}{Using goal-driven deep learning models to understand\\n  sensory cortex}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature neuroscience}}\\n  \\\\textbf{\\\\bibinfo{volume}{19}}, \\\\bibinfo{pages}{356} (\\\\bibinfo{year}{2016}).\\n\\n\\\\bibitem{serre2019deep}\\n\\\\bibinfo{author}{Serre, T.}\\n\\\\newblock \\\\bibinfo{title}{Deep learning: the good, the bad, and the ugly}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Annual Review of Vision Science}}\\n  \\\\textbf{\\\\bibinfo{volume}{5}}, \\\\bibinfo{pages}{399--426}\\n  (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{ilyas2019adversarial}\\n\\\\bibinfo{author}{Ilyas, A.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Adversarial examples are not bugs, they are\\n  features}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Advances in Neural Information\\n  Processing Systems}}, \\\\bibinfo{pages}{125--136} (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{xie2020adversarial}\\n\\\\bibinfo{author}{Xie, C.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Adversarial examples improve image recognition}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the IEEE/CVF Conference\\n  on Computer Vision and Pattern Recognition}}, \\\\bibinfo{pages}{819--828}\\n  (\\\\bibinfo{year}{2020}).\\n\\n\\\\bibitem{bracci2019ventral}\\n\\\\bibinfo{author}{Bracci, S.}, \\\\bibinfo{author}{Ritchie, J.~B.},\\n  \\\\bibinfo{author}{Kalfas, I.} \\\\& \\\\bibinfo{author}{de~Beeck, H. P.~O.}\\n\\\\newblock \\\\bibinfo{title}{The ventral visual pathway represents animal\\n  appearance over animacy, unlike human behavior and deep neural networks}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Journal of Neuroscience}}\\n  \\\\textbf{\\\\bibinfo{volume}{39}}, \\\\bibinfo{pages}{6513--6525}\\n  (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{wardle2020rapid}\\n\\\\bibinfo{author}{Wardle, S.~G.}, \\\\bibinfo{author}{Taubert, J.},\\n  \\\\bibinfo{author}{Teichmann, L.} \\\\& \\\\bibinfo{author}{Baker, C.~I.}\\n\\\\newblock \\\\bibinfo{title}{Rapid and dynamic processing of face pareidolia in\\n  the human brain}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature Communications}}\\n  \\\\textbf{\\\\bibinfo{volume}{11}}, \\\\bibinfo{pages}{1--14} (\\\\bibinfo{year}{2020}).\\n\\n\\\\bibitem{bao2020map}\\n\\\\bibinfo{author}{Bao, P.}, \\\\bibinfo{author}{She, L.}, \\\\bibinfo{author}{McGill,\\n  M.} \\\\& \\\\bibinfo{author}{Tsao, D.~Y.}\\n\\\\newblock \\\\bibinfo{title}{A map of object space in primate inferotemporal\\n  cortex}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature}} \\\\bibinfo{pages}{1--6}\\n  (\\\\bibinfo{year}{2020}).\\n\\n\\\\bibitem{bashivan2019neural}\\n\\\\bibinfo{author}{Bashivan, P.}, \\\\bibinfo{author}{Kar, K.} \\\\&\\n  \\\\bibinfo{author}{DiCarlo, J.~J.}\\n\\\\newblock \\\\bibinfo{title}{Neural population control via deep image synthesis}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Science}} \\\\textbf{\\\\bibinfo{volume}{364}},\\n  \\\\bibinfo{pages}{eaav9436} (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{walker2019inception}\\n\\\\bibinfo{author}{Walker, E.~Y.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Inception loops discover what excites neurons most\\n  using deep predictive models}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature neuroscience}}\\n  \\\\textbf{\\\\bibinfo{volume}{22}}, \\\\bibinfo{pages}{2060--2065}\\n  (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{sheinberg1997role}\\n\\\\bibinfo{author}{Sheinberg, D.~L.} \\\\& \\\\bibinfo{author}{Logothetis, N.~K.}\\n\\\\newblock \\\\bibinfo{title}{The role of temporal cortical areas in perceptual\\n  organization}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Proceedings of the National Academy of\\n  Sciences}} \\\\textbf{\\\\bibinfo{volume}{94}}, \\\\bibinfo{pages}{3408--3413}\\n  (\\\\bibinfo{year}{1997}).\\n\\n\\\\bibitem{afraz2006microstimulation}\\n\\\\bibinfo{author}{Afraz, S.-R.}, \\\\bibinfo{author}{Kiani, R.} \\\\&\\n  \\\\bibinfo{author}{Esteky, H.}\\n\\\\newblock \\\\bibinfo{title}{Microstimulation of inferotemporal cortex influences\\n  face categorization}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature}} \\\\textbf{\\\\bibinfo{volume}{442}},\\n  \\\\bibinfo{pages}{692--695} (\\\\bibinfo{year}{2006}).\\n\\n\\\\bibitem{rajalingham2019reversible}\\n\\\\bibinfo{author}{Rajalingham, R.} \\\\& \\\\bibinfo{author}{DiCarlo, J.~J.}\\n\\\\newblock \\\\bibinfo{title}{Reversible inactivation of different millimeter-scale\\n  regions of primate it results in different patterns of core object\\n  recognition deficits}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Neuron}} \\\\textbf{\\\\bibinfo{volume}{102}},\\n  \\\\bibinfo{pages}{493--505} (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{iaccarino2016gamma}\\n\\\\bibinfo{author}{Iaccarino, H.~F.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Gamma frequency entrainment attenuates amyloid load\\n  and modifies microglia}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature}} \\\\textbf{\\\\bibinfo{volume}{540}},\\n  \\\\bibinfo{pages}{230--235} (\\\\bibinfo{year}{2016}).\\n\\n\\\\bibitem{mahendran2016visualizing}\\n\\\\bibinfo{author}{Mahendran, A.} \\\\& \\\\bibinfo{author}{Vedaldi, A.}\\n\\\\newblock \\\\bibinfo{title}{Visualizing deep convolutional neural networks using\\n  natural pre-images}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{International Journal of Computer Vision}}\\n  \\\\textbf{\\\\bibinfo{volume}{120}}, \\\\bibinfo{pages}{233--255}\\n  (\\\\bibinfo{year}{2016}).\\n\\n\\\\bibitem{papernot2017practical}\\n\\\\bibinfo{author}{Papernot, N.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Practical black-box attacks against machine\\n  learning}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the 2017 ACM on Asia\\n  conference on computer and communications security}},\\n  \\\\bibinfo{pages}{506--519} (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{liu2017delving}\\n\\\\bibinfo{author}{Liu, Y.}, \\\\bibinfo{author}{Chen, X.}, \\\\bibinfo{author}{Liu,\\n  C.} \\\\& \\\\bibinfo{author}{Song, D.}\\n\\\\newblock \\\\bibinfo{title}{Delving into transferable adversarial examples and\\n  black-box attacks}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the International\\n  Conference on Learning Representations (ICLR)}} (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{chen2017zoo}\\n\\\\bibinfo{author}{Chen, P.-Y.}, \\\\bibinfo{author}{Zhang, H.},\\n  \\\\bibinfo{author}{Sharma, Y.}, \\\\bibinfo{author}{Yi, J.} \\\\&\\n  \\\\bibinfo{author}{Hsieh, C.-J.}\\n\\\\newblock \\\\bibinfo{title}{Zoo: Zeroth order optimization based black-box\\n  attacks to deep neural networks without training substitute models}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the 10th ACM Workshop on\\n  Artificial Intelligence and Security}}, \\\\bibinfo{pages}{15--26}\\n  (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{ilyas2018black}\\n\\\\bibinfo{author}{Ilyas, A.}, \\\\bibinfo{author}{Engstrom, L.},\\n  \\\\bibinfo{author}{Athalye, A.} \\\\& \\\\bibinfo{author}{Lin, J.}\\n\\\\newblock \\\\bibinfo{title}{Black-box adversarial attacks with limited queries\\n  and information}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{arXiv preprint arXiv:1804.08598}}\\n  (\\\\bibinfo{year}{2018}).\\n\\n\\\\bibitem{narodytska2017simple}\\n\\\\bibinfo{author}{{Narodytska}, N.} \\\\& \\\\bibinfo{author}{{Kasiviswanathan}, S.}\\n\\\\newblock \\\\bibinfo{title}{Simple black-box adversarial attacks on deep neural\\n  networks}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{2017 IEEE Conference on Computer Vision\\n  and Pattern Recognition Workshops (CVPRW)}}, \\\\bibinfo{pages}{1310--1318}\\n  (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{goodfellow2017attacking}\\n\\\\bibinfo{author}{Goodfellow, I.} \\\\& \\\\bibinfo{author}{Papernot, N.}\\n\\\\newblock \\\\bibinfo{title}{Is attacking machine learning easier than defending\\n  it}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Blog post on Feb}}\\n  \\\\textbf{\\\\bibinfo{volume}{15}}, \\\\bibinfo{pages}{2017} (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{tramer2020fundamental}\\n\\\\bibinfo{author}{Tram{\\\\`e}r, F.}, \\\\bibinfo{author}{Behrmann, J.},\\n  \\\\bibinfo{author}{Carlini, N.}, \\\\bibinfo{author}{Papernot, N.} \\\\&\\n  \\\\bibinfo{author}{Jacobsen, J.-H.}\\n\\\\newblock \\\\bibinfo{title}{Fundamental tradeoffs between invariance and\\n  sensitivity to adversarial perturbations}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{International Conference on Machine\\n  Learning}}, \\\\bibinfo{pages}{9561--9571} (\\\\bibinfo{organization}{PMLR},\\n  \\\\bibinfo{year}{2020}).\\n\\n\\\\bibitem{zhu2017unpaired}\\n\\\\bibinfo{author}{Zhu, J.-Y.}, \\\\bibinfo{author}{Park, T.},\\n  \\\\bibinfo{author}{Isola, P.} \\\\& \\\\bibinfo{author}{Efros, A.~A.}\\n\\\\newblock \\\\bibinfo{title}{Unpaired image-to-image translation using\\n  cycle-consistent adversarial networks}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the IEEE international\\n  conference on computer vision}}, \\\\bibinfo{pages}{2223--2232}\\n  (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{geirhos2018generalisation}\\n\\\\bibinfo{author}{Geirhos, R.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Generalisation in humans and deep neural networks}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{arXiv preprint arXiv:1808.08750}}\\n  (\\\\bibinfo{year}{2018}).\\n\\n\\\\bibitem{koenderink2017eidolons}\\n\\\\bibinfo{author}{Koenderink, J.}, \\\\bibinfo{author}{Valsecchi, M.},\\n  \\\\bibinfo{author}{van Doorn, A.}, \\\\bibinfo{author}{Wagemans, J.} \\\\&\\n  \\\\bibinfo{author}{Gegenfurtner, K.}\\n\\\\newblock \\\\bibinfo{title}{Eidolons: Novel stimuli for vision research}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Journal of Vision}}\\n  \\\\textbf{\\\\bibinfo{volume}{17}}, \\\\bibinfo{pages}{7--7} (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{silver2017mastering}\\n\\\\bibinfo{author}{Silver, D.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Mastering the game of go without human knowledge}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{nature}} \\\\textbf{\\\\bibinfo{volume}{550}},\\n  \\\\bibinfo{pages}{354--359} (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{xie2017aggregated}\\n\\\\bibinfo{author}{Xie, S.}, \\\\bibinfo{author}{Girshick, R.},\\n  \\\\bibinfo{author}{Doll{\\\\'a}r, P.}, \\\\bibinfo{author}{Tu, Z.} \\\\&\\n  \\\\bibinfo{author}{He, K.}\\n\\\\newblock \\\\bibinfo{title}{Aggregated residual transformations for deep neural\\n  networks}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the IEEE conference on\\n  computer vision and pattern recognition}}, \\\\bibinfo{pages}{1492--1500}\\n  (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{huang2017densely}\\n\\\\bibinfo{author}{Huang, G.}, \\\\bibinfo{author}{Liu, Z.}, \\\\bibinfo{author}{Van\\n  Der~Maaten, L.} \\\\& \\\\bibinfo{author}{Weinberger, K.~Q.}\\n\\\\newblock \\\\bibinfo{title}{Densely connected convolutional networks}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the IEEE conference on\\n  computer vision and pattern recognition}}, \\\\bibinfo{pages}{4700--4708}\\n  (\\\\bibinfo{year}{2017}).\\n\\n\\\\bibitem{hu2018squeeze}\\n\\\\bibinfo{author}{Hu, J.}, \\\\bibinfo{author}{Shen, L.} \\\\& \\\\bibinfo{author}{Sun,\\n  G.}\\n\\\\newblock \\\\bibinfo{title}{Squeeze-and-excitation networks}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the IEEE conference on\\n  computer vision and pattern recognition}}, \\\\bibinfo{pages}{7132--7141}\\n  (\\\\bibinfo{year}{2018}).\\n\\n\\\\bibitem{richards2019deep}\\n\\\\bibinfo{author}{Richards, B.~A.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{A deep learning framework for neuroscience}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature neuroscience}}\\n  \\\\textbf{\\\\bibinfo{volume}{22}}, \\\\bibinfo{pages}{1761--1770}\\n  (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{geirhos_2018}\\n\\\\bibinfo{author}{Geirhos, R.}\\n\\\\newblock \\\\bibinfo{title}{generalisation-humans-dnns} (\\\\bibinfo{year}{2018}).\\n\\\\newblock\\n  \\\\urlprefix\\\\url{https://github.com/rgeirhos/generalisation-humans-DNNs/commits/master}.\\n\\n\\\\bibitem{gestaltrevision_2016}\\n\\\\bibinfo{author}{Gestaltrevision}.\\n\\\\newblock \\\\bibinfo{title}{Eidolon} (\\\\bibinfo{year}{2016}).\\n\\\\newblock\\n  \\\\urlprefix\\\\url{github.com/gestaltrevision/Eidolon/tree/8af01f424c2e5776479e05e42b699c4adef25f88}.\\n\\n\\\\bibitem{zhuang2021unsupervised}\\n\\\\bibinfo{author}{Zhuang, C.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Unsupervised neural network models of the ventral\\n  visual stream}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Proceedings of the National Academy of\\n  Sciences}} \\\\textbf{\\\\bibinfo{volume}{118}} (\\\\bibinfo{year}{2021}).\\n\\n\\\\bibitem{benjamini2006adaptive}\\n\\\\bibinfo{author}{Benjamini, Y.}, \\\\bibinfo{author}{Krieger, A.~M.} \\\\&\\n  \\\\bibinfo{author}{Yekutieli, D.}\\n\\\\newblock \\\\bibinfo{title}{Adaptive linear step-up procedures that control the\\n  false discovery rate}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Biometrika}} \\\\textbf{\\\\bibinfo{volume}{93}},\\n  \\\\bibinfo{pages}{491--507} (\\\\bibinfo{year}{2006}).\\n\\n\\\\bibitem{loshchilov2014computationally}\\n\\\\bibinfo{author}{Loshchilov, I.}\\n\\\\newblock \\\\bibinfo{title}{A computationally efficient limited memory cma-es for\\n  large scale optimization}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the 2014 Annual\\n  Conference on Genetic and Evolutionary Computation}},\\n  \\\\bibinfo{pages}{397--404} (\\\\bibinfo{year}{2014}).\\n\\n\\\\bibitem{2020SciPy-NMeth}\\n\\\\bibinfo{author}{Virtanen, P.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{{{SciPy} 1.0: Fundamental Algorithms for Scientific\\n  Computing in Python}}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature Methods}}\\n  \\\\textbf{\\\\bibinfo{volume}{17}}, \\\\bibinfo{pages}{261--272}\\n  (\\\\bibinfo{year}{2020}).\\n\\n\\\\bibitem{scikit-learn}\\n\\\\bibinfo{author}{Pedregosa, F.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Scikit-learn: Machine learning in {P}ython}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Journal of Machine Learning Research}}\\n  \\\\textbf{\\\\bibinfo{volume}{12}}, \\\\bibinfo{pages}{2825--2830}\\n  (\\\\bibinfo{year}{2011}).\\n\\n\\\\bibitem{seabold2010statsmodels}\\n\\\\bibinfo{author}{Seabold, S.} \\\\& \\\\bibinfo{author}{Perktold, J.}\\n\\\\newblock \\\\bibinfo{title}{statsmodels: Econometric and statistical modeling\\n  with python}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{9th Python in Science Conference}}\\n  (\\\\bibinfo{year}{2010}).\\n\\n\\\\bibitem{Waskom2021}\\n\\\\bibinfo{author}{Waskom, M.~L.}\\n\\\\newblock \\\\bibinfo{title}{seaborn: statistical data visualization}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Journal of Open Source Software}}\\n  \\\\textbf{\\\\bibinfo{volume}{6}}, \\\\bibinfo{pages}{3021} (\\\\bibinfo{year}{2021}).\\n\\\\newblock \\\\urlprefix\\\\url{https://doi.org/10.21105/joss.03021}.\\n\\n\\\\bibitem{pedregosa2011scikit}\\n\\\\bibinfo{author}{Pedregosa, F.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Scikit-learn: Machine learning in python}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{the Journal of machine Learning research}}\\n  \\\\textbf{\\\\bibinfo{volume}{12}}, \\\\bibinfo{pages}{2825--2830}\\n  (\\\\bibinfo{year}{2011}).\\n\\n\\\\bibitem{kar2019evidence}\\n\\\\bibinfo{author}{Kar, K.}, \\\\bibinfo{author}{Kubilius, J.},\\n  \\\\bibinfo{author}{Schmidt, K.}, \\\\bibinfo{author}{Issa, E.~B.} \\\\&\\n  \\\\bibinfo{author}{DiCarlo, J.~J.}\\n\\\\newblock \\\\bibinfo{title}{Evidence that recurrent circuits are critical to the\\n  ventral streams execution of core object recognition behavior}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Nature neuroscience}}\\n  \\\\textbf{\\\\bibinfo{volume}{22}}, \\\\bibinfo{pages}{974--983}\\n  (\\\\bibinfo{year}{2019}).\\n\\n\\\\bibitem{kurakin2016adversarial}\\n\\\\bibinfo{author}{Kurakin, A.}, \\\\bibinfo{author}{Goodfellow, I.},\\n  \\\\bibinfo{author}{Bengio, S.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Adversarial examples in the physical world}\\n  (\\\\bibinfo{year}{2016}).\\n\\n\\\\bibitem{alexnet}\\n\\\\bibinfo{author}{Krizhevsky, A.}, \\\\bibinfo{author}{Sutskever, I.} \\\\&\\n  \\\\bibinfo{author}{Hinton, G.~E.}\\n\\\\newblock \\\\bibinfo{title}{Imagenet classification with deep convolutional\\n  neural networks}.\\n\\\\newblock In \\\\bibinfo{editor}{Pereira, F.}, \\\\bibinfo{editor}{Burges, C. J.~C.},\\n  \\\\bibinfo{editor}{Bottou, L.} \\\\& \\\\bibinfo{editor}{Weinberger, K.~Q.} (eds.)\\n  \\\\emph{\\\\bibinfo{booktitle}{Advances in Neural Information Processing\\n  Systems}}, vol.~\\\\bibinfo{volume}{25} (\\\\bibinfo{publisher}{Curran Associates,\\n  Inc.}, \\\\bibinfo{year}{2012}).\\n\\\\newblock\\n  \\\\urlprefix\\\\url{https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}.\\n\\n\\\\bibitem{xie_noisystudent}\\n\\\\bibinfo{author}{Xie, Q.}, \\\\bibinfo{author}{Luong, M.-T.},\\n  \\\\bibinfo{author}{Hovy, E.} \\\\& \\\\bibinfo{author}{Le, Q.~V.}\\n\\\\newblock \\\\bibinfo{title}{Self-training with noisy student improves imagenet\\n  classification}.\\n\\\\newblock In \\\\emph{\\\\bibinfo{booktitle}{2020 IEEE/CVF Conference on Computer\\n  Vision and Pattern Recognition (CVPR)}}, \\\\bibinfo{pages}{10684--10695}\\n  (\\\\bibinfo{year}{2020}).\\n\\n\\\\bibitem{radford_CLIP}\\n\\\\bibinfo{author}{Radford, A.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Learning transferable visual models from natural\\n  language supervision}.\\n\\\\newblock In \\\\bibinfo{editor}{Meila, M.} \\\\& \\\\bibinfo{editor}{Zhang, T.} (eds.)\\n  \\\\emph{\\\\bibinfo{booktitle}{Proceedings of the 38th International Conference on\\n  Machine Learning}}, vol. \\\\bibinfo{volume}{139} of\\n  \\\\emph{\\\\bibinfo{series}{Proceedings of Machine Learning Research}},\\n  \\\\bibinfo{pages}{8748--8763} (\\\\bibinfo{publisher}{PMLR},\\n  \\\\bibinfo{year}{2021}).\\n\\\\newblock \\\\urlprefix\\\\url{https://proceedings.mlr.press/v139/radford21a.html}.\\n\\n\\\\bibitem{KubiliusSchrimpf2019CORnet}\\n\\\\bibinfo{author}{Kubilius, J.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{{Brain-Like Object Recognition with High-Performing\\n  Shallow Recurrent ANNs}}.\\n\\\\newblock In \\\\bibinfo{editor}{Wallach, H.} \\\\emph{et~al.} (eds.)\\n  \\\\emph{\\\\bibinfo{booktitle}{Neural Information Processing Systems (NeurIPS)}},\\n  \\\\bibinfo{pages}{12785----12796} (\\\\bibinfo{publisher}{Curran Associates,\\n  Inc.}, \\\\bibinfo{year}{2019}).\\n\\\\newblock\\n  \\\\urlprefix\\\\url{http://papers.nips.cc/paper/9441-brain-like-object-recognition-with-high-performing-shallow-recurrent-anns}.\\n\\n\\\\bibitem{dapello2020simulating}\\n\\\\bibinfo{author}{Dapello, J.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Simulating a primary visual cortex at the front of\\n  cnns improves robustness to image perturbations}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Advances in Neural Information Processing\\n  Systems}} \\\\textbf{\\\\bibinfo{volume}{33}}, \\\\bibinfo{pages}{13073--13087}\\n  (\\\\bibinfo{year}{2020}).\\n\\n\\\\bibitem{Kreiman21_vision}\\n\\\\bibinfo{author}{Kreiman, G.}\\n\\\\newblock \\\\emph{\\\\bibinfo{title}{Biological and Computer Vision}}\\n  (\\\\bibinfo{publisher}{Cambridge University Press},\\n  \\\\bibinfo{address}{Cambridge, UK}, \\\\bibinfo{year}{2021}).\\n\\\\newblock\\n  \\\\urlprefix\\\\url{https://www.cambridge.org/core/books/biological-and-computer-vision/BB7E68A69AFE7A322F68F3C4A297F3CF}.\\n\\n\\\\bibitem{PONCE2019999}\\n\\\\bibinfo{author}{Ponce, C.~R.} \\\\emph{et~al.}\\n\\\\newblock \\\\bibinfo{title}{Evolving images for visual neurons using a deep\\n  generative network reveals coding principles and neuronal preferences}.\\n\\\\newblock \\\\emph{\\\\bibinfo{journal}{Cell}} \\\\textbf{\\\\bibinfo{volume}{177}},\\n  \\\\bibinfo{pages}{999--1009.e10} (\\\\bibinfo{year}{2019}).\\n\\\\newblock\\n  \\\\urlprefix\\\\url{https://www.sciencedirect.com/science/article/pii/S0092867419303915}.\\n\\n\\\\end{thebibliography}\\n\",\n",
       "  'bibliography_bib': '@inproceedings{he2015delving,\\n  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},\\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\\n  booktitle={Proceedings of the IEEE international conference on computer vision},\\n  pages={1026--1034},\\n  year={2015}\\n}\\n\\n@article{lecun2015deep,\\n  title={Deep learning},\\n  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},\\n  journal={nature},\\n  volume={521},\\n  number={7553},\\n  pages={436--444},\\n  year={2015},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{yamins2014performance,\\n  title={Performance-optimized hierarchical models predict neural responses in higher visual cortex},\\n  author={Yamins, Daniel LK and Hong, Ha and Cadieu, Charles F and Solomon, Ethan A and Seibert, Darren and DiCarlo, James J},\\n  journal={Proceedings of the National Academy of Sciences},\\n  volume={111},\\n  number={23},\\n  pages={8619--8624},\\n  year={2014},\\n  publisher={National Acad Sciences}\\n}\\n\\n@article{cadena2019deep,\\n  title={Deep convolutional models improve predictions of macaque V1 responses to natural images},\\n  author={Cadena, Santiago A and Denfield, George H and Walker, Edgar Y and Gatys, Leon A and Tolias, Andreas S and Bethge, Matthias and Ecker, Alexander S},\\n  journal={PLoS computational biology},\\n  volume={15},\\n  number={4},\\n  pages={e1006897},\\n  year={2019},\\n  publisher={Public Library of Science}\\n}\\n\\n@article{szegedy2013intriguing,\\n  title={Intriguing properties of neural networks},\\n  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},\\n  journal={arXiv preprint arXiv:1312.6199},\\n  year={2013}\\n}\\n\\n@article{goodfellow2014explaining,\\n  title={Explaining and harnessing adversarial examples},\\n  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},\\n  journal={arXiv preprint arXiv:1412.6572},\\n  year={2014}\\n}\\n\\n@inproceedings{elsayed2018adversarial,\\n  title={Adversarial examples that fool both computer vision and time-limited humans},\\n  author={Elsayed, Gamaleldin and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alexey and Goodfellow, Ian and Sohl-Dickstein, Jascha},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={3910--3920},\\n  year={2018}\\n}\\n\\n@article{zhou2019humans,\\n  title={Humans can decipher adversarial images},\\n  author={Zhou, Zhenglong and Firestone, Chaz},\\n  journal={Nature communications},\\n  volume={10},\\n  number={1},\\n  pages={1--9},\\n  year={2019},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@inproceedings{berardino2017eigen,\\n  title={Eigen-distortions of hierarchical representations},\\n  author={Berardino, Alexander and Laparra, Valero and Ball{\\\\\\'e}, Johannes and Simoncelli, Eero},\\n  booktitle={Advances in neural information processing systems},\\n  pages={3530--3539},\\n  year={2017}\\n}\\n\\n@inproceedings{klindt2017neural,\\n  title={Neural system identification for large populations separating what and where},\\n  author={Klindt, David and Ecker, Alexander S and Euler, Thomas and Bethge, Matthias},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={3506--3516},\\n  year={2017}\\n}\\n\\n@inproceedings{he2016deep,\\n  title={Deep residual learning for image recognition},\\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\\n  pages={770--778},\\n  year={2016}\\n}\\n\\n@article{schrimpf2018brain,\\n  title={Brain-score: Which artificial neural network for object recognition is most brain-like?},\\n  author={Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J and Rajalingham, Rishi and Issa, Elias B and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Schmidt, Kailyn and others},\\n  journal={BioRxiv},\\n  pages={407007},\\n  year={2018},\\n  publisher={Cold Spring Harbor Laboratory}\\n}\\n\\n@article{moeller2008patches,\\n  title={Patches with links: a unified system for processing faces in the macaque temporal lobe},\\n  author={Moeller, Sebastian and Freiwald, Winrich A and Tsao, Doris Y},\\n  journal={Science},\\n  volume={320},\\n  number={5881},\\n  pages={1355--1359},\\n  year={2008},\\n  publisher={American Association for the Advancement of Science}\\n}\\n\\n@inproceedings{deng2009imagenet,\\n  title={Imagenet: A large-scale hierarchical image database},\\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\\n  pages={248--255},\\n  year={2009},\\n  organization={Ieee}\\n}\\n\\n@article{konkle2010conceptual,\\n  title={Conceptual distinctiveness supports detailed visual long-term memory for real-world objects},\\n  author={Konkle, Talia and Brady, Timothy F and Alvarez, George A and Oliva, Aude},\\n  journal={Journal of experimental Psychology: general},\\n  volume={139},\\n  number={3},\\n  pages={558},\\n  year={2010},\\n  publisher={American Psychological Association}\\n}\\n\\n@article{ma2015chicago,\\n  title={The Chicago face database: A free stimulus set of faces and norming data},\\n  author={Ma, Debbie S and Correll, Joshua and Wittenbrink, Bernd},\\n  journal={Behavior research methods},\\n  volume={47},\\n  number={4},\\n  pages={1122--1135},\\n  year={2015},\\n  publisher={Springer}\\n}\\n\\n@article{tsao2006cortical,\\n  title={A cortical region consisting entirely of face-selective cells},\\n  author={Tsao, Doris Y and Freiwald, Winrich A and Tootell, Roger BH and Livingstone, Margaret S},\\n  journal={Science},\\n  volume={311},\\n  number={5761},\\n  pages={670--674},\\n  year={2006},\\n  publisher={American Association for the Advancement of Science}\\n}\\n\\n@article{mcinnes2018umap,\\n  title={{UMAP}: Uniform manifold approximation and projection for dimension reduction},\\n  author={McInnes, Leland and Healy, John and Melville, James},\\n  journal={arXiv preprint arXiv:1802.03426},\\n  year={2018}\\n}\\n\\n@article{zhang2019theoretically,\\n  title={Theoretically principled trade-off between robustness and accuracy},\\n  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric P and Ghaoui, Laurent El and Jordan, Michael I},\\n  journal={arXiv preprint arXiv:1901.08573},\\n  year={2019}\\n}\\n\\n@article{tramer2017ensemble,\\n  title={Ensemble adversarial training: Attacks and defenses},\\n  author={Tram{\\\\`e}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},\\n  journal={arXiv preprint arXiv:1705.07204},\\n  year={2017}\\n}\\n\\n@article{yamins2016goal,\\n  title={Using goal-driven deep learning models to understand sensory cortex},\\n  author={Yamins, Daniel LK and DiCarlo, James J},\\n  journal={Nature neuroscience},\\n  volume={19},\\n  number={3},\\n  pages={356},\\n  year={2016},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{serre2019deep,\\n  title={Deep learning: the good, the bad, and the ugly},\\n  author={Serre, Thomas},\\n  journal={Annual Review of Vision Science},\\n  volume={5},\\n  pages={399--426},\\n  year={2019},\\n  publisher={Annual Reviews}\\n}\\n\\n@inproceedings{ilyas2019adversarial,\\n  title={Adversarial examples are not bugs, they are features},\\n  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={125--136},\\n  year={2019}\\n}\\n\\n@inproceedings{xie2020adversarial,\\n  title={Adversarial examples improve image recognition},\\n  author={Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L and Le, Quoc V},\\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\\n  pages={819--828},\\n  year={2020}\\n}\\n\\n@article{bracci2019ventral,\\n  title={The ventral visual pathway represents animal appearance over animacy, unlike human behavior and deep neural networks},\\n  author={Bracci, Stefania and Ritchie, J Brendan and Kalfas, Ioannis and de Beeck, Hans P Op},\\n  journal={Journal of Neuroscience},\\n  volume={39},\\n  number={33},\\n  pages={6513--6525},\\n  year={2019},\\n  publisher={Soc Neuroscience}\\n}\\n\\n@article{wardle2020rapid,\\n  title={Rapid and dynamic processing of face pareidolia in the human brain},\\n  author={Wardle, Susan G and Taubert, Jessica and Teichmann, Lina and Baker, Chris I},\\n  journal={Nature Communications},\\n  volume={11},\\n  number={1},\\n  pages={1--14},\\n  year={2020},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{bao2020map,\\n  title={A map of object space in primate inferotemporal cortex},\\n  author={Bao, Pinglei and She, Liang and McGill, Mason and Tsao, Doris Y},\\n  journal={Nature},\\n  pages={1--6},\\n  year={2020},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{bashivan2019neural,\\n  title={Neural population control via deep image synthesis\\n  },\\n  author={Bashivan, Pouya and Kar, Kohitij and DiCarlo, James J},\\n  journal={Science},\\n  volume={364},\\n  number={6439},\\n  pages={eaav9436},\\n  year={2019},\\n  publisher={American Association for the Advancement of Science}\\n}\\n\\n@article{walker2019inception,\\n  title={Inception loops discover what excites neurons most using deep predictive models},\\n  author={Walker, Edgar Y and Sinz, Fabian H and Cobos, Erick and Muhammad, Taliah and Froudarakis, Emmanouil and Fahey, Paul G and Ecker, Alexander S and Reimer, Jacob and Pitkow, Xaq and Tolias, Andreas S},\\n  journal={Nature neuroscience},\\n  volume={22},\\n  number={12},\\n  pages={2060--2065},\\n  year={2019},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{sheinberg1997role,\\n  title={The role of temporal cortical areas in perceptual organization},\\n  author={Sheinberg, David L and Logothetis, Nikos K},\\n  journal={Proceedings of the National Academy of Sciences},\\n  volume={94},\\n  number={7},\\n  pages={3408--3413},\\n  year={1997},\\n  publisher={National Acad Sciences}\\n}\\n\\n@article{afraz2006microstimulation,\\n  title={Microstimulation of inferotemporal cortex influences face categorization},\\n  author={Afraz, Seyed-Reza and Kiani, Roozbeh and Esteky, Hossein},\\n  journal={Nature},\\n  volume={442},\\n  number={7103},\\n  pages={692--695},\\n  year={2006},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{rajalingham2019reversible,\\n  title={Reversible inactivation of different millimeter-scale regions of primate IT results in different patterns of core object recognition deficits},\\n  author={Rajalingham, Rishi and DiCarlo, James J},\\n  journal={Neuron},\\n  volume={102},\\n  number={2},\\n  pages={493--505},\\n  year={2019},\\n  publisher={Elsevier}\\n}\\n\\n@article{iaccarino2016gamma,\\n  title={Gamma frequency entrainment attenuates amyloid load and modifies microglia},\\n  author={Iaccarino, Hannah F and Singer, Annabelle C and Martorell, Anthony J and Rudenko, Andrii and Gao, Fan and Gillingham, Tyler Z and Mathys, Hansruedi and Seo, Jinsoo and Kritskiy, Oleg and Abdurrob, Fatema and others},\\n  journal={Nature},\\n  volume={540},\\n  number={7632},\\n  pages={230--235},\\n  year={2016},\\n  publisher={Nature Publishing Group}\\n}\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n@article{mahendran2016visualizing,\\n  title={Visualizing deep convolutional neural networks using natural pre-images},\\n  author={Mahendran, Aravindh and Vedaldi, Andrea},\\n  journal={International Journal of Computer Vision},\\n  volume={120},\\n  number={3},\\n  pages={233--255},\\n  year={2016},\\n  publisher={Springer}\\n}\\n\\n@inproceedings{papernot2017practical,\\n  title={Practical black-box attacks against machine learning},\\n  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},\\n  booktitle={Proceedings of the 2017 ACM on Asia conference on computer and communications security},\\n  pages={506--519},\\n  year={2017}\\n}\\n\\n@inproceedings{liu2017delving,\\n  title={Delving into transferable adversarial examples and black-box attacks},\\n  author={Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},\\n  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},\\n  year={2017}\\n}\\n\\n@inproceedings{chen2017zoo,\\n  title={Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},\\n  author={Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},\\n  booktitle={Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},\\n  pages={15--26},\\n  year={2017}\\n}\\n\\n@article{ilyas2018black,\\n  title={Black-box adversarial attacks with limited queries and information},\\n  author={Ilyas, Andrew and Engstrom, Logan and Athalye, Anish and Lin, Jessy},\\n  journal={arXiv preprint arXiv:1804.08598},\\n  year={2018}\\n}\\n\\n@inproceedings{narodytska2017simple,\\n  author={N. {Narodytska} and S. {Kasiviswanathan}},\\n  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, \\n  title={Simple Black-Box Adversarial Attacks on Deep Neural Networks}, \\n  year={2017},\\n  volume={},\\n  number={},\\n  pages={1310--1318}\\n }\\n\\n@article{goodfellow2017attacking,\\n  title={Is attacking machine learning easier than defending it},\\n  author={Goodfellow, Ian and Papernot, Nicolas},\\n  journal={Blog post on Feb},\\n  volume={15},\\n  pages={2017},\\n  year={2017}\\n}\\n\\n@inproceedings{tramer2020fundamental,\\n  title={Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations},\\n  author={Tram{\\\\`e}r, Florian and Behrmann, Jens and Carlini, Nicholas and Papernot, Nicolas and Jacobsen, J{\\\\\"o}rn-Henrik},\\n  booktitle={International Conference on Machine Learning},\\n  pages={9561--9571},\\n  year={2020},\\n  organization={PMLR}\\n}\\n\\n@inproceedings{zhu2017unpaired,\\n  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},\\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\\n  booktitle={Proceedings of the IEEE international conference on computer vision},\\n  pages={2223--2232},\\n  year={2017}\\n}\\n\\n@article{geirhos2018generalisation,\\n  title={Generalisation in humans and deep neural networks},\\n  author={Geirhos, Robert and Temme, Carlos R Medina and Rauber, Jonas and Sch{\\\\\"u}tt, Heiko H and Bethge, Matthias and Wichmann, Felix A},\\n  journal={arXiv preprint arXiv:1808.08750},\\n  year={2018}\\n}\\n\\n@article{koenderink2017eidolons,\\n  title={Eidolons: Novel stimuli for vision research},\\n  author={Koenderink, Jan and Valsecchi, Matteo and van Doorn, Andrea and Wagemans, Johan and Gegenfurtner, Karl},\\n  journal={Journal of Vision},\\n  volume={17},\\n  number={2},\\n  pages={7--7},\\n  year={2017},\\n  publisher={The Association for Research in Vision and Ophthalmology}\\n}\\n\\n@article{silver2017mastering,\\n  title={Mastering the game of go without human knowledge},\\n  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},\\n  journal={nature},\\n  volume={550},\\n  number={7676},\\n  pages={354--359},\\n  year={2017},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@inproceedings{xie2017aggregated,\\n  title={Aggregated residual transformations for deep neural networks},\\n  author={Xie, Saining and Girshick, Ross and Doll{\\\\\\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},\\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\\n  pages={1492--1500},\\n  year={2017}\\n}\\n\\n@inproceedings{huang2017densely,\\n  title={Densely connected convolutional networks},\\n  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},\\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\\n  pages={4700--4708},\\n  year={2017}\\n}\\n\\n@inproceedings{hu2018squeeze,\\n  title={Squeeze-and-excitation networks},\\n  author={Hu, Jie and Shen, Li and Sun, Gang},\\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\\n  pages={7132--7141},\\n  year={2018}\\n}\\n\\n@article{richards2019deep,\\n  title={A deep learning framework for neuroscience},\\n  author={Richards, Blake A and Lillicrap, Timothy P and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and de Berker, Archy and Ganguli, Surya and others},\\n  journal={Nature neuroscience},\\n  volume={22},\\n  number={11},\\n  pages={1761--1770},\\n  year={2019},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@misc{geirhos_2018, title={generalisation-humans-DNNs}, url={https://github.com/rgeirhos/generalisation-humans-DNNs/commits/master}, journal={GitHub}, author={Geirhos, Robert}, year={2018}, month={Aug}}\\n\\n@misc{gestaltrevision_2016, title={Eidolon}, url={github.com/gestaltrevision/Eidolon/tree/8af01f424c2e5776479e05e42b699c4adef25f88}, journal={GitHub}, author={Gestaltrevision}, year={2016}, month={Aug}}\\n\\n@article{zhuang2021unsupervised,\\n  title={Unsupervised neural network models of the ventral visual stream},\\n  author={Zhuang, Chengxu and Yan, Siming and Nayebi, Aran and Schrimpf, Martin and Frank, Michael C and DiCarlo, James J and Yamins, Daniel LK},\\n  journal={Proceedings of the National Academy of Sciences},\\n  volume={118},\\n  number={3},\\n  year={2021},\\n  publisher={National Acad Sciences}\\n}\\n\\n@article{benjamini2006adaptive,\\n  title={Adaptive linear step-up procedures that control the false discovery rate},\\n  author={Benjamini, Yoav and Krieger, Abba M and Yekutieli, Daniel},\\n  journal={Biometrika},\\n  volume={93},\\n  number={3},\\n  pages={491--507},\\n  year={2006},\\n  publisher={Oxford University Press}\\n}\\n\\n@inproceedings{loshchilov2014computationally,\\n  title={A computationally efficient limited memory CMA-ES for large scale optimization},\\n  author={Loshchilov, Ilya},\\n  booktitle={Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},\\n  pages={397--404},\\n  year={2014}\\n}\\n\\n@ARTICLE{2020SciPy-NMeth,\\n  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and\\n            Haberland, Matt and Reddy, Tyler and Cournapeau, David and\\n            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and\\n            Bright, Jonathan and {van der Walt}, St{\\\\\\'e}fan J. and\\n            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and\\n            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and\\n            Kern, Robert and Larson, Eric and Carey, C J and\\n            Polat, {\\\\.I}lhan and Feng, Yu and Moore, Eric W. and\\n            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and\\n            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and\\n            Harris, Charles R. and Archibald, Anne M. and\\n            Ribeiro, Ant{\\\\^o}nio H. and Pedregosa, Fabian and\\n            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},\\n  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific\\n            Computing in Python}},\\n  journal = {Nature Methods},\\n  year    = {2020},\\n  volume  = {17},\\n  pages   = {261--272},\\n  adsurl  = {https://rdcu.be/b08Wh},\\n  doi     = {10.1038/s41592-019-0686-2},\\n}\\n\\n@article{scikit-learn,\\n title={Scikit-learn: Machine Learning in {P}ython},\\n author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\\n journal={Journal of Machine Learning Research},\\n volume={12},\\n pages={2825--2830},\\n year={2011}\\n}\\n\\n@inproceedings{seabold2010statsmodels,\\n  title={statsmodels: Econometric and statistical modeling with python},\\n  author={Seabold, Skipper and Perktold, Josef},\\n  booktitle={9th Python in Science Conference},\\n  year={2010},\\n}\\n\\n@article{Waskom2021,\\n    doi = {10.21105/joss.03021},\\n    url = {https://doi.org/10.21105/joss.03021},\\n    year = {2021},\\n    publisher = {The Open Journal},\\n    volume = {6},\\n    number = {60},\\n    pages = {3021},\\n    author = {Michael L. Waskom},\\n    title = {seaborn: statistical data visualization},\\n    journal = {Journal of Open Source Software}\\n }\\n \\n @article{pedregosa2011scikit,\\n  title={Scikit-learn: Machine learning in Python},\\n  author={Pedregosa, Fabian and Varoquaux, Ga{\\\\\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},\\n  journal={the Journal of machine Learning research},\\n  volume={12},\\n  pages={2825--2830},\\n  year={2011},\\n  publisher={JMLR. org}\\n}\\n\\n@article{kar2019evidence,\\n  title={Evidence that recurrent circuits are critical to the ventral streams execution of core object recognition behavior},\\n  author={Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B and DiCarlo, James J},\\n  journal={Nature neuroscience},\\n  volume={22},\\n  number={6},\\n  pages={974--983},\\n  year={2019},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@misc{kurakin2016adversarial,\\n  title={Adversarial examples in the physical world},\\n  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy and others},\\n  year={2016}\\n}\\n\\n@inproceedings{alexnet,\\n author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},\\n booktitle = {Advances in Neural Information Processing Systems},\\n editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},\\n pages = {},\\n publisher = {Curran Associates, Inc.},\\n title = {ImageNet Classification with Deep Convolutional Neural Networks},\\n url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},\\n volume = {25},\\n year = {2012}\\n}\\n\\n@INPROCEEDINGS{xie_noisystudent,\\n  author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},\\n  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, \\n  title={Self-Training With Noisy Student Improves ImageNet Classification}, \\n  year={2020},\\n  volume={},\\n  number={},\\n  pages={10684-10695},\\n  doi={10.1109/CVPR42600.2020.01070}}\\n\\n\\n\\n@InProceedings{radford_CLIP,\\n  title = \\t {Learning Transferable Visual Models From Natural Language Supervision},\\n  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},\\n  booktitle = \\t {Proceedings of the 38th International Conference on Machine Learning},\\n  pages = \\t {8748--8763},\\n  year = \\t {2021},\\n  editor = \\t {Meila, Marina and Zhang, Tong},\\n  volume = \\t {139},\\n  series = \\t {Proceedings of Machine Learning Research},\\n  month = \\t {18--24 Jul},\\n  publisher =    {PMLR},\\n  pdf = \\t {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},\\n  url = \\t {https://proceedings.mlr.press/v139/radford21a.html},\\n}\\n\\n@inproceedings{KubiliusSchrimpf2019CORnet,\\narchivePrefix = {arXiv},\\narxivId = {1909.06161},\\nauthor = {Kubilius, Jonas and Schrimpf, Martin and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Schmidt, Kailyn and Nayebi, Aran and Bear, Daniel and Yamins, Daniel L. K. and DiCarlo, James J.},\\nbooktitle = {Neural Information Processing Systems (NeurIPS)},\\neditor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and D\\'Alch{\\\\\\'{e}}-Buc, F. and Fox, E. and Garnett, R.},\\npages = {12785----12796},\\npublisher = {Curran Associates, Inc.},\\ntitle = {{Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs}},\\nurl = {http://papers.nips.cc/paper/9441-brain-like-object-recognition-with-high-performing-shallow-recurrent-anns},\\nyear = {2019}\\n}\\n\\n@article{dapello2020simulating,\\n  title={Simulating a primary visual cortex at the front of CNNs improves robustness to image perturbations},\\n  author={Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David and DiCarlo, James J},\\n  journal={Advances in Neural Information Processing Systems},\\n  volume={33},\\n  pages={13073--13087},\\n  year={2020}\\n}\\n\\n@book {Kreiman21_vision,\\n\\ttitle = {Biological and Computer Vision},\\n\\tyear = {2021},\\n\\tmonth = {02/2021},\\n\\tpublisher = {Cambridge University Press},\\n\\torganization = {Cambridge University Press},\\n\\taddress = {Cambridge, UK},\\n\\tisbn = {978-1108705004},\\n\\tissn = {1108705006},\\n\\tdoi = {10.1017/9781108649995},\\n\\turl = {https://www.cambridge.org/core/books/biological-and-computer-vision/BB7E68A69AFE7A322F68F3C4A297F3CF},\\n\\tauthor = {Gabriel Kreiman}\\n}\\n\\n@article{PONCE2019999,\\ntitle = {Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences},\\njournal = {Cell},\\nvolume = {177},\\nnumber = {4},\\npages = {999-1009.e10},\\nyear = {2019},\\nissn = {0092-8674},\\ndoi = {https://doi.org/10.1016/j.cell.2019.04.005},\\nurl = {https://www.sciencedirect.com/science/article/pii/S0092867419303915},\\nauthor = {Carlos R. Ponce and Will Xiao and Peter F. Schade and Till S. Hartmann and Gabriel Kreiman and Margaret S. Livingstone},\\nkeywords = {neural networks, inferotemporal cortex, generative adversarial network},\\n\\n}\\n',\n",
       "  'arxiv_citations': {'1312.6199': True,\n",
       "   '1412.6572': True,\n",
       "   '1802.03426': True,\n",
       "   '1901.08573': True,\n",
       "   '1705.07204': True,\n",
       "   '1804.08598': True,\n",
       "   '1808.08750': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Adversarial examples',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #138',\n",
       "   'newsletter_url': 'https://mailchi.mp/e7cec4ed9117/an-138why-ai-governance-should-find-problems-rather-than-just-solving-them',\n",
       "   'summarizer': 'Rohin',\n",
       "   'summary': 'It turns out that you can create adversarial examples for monkeys! The task: classifying a given face as coming from a monkey vs. a human. The method is pretty simple: train a neural network to predict what monkeys would do, and then find adversarial examples for monkeys. These examples dont transfer perfectly, but they transfer enough that it seems reasonable to call them adversarial examples. In fact, these adversarial examples also make humans make the wrong classification reasonably often (though not as often as with monkeys), when given about 1 second to classify (a fairly long amount of time). Still, it is clear that the monkeys and humans are much more behaviorally robust than the neural networks.',\n",
       "   'opinion': \"First, a nitpick: the adversarially modified images are pretty significantly modified, such that you now have to wonder whether we should say that the humans are getting the answer wrong, or that the image has been modified meaningfully enough that there is no longer a right answer (as is arguably the case with the infamous [cat-dog](https://twitter.com/goodfellow_ian/status/966853052140470272)). The authors do show that e.g. Gaussian noise of the same magnitude doesn't degrade human performance, which is a good sanity check, but doesnt negate this point.\\n\\nNonetheless, I liked this paper -- it seems like good evidence that neural networks and biological brains are picking up on similar features. My preferred explanation is that these are the natural features for our environment, though other explanations are possible, e.g. perhaps brains and neural networks are sufficiently similar architectures that they do similar things. Note however that they do require a _grey-box_ approach, where they first train the neural network to predict the monkey's neuronal responses. When they instead use a neural network trained to classify human faces vs. monkey faces, the resulting adversarial images do not cause misclassifications in monkeys. So they do need to at least finetune the final layer for this to work, and thus there is at least some difference between the neural networks and monkey brains.\",\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '2011.05623v3',\n",
       "   'arxiv_id': '2011.05623',\n",
       "   'title': 'Fooling the primate brain with minimal, targeted image manipulation',\n",
       "   'authors': ['Li Yuan',\n",
       "    'Will Xiao',\n",
       "    'Giorgia Dellaferrera',\n",
       "    'Gabriel Kreiman',\n",
       "    'Francis E. H. Tay',\n",
       "    'Jiashi Feng',\n",
       "    'Margaret S. Livingstone'],\n",
       "   'date_published': '2020-11-11 08:30:54+00:00',\n",
       "   'data_last_modified': '2022-03-30 05:36:53+00:00',\n",
       "   'url': 'http://arxiv.org/abs/2011.05623v3',\n",
       "   'abstract': \"Artificial neural networks (ANNs) are considered the current best models of biological vision. ANNs are the best predictors of neural activity in the ventral stream; moreover, recent work has demonstrated that ANN models fitted to neuronal activity can guide the synthesis of images that drive pre-specified response patterns in small neuronal populations. Despite the success in predicting and steering firing activity, these results have not been connected with perceptual or behavioral changes. Here we propose an array of methods for creating minimal, targeted image perturbations that lead to changes in both neuronal activity and perception as reflected in behavior. We generated 'deceptive images' of human faces, monkey faces, and noise patterns so that they are perceived as a different, pre-specified target category, and measured both monkey neuronal responses and human behavior to these images. We found several effective methods for changing primate visual categorization that required much smaller image change compared to untargeted noise. Our work shares the same goal with adversarial attack, namely the manipulation of images with minimal, targeted noise that leads ANN models to misclassify the images. Our results represent a valuable step in quantifying and characterizing the differences in perturbation robustness of biological and artificial vision.\",\n",
       "   'author_comment': 'None',\n",
       "   'journal_ref': 'None',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'q-bio.NC',\n",
       "   'categories': \"['q-bio.NC', 'cs.CV', 'cs.NE', 'eess.IV']\",\n",
       "   'individual_summary': \"Title: Adversarial images for the primate brain\\nAuthors: Li Yuan, Will Xiao, Gabriel Kreiman, Francis E.H. Tay, Jiashi Feng, Margaret S. Livingstone\\nPaper abstract: Artificial neural networks (ANNs) are considered the current best models of biological vision. ANNs are the best predictors of neural activity in the ventral stream; moreover, recent work has demonstrated that ANN models fitted to neuronal activity can guide the synthesis of images that drive pre-specified response patterns in small neuronal populations. Despite the success in predicting and steering firing activity, these results have not been connected with perceptual or behavioral changes. Here we propose an array of methods for creating minimal, targeted image perturbations that lead to changes in both neuronal activity and perception as reflected in behavior. We generated 'deceptive images' of human faces, monkey faces, and noise patterns so that they are perceived as a different, pre-specified target category, and measured both monkey neuronal responses and human behavior to these images. We found several effective methods for changing primate visual categorization that required much smaller image change compared to untargeted noise. Our work shares the same goal with adversarial attack, namely the manipulation of images with minimal, targeted noise that leads ANN models to misclassify the images. Our results represent a valuable step in quantifying and characterizing the differences in perturbation robustness of biological and artificial vision.\\nSummary: It turns out that you can create adversarial examples for monkeys! The task: classifying a given face as coming from a monkey vs. a human. The method is pretty simple: train a neural network to predict what monkeys would do, and then find adversarial examples for monkeys. These examples dont transfer perfectly, but they transfer enough that it seems reasonable to call them adversarial examples. In fact, these adversarial examples also make humans make the wrong classification reasonably often (though not as often as with monkeys), when given about 1 second to classify (a fairly long amount of time). Still, it is clear that the monkeys and humans are much more behaviorally robust than the neural networks.\\nMy opinion: First, a nitpick: the adversarially modified images are pretty significantly modified, such that you now have to wonder whether we should say that the humans are getting the answer wrong, or that the image has been modified meaningfully enough that there is no longer a right answer (as is arguably the case with the infamous [cat-dog](https://twitter.com/goodfellow_ian/status/966853052140470272)). The authors do show that e.g. Gaussian noise of the same magnitude doesn't degrade human performance, which is a good sanity check, but doesnt negate this point.\\n\\nNonetheless, I liked this paper -- it seems like good evidence that neural networks and biological brains are picking up on similar features. My preferred explanation is that these are the natural features for our environment, though other explanations are possible, e.g. perhaps brains and neural networks are sufficiently similar architectures that they do similar things. Note however that they do require a _grey-box_ approach, where they first train the neural network to predict the monkey's neuronal responses. When they instead use a neural network trained to classify human faces vs. monkey faces, the resulting adversarial images do not cause misclassifications in monkeys. So they do need to at least finetune the final layer for this to work, and thus there is at least some difference between the neural networks and monkey brains.\",\n",
       "   'paper_text': '',\n",
       "   'text': 'HIGHLIGHTS\\n[Solving for X? Towards a problem-finding framework to ground long-term governance strategies for artificial intelligence](https://www.researchgate.net/profile/Matthijs_Maas/publication/342774816_\\'Solving_for_X\\'_Towards_a_problem-finding_framework_to_ground_long-term_governance_strategies_for_artificial_intelligence/links/5fbbd04592851c933f517ad3/Solving-for-X-Towards-a-problem-finding-framework-to-ground-long-term-governance-strategies-for-artificial-intelligence.pdf) *(Hin-Yan Liu et al)* (summarized by Rohin): The typical workflow in governance research might go something like this: first, choose an existing problem to work on; second, list out possible governance mechanisms that could be applied to the problem; third, figure out which of these is best. We might call this the *problem-solving* approach. However, such an approach has several downsides:1. Such an approach will tend to use existing analogies and metaphors used for that problem, even when they are no longer appropriate.2. If there are problems which arent obvious given current frameworks for governance, this approach wont address them.3. Usually, solutions under this approach build on earlier, allegedly similar problems and their solutions, leading to path-dependencies in what kind of solutions are being sought. This makes it harder to identify and/or pursue new classes of solutions4. It is hard to differentiate between problems that are symptoms vs. problems that are root causes in such a framework, since not much thought is put into comparisons across problems5. Framing our job as solving an existing set of problems lulls us into a false sense of security, as it makes us think we understand the situation better than we actually do (if only we solved these problems, wed be done; nothing else would come up).The core claim of this paper is that we should also invest in a *problem-finding* approach, in which we do not assume that we even know what the problem is, and are trying to figure it out in advance before it arises. This distinction between problem-solving and problem-finding is analogous to the distinction between normal science and paradigm-changing science, between exploitation and exploration, and between addressing problems and pursuing mysteries. Including a problem-finding approach in our portfolio of research techniques helps mitigate the five disadvantages listed above. One particularly nice advantage is that it can help avoid the [Collingridge dilemma](https://en.wikipedia.org/wiki/Collingridge_dilemma): by searching for problems in advance, we can control them before they get entrenched in society (when they would be harder to control).The authors then propose a classification of governance research, where levels 0 and 1 correspond to problem-solving and levels 2 and 3 correspond to problem-finding:- **Business as usual** (level 0): There is no need to change the existing governance structures; they will naturally handle any problems that arise.- **Puzzle-solving** (level 1): Aims to solve the problem at hand (something like deepfakes), possibly by changing the existing governance structures.- **Disruptor-finding** (level 2): Searches for properties of AI systems that would be hard to accommodate with the existing governance tools, so that we can prepare in advance.- **Charting macrostrategic trajectories** (level 3): Looks for crucial considerations about how AI could affect the trajectory of the world.These are not just meant to apply to AGI. For example, autonomous weapons may make it easier to predict and preempt conflict, in which case rather than very visible drone strikes we may instead have invisible high-tech wars. This may lessen the reputational penalties of war, and so we may need to increase scrutiny of, and accountability for, this sort of hidden violence. This is a central example of a level 2 consideration.The authors note that we could extend the framework even further to cases where governance research fails: at level -1, governance stays fixed and unchanging in its current form, either because reality is itself not changing, or because the governance got locked in for some reason. Conversely, at level 4, we are unable to respond to governance challenges, either because we cannot see the problems at all, or because we cannot comprehend them, or because we cannot control them despite understanding them. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** One technique I like a lot is backchaining: starting from the goal you are trying to accomplish, and figuring out what actions or intermediate subgoals would most help accomplish that goal. Ive spent a lot of time doing this sort of thing with AI alignment. This paper feels like it is advocating the same for AI governance, but also gives a bunch of concrete examples of what this sort of work might look like. Im hoping that it inspires a lot more governance work of the problem-finding variety; this does seem quite neglected to me right now.One important caveat to all of this is that I am not a governance researcher and dont have experience actually trying to do such research, so its not unlikely that even though I think this sounds like good meta-research advice, it is actually missing the mark in a way I failed to see.While I do recommend reading through the paper, I should warn you that it is rather dense and filled with jargon, at least from my perspective as an outsider. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n TECHNICAL AI ALIGNMENT\\n\\ufeff\\n\\n ITERATED AMPLIFICATION\\n[Epistemology of HCH](https://www.alignmentforum.org/posts/CDSXoC54CjbXQNLGr/epistemology-of-hch) *(Adam Shimi)* (summarized by Rohin): This post identifies and explores three perspectives one can take on [HCH](https://www.alignmentforum.org/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch) ([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34)):1. **Philosophical abstraction:** In this perspective, HCH is an operationalization of the concept of ones enlightened judgment.2. **Intermediary alignment scheme:** Here we consider HCH as a scheme that arguably would be aligned if we could build it.3. **Model of computation:** By identifying the human in HCH with some computation primitive (e.g. arbitrary polynomial-time algorithms), we can think of HCH as a particular theoretical model of computation that can be done using that primitive.\\ufeff\\n\\n MESA OPTIMIZATION\\n[Fixing The Good Regulator Theorem](https://www.alignmentforum.org/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) *(John Wentworth)* (summarized by Rohin): Consider a setting in which we must extract information from some data X to produce model M, so that we can later perform some task Z in a system S while only having access to M. We assume that the task depends only on S and not on X (except inasmuch as X affects S). As a concrete example, we might consider gradient descent extracting information from a training dataset (X) and encoding it in neural network weights (M), which can later be used to classify new test images (Z) taken in the world (S) without looking at the training dataset.The key question: when is it reasonable to call M a model of S?1. If we assume that this process is done optimally, then M must contain all information in X that is needed for optimal performance on Z.2. If we assume that every aspect of S is important for optimal performance on Z, then M must contain all information about S that it is possible to get. Note that it is usually important that Z contains some new input (e.g. test images to be classified) to prevent M from hardcoding solutions to Z without needing to infer properties of S.3. If we assume that M contains *no more* information than it needs, then it must contain exactly the information about S that can be deduced from X.It seems reasonable to say that in this case we constructed a model M of the system S from the source X \"as well as possible\". This post formalizes this conceptual argument and presents it as a refined version of the [Good Regulator Theorem](http://pespmc1.vub.ac.be/books/Conant_Ashby.pdf).Returning to the neural net example, this argument suggests that since neural networks are trained on data from the world, their weights will encode information about the world and can be thought of as a model of the world.\\ufeff\\n\\n PREVENTING BAD BEHAVIOR\\n[Shielding Atari Games with Bounded Prescience](http://arxiv.org/abs/2101.08153) *(Mirco Giacobbe et al)* (summarized by Rohin): In order to study agents trained for Atari, the authors write down several safety properties using the internals of the ALE simulator that agents should satisfy. They then test several agents trained with deep RL algorithms to see how well they perform on these safety properties. They find that the agents only successfully satisfy 4 out of their 43 properties all the time, whereas for 24 of the properties, all agents fail at least some of the time (and frequently they fail on every single rollout tested).This even happens for some properties that should be easy to satisfy. For example, in the game Assault, the agent loses a life if its gun ever overheats, but avoiding this is trivial: just dont use the gun when the display shows that the gun is about to overheat.The authors implement a bounded shielding approach, which basically simulates actions up to N timesteps in the future, and then only takes actions from the ones that dont lead to an unsafe state (if that is possible). With N = 1 this is enough to avoid the failure described above with Assault. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** I liked the analysis of what safety properties agents failed to satisfy, and the fact that agents sometimes fail the obvious or easy safety properties suggests that the bounded shielding approach can actually be useful in practice. Nonetheless, I still prefer the approach of finding an [inductive safety invariant](http://arxiv.org/abs/2009.12612) ([AN #124](https://mailchi.mp/d1da78ed4aac/an-124provably-safe-exploration-through-shielding)), as it provides a guarantee of safety throughout the episode, rather than only for the next N timesteps. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n ADVERSARIAL EXAMPLES\\n[Adversarial images for the primate brain](https://arxiv.org/abs/2011.05623) *(Li Yuan et al)* (summarized by Rohin) (H/T Xuan): It turns out that you can create adversarial examples for monkeys! The task: classifying a given face as coming from a monkey vs. a human. The method is pretty simple: train a neural network to predict what monkeys would do, and then find adversarial examples for monkeys. These examples dont transfer perfectly, but they transfer enough that it seems reasonable to call them adversarial examples. In fact, these adversarial examples also make humans make the wrong classification reasonably often (though not as often as with monkeys), when given about 1 second to classify (a fairly long amount of time). Still, it is clear that the monkeys and humans are much more behaviorally robust than the neural networks. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** First, a nitpick: the adversarially modified images are pretty significantly modified, such that you now have to wonder whether we should say that the humans are getting the answer wrong, or that the image has been modified meaningfully enough that there is no longer a right answer (as is arguably the case with the infamous [cat-dog](https://twitter.com/goodfellow_ian/status/966853052140470272)). The authors do show that e.g. Gaussian noise of the same magnitude doesn\\'t degrade human performance, which is a good sanity check, but doesnt negate this point.Nonetheless, I liked this paper -- it seems like good evidence that neural networks and biological brains are picking up on similar features. My preferred explanation is that these are the natural features for our environment, though other explanations are possible, e.g. perhaps brains and neural networks are sufficiently similar architectures that they do similar things. Note however that they do require a *grey-box* approach, where they first train the neural network to predict the monkey\\'s neuronal responses. When they instead use a neural network trained to classify human faces vs. monkey faces, the resulting adversarial images do not cause misclassifications in monkeys. So they do need to at least finetune the final layer for this to work, and thus there is at least some difference between the neural networks and monkey brains. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n FORECASTING\\n[2020 Survey of Artificial General Intelligence Projects for Ethics, Risk, and Policy](http://gcrinstitute.org/2020-survey-of-artificial-general-intelligence-projects-for-ethics-risk-and-policy/) *(McKenna Fitzgerald et al)* (summarized by Flo): This is a survey of AGI research and development (R&D) projects, based on public information like publications and websites. The survey finds 72 such projects active in 2020 compared to 70 projects active in 2017. This corresponds to 15 new projects and 13 projects that shut down since 2017. Almost half of the projects are US-based (and this is fewer than in 2017!), and most of the rest is based in US-allied countries. Around half of the projects publish open-source code. Many projects are interconnected via shared personnel or joint projects and only a few have identifiable military connections (fewer than in 2017). All of these factors might facilitate cooperation around safety. The projects form three major clusters: 1) corporate projects active on AGI safety 2) academic projects not active on AGI safety and 3) small corporations not active on AGI safety. Most of the projects are rather small and project size varies a lot, with the largest projects having more than 100 times as many employees as the smallest ones. While the share of projects with a humanitarian focus has increased to more than half, only a small but growing number is active on safety. Compared to 2017, the share of corporate projects has increased, and there are fewer academic projects. While academic projects are more likely to focus on knowledge expansion rather than humanitarian goals, corporate projects seem more likely to prioritize profit over public interest and safety. Consequently, corporate governance might be especially important. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Flo\\'s opinion:** These kinds of surveys seem important to conduct, even if they don\\'t always deliver very surprising results. That said, I was surprised by the large amount of small AGI projects (for which I expect the chances of success to be tiny) and the overall small amount of Chinese AGI projects.  |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [How The Hell Do We Create General-Purpose Robots?](https://howthehell.substack.com/p/general-purpose-robots) *(Sergey Alexashenko)* (summarized by Rohin): A **general-purpose robot** (GPR) is one that can execute simple commands like unload the dishwasher or paint the wall. This post outlines an approach to get to such robots, and estimates how much it would cost to get there.On the hardware side, we need to have hardware for the body, sensors, and brain. The body is ready; the Spot robot from Boston Dynamics seems like a reasonable candidate. On sensors, we have vision, hearing and lidar covered; however, we dont have great sensors for touch yet. That being said, it seems possible to get by with bad sensors for touch, and compensate with vision. Finally, for the brain, even if we cant put enough chips on the robot itself, we can use more compute via the cloud.For software, in principle a large enough neural network should suffice; all of the skills involved in GPRs have already been demonstrated by neural nets, just not as well as would be necessary. (In particular, we dont need to posit AGI.) The big issue is that we dont know how to train such a network. (We cant train in the real world, as that is way too slow.)With a big enough investment, it seems plausible that we could build a simulator in which the robot could learn. The simulator would have to be physically realistic and diverse, which is quite a challenge. But we dont have to write down physically accurate models of all objects: instead, we can *virtualize* objects. Specifically, we interact with an object for a couple of minutes, and then use the resulting data to build a model of the object in our simulation. (You could imagine an AlphaFold-like system that does this very well.)The author then runs some Fermi estimates and concludes that it might cost around $42 billion for the R&D in such a project (though it may not succeed), and concludes that this would clearly be worth it given the huge economic benefits. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** This outline seems pretty reasonable to me. There are a lot of specific points to nitpick with; for example, I am not convinced that we can just use cloud compute. It seems plausible that manipulation tasks require quick, iterative feedback, where the latency of cloud compute would be unacceptable. (Indeed, the quick, iterative feedback of touch is exactly why it is such a valuable sensor.) Nonetheless, I broadly like the outlined plan and it feels like these sorts of nitpicks are things that we will be able to solve as we work on the problem.I am more skeptical of the cost estimate, which seems pretty optimistic to me. The author basically took existing numbers and then multiplied them by some factor for the increased hardness; I think that those factors are too low (for the AI aspects, idk about the robot hardware aspects), and I think that there are probably lots of other significant invisible costs that arent being counted here. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n NEWS\\n[Postdoc role at CHAI](https://humancompatible.ai/jobs#postdoc-specializing-in-ai-safety-and-control) *(CHAI)* (summarized by Rohin): The Center for Human-Compatible AI (where I did my PhD) is looking for postdocs. Apply [here](https://forms.gle/8w9Jfjr3X86osAvTA).[Apply to EA Funds now](https://forum.effectivealtruism.org/posts/NfkdSooNiHcdCBSJs/apply-to-ea-funds-now-1) *(Jonas Vollmer)* (summarized by Rohin): EA Funds applications are open until the deadline of March 7. This includes the Long-Term Future Fund (LTFF), which often provides grants to people working on AI alignment. Im told that LTFF is constrained by high-quality applications, and that applying only takes a few hours, so it is probably best to err on the side of applying. |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI\\'m always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2021 Alignment Newsletter, All rights reserved.*\\n\\n**'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2110.10819v1',\n",
       "  'title': 'Shaking the foundations: delusions in sequence models for interaction and control',\n",
       "  'authors': ['Pedro A. Ortega',\n",
       "   'Markus Kunesch',\n",
       "   'Grgoire Deltang',\n",
       "   'Tim Genewein',\n",
       "   'Jordi Grau-Moya',\n",
       "   'Joel Veness',\n",
       "   'Jonas Buchli',\n",
       "   'Jonas Degrave',\n",
       "   'Bilal Piot',\n",
       "   'Julien Perolat',\n",
       "   'Tom Everitt',\n",
       "   'Corentin Tallec',\n",
       "   'Emilio Parisotto',\n",
       "   'Tom Erez',\n",
       "   'Yutian Chen',\n",
       "   'Scott Reed',\n",
       "   'Marcus Hutter',\n",
       "   'Nando de Freitas',\n",
       "   'Shane Legg'],\n",
       "  'date_published': '2021-10-20 23:31:05+00:00',\n",
       "  'data_last_modified': '2021-10-20 23:31:05+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2110.10819v1',\n",
       "  'abstract': 'The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models \"lack the understanding of the cause and effect of their actions\" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.',\n",
       "  'author_comment': 'DeepMind Tech Report, 16 pages, 4 figures',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.AI'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': './main.tex',\n",
       "  'text': '---\\nauthor:\\n- Pedro A. Ortega\\n- Markus Kunesch\\n- Grgoire Deltang\\n- Tim Genewein\\n- Jordi Grau-Moya\\n- Joel Veness\\n- Jonas Buchli\\n- Jonas Degrave\\n- Bilal Piot\\n- Julien Perolat\\n- Tom Everitt\\n- Corentin Tallec\\n- Emilio Parisotto\\n- Tom Erez\\n- Yutian Chen\\n- Scott Reed\\n- Marcus Hutter\\n- Nando de Freitas\\n- Shane Legg\\nbibliography:\\n- main.bib\\ntitle: \"Shaking the foundations: delusions in sequence models for interaction and control\"\\n---\\n\\n::: {.epigraph}\\n-   What\\'s on the menu?\\n\\n-   Cheeseburgers are on the menu.\\n\\n-   No, it\\'s tacos.\\n\\n-   Tacos are on the menu.\\n\\n-   What\\'s on the menu?\\n\\n-   Cheeseburgers are on the menu.\\n\\n-   No, I said it was tacos.\\n\\n-   Tacos are on the menu.\\n\\n-   What\\'s on the menu?\\n\\n-   Cheeseburgers are on the menu.\\n\\n*--- Imagined conversation with a fast food AI.*\\n:::\\n\\nIntroduction\\n============\\n\\nSuppose we are given a probabilistic model $P$ that captures a joint distribution over a stochastic sequence $X_1, X_2, \\\\ldots, X_T$. We can use this probabilistic model to make sequential predictions: if we have observed a history $X_1=x_1$, ..., $X_t=x_t$ up to time\\xa0$t$, then we can predict the observation $X_{t+1}$ at time $t+1$ by conditioning the model on the past observations, that is, $$P(X_{t+1}=x_{t+1} \\\\mid x_1, \\\\ldots, x_t).$$ This prediction scheme works well under very broad conditions. In fact, it is optimal in expectation with respect to the log-loss; and if the sequence is governed by latent variables (which is the case, for instance, when the sequence is exchangeable) then the scheme corresponds to Bayesian prediction\\xa0[@cover1999elements].\\n\\nIf we are interested in purposeful adaptive behavior (rather than mere prediction), it is natural to ask whether the above auto-regressive prediction scheme extends to the *interactive setting*, where the model is used both for generating and predicting data (see e.g.\\xa0[@janner2021reinforcement; @chen2021decision]). For instance assume we are given a probabilistic model for a stochastic sequence $A_1, O_1, A_2, O_2, \\\\ldots, A_T, O_T$ describing sequential interactions between an expert and their environment, who issue the actions\\xa0$A_t$ and observations\\xa0$O_t$ respectively. Such a model could have been built from recorded demonstrations of an expert performing one or more tasks---say, a driver navigating a vehicle, a guided robot arm manipulating objects, or an interactive language model. First we remark that, as discussed before, we can use this model to passively predict the sequential expert-environment interactions. If we observe a sequence of past interactions $a_{1:t}, o_{1:t}$ between the expert and the environment, we can predict the expert\\'s next action using the conditional probability $$P(A_{t+1}=a_{t+1} \\\\mid  a_{1:t}, o_{1:t}).$$ The predictions made by this model will converge quickly even if we are initially uncertain about the task the expert is performing.\\n\\nBut assume now that we want to use this model for *imitation*. The rationale is to imitate the expert (or act as if one were the expert) by following the actions predicted by the model. The hope is that, as in the prediction case, if the sequence is governed by latent variables that characterize the task properties and the intentions of the expert, then the model will serve as an adaptive policy that converges quickly. More precisely, given past interactions\\xa0$a_{1:t}$ and\\xa0$o_{1:t}$, can we use this model for iteratively *choosing* the next action\\xa0$A_{t+1}$ by sampling it from $$a_{t+1} \\\\sim P(A_{t+1} \\\\mid a_{1:t}, o_{1:t}),$$ where $a_{1:t}$ are the previous actions generated by the model and $o_{1:t}$ are the past observations produced by the environment? This is a simple question with far-reaching implications on foundation models\\xa0[@Bommasani:21foundation], language agents\\xa0[@brown2020language], behavior cloning\\xa0[@ho2016generative], inverse reinforcement learning\\xa0[@ng2000algorithms; @abbeel2004apprenticeship], goal-/return- conditioned policies\\xa0[@veness2015compress; @janner2021reinforcement; @chen2021decision]), and so forth. For instance, it would allow the construction of highly-general adaptive agents by simply building a probabilistic model from observed demonstrations over multiple tasks and domains *without* relying on expensive reward maximization procedures.\\n\\nUnfortunately, the answer to this question is \"no\"---at least not without assumptions (e.g.\\xa0that the task is a causally-sufficient Markov Decision Process and the expert policy is stationary) and not under the same general conditions under which passive sequence prediction works[^1]. Unlike in control and reinforcement learning, here the actions are random variables, and as such embedded in a network of causal and probabilistic dependencies. In this report we will see that, while we can condition the model on observations as usual, actions (or action groups) must be treated as causal interventions. The reason is subtle: the model update triggered by the collected data differs depending upon whether the data was generated by the model itself (i.e.\\xa0actions) or outside of it (i.e.\\xa0observations), and mixing them up leads to wrong inferences. These take the form of *self-delusions* where the model takes its own actions as evidence about the world (for instance, believing that \"Cheeseburgers are on the menu\" after sampling this sentence from the predictive distribution) due to the presence of confounding variables. This problem[^2] was first pointed out in\\xa0[@ortega2010bayesian; @ortega2010minimum], and related points were made in\\xa0[@rezende2020causally].\\n\\nA minimal example {#sec:single}\\n=================\\n\\nTo illustrate the self-delusion problem, we need an example with at least three random variables. For this, consider the following *prize-or-frog* problem. There are two boxes (1 & 2), one containing a prize and the other a frog respectively. The objective is to open the box containing the prize.\\n\\nSuppose we build a probabilistic model from data generated by an expert who opens the correct box when told where the prize is. For simplicity, we assume the two configurations are equiprobable. The full model is a joint distribution $P(\\\\Theta, A, O)$ over the box configuration\\xa0$\\\\Theta$, the chosen box\\xa0$A$ (1\\xa0or\\xa02), and the observed content\\xa0$O$ ($\\\\pm 1$ reward). See Figure\\xa0[\\\\[fig:boxes\\\\]](#fig:boxes){reference-type=\"ref\" reference=\"fig:boxes\"}a for a depiction. Formally, we have\\n\\n$$P(\\\\Theta=\\\\theta) = \\\\frac{1}{2};\\n \\\\quad\\n P(A=a \\\\mid \\\\Theta=\\\\theta) =\\n \\\\begin{cases}\\n   1 & \\\\text{if $a=\\\\theta$,} \\\\\\\\\\n   0 & \\\\text{otherwise;}\\n \\\\end{cases}\\n \\\\quad\\n P(O=o \\\\mid \\\\Theta=\\\\theta, A=a) =\\n \\\\begin{cases}\\n   1 & \\\\text{if ($o=+1$ and $a=\\\\theta$)} \\\\\\\\\\n     & \\\\text{or ($o=-1$ and $a\\\\neq\\\\theta$),} \\\\\\\\\\n   0 & \\\\text{otherwise.}\\n \\\\end{cases}$$\\n\\nFirst we verify that the model works as intended for sequential prediction---that is, when the agent uses the model to sequentially predict the interactions between the expert and the task. We consider two cases: the fully observable case, and a partially observable where we do not see the task parameter.\\n\\n#### Fully observable, passive.\\n\\nThis is a sequential prediction problem that proceeds in three steps. In each step, we predict the value of a random variable and then observe its outcome, following the order $\\\\Theta$, $A$, and $O$. The predictions are made using the conditional probability distributions $P(\\\\Theta)$, $P(A \\\\mid \\\\Theta)$, and $P(O \\\\mid \\\\Theta, A)$ respectively. Clearly, only the first prediction is uncertain---i.e.\\xa0$P(\\\\Theta=1) = P(\\\\Theta=2) = \\\\tfrac{1}{2}$; after $\\\\Theta$ is revealed, $A$ and $O$ can be predicted with certainty.\\n\\n#### Partially observable, passive.\\n\\nThis time the configuration $\\\\Theta$ is latent, and thus we are only required to predict the action\\xa0$A$ and the observation\\xa0$O$ using the marginal joint distribution $$P(a, o) = \\\\sum_\\\\theta P(a, o \\\\mid \\\\theta) P(\\\\theta).$$ This marginal distribution can be derived as shown or learned directly from data, for instance (unintentionally) when\\xa0$\\\\Theta$ is a hidden latent we are unaware of. Accordingly, we make our sequential predictions using the conditional probability distributions $P(A)$ and $P(O \\\\mid A)$. The first prediction is uniform, that is $$\\\\label{eq:first-action}\\n  P(a) = \\\\sum_\\\\theta P(a \\\\mid \\\\theta) P(\\\\theta) = \\\\frac{1}{2},$$ which makes sense because we do not know the configuration $\\\\Theta$. However, once the expert issues the action $A=a$, we can infer the unique value of $\\\\Theta=\\\\theta$ compatible with this choice. Given $A=a$, the subsequent observation\\xa0$O=+1$ occurs with certainty because of the dependency between $\\\\Theta$ and $A$. Mathematically, this is $$P(o \\\\mid a) \\n      = \\\\sum_\\\\theta P(o \\\\mid \\\\theta, a) P(\\\\theta \\\\mid a) =\\n      \\\\begin{cases}\\n        1 & \\\\text{if $o = +1$},\\\\\\\\\\n        0 & \\\\text{if $o = -1$},\\n      \\\\end{cases}$$ where $P(\\\\theta \\\\mid a) = \\\\delta[\\\\theta = a]$ is the posterior probability of the configuration given the expert\\'s action.\\n\\nNow that we know the model works well for prediction, we will use the model in an interactive way for imitation, i.e.\\xa0where we choose the action\\xa0$A$ following our sequential model instead of letting the expert choose it. Again we will consider both the fully and partially observable cases separately.\\n\\n#### Fully observable, interactive.\\n\\nIn the fully observable case we will first observe\\xa0$\\\\Theta$, then choose\\xa0$A$, and finally observe\\xa0$O$. Clearly, once the value of\\xa0$\\\\Theta$ is known, acting amounts to imitating the expert: $$P(a \\\\mid \\\\theta) = \\n    \\\\begin{cases}\\n      1 & \\\\text{if $a = \\\\theta$,}\\\\\\\\\\n      0 & \\\\text{otherwise.}\\n    \\\\end{cases}$$ After the action is chosen, the prediction of the outcome\\xa0$O$ follows as usual.\\n\\n#### Partially observable, interactive---and the self-delusion problem.\\n\\nIn this case we are required to first issue the action $A$ and then observe the outcome $O$, following the marginal distribution $P(A, O)$. Crucially, we do not observe $\\\\Theta$.\\n\\nSince we do not know the configuration of the task, the model is uncertain about which action to pick, suggesting $P(a)=\\\\frac{1}{2}$ as calculated in [\\\\[eq:first-action\\\\]](#eq:first-action){reference-type=\"eqref\" reference=\"eq:first-action\"}. Sticking to the recommendation made by the model, we sample the action from $P(A)$. This immediately leads to a problem: *whichever action we choose will convince ourselves of the box configuration*! That is, $$P(\\\\theta \\\\mid a) =\\n    \\\\begin{cases}\\n      1 & \\\\text{if $\\\\theta=a$,}\\\\\\\\\\n      0 & \\\\text{otherwise,}\\n    \\\\end{cases}$$ because there is only one configuration that is consistent with the choice of the action. This *delusion* will impact our inferences downstream: we now predict with certainty that we will obtain the prize $P(O=1 \\\\mid a) = 1$ no matter which action we took, whereas clearly the correct inference is that we still don\\'t know.\\n\\n#### Addressing the delusion.\\n\\nHow did this delusion happen and does one circumvent it? The problem lies in the causal structure: $\\\\Theta$ causally precedes $A$ and $O$, and $A$ precedes $O$ (Figure\\xa0[\\\\[fig:boxes\\\\]](#fig:boxes){reference-type=\"ref\" reference=\"fig:boxes\"}b). Crucially, the causal structure implies that knowing the configuration\\xa0$\\\\Theta$ is a precondition for choosing the action\\xa0$A$, just as the expert did. But when we choose the action using\\xa0$P(A)$ without knowing\\xa0$\\\\Theta$ we are violating the causal requisite. This is why conditioning on the generated action will not only provide evidence about the causal future, but erroneously also about the causal past (Figure\\xa0[\\\\[fig:boxes-dag\\\\]](#fig:boxes-dag){reference-type=\"ref\" reference=\"fig:boxes-dag\"}a). In short, $\\\\Theta$ is a *confounder* [@pearl2009causality].\\n\\nTo avoid this mistake, one must incorporate the causal independence constraint introduced by the choice of the action\\xa0$A$ without the knowledge of\\xa0$\\\\Theta$, formally written as $\\\\Theta ~\\\\bot\\\\!\\\\!\\\\!\\\\bot~A$. Mathematically, this is done by treating the action as a causal intervention on\\xa0$P$ using the *do-operator*, resulting in the posterior distribution $P(\\\\Theta \\\\mid \\\\mathrm{do}(A))$. A calculation shows that intervening on $A$ does not change the beliefs about $\\\\Theta$: $$P(\\\\theta \\\\mid \\\\mathrm{do}(a))\\n  = \\\\mathrm{do}(a) \\\\biggl\\\\{ \\n  \\\\frac{ P(a \\\\mid \\\\theta) P(\\\\theta) }\\n  { \\\\sum_{\\\\theta\\'} P(a \\\\mid \\\\theta\\') P(\\\\theta\\') }\\n  \\\\biggr\\\\}\\n  = \\\\frac{ P(\\\\mathrm{do}(a) \\\\mid \\\\theta) P(\\\\theta) }\\n  { \\\\sum_{\\\\theta\\'} P(\\\\mathrm{do}(a) \\\\mid \\\\theta\\') P(\\\\theta\\') }\\n  = P(\\\\theta),$$ where we used Bayes\\' rule and then the fact that $P(\\\\mathrm{do}(a) \\\\mid \\\\theta) = Q(a \\\\mid \\\\theta) = Q(a)$ for some $Q$ where $\\\\Theta ~\\\\bot\\\\!\\\\!\\\\!\\\\bot~A$. Unlike conditioning, intervening on the generated action breaks the causal link between\\xa0$\\\\Theta$ and\\xa0$A$, which in turn correctly provides evidence about the causal future, but not on the past\\xa0(Figure\\xa0[\\\\[fig:boxes-dag\\\\]](#fig:boxes-dag){reference-type=\"ref\" reference=\"fig:boxes-dag\"}b). Consequently, the prediction of the observation $O$ is given by the posterior predictive $P(O \\\\mid \\\\mathrm{do}(A))$ which can be obtained via the back-door adjustment formula: $$P(o \\\\mid \\\\mathrm{do}(a)) \\n  = \\\\sum_\\\\theta P(o \\\\mid \\\\theta, \\\\mathrm{do}(a)) P(\\\\theta \\\\mid \\\\mathrm{do}(a))\\n  = \\\\sum_\\\\theta P(o \\\\mid \\\\theta, a) P(\\\\theta)\\n  = \\\\frac{1}{2}.$$ The result makes sense: now, selecting a box at random using the model does not provide evidence about the configuration, which in turn leaves us uncertain about the outcome.\\n\\nWe stress that, unlike in standard conditioning, to evaluate an intervention we need to know the causal structure. For instance, had the graph been the same except that\\xa0$A$ causally precedes\\xa0$\\\\Theta$, then the conditional probability $P(\\\\theta \\\\mid \\\\mathrm{do}(a))$ would have been $$P(\\\\theta \\\\mid \\\\mathrm{do}(a))\\n  = \\\\mathrm{do}(a) \\\\biggl\\\\{ \\n  \\\\frac{ P(a) P(\\\\theta \\\\mid a) }\\n  { \\\\sum_{\\\\theta\\'} P(a) P(\\\\theta\\' \\\\mid a) }\\n  \\\\biggr\\\\}\\n  = \\\\frac{ P(\\\\mathrm{do}(a)) P(\\\\theta \\\\mid \\\\mathrm{do}(a)) }\\n  { \\\\sum_{\\\\theta\\'} P(\\\\mathrm{do}(a)) P(\\\\theta\\' \\\\mid \\\\mathrm{do}(a)) }\\n  = P(\\\\theta \\\\mid a) = \\\\delta_a^\\\\theta.$$ In this case choosing the action does fix the box configuration, consistent with the new causal story.\\n\\n#### Back to fully observable, interactive.\\n\\nHaving seen that actions are causal interventions one might ask: why did we not intervene in the fully observable case? The answer is: we did. The difference is that in the fully observable case, conditioning and intervening leads to the same posterior distribution: $$P(O \\\\mid \\\\Theta, A) = P(O \\\\mid \\\\Theta, \\\\mathrm{do}(A)).$$ This is seen by noticing that conditioning on\\xa0$A$ only produces a flow of information that travels downstream. The upward flow of information is blocked, since we have already conditioned on $\\\\Theta$ (Figure\\xa0[\\\\[fig:boxes-dag\\\\]](#fig:boxes-dag){reference-type=\"ref\" reference=\"fig:boxes-dag\"}c).\\n\\n#### Summary.\\n\\nWhen using a probabilistic model in an interactive setting, actions (i.e.\\xa0data generated by the model) are causal interventions, whilst observations (i.e.\\xa0data generated externally) are standard Bayesian conditions. This prevents incorrect inferences (=delusions) via hidden confounders. Hence, one cannot learn from one\\'s own actions, but only from their effects. Furthermore, to evaluate an intervention we need to know the underlying causal structure.\\n\\nIn our example situation we used a random configuration, but otherwise the expert choices and outcomes are deterministic. However, the reader can easily verify that the problem and the solution hold in the general case when the expert choices and the outcomes are stochastic.\\n\\nThe source of the delusion can also be seen by comparing the two posterior beliefs $P(\\\\theta \\\\mid a, o)$ and $P(\\\\theta \\\\mid \\\\mathrm{do}(a), o)$ over the task parameter, that is, where the past action is treated as a standard condition and a causal intervention respectively (differences are highlighted): $$\\\\begin{aligned}\\n\\\\label{eq:delusion}\\n  P(\\\\theta \\\\mid {\\\\color{red} a}, o)\\n  &= \\\\frac{ P(\\\\theta) {\\\\color{red}P(a \\\\mid \\\\theta)} P(o \\\\mid \\\\theta, a) }{\\n  \\\\sum_{\\\\theta\\'} P(\\\\theta\\') {\\\\color{red}P(a \\\\mid \\\\theta\\')} P(o \\\\mid \\\\theta\\', a)}\\n  &\\n  P(\\\\theta \\\\mid {\\\\color{red} \\\\mathrm{do}(a)}, o)\\n  &= \\\\frac{ P(\\\\theta) P(o \\\\mid \\\\theta, a) }{\\n  \\\\sum_{\\\\theta\\'} P(\\\\theta\\') P(o \\\\mid \\\\theta\\', a)}.\\\\end{aligned}$$ Here we can clearly see that the intervention causes the dismissal of the evidence $P(a \\\\mid \\\\theta)$ produced by the self-generated action\\xa0$A=a$.\\n\\nIn control and RL, the controller/agent is never included into the probabilistic model, but regarded as an external process. Actions are not treated as random variables but as indexes over families of distributions. Hence, when the control/RL practitioner \"conditions\" on an action, technically they choose a distribution from a family parameterized by the actions, which results in the (correct) second posterior belief formula in\\xa0[\\\\[eq:delusion\\\\]](#eq:delusion){reference-type=\"eqref\" reference=\"eq:delusion\"}.\\n\\nSampling an action from the posterior predictive $P(a)$ amounts to sampling from $$P(a) = \\\\sum_\\\\theta P(a \\\\mid \\\\theta) P(\\\\theta).$$ The sum (or integral, if $\\\\Theta$ were continuous) can render the direct calculation of $P(a)$ intractable. An equivalent and algorithmically easier way to obtain an action using Monte-Carlo simulation is to first sample $\\\\theta \\\\sim P(\\\\theta)$ and then sample the action from the expert policy $a \\\\sim P(a \\\\mid \\\\theta)$. Note that this is Thompson Sampling---not as a heuristic, but derived from first principles [@ortega2010bayesian; @ortega2010minimum].\\n\\nThis is also the reason why goal-conditioned planning does not work in the general case. Suppose the interactions are generated from the process $\\\\Theta, A, O, G$, where $G$ is a random variable characterizing the final state that depends causally on\\xa0$\\\\Theta$. This could e.g.\\xa0be an explicit final state or a function computed from the interactions, such as the return. In goal-conditioned planning, one conditions on the desired goal $G=g$ and then acts according to the posterior predictive, i.e.\\xa0$P(A \\\\mid G=g)$. The problem is that the choice of the goal $G=g$ is an action, and conditioning on it will reveal false information about the latent task parameter\\xa0$\\\\Theta$, which is delusion.\\n\\nThe breakdown in the partially observable case relied on the presence of an unobservable latent factor, which in general cannot be avoided. However, often there are domains where the observation is so rich that it (implicitly) contains the task identifier. This essentially brings the problem back to the fully observable case, limiting the impact of the delusion.\\n\\nThe role played by interventions in our discussion has a parallel in the brain sciences. The central motor system, when issuing a motor signal, generates an efferent copy which is sent to the sensory system. This enables the sensory system to take into account what the intended actions are, so that these are not mistaken for environmental actions. This mechanism is hypothesized to be the reason why one can\\'t tickle oneself [@blakemore2000can].\\n\\nThe sequential case {#sec:sequential}\\n===================\\n\\nA straightforward way of extending the previous example to sequential decisions is by formalizing the interactions as a sequential game because they reflect the causal dependencies. We will do this in two steps: first we formulate the idealized, causally sufficient game which includes all the task parameters an expert needs in order to make their decisions, and then we use it to derive an adaptive policy for when these parameters are latent. The fact that this actually works is non-trivial.\\n\\nWe start by considering a stochastic process\\xa0$P$ with\\xa0$T$ rounds over the random variable triplets $$\\\\Theta_1, A_1, O_1, \\\\Theta_2, A_2, O_2, \\\\ldots, \\\\Theta_T, A_T, O_T,$$ where\\xa0the $\\\\Theta_t$ and $O_t$ are moves taken by Nature, and the\\xa0$A_t$ are moves taken by an expert. Here we chose this triplet structure only for convenience: one can easily envision games with other structures better suited for the desired application domain. For instance, a natural language domain might require hierarchically structured variables (say, topic, sentence, and word markers). Figure\\xa0[\\\\[fig:sequential-game\\\\]](#fig:sequential-game){reference-type=\"ref\" reference=\"fig:sequential-game\"}a shows an example with binary random variables and $T=2$ rounds and Figure\\xa0[\\\\[fig:sequential-game\\\\]](#fig:sequential-game){reference-type=\"ref\" reference=\"fig:sequential-game\"}b shows its associated causal Bayesian network. As in the prize-or-frog game, we interpret the $\\\\Theta$\\'s as task/expert-intention parameters; only here they are generated as the process unfolds. The process represents a game with complete information, where every move is observable by both parties. That is, in every time step\\xa0$t+1$, the expert sees all the past moves and chooses an action from the conditional distribution (i.e.\\xa0the expert policy) $$P(A_{t+1} \\\\mid \\\\theta_{1:t+1}, a_{1:t}, o_{1:t}).$$ Variations of such a game could represent, for instance, an agent playing a multi-armed bandit with known reward distributions; an agent controlling a known MDP; or a card player who sees the hands of their opponents (the opponent players were folded into a single environment).\\n\\nNow let us assume we use this model to mimic the interactions, but without seeing the task parameters. The interaction process is thus the marginal process over the random variables $$A_1, O_1, A_2, O_2, \\\\ldots, A_T, O_T.$$ In the language of game theory, we have turned the original, perfect information game into a game with imperfect information. In this more challenging game, Nature can make hidden moves, choosing the task parameters\\xa0$\\\\Theta_t$ secretly. From game theory, we also know that hidden moves spawn *information sets*, i.e.\\xa0game states that are indistinguishable (highlighted in Figure\\xa0[\\\\[fig:sequential-game\\\\]](#fig:sequential-game){reference-type=\"ref\" reference=\"fig:sequential-game\"}a) and hence do not admit the same state-specific actions that the expert would choose [@vonNeumann1947theory; @osborne1994course].\\n\\nGiven the above limitations, how do we choose our next action? As we have seen in the prize-or-frog example, we cannot sample actions from $P(A_{t+1} \\\\mid a_{1:t}, o_{1:t})$ because conditioning on past actions leads to delusions. Instead, we must choose our next action using $$\\\\label{eq:action-pred}\\n  P(A_{t+1} \\\\mid \\\\mathrm{do}(a_{1:t}), o_{1:t})$$ where past actions are incorporated as causal interventions and past observations as standard conditions. This leads to inferences that are consistent with the causal constraints imposed by the information sets. Compare the causal Bayesian network before and after the interactions in Figures\\xa0[\\\\[fig:sequential-game\\\\]](#fig:sequential-game){reference-type=\"ref\" reference=\"fig:sequential-game\"}b &\\xa0c respectively. In addition, the resulting policy is adaptive---essentially, it is Thompson sampling---and will converge to the expert\\'s policy under broad conditions [@leike2016thompson; @osband2017posterior; @ortega2010bayesian].\\n\\nIf instead we had started directly from the marginal process over the interactions without knowing the underlying causal dependencies on the confounders\\xa0$\\\\Theta_t$ (see Figure\\xa0[\\\\[fig:sequential-game\\\\]](#fig:sequential-game){reference-type=\"ref\" reference=\"fig:sequential-game\"}d), then we would have been incapable of evaluating the correct result of interventions. This is why possessing a causally sufficient model is of paramount importance.\\n\\n#### Summary.\\n\\nIf we want to imitate an expert who interacts with a task class, we can do so by constructing a causally sufficient model of their sequential interactions, and then sample our actions from the conditional\\xa0[\\\\[eq:action-pred\\\\]](#eq:action-pred){reference-type=\"eqref\" reference=\"eq:action-pred\"} which treats past actions as interventions. In short, imitation requires knowing the reasons behind actions. This will produce an adaptive policy that converges to the expert\\'s policy under broad conditions.\\n\\nInterestingly, by introducing the notion of information sets for describing hidden moves, Von\\xa0Neumann shows to have been acutely aware of the associated causal problem already in 1944 [@vonNeumann1947theory Chapter\\xa07].\\n\\nWhile the conditional distribution\\xa0[\\\\[eq:action-pred\\\\]](#eq:action-pred){reference-type=\"eqref\" reference=\"eq:action-pred\"} looks simple, it is worth expressing it in terms of conditional probabilities that are free from interventions. For completeness, here we provide its recursive definition. The posterior predictive over the next action is $$P(a_{t+1} \\\\mid \\\\mathrm{do}(a_{1:t}), o_{1:t})\\n  = \\\\sum_{\\\\theta_{1:t+1}} P(a_{t+1} \\\\mid \\\\theta_{1:t+1}, a_{1:t}, o_{1:t}) \\\\, P(\\\\theta_{1:t+1} \\\\mid \\\\mathrm{do}(a_{1:t}), o_{1:t}),$$ i.e.\\xa0a weighted superposition of expert actions weighted by the posterior probabilities of the task parameters. The posterior $P(\\\\theta_{1:t+1} \\\\mid \\\\mathrm{do}(a_{1:t}), o_{1:t})$ in turn is given by the recursive expression $$P(\\\\theta_{1:t+1} \\\\mid \\\\mathrm{do}(a_{1:t}), o_{1:t})\\n  \\\\propto \\n    P(\\\\theta_{t+1} \\\\mid \\\\theta_{1:t}, a_{1:t}, o_{1:t})\\n    \\\\, P(o_t \\\\mid \\\\theta_{1:t}, a_{1:t}, o_{1:t-1})\\n    \\\\, P(\\\\theta_{1:t} \\\\mid \\\\mathrm{do}(a_{1:t-1}), o_{1:t-1}),$$ That is, proportional to the product of the probability of choosing the last task parameter, the likelihood of the observation, and the prior probability of the remaining task parameters. The likelihood of the action is absent from the product because it was treated as an intervention. This derivation makes use of standard probability and the rules of causal calculus. It is easy to see that if the recursive equation is \"unrolled\", the result will be entirely free of expressions containing interventions.\\n\\nLearning how to play a multi-armed bandit is the quintessential introductory reinforcement learning problem [@sutton2018reinforcement], exemplifying the exploration-exploitation dilemma an agent must face when dealing with uncertainty. We built a model of interactions between a noisy expert and a multi-armed bandit with $5$\\xa0arms where each arm delivers a Bernoulli-distributed reward. The interaction process is $\\\\Theta, A_1, O_1, A_2, O_2, \\\\ldots$ where $\\\\Theta$ is the hidden choice of the best arm ($\\\\theta \\\\in \\\\{1, 2, 3, 4, 5\\\\}$), and the $A_t$ and $O_t$ are the expert actions and the bandit rewards respectively. The causal probabilistic model is specified by the following mechanisms, $$P(\\\\Theta=\\\\theta) = \\\\frac{1}{5};\\\\quad\\n  P(A_t=a_t \\\\mid \\\\theta) = \\n  \\\\begin{cases}\\n    0.6 & \\\\text{if $a_t=\\\\theta$,} \\\\\\\\\\n    0.1 & \\\\text{otherwise;}\\n  \\\\end{cases}\\\\quad\\n  P(O_t=1 \\\\mid \\\\theta, a_t) =\\n  \\\\begin{cases}\\n    0.75 & \\\\text{if $a_t=\\\\theta$,} \\\\\\\\\\n    0.25 & \\\\text{otherwise.}\\n  \\\\end{cases}$$ Since these are the mechanisms, the $A_t$ are independent of the past given $\\\\Theta$, and so are the $O_t$ given the previous action $A_t$ and the parameter $\\\\Theta$. The table below shows the result of using this model for imitating the expert during twenty rounds without knowing\\xa0$\\\\Theta$. It compares two cases: when past actions are treated as standard conditions and as causal interventions with three trajectories each. For simplicity, the best arm was labelled as arm\\xa01.\\n\\n   Conditioning: $a_{t+1} \\\\sim P(a_{t+1} \\\\mid a_{1:t}, o_{1:t})$                                                   Intervening: $a_{t+1} \\\\sim P(a_{t+1} \\\\mid \\\\mathrm{do}(a_{1:t}), o_{1:t})$  \\n  --------------------------------------------------------------- -------------------------------------------- -- --------------------------------------------------------------------------- --------------------------------------------\\n                             Actions:                             3 4 3 3 3\\xa0\\xa03 3 3 3 3\\xa0\\xa03 3 3 3 3\\xa0\\xa03 3 3 3 3                                       Actions:                                   1 4 4 3 1\\xa0\\xa01 1 5 3 1\\xa0\\xa01 1 4 1 1\\xa0\\xa01 1 1 1 1\\n                             Rewards:                             1 0 0 0 0\\xa0\\xa00 0 1 0 0\\xa0\\xa00 0 1 0 0\\xa0\\xa00 1 0 1 0                                       Rewards:                                   0 0 0 1 1\\xa0\\xa01 0 0 0 1\\xa0\\xa01 1 0 1 1\\xa0\\xa01 1 1 0 1\\n                                                                                                                                                                                              \\n                             Actions:                             2 1 2 1 2\\xa0\\xa01 1 1 1 1\\xa0\\xa01 1 1 1 1\\xa0\\xa01 1 1 1 1                                       Actions:                                   5 4 5 3 4\\xa0\\xa05 3 3 2 3\\xa0\\xa01 1 1 5 5\\xa0\\xa01 1 1 1 1\\n                             Rewards:                             0 1 0 1 0\\xa0\\xa01 1 1 1 0\\xa0\\xa01 1 1 1 1\\xa0\\xa01 1 0 1 1                                       Rewards:                                   1 0 0 1 0\\xa0\\xa00 0 1 0 0\\xa0\\xa01 0 1 1 0\\xa0\\xa01 1 1 0 1\\n                                                                                                                                                                                              \\n                             Actions:                             5 5 2 2 2\\xa0\\xa02 5 2 4 1\\xa0\\xa02 2 2 2 2\\xa0\\xa02 2 2 2 5                                       Actions:                                   2 1 1 1 5\\xa0\\xa04 5 4 1 4\\xa0\\xa05 1 1 1 1\\xa0\\xa01 4 1 1 1\\n                             Rewards:                             0 0 1 0 1\\xa0\\xa00 0 0 1 1\\xa0\\xa01 1 0 0 1\\xa0\\xa00 0 0 0 0                                       Rewards:                                   0 1 1 0 0\\xa0\\xa00 0 0 0 0\\xa0\\xa01 1 1 1 1\\xa0\\xa00 0 1 1 1\\n\\nThe two resulting strategies differ significantly. Treating actions as conditions leads to self-delusion---evidenced by a tendency to repeating previously chosen actions, often converging to the wrong arm. In contrast, treating actions as interventions leads to a richer exploratory strategy which identifies the best arm.\\n\\nIn language modelling, the actions and observations are both language tokens, such as words. The web is full of the actions (text) produced by many other agents, mostly people, but recently machines too, such as GPT3 [@brown2020language]. Language models $P(x_t \\\\mid x_{1:t-1})$ are often pre-trained with self-supervised learning techniques. These pre-trained models are agents that can generate actions by conditioning on previous actions. Their environment is an API that allows for human experts to prompt the agents with text.\\n\\nConsider a pre-trained language model whose job is to predict the fourth word given a sequence of three words proposed by us (experts). Without any loss of generality, we again introduce the variable $\\\\theta$ to capture information that is available to the experts, but not to the agent. This information could represent the intention, emotional state or any other information about the experts that the language model has no access to via the observations. If we knew the posterior distribution of $\\\\theta$, we could easily marginalise it out using the rules of probability, and predict the fourth word as follows: $$P(x_4 \\\\mid x_1, x_2, x_3) = \\n\\\\int P(x_4 \\\\mid \\\\theta, x_1, x_2, x_3) \\n\\\\, P(\\\\theta \\\\mid x_1, x_2, x_3) \\\\, d\\\\theta,\\n\\\\label{eq:pred1}$$ where the posterior distribution $P(\\\\theta \\\\mid x_1, x_2, x_3)$, representing the model\\'s beliefs about $\\\\theta$, is given by $$P(\\\\theta \\\\mid x_1, x_2, x_3) \\n  \\\\propto P(x_3 \\\\mid \\\\theta, x_1, x_2)\\n    \\\\, P(x_2 \\\\mid \\\\theta, x_1) \\\\, P(x_1 \\\\mid \\\\theta) \\\\, P(\\\\theta). \\n\\\\label{eq:post1}$$ Note that in practice we don\\'t know the shape of $\\\\theta$ and indeed it may even be an unknown unknown. Therefore we don\\'t compute the prediction using the right hand side of equation\\xa0([\\\\[eq:pred1\\\\]](#eq:pred1){reference-type=\"ref\" reference=\"eq:pred1\"}), but instead train the predictive model on the left hand side of this equation.\\n\\nThe problem arises when we consider interaction between the pre-trained language model and the environment. Suppose we use a language model API to enter the first word $x_1$, but this time let the model use its own generated second word $x_2$, and then force the model to use our third word $x_3$. We then try to predict the fourth word as before. That is, while $x_1$ and $x_3$ are expert data, $x_2$ is an intervention produced by the language agent. Hence, by the arguments introduced earlier in the minimal example section, we must adopt the following model to avoid delusions: $$P(x_4 \\\\mid x_1,\\\\mathrm{do}(x_2),x_3) = \\n  \\\\int P(x_4 \\\\mid \\\\theta,x_1,x_2,x_3) \\n    \\\\, P(\\\\theta \\\\mid x_1,\\\\mathrm{do}(x_2),x_3) \\\\, d\\\\theta$$ The posterior distribution $P(\\\\theta \\\\mid x_1,\\\\mathrm{do}(x_2),x_3)$, with $x_2$ being treated as an intervention, is given by $$P(\\\\theta \\\\mid x_1,\\\\mathrm{do}(x_2),x_3) \\n  \\\\propto P(x_3 \\\\mid \\\\theta,x_1,x_2)\\n    \\\\, P(x_1 \\\\mid \\\\theta) \\\\, P(\\\\theta) \\n\\\\label{eq:post2}$$ Note that $P(x_2 \\\\mid \\\\theta,x_1)$ appears in equation\\xa0([\\\\[eq:post1\\\\]](#eq:post1){reference-type=\"ref\" reference=\"eq:post1\"}) but not in equation\\xa0([\\\\[eq:post2\\\\]](#eq:post2){reference-type=\"ref\" reference=\"eq:post2\"}) because in this latter equation what we have is $P(\\\\mathrm{do}(x_2) \\\\mid \\\\theta,x_1)$ and this term cancels out in the expression for the posterior distribution as we discussed earlier. This can be understood as an application of the back-door criterion to the causal graph, whereby we delete all links pointing to $x_2$. That is, $x_2$ is fixed, and not a random variable. Hence, $$P(x_4 \\\\mid x_1,x_2,x_3) \\\\neq P(x_4 \\\\mid x_1,\\\\mathrm{do}(x_2),x_3).$$ If the model acted in the past, its actions must be treated as causal interventions and not as observations. In the extreme case imagine that the language model generates a lot of text and that this text is added to the dataset, say a web corpus. Then relearning from this dataset will only confirm the model\\'s biases, that is, its delusions.\\n\\nHow important is this in practice? If the observations are very informative about the latent information $\\\\theta$, then the difference between the two predictive models, described above, will be small. However, when the observations are not informative about $\\\\theta$, or ambiguous, and the agent cannot collect data online to reduce its ignorance, the two predictive models can be very different, giving rise to absurd dialogues as the one in the introduction of this paper.\\n\\nSequential models built from data\\n=================================\\n\\nSo far we have assumed that we are in possession of a complete causal-probabilistic model over interaction sequences on which we can arbitrarily operate by marginalizing, conditioning, and intervening on any desired random variable. We now turn our attention to learning such models from data through regression, where the models typically have rigid function signatures. For instance, say we learn a function\\xa0$f$ that computes the probabilities of a Markov transition kernel $$P(X_{t+1}=x_{t+1} \\\\mid X_t=x_t) = f(x_{t+1}, x_t).$$ Using $f$, we cannot compute $P(X_{t+1} \\\\mid \\\\mathrm{do}(x_t))$. This is the type of challenge we\\'ll face next.\\n\\nMeta-learning and \"counterfactual teaching\"\\n-------------------------------------------\\n\\nWe can learn a sequential model for control using memory-based meta-learning [@duan2016rl; @wang2016learning] (as was done to train e.g.\\xa0GPT-3 [@brown2020language]). If we have a collection of tasks and an expert, and a prior distribution over task parameters, then we can meta-train an agent system with memory to learn an adaptive policy over the task class. An agent with memory could be e.g.\\xa0a recurrent neural architecture based on LSTM cells or a higher-order Markov model based on transformers. Once the agent has been meta-trained, it can then be deployed at test time with fixed parameters; importantly, the resulting agent will follow an adaptive policy.\\n\\n#### Setup.\\n\\nFor simplicity here we restrict ourselves to interaction processes with one initial latent parameter, i.e.\\xa0$\\\\Theta, A_1, O_1, \\\\ldots, A_T, O_T$. Formally, we need the following ingredients: A *prior distribution over task parameters*\\xa0$Q(\\\\Theta)$; a *collection of tasks*, where each task is represented by a conditional probability distribution\\xa0$Q(O \\\\mid \\\\Theta, W)$ over the next observation\\xa0$O$ given the task parameter\\xa0$\\\\Theta$ and the current memory state\\xa0$W$; an *expert*, represented by a conditional probability distribution\\xa0$Q(A \\\\mid \\\\Theta, E)$ over the next ideal action $A$ given the task parameter\\xa0$\\\\Theta$ and the current memory state\\xa0$E$; and an *agent*, represented by learnable conditional probability distributions\\xa0$P(A \\\\mid M)$ and\\xa0$P(O \\\\mid M)$ predicting the next action or observation given a memory state\\xa0$M$. In all of the three, the memory states $W$, $E$ and $M$ act as the sufficient statistics of the past interactions.\\n\\nWithout loss of generality, we can implement our agent using four trainable functions, $$\\\\begin{aligned}\\n  P(A \\\\mid M=m) &= f_A(m) &\\n  P(M\\' \\\\mid M=m, A=a) &= g_A(a, m) \\\\\\\\\\n  P(O \\\\mid M=m) &= f_O(m) &\\n  P(M\\' \\\\mid M=m, O=o) &= g_O(o, m)\\\\end{aligned}$$ where\\xa0$f_A$ and\\xa0$f_O$ implement the policy and the prediction, and where\\xa0$g_A$ and\\xa0$g_O$ are their associated memory transition kernels.\\n\\n#### Training.\\n\\nThe challenge is to meta-train the agent so that we address the following two problems:\\n\\n-   the observation predictions regress the Bayesian posterior predictive;\\n\\n-   and the action probabilities regress the posterior predictive, but respecting the causal constraints before and after choosing the action.\\n\\nPart\\xa0(a) is solved by minimizing the log-loss of the predictions in the standard way. We call this *factual teaching*. To address\\xa0(b), the agent first predicts the expert\\'s action and then samples its own action from it. Subsequently the expert reveals their action, inducing a log-loss penalty for the agent. We call this scheme *counterfactual teaching*. This training setup makes sure that the resulting agent, at deployment time, imitates the expert while treating past actions as interventions (see Appendix\\xa0[6](#sec:fcf-teaching){reference-type=\"ref\" reference=\"sec:fcf-teaching\"}). The computation graph is shown in Figure\\xa0[\\\\[fig:computation-graph\\\\]](#fig:computation-graph){reference-type=\"ref\" reference=\"fig:computation-graph\"}.\\n\\nIt is worth being clear about what this achieves. If the agent is trained using factual/counterfactual teaching to regress the observation/action probabilities, then upon convergence the functions\\xa0$f_A$ and\\xa0$f_O$ will approximate[^3] $$\\\\begin{aligned}\\n  f_A(m_t) &\\\\approx Q(A_{t+1} \\\\mid \\\\mathrm{do}(a)_{1:t}, o_{1:t}) \\\\\\\\\\n  \\\\text{and}\\\\quad\\n  f_O(m\\'_t) &\\\\approx Q(O_{t+1} \\\\mid \\\\mathrm{do}(a)_{1:t+1}, o_{1:t}),\\\\end{aligned}$$ where\\xa0$m_t$, and $m\\'_t$ are the sufficient statistics for the interaction histories and where all the past actions---and only those---have been intervened on. This avoids the delusions pointed out in Sections\\xa0[2](#sec:single){reference-type=\"ref\" reference=\"sec:single\"}\\xa0&\\xa0[3](#sec:sequential){reference-type=\"ref\" reference=\"sec:sequential\"}. Other probabilities, such as $$Q(A_{t+1}=a_{t+1} \\\\mid a_{1:t}, o_{1:t})\\n\\\\quad \\\\text{or} \\\\quad\\nQ(A_{t+1}=a_{t+1} \\\\mid a_{1:t}, \\\\mathrm{do}(o)_{1:t}),$$ where we condition on actions or where we intervene on the observations respectively, are not captured.\\n\\n#### Summary.\\n\\nWhen training a function approximator to learn an adaptive policy, the causal distinction between actions and observations translates into using counterfactual and factual teaching signals respectively. This ensures that predictions are amortized using the correct weighting of past histories that mix conditioning and intervening.\\n\\nRather than using an architecture with explicit internal memory, alternatively we can also train a single function implementing a sequence predictor $$P(X_{t+1} \\\\mid X_1=x_1, \\\\ldots, X_t=x_t) = f([x_1, x_2, \\\\ldots, x_t]).$$ In this approach the architecture does not distinguish between actions and observations, treating them uniformly as symbols. Since the entire past is provided as prediction context, no explicit memory is required because the past is trivially a sufficient statistic of itself. Even when the context is truncated to the most recent\\xa0$K$ symbols (=$K$-order Markov) it could still provide a good approximation to the ideal sufficient statistic (provided\\xa0$K$ is sufficiently large).\\n\\nFor all practical purposes, a given symbol\\xa0$x$ and its intervened version\\xa0$\\\\mathrm{do}(x)$ are best seen as entirely different symbols. For instance, $$P(X_4 \\\\mid \\\\mathrm{do}(x_1), x_2, x_3),\\n  \\\\qquad\\n  P(X_4 \\\\mid x_1, \\\\mathrm{do}(x_2), x_3),\\n  \\\\qquad\\\\text{and}\\\\qquad\\n  P(X_4 \\\\mid x_1, \\\\mathrm{do}(x_2), \\\\mathrm{do}(x_3)),$$ are three different conditional probability distributions because they differ in their intervened variables: each must be meta-learned separately, carefully switching between factual and counterfactual teaching signals. Thus in the previous example, assuming that $X_1$, $X_2$, and $X_3$ are binary, then there are not $2^3=8$, but $4^3=64$ different pasts of length 3, because each random variable has\\xa02 standard and\\xa02 intervened possible values.\\n\\n[\\\\[rem:distinguish\\\\]]{#rem:distinguish label=\"rem:distinguish\"} In the example given we explicitly distinguished between the task and the expert as providers of the factual and counterfactual teaching signals respectively. However, in many cases one cannot or does not want to draw this distinction. A timely example is given by interactive language models. Tokens generated by the model are actions to be treated as causal interventions, while tokens provided by the user are standard observations. Since we cannot tell ahead of time which party will produce which part of speech, it would be necessary to amortize each token both in its standard and intervened form using factual and counterfactual teaching signals.\\n\\n[\\\\[rem:delude\\\\]]{#rem:delude label=\"rem:delude\"} If we do not distinguish between actions and observations, then a language model will delude itself. In the opening epigraph of this report, the sentences generated by the model were incorporated into the prediction context as standard conditions.\\n\\nThe requirement of soliciting an expert\\'s action online from a trajectory generated by an agent for counterfactual teaching is similar to the Dagger algorithm [@ross2011reduction] where an expert provides supervising signals for imitation learning in states reached by a student agent. However, they come from different motivations. Dagger is designed to reduce the extrapolation error due to distribution shift at test time, and doesn\\'t consider the existence of latent parameters. The benefit of Dagger over vanilla imitation learning diminishes if a student can predict the expert\\'s action distribution exactly. In contrast, it becomes necessary to request counterfactual learning signals from an expert in this study at the presence of latent variables. The bias always exists in this case no matter how expressive the agent model is and how much data is available.\\n\\n[\\\\[rem:onlydoneeded\\\\]]{#rem:onlydoneeded label=\"rem:onlydoneeded\"} For the purpose of learning an adaptive policy, it appears that all we want is estimate the action distribution $P(A_{t+1} \\\\mid \\\\mathrm{do}(a)_{1:t}, o_{1:t})$. Therefore, training our model on the loss on $f_A(m)$ might be enough, although the loss defined for observation prediction could be useful as an auxiliary loss.\\n\\nOffline Adaptation and Control\\n------------------------------\\n\\nNow our goal is to train an agent to imitate an expert who is not available. Instead, we only have a dataset containing demonstrations, i.e.\\xa0trajectories of interactions between the expert and the tasks, from which we want to build our (causal-probabilistic) sequential model. This limitation does not impact learning a sequential predictor, but it does affect the problem of learning an adaptive policy in a fundamental way. Currently this is an open problem, and we will limit ourselves to merely pointing out the main difficulty.\\n\\nAs before we assume the interaction process is $\\\\Theta, A_1, O_1, \\\\ldots, A_T, O_T$, but now we are only provided with sampled trajectories of the form $\\\\tau = a_1, o_1, \\\\ldots, a_T, o_T$ where the task parameter\\xa0$\\\\theta$ is latent. Meta-training an agent for sequential prediction is easy: we select a trajectory at random, and then we sequentially provide factual teaching signals for the agent\\'s predictions. However, in the interactive case, if we do not know the task parameter\\xa0$\\\\theta$, then we cannot supply counterfactual teaching signals. To see this, imagine we attempt to meta-train following the approach of the prediction case, and sample a trajectory $\\\\tau = \\\\bar{a}_1, o_1, \\\\ldots, \\\\bar{a}_T, o_T$ from the dataset. Here the $\\\\bar{a}_t$ are the expert actions. The agent starts by forecasting the first action using\\xa0$P(A_1)$. To implement the counterfactual teaching signal, we penalize the prediction using the expert\\'s action as $\\\\ell = -\\\\log P(A_1=\\\\bar{a}_1)$ and let the agent pick its own action $a_1 \\\\sim P(A_1)$. If the two actions differ, i.e.\\xa0$a_1 \\\\neq \\\\bar{a}_1$, then the agent\\'s choice is incompatible with our sampled trajectory\\xa0$\\\\tau$ and hence we cannot use it to continue the sequence. This problem cannot be circumvented by replacing\\xa0$\\\\tau$ with another sequence\\xa0$\\\\tau\\'$ from the dataset that starts with\\xa0$a_1$, because $\\\\tau\\'$ might not have been generated by a task-expert combination having the same latent parameter\\xa0$\\\\theta$ as the original trajectory\\xa0$\\\\tau$.\\n\\nThis is not a problem of data sparsity: even if the dataset is rich enough to contain every possible interaction sequence (with different multiplicities) the above problem persists because the data has been collected under the presence of the confounding factor\\xa0$\\\\Theta$. To address the problem, it is necessary to incorporate assumptions (e.g.\\xa0a set of hypotheses) about the confounding variable, but currently it is unclear how to incorporate this seamlessly into a meta-learning scheme.\\n\\n#### Summary.\\n\\nIn general, we cannot meta-train an agent only from expert demonstrations to imitate the expert at deployment time, because said demonstrations could depend on unknown confounding variables. To train an agent using expert demonstrations, it is necessary to induce causal models which propose the necessary confounders for explaining the data.\\n\\nConclusion\\n==========\\n\\nIn this report we have explored the use of sequence models for control. This was motivated by the recent enormous success of large language models. We have shown that a naive use, where actions are chosen according to the predicted next action conditioned on the past, does not work in general because it creates delusions, i.e.\\xa0situations in which the agent mistakes its own actions for evidence about the task. These delusions occur whenever there are confounding factors that obscure the cause-effect relationships between actions and observations.\\n\\nTo overcome this, actions must be treated as causal interventions after being generated. Doing so introduces the correct causal constraints so that the agent can only learn about the task through the effects of its actions. However, performing interventions requires having access to the causal model. We then explored meta-learning a sequence model suitable for adaptive imitation. The main challenge here is to make sure that actions and observations are regressed as interventions and conditions respectively, and we have shown that this can be achieved via counterfactual and factual teaching signals respectively.\\n\\nFinally, if the sequence does not possess any latent confounders, then conditioning on the actions will not lead to self-delusions. In practice, there are many domains which could not be significantly affected by self-delusions. For instance, if the observations are sufficiently informative (with respect to the latent task and expert intentions), then the effect on the posterior over sequences can be reasonably assumed to be negligible.\\n\\nWhy does factual/counterfactual teaching work? {#sec:fcf-teaching}\\n==============================================\\n\\nWe explain why the computation graph described in Figure\\xa0[\\\\[fig:computation-graph\\\\]](#fig:computation-graph){reference-type=\"ref\" reference=\"fig:computation-graph\"} leads to the regression of the correctly-conditioned and -intervened conditional probabilities. During training, each sample trajectory $\\\\theta, a_1, \\\\bar{a}_1, o_1, a_2, \\\\bar{a}_2, o_2, \\\\ldots, a_T, \\\\bar{a}_T, o_T$ is generated by taking turns between the agent, the expert, and the task. For simplicity we assume that all the alphabets are finite and that the function approximator implementing the agent can represent the optimal solution. The first symbol\\xa0$\\\\theta$ is unobserved by the agent but observed by the task and the expert. The observations $o_t$ and the expert actions $\\\\bar{a}_t$ are used to compute losses, but the expert actions are immediately discarded thereafter and do not influence the remaining course of the realization. The probability of such a trajectory is $$B(\\\\theta, a_{\\\\leq T}, \\\\bar{a}_{\\\\leq T}, o_{\\\\leq T}) :=\\n  Q(\\\\theta)\\n    \\\\biggl( \\\\prod_{\\\\tau=1}^T \\n      P(A_\\\\tau = a_\\\\tau \\\\mid a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(A_\\\\tau = \\\\bar{a}_\\\\tau \\\\mid \\\\theta, a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(O_\\\\tau = o_\\\\tau \\\\mid \\\\theta, a_{\\\\leq \\\\tau}, o_{<\\\\tau})\\n    \\\\biggr),$$ where\\xa0$B$ stands for \"braided\" distribution, $P$ are the agent\\'s probabilities and $Q$ are the environmental/expert probabilities. The probabilities of the right-hand-side correspond to the causal mechanisms. At time $t$, the agent gets two training signals, namely a factual and counterfactual one, given by $$\\\\ell(o_t) := - \\\\log P(O_t = o_t \\\\mid a_{\\\\leq t}, o_{<t})\\n  \\\\qquad\\\\text{and}\\\\qquad\\n  \\\\ell(\\\\bar{a}_t) := - \\\\log P(A_t = \\\\bar{a}_t \\\\mid a_{<t}, o_{<t}),$$ obtained with probabilities $$\\\\sum_\\\\theta \\n  B(\\\\theta, a_{\\\\leq t}, o_{<t})\\n  Q(O_t = o_t \\\\mid \\\\theta, a_{\\\\leq t}, o_{<t})\\n  \\\\qquad\\\\text{and}\\\\qquad\\n  \\\\sum_\\\\theta\\n  B(\\\\theta, a_{<t}, o_{<t})\\n  Q(A_t = \\\\bar{a}_t \\\\mid \\\\theta, a_{<t}, o_{<t})$$ respectively. Note that the marginal distributions on $B$ were derived from the joint in the usual way, and that we average over the latent parameter $\\\\Theta$ because it is unobserved by the agent.\\n\\nThe training setup implies that each loss term is optimized only w.r.t.\\xa0a subset of the conditional probabilities. If we use backpropagation to optimize the loss in an end-to-end fashion, then the computation graph in Figure\\xa0[\\\\[fig:computation-graph\\\\]](#fig:computation-graph){reference-type=\"ref\" reference=\"fig:computation-graph\"}c (page\\xa0) implies that direct links such as $P_A \\\\rightarrow \\\\ell(\\\\bar{A})$ propagate gradients, but stochastic links such as $P_A \\\\rightsquigarrow A$ don\\'t. By tracing the gradient paths originating in each loss, one sees that the minimization of the log-losses $\\\\ell(\\\\bar{a}_t)$ and $\\\\ell(o_t)$ at time\\xa0$t$ *only depend* on the conditional probabilities $P(A_t = \\\\bar{a}_t \\\\mid a_{<t}, o_{<t})$ for all $\\\\bar{a}_t$ and $P(O_t = o_t \\\\mid a_{\\\\leq t}, o_{<t})$ for all $o_t$ respectively. Therefore, it is sufficient to analyze the log-losses separately and w.r.t.\\xa0to given fixed histories $a_{<t}, o_{<t}$ averaged over $\\\\theta$. Let us investigate actions and observations in turn.\\n\\nFor an action at time $t$, we have the expected log-loss: $$\\\\begin{aligned}\\n  L(A_t) \\n  &:= \\\\sum_{\\\\bar{a}_t} \\\\sum_\\\\theta\\n    B(\\\\theta, a_{<t}, o_{<t})\\n    Q(\\\\bar{a}_t \\\\mid \\\\theta, a_{<t}, o_{<t})\\n    \\\\ell(\\\\bar{a}_t) \\\\\\\\\\n  &\\\\stackrel{(a)}{=} - \\\\sum_{\\\\bar{a}_t} \\\\sum_\\\\theta\\n    Q(\\\\theta)\\n    \\\\biggl( \\\\prod_{\\\\tau=1}^{t-1} \\n      P(a_\\\\tau \\\\mid a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(o_\\\\tau \\\\mid \\\\theta, a_{\\\\leq \\\\tau}, o_{<\\\\tau})\\n    \\\\biggr)\\n    Q(\\\\bar{a}_t \\\\mid \\\\theta, a_{<t}, o_{<t})\\n    \\\\log P(\\\\bar{a}_t \\\\mid a_{<t}, o_{<t}) \\\\\\\\\\n  &\\\\stackrel{(b)}{=} - \\\\sum_{\\\\bar{a}_t} \\\\sum_\\\\theta\\n    Q(\\\\theta \\\\mid \\\\hat{a}_{<t}) \\n    \\\\biggl( \\\\prod_{\\\\tau=1}^{t-1} \\n      P(a_\\\\tau \\\\mid a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(o_\\\\tau \\\\mid \\\\theta, \\\\hat{a}_{<t}, o_{<\\\\tau})\\n    \\\\biggr)\\n    Q(\\\\bar{a}_t \\\\mid \\\\theta, \\\\hat{a}_{<t}, o_{<t})\\n    \\\\log P(\\\\bar{a}_t \\\\mid a_{<t}, o_{<t}) \\\\\\\\\\n  &\\\\stackrel{(c)}{=} \\n    \\\\biggl( \\\\prod_{\\\\tau=1}^{t-1} \\n      P(a_\\\\tau \\\\mid a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(o_\\\\tau \\\\mid \\\\hat{a}_{<t}, o_{<\\\\tau})\\n    \\\\biggr)\\n    \\\\biggl\\\\{ -\\\\sum_{\\\\bar{a}_t}\\n    Q(\\\\bar{a}_t \\\\mid \\\\hat{a}_{<t}, o_{<t})\\n    \\\\log P(\\\\bar{a}_t \\\\mid a_{<t}, o_{<t}) \\\\biggr\\\\}.\\\\end{aligned}$$ Equality\\xa0(a) is obtained by substituting the definitions. Equality\\xa0(b) is obtained in two steps. First we convert the action conditions\\xa0$a_\\\\tau$ to action interventions\\xa0$\\\\hat{a}_\\\\tau := \\\\mathrm{do}(a)_{\\\\tau}$ in the $Q$ probabilities. This works because the $Q$ probabilities are the causal factors. Then we introduce the remaining action interventions up to time $t-1$ to get $\\\\hat{a}_{<t} = \\\\mathrm{do}(a_{<t})$. One can do this because the probability arguments are independent of the actions downstream *after* intervention. Finally, equality\\xa0(c) results from using the chain rule on the $Q$-terms to obtain $Q(\\\\theta, \\\\mathrm{do}(a_{<t}), o_{<t})$, then marginalizing out $\\\\theta$, and finally using the chain rule again to break the joint probabilities into conditional probabilities, plus a cosmetic re-grouping of the terms.\\n\\nRewritten in this way reveals that $L(A_t)$ is an expectation over a collection of cross-entropies between $Q(A_t \\\\mid \\\\mathrm{do}({a}_{<t}), o_{<t})$ and $P(A_t \\\\mid a_{<t}, o_{<t})$, where the expectation is taken over all histories of length $t-1$. Hence, minimizing $L(A_t)$ w.r.t.\\xa0$P(A_t \\\\mid a_{<t}, o_{<t})$ implies $$\\\\label{eq:fcf-action}\\n  P(a_t \\\\mid a_{<t}, o_{<t}) \\n  = Q(a_t \\\\mid \\\\mathrm{do}(a_{<t}), o_{<t})$$ for all histories $a_{<t}, o_{<t}$ of length $t-1$. In particular, note how $Q(a_t \\\\mid \\\\mathrm{do}(a_{<t}), o_{<t})$ contains *intervened* past actions in the conditional.\\n\\nFollowing an analogous argument for the observation at time $t$, we get: $$\\\\begin{aligned}\\n  L(O_t)\\n  &:=\\\\sum_{o_t} \\\\sum_\\\\theta \\n    B(\\\\theta, a_{<t}, o_{<t})\\n    Q(A_t = \\\\bar{a}_t \\\\mid \\\\theta, a_{<t}, o_{<t})\\n    \\\\ell(o_t) \\\\\\\\\\n  &= - \\\\sum_{o_t} \\\\sum_\\\\theta \\n    Q(\\\\theta)\\n    \\\\biggl( \\\\prod_{\\\\tau=1}^t\\n      P(a_\\\\tau \\\\mid a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(o_\\\\tau \\\\mid \\\\theta, a_{\\\\leq \\\\tau}, o_{<\\\\tau})\\n    \\\\biggr)\\n    \\\\log P(o_t \\\\mid a_{\\\\leq t}, o_{<t}) \\\\\\\\\\n  &= - \\\\sum_{o_t} \\\\sum_\\\\theta \\n    \\\\biggl( \\\\prod_{\\\\tau=1}^t \\n      P(a_\\\\tau \\\\mid a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(o_\\\\tau \\\\mid \\\\theta, \\\\hat{a}_{\\\\leq t}, o_{<\\\\tau})\\n    \\\\biggr)\\n    \\\\log P(o_t \\\\mid a_{\\\\leq t}, o_{<t}) \\\\\\\\\\n  &= \\\\biggl( \\\\prod_{\\\\tau=1}^{t-1} \\n      P(a_\\\\tau \\\\mid a_{<\\\\tau}, o_{<\\\\tau})\\n      Q(o_\\\\tau \\\\mid \\\\hat{a}_{\\\\leq t}, o_{<\\\\tau})\\n    \\\\biggr)\\n    P(a_t \\\\mid a_{<t}, o_{<t})\\n    \\\\biggl\\\\{ -\\\\sum_{o_t}\\n    Q(o_t \\\\mid \\\\hat{a}_{\\\\leq t}, o_{<t})\\n    \\\\log P(o_t \\\\mid a_{\\\\leq t}, o_{<t}) \\\\biggr\\\\}.\\\\end{aligned}$$ As before, minimizing $L(O_t)$ w.r.t.\\xa0$P(o_t \\\\mid a_{\\\\leq t}, o_{<t})$ implies $$\\\\label{eq:fcf-observation}\\n  P(o_t \\\\mid a_{\\\\leq t}, o_{<t})\\n  = Q(o_t \\\\mid \\\\mathrm{do}(a_{\\\\leq t}), o_{<t}).$$ Hence, [\\\\[eq:fcf-action\\\\]](#eq:fcf-action){reference-type=\"eqref\" reference=\"eq:fcf-action\"} and [\\\\[eq:fcf-observation\\\\]](#eq:fcf-observation){reference-type=\"eqref\" reference=\"eq:fcf-observation\"} together show that the use of factual and counterfactual loss signals leads to conditional probabilities that correctly treat past actions as interventions.\\n\\n[^1]: It does work in some restricted settings discussed later.\\n\\n[^2]: The self-delusions discussed here are unrelated to the problem arising from observational confounding [@arjovsky2019invariant; @zolna2019task; @deletang2021causal].\\n\\n[^3]: Assuming the function classes are rich enough.\\n',\n",
       "  'bibliography_bbl': \"\\\\begin{thebibliography}{10}\\n\\n\\\\bibitem{abbeel2004apprenticeship}\\nP.~Abbeel and A.~Y. Ng.\\n\\\\newblock Apprenticeship learning via inverse reinforcement learning.\\n\\\\newblock In {\\\\em Proceedings of the twenty-first international conference on\\n  Machine learning}, page~1, 2004.\\n\\n\\\\bibitem{arjovsky2019invariant}\\nM.~Arjovsky, L.~Bottou, I.~Gulrajani, and D.~Lopez-Paz.\\n\\\\newblock Invariant risk minimization.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1907.02893}, 2019.\\n\\n\\\\bibitem{blakemore2000can}\\nS.-J. Blakemore, D.~Wolpert, and C.~Frith.\\n\\\\newblock Why can't you tickle yourself?\\n\\\\newblock {\\\\em Neuroreport}, 11(11):R11--R16, 2000.\\n\\n\\\\bibitem{Bommasani:21foundation}\\nR.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S.\\n  Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill, E.~Brynjolfsson, S.~Buch,\\n  D.~Card, R.~Castellon, N.~Chatterji, A.~S. Chen, K.~Creel, J.~Q. Davis,\\n  D.~Demszky, C.~Donahue, M.~Doumbouya, E.~Durmus, S.~Ermon, J.~Etchemendy,\\n  K.~Ethayarajh, L.~Fei{-}Fei, C.~Finn, T.~Gale, L.~Gillespie, K.~Goel, N.~D.\\n  Goodman, S.~Grossman, N.~Guha, T.~Hashimoto, P.~Henderson, J.~Hewitt, D.~E.\\n  Ho, J.~Hong, K.~Hsu, J.~Huang, T.~Icard, S.~Jain, D.~Jurafsky, P.~Kalluri,\\n  S.~Karamcheti, G.~Keeling, F.~Khani, O.~Khattab, P.~W. Koh, M.~S. Krass,\\n  R.~Krishna, R.~Kuditipudi, and et~al.\\n\\\\newblock On the opportunities and risks of foundation models.\\n\\\\newblock {\\\\em arXiv preprint arXiv:2108.07258}, 2021.\\n\\n\\\\bibitem{brown2020language}\\nT.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,\\n  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.\\n\\\\newblock Language models are few-shot learners.\\n\\\\newblock {\\\\em arXiv preprint arXiv:2005.14165}, 2020.\\n\\n\\\\bibitem{chen2021decision}\\nL.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,\\n  A.~Srinivas, and I.~Mordatch.\\n\\\\newblock Decision transformer: Reinforcement learning via sequence modeling.\\n\\\\newblock {\\\\em arXiv preprint arXiv:2106.01345}, 2021.\\n\\n\\\\bibitem{cover1999elements}\\nT.~M. Cover.\\n\\\\newblock {\\\\em Elements of information theory}.\\n\\\\newblock John Wiley \\\\& Sons, 1999.\\n\\n\\\\bibitem{deletang2021causal}\\nG.~D{\\\\'e}letang, J.~Grau-Moya, M.~Martic, T.~Genewein, T.~McGrath, V.~Mikulik,\\n  M.~Kunesch, S.~Legg, and P.~A. Ortega.\\n\\\\newblock Causal analysis of agent behavior for ai safety.\\n\\\\newblock {\\\\em arXiv preprint arXiv:2103.03938}, 2021.\\n\\n\\\\bibitem{duan2016rl}\\nY.~Duan, J.~Schulman, X.~Chen, P.~L. Bartlett, I.~Sutskever, and P.~Abbeel.\\n\\\\newblock Rl$^2$: Fast reinforcement learning via slow reinforcement learning.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1611.02779}, 2016.\\n\\n\\\\bibitem{ho2016generative}\\nJ.~Ho and S.~Ermon.\\n\\\\newblock Generative adversarial imitation learning.\\n\\\\newblock {\\\\em Advances in neural information processing systems},\\n  29:4565--4573, 2016.\\n\\n\\\\bibitem{janner2021reinforcement}\\nM.~Janner, Q.~Li, and S.~Levine.\\n\\\\newblock Reinforcement learning as one big sequence modeling problem.\\n\\\\newblock {\\\\em arXiv preprint arXiv:2106.02039}, 2021.\\n\\n\\\\bibitem{leike2016thompson}\\nJ.~Leike, T.~Lattimore, L.~Orseau, and M.~Hutter.\\n\\\\newblock Thompson sampling is asymptotically optimal in general environments.\\n\\\\newblock In {\\\\em Proceedings of the Thirty-Second Conference on Uncertainty in\\n  Artificial Intelligence, {UAI} 2016}. {AUAI} Press, 2016.\\n\\n\\\\bibitem{ng2000algorithms}\\nA.~Y. Ng, S.~J. Russell, et~al.\\n\\\\newblock Algorithms for inverse reinforcement learning.\\n\\\\newblock In {\\\\em Icml}, volume~1, page~2, 2000.\\n\\n\\\\bibitem{ortega2010bayesian}\\nP.~Ortega and D.~Braun.\\n\\\\newblock A {B}ayesian rule for adaptive control based on causal interventions.\\n\\\\newblock In {\\\\em Third Conference on Artificial General Intelligence (AGI\\n  2010)}, pages 121--126. Atlantis Press, 2010.\\n\\n\\\\bibitem{ortega2010minimum}\\nP.~A. Ortega and D.~A. Braun.\\n\\\\newblock A minimum relative entropy principle for learning and acting.\\n\\\\newblock {\\\\em Journal of Artificial Intelligence Research}, 38:475--511, 2010.\\n\\n\\\\bibitem{osband2017posterior}\\nI.~Osband and B.~Van~Roy.\\n\\\\newblock Why is posterior sampling better than optimism for reinforcement\\n  learning?\\n\\\\newblock In {\\\\em International conference on machine learning}, pages\\n  2701--2710. PMLR, 2017.\\n\\n\\\\bibitem{osborne1994course}\\nM.~J. Osborne and A.~Rubinstein.\\n\\\\newblock {\\\\em A course in game theory}.\\n\\\\newblock MIT press, 1994.\\n\\n\\\\bibitem{pearl2009causality}\\nJ.~Pearl.\\n\\\\newblock {\\\\em Causality}.\\n\\\\newblock Cambridge university press, 2009.\\n\\n\\\\bibitem{rezende2020causally}\\nD.~J. Rezende, I.~Danihelka, G.~Papamakarios, N.~R. Ke, R.~Jiang, T.~Weber,\\n  K.~Gregor, H.~Merzic, F.~Viola, J.~Wang, et~al.\\n\\\\newblock Causally correct partial models for reinforcement learning.\\n\\\\newblock {\\\\em arXiv preprint arXiv:2002.02836}, 2020.\\n\\n\\\\bibitem{ross2011reduction}\\nS.~Ross, G.~Gordon, and D.~Bagnell.\\n\\\\newblock A reduction of imitation learning and structured prediction to\\n  no-regret online learning.\\n\\\\newblock In {\\\\em Proceedings of the fourteenth international conference on\\n  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and\\n  Conference Proceedings, 2011.\\n\\n\\\\bibitem{sutton2018reinforcement}\\nR.~S. Sutton and A.~G. Barto.\\n\\\\newblock {\\\\em Reinforcement learning: An introduction}.\\n\\\\newblock MIT press, 2018.\\n\\n\\\\bibitem{veness2015compress}\\nJ.~Veness, M.~G. Bellemare, M.~Hutter, A.~Chua, and G.~Desjardins.\\n\\\\newblock Compress and control.\\n\\\\newblock In {\\\\em Twenty-Ninth AAAI Conference on Artificial Intelligence},\\n  2015.\\n\\n\\\\bibitem{vonNeumann1947theory}\\nJ.~Von~Neumann and O.~Morgenstern.\\n\\\\newblock {\\\\em Theory of games and economic behavior, 2nd rev}.\\n\\\\newblock Princeton university press, 1947.\\n\\n\\\\bibitem{wang2016learning}\\nJ.~X. Wang, Z.~Kurth-Nelson, D.~Tirumala, H.~Soyer, J.~Z. Leibo, R.~Munos,\\n  C.~Blundell, D.~Kumaran, and M.~Botvinick.\\n\\\\newblock Learning to reinforcement learn.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1611.05763}, 2016.\\n\\n\\\\bibitem{zolna2019task}\\nK.~Zolna, S.~Reed, A.~Novikov, S.~G. Colmenarejo, D.~Budden, S.~Cabi, M.~Denil,\\n  N.~de~Freitas, and Z.~Wang.\\n\\\\newblock Task-relevant adversarial imitation learning.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1910.01077}, 2019.\\n\\n\\\\end{thebibliography}\\n\",\n",
       "  'bibliography_bib': '@inproceedings{veness2015,\\nauthor = {Veness, Joel and Bellemare, Marc G. and Hutter, Marcus and Chua, Alvin and Desjardins, Guillaume},\\ntitle = {Compress and Control},\\nyear = {2015},\\nisbn = {0262511290},\\npublisher = {AAAI Press},\\nabstract = {This paper describes a new information-theoretic policy evaluation technique for reinforcement\\nlearning. This technique converts any compression or density model into a corresponding\\nestimate of value. Under appropriate stationarity and ergodicity conditions, we show\\nthat the use of a sufficiently powerful model gives rise to a consistent value function\\nestimator. We also study the behavior of this technique when applied to various Atari\\n2600 video games, where the use of suboptimal modeling techniques is unavoidable.\\nWe consider three fundamentally different models, all too limited to perfectly model\\nthe dynamics of the system. Remarkably, we find that our technique provides sufficiently\\naccurate value estimates for effective on-policy control. We conclude with a suggestive\\nstudy highlighting the potential of our technique to scale to large problems.},\\nbooktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},\\npages = {30163023},\\nnumpages = {8},\\nlocation = {Austin, Texas},\\nseries = {AAAI\\'15}\\n}\\n\\n\\n@inproceedings{ortega2010bayesian,\\n  title={A {B}ayesian rule for adaptive control based on causal interventions},\\n  author={Ortega, PA and Braun, DA},\\n  booktitle={Third Conference on Artificial General Intelligence (AGI 2010)},\\n  pages={121--126},\\n  year={2010},\\n  organization={Atlantis Press}\\n}\\n\\n@article{ortega2010minimum,\\n  title={A minimum relative entropy principle for learning and acting},\\n  author={Ortega, Pedro A and Braun, Daniel A},\\n  journal={Journal of Artificial Intelligence Research},\\n  volume={38},\\n  pages={475--511},\\n  year={2010}\\n}\\n\\n@book{vonNeumann1947theory,\\n  title={Theory of games and economic behavior, 2nd rev},\\n  author={Von Neumann, John and Morgenstern, Oskar},\\n  year={1947},\\n  publisher={Princeton university press}\\n}\\n\\n@book{savage1972foundations,\\n  title={The foundations of statistics},\\n  author={Savage, Leonard J},\\n  year={1972},\\n  publisher={Courier Corporation}\\n}\\n\\n@book{pearl2009causality,\\n  title={Causality},\\n  author={Pearl, Judea},\\n  year={2009},\\n  publisher={Cambridge university press}\\n}\\n\\n@book{sutton2018reinforcement,\\n  title={Reinforcement learning: An introduction},\\n  author={Sutton, Richard S and Barto, Andrew G},\\n  year={2018},\\n  publisher={MIT press}\\n}\\n\\n@book{hutter2004universal,\\n  title={Universal artificial intelligence: Sequential decisions based on algorithmic probability},\\n  author={Hutter, Marcus},\\n  year={2004},\\n  publisher={Springer Science \\\\& Business Media}\\n}\\n\\n@book{bellman1957dynamic,\\nauthor = {Bellman, Richard Ernest},\\ntitle = {Dynamic Programming},\\nyear = {1957},\\nisbn = {0486428095},\\npublisher = {Princeton University Press},\\n}\\n\\n@article{duan2016rl,\\n  title={Rl$^2$: Fast reinforcement learning via slow reinforcement learning},\\n  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},\\n  journal={arXiv preprint arXiv:1611.02779},\\n  year={2016}\\n}\\n\\n@article{wang2016learning,\\n  title={Learning to reinforcement learn},\\n  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},\\n  journal={arXiv preprint arXiv:1611.05763},\\n  year={2016}\\n}\\n\\n@book{mackay2003information,\\n  title={Information theory, inference and learning algorithms},\\n  author={MacKay, David JC and Mac Kay, David JC},\\n  year={2003},\\n  publisher={Cambridge university press}\\n}\\n\\n@book{cover1999elements,\\n  title={Elements of information theory},\\n  author={Cover, Thomas M},\\n  year={1999},\\n  publisher={John Wiley \\\\& Sons}\\n}\\n\\n@inproceedings{ortega2021adaptive,\\n  title={Adaptive Coding of Actions and Observations},\\n  author={Ortega, Pedro A and Braun, Daniel A},\\n  booktitle={NIPS Workshop on Information in Perception and Action},\\n  year={2012},\\n}\\n\\n@inproceedings{leike2016thompson,\\n  author    = {Jan Leike and\\n               Tor Lattimore and\\n               Laurent Orseau and\\n               Marcus Hutter},\\n  title     = {Thompson Sampling is Asymptotically Optimal in General Environments},\\n  booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial\\n               Intelligence, {UAI} 2016},\\n  publisher = {{AUAI} Press},\\n  year      = {2016},\\n}\\n\\n@article{brown2020language,\\n  title={Language models are few-shot learners},\\n  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},\\n  journal={arXiv preprint arXiv:2005.14165},\\n  year={2020}\\n}\\n\\n@book{osborne1994course,\\n  title={A course in game theory},\\n  author={Osborne, Martin J and Rubinstein, Ariel},\\n  year={1994},\\n  publisher={MIT press}\\n}\\n\\n@article{janner2021reinforcement,\\n  title={Reinforcement Learning as One Big Sequence Modeling Problem},\\n  author={Janner, Michael and Li, Qiyang and Levine, Sergey},\\n  journal={arXiv preprint arXiv:2106.02039},\\n  year={2021}\\n}\\n\\n@inproceedings{ross2011reduction,\\n  title={A reduction of imitation learning and structured prediction to no-regret online learning},\\n  author={Ross, St{\\\\\\'e}phane and Gordon, Geoffrey and Bagnell, Drew},\\n  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},\\n  pages={627--635},\\n  year={2011},\\n  organization={JMLR Workshop and Conference Proceedings}\\n}\\n\\n@article{blakemore2000can,\\n  title={Why can\\'t you tickle yourself?},\\n  author={Blakemore, Sarah-Jayne and Wolpert, Daniel and Frith, Chris},\\n  journal={Neuroreport},\\n  volume={11},\\n  number={11},\\n  pages={R11--R16},\\n  year={2000},\\n  publisher={LWW}\\n}\\n\\n@inproceedings{abbeel2004apprenticeship,\\n  title={Apprenticeship learning via inverse reinforcement learning},\\n  author={Abbeel, Pieter and Ng, Andrew Y},\\n  booktitle={Proceedings of the twenty-first international conference on Machine learning},\\n  pages={1},\\n  year={2004}\\n}\\n\\n@inproceedings{ng2000algorithms,\\n  title={Algorithms for inverse reinforcement learning.},\\n  author={Ng, Andrew Y and Russell, Stuart J and others},\\n  booktitle={Icml},\\n  volume={1},\\n  pages={2},\\n  year={2000}\\n}\\n\\n@article{ho2016generative,\\n  title={Generative adversarial imitation learning},\\n  author={Ho, Jonathan and Ermon, Stefano},\\n  journal={Advances in neural information processing systems},\\n  volume={29},\\n  pages={4565--4573},\\n  year={2016}\\n}\\n\\n@article{zolna2019task,\\n  title={Task-relevant adversarial imitation learning},\\n  author={Zolna, Konrad and Reed, Scott and Novikov, Alexander and Colmenarejo, Sergio Gomez and Budden, David and Cabi, Serkan and Denil, Misha and de Freitas, Nando and Wang, Ziyu},\\n  journal={arXiv preprint arXiv:1910.01077},\\n  year={2019}\\n}\\n\\n@article{deletang2021causal,\\n  title={Causal Analysis of Agent Behavior for AI Safety},\\n  author={D{\\\\\\'e}letang, Gr{\\\\\\'e}goire and Grau-Moya, Jordi and Martic, Miljan and Genewein, Tim and McGrath, Tom and Mikulik, Vladimir and Kunesch, Markus and Legg, Shane and Ortega, Pedro A},\\n  journal={arXiv preprint arXiv:2103.03938},\\n  year={2021}\\n}\\n\\n@article{arjovsky2019invariant,\\n  title={Invariant risk minimization},\\n  author={Arjovsky, Martin and Bottou, L{\\\\\\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},\\n  journal={arXiv preprint arXiv:1907.02893},\\n  year={2019}\\n}\\n\\n@article{chen2021decision,\\n  title={Decision transformer: Reinforcement learning via sequence modeling},\\n  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Michael and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},\\n  journal={arXiv preprint arXiv:2106.01345},\\n  year={2021}\\n}\\n\\n@inproceedings{veness2015compress,\\n  title={Compress and control},\\n  author={Veness, Joel and Bellemare, Marc G and Hutter, Marcus and Chua, Alvin and Desjardins, Guillaume},\\n  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},\\n  year={2015}\\n}\\n\\n@article{rezende2020causally,\\n  title={Causally correct partial models for reinforcement learning},\\n  author={Rezende, Danilo J and Danihelka, Ivo and Papamakarios, George and Ke, Nan Rosemary and Jiang, Ray and Weber, Theophane and Gregor, Karol and Merzic, Hamza and Viola, Fabio and Wang, Jane and others},\\n  journal={arXiv preprint arXiv:2002.02836},\\n  year={2020}\\n}\\n\\n@inproceedings{osband2017posterior,\\n  title={Why is posterior sampling better than optimism for reinforcement learning?},\\n  author={Osband, Ian and Van Roy, Benjamin},\\n  booktitle={International conference on machine learning},\\n  pages={2701--2710},\\n  year={2017},\\n  organization={PMLR}\\n}\\n\\n@Article{Hutter:11uiphil,\\n  author =       \"Samuel Rathmanner and Marcus Hutter\",\\n  title =        \"A Philosophical Treatise of Universal Induction\",\\n  journal =      \"Entropy\",\\n  volume =       \"13\",\\n  number =       \"6\",\\n  pages =        \"1076--1136\",\\n  _month =        jun,\\n  year =         \"2011\",\\n  bibtex =       \"http://www.hutter1.net/official/bib.htm#uiphil\",\\n  url =          \"http://arxiv.org/abs/1105.5721\",\\n  pdf =          \"http://www.hutter1.net/publ/uiphil.pdf\",\\n  latex =        \"http://www.hutter1.net/publ/uiphil.zip\",\\n  slides =       \"http://www.hutter1.net/publ/suiphil.pdf\",\\n  video1 =       \"http://www.youtube.com/watch?v=gb4oXRsw3yA\",\\n  video2 =       \"http://www.youtube.com/watch?v=Q_cHUpwpdFo\",\\n  project =      \"http://www.hutter1.net/official/projects.htm#ait\",\\n  doi =          \"10.3390/e13061076\",\\n  issn =         \"1099-4300\",\\n  keywords =     \"sequence prediction; inductive inference; Bayes rule;\\n                  Solomonoff prior; Kolmogorov complexity; Occam\\'s razor;\\n                  philosophical issues; confirmation theory; Black raven paradox.\",\\n}\\n\\n@InCollection{Hutter:12uaigentle,\\n  author =       \"Marcus Hutter\",\\n  title =        \"One Decade of Universal Artificial Intelligence\",\\n  booktitle =    \"Theoretical Foundations of Artificial General Intelligence\",\\n  pages =        \"67--88\",\\n  _editor =       \"Pei Wang and Ben Goertzel\",\\n  publisher =    \"Atlantis Press\",\\n  _month =        sep,\\n  year =         \"2012\",\\n  bibtex =       \"http://www.hutter1.net/official/bib.htm#uaigentle\",\\n  url =          \"http://arxiv.org/abs/1202.6153\",\\n  pdf =          \"http://www.hutter1.net/publ/uaigentle.pdf\",\\n  latex =        \"http://www.hutter1.net/publ/uaigentle.zip\",\\n  slides =       \"http://www.hutter1.net/publ/suaigentle.pdf\",\\n  slides2 =      \"http://www.hutter1.net/publ/suai4lay.pdf\",\\n  video =        \"http://vimeo.com/7321732\",\\n  video2 =       \"http://www.youtube.com/watch?v=I-vx5zbOOXI\",\\n  http =         \"http://2012.singularitysummit.com.au/2012/08/universal-artificial-intelligence/\",\\n  project =      \"http://www.hutter1.net/official/projects.htm#uai\",\\n  interview =    \"http://www.youtube.com/watch?v=a2tgUXm_txw\",\\n  doi =          \"10.2991/978-94-91216-62-6_5\",\\n  isbn =         \"978-94-91216-61-9(print) 978-94-91216-62-6(online)\",\\n  keywords =     \"artificial intelligence; reinforcement learning;\\n                  algorithmic information theory; sequential decision theory;\\n                  universal induction; rational agents; foundations.\",\\n}\\n\\n@InCollection{Hutter:17unilearn,\\n  author =       \"Marcus Hutter\",\\n  title =        \"Universal Learning Theory\",\\n  booktitle =    \"Encyclopedia of Machine Learning and Data Mining\",\\n  pages =        \"1295--1304\",\\n  editor =       \"C. Sammut and G. Webb\",\\n  publisher =    \"Springer\",\\n  _month =        aug,\\n  year =         \"2017\",\\n  edition =      \"2nd\",\\n  bibtex =       \"http://www.hutter1.net/official/bib.htm#unilearn\",\\n  url =          \"http://arxiv.org/abs/1102.2467\",\\n  pdf =          \"http://www.hutter1.net/publ/unilearn.pdf\",\\n  latex =        \"http://www.hutter1.net/publ/unilearn.tex\",\\n  slides =       \"http://www.hutter1.net/ai/susp.pdf\",\\n  project =      \"http://www.hutter1.net/official/projects.htm#ait\",\\n  doi =          \"10.1007/978-1-4899-7687-1_867\",\\n  isbn =         \"978-1-4899-7686-4\",\\n  keywords =     \"Algorithmic probability; Ray Solomonoff; induction;\\n                  prediction; decision; action; Turing machine;\\n                  Kolmogorov complexity; universal prior; Bayes\\' rule.\",\\n}\\n\\n@Article{Hutter:21oimilearn,\\n  author =       \"Michael K. Cohen and Marcus Hutter and Neel Nanda\",\\n  title =        \"Fully General Online Imitation Learning\",\\n  journal =      \"arXiv\",\\n  _month =        feb,\\n  year =         \"2021\",\\n  bibtex =       \"http://www.hutter1.net/official/bib.htm#oimilearn\",\\n  url =          \"http://arxiv.org/abs/2102.08686\",\\n  pdf =          \"http://www.hutter1.net/publ/oimilearn.pdf\",\\n  project =      \"http://www.hutter1.net/official/projects.htm#safe\",\\n}\\n\\n@article{Bommasani:21foundation,\\n  author    = {Rishi Bommasani and\\n               Drew A. Hudson and\\n               Ehsan Adeli and\\n               Russ Altman and\\n               Simran Arora and\\n               Sydney von Arx and\\n               Michael S. Bernstein and\\n               Jeannette Bohg and\\n               Antoine Bosselut and\\n               Emma Brunskill and\\n               Erik Brynjolfsson and\\n               Shyamal Buch and\\n               Dallas Card and\\n               Rodrigo Castellon and\\n               Niladri Chatterji and\\n               Annie S. Chen and\\n               Kathleen Creel and\\n               Jared Quincy Davis and\\n               Dorottya Demszky and\\n               Chris Donahue and\\n               Moussa Doumbouya and\\n               Esin Durmus and\\n               Stefano Ermon and\\n               John Etchemendy and\\n               Kawin Ethayarajh and\\n               Li Fei{-}Fei and\\n               Chelsea Finn and\\n               Trevor Gale and\\n               Lauren Gillespie and\\n               Karan Goel and\\n               Noah D. Goodman and\\n               Shelby Grossman and\\n               Neel Guha and\\n               Tatsunori Hashimoto and\\n               Peter Henderson and\\n               John Hewitt and\\n               Daniel E. Ho and\\n               Jenny Hong and\\n               Kyle Hsu and\\n               Jing Huang and\\n               Thomas Icard and\\n               Saahil Jain and\\n               Dan Jurafsky and\\n               Pratyusha Kalluri and\\n               Siddharth Karamcheti and\\n               Geoff Keeling and\\n               Fereshte Khani and\\n               Omar Khattab and\\n               Pang Wei Koh and\\n               Mark S. Krass and\\n               Ranjay Krishna and\\n               Rohith Kuditipudi and\\n               et al.},\\n  title     = {On the Opportunities and Risks of Foundation Models},\\n   journal={arXiv preprint arXiv:2108.07258},\\n  year      = {2021},\\n  url       = {https://arxiv.org/abs/2108.07258}\\n}\\n\\n@Article{Hutter:21ckillcat,\\n  author =       \"Michael K. Cohen and Marcus Hutter and Elliot Catt\",\\n  title =        \"Curiosity Killed or Incapacitated the Cat and the Asymptotically Optimal Agent\",\\n  journal =      \"IEEE Journal on Selected Areas in Information Theory\",\\n  volume =       \"2\",\\n  number =       \"2\",\\n  pages =        \"665--677\",\\n  publisher =    \"IEEE\",\\n  _month =        may,\\n  year =         \"2021\",\\n  bibtex =       \"http://www.hutter1.net/official/bib.htm#ckillcat\",\\n  url =          \"http://arxiv.org/abs/2006.03357\",\\n  pdf =          \"http://www.hutter1.net/publ/ckillcat.pdf\",\\n  project =      \"http://www.hutter1.net/official/projects.htm#safe\",\\n  issn =         \"2641-8770\",\\n  doi =          \"10.1109/JSAIT.2021.3079722\",\\n  keywords =     \"Artificial intelligence; learning; autonomous agents;\\n                  Bayes methods; information theory; inference algorithms; \\n                  history; reinforcement learning; Markov processes\",\\n}\\n\\n@Article{Hutter:21bomaix,\\n  author =       \"Michael K. Cohen and Badri Vellambi and Marcus Hutter\",\\n  title =        \"Intelligence and Unambitiousness Using Algorithmic Information Theory\",\\n  journal =      \"IEEE Journal on Selected Areas in Information Theory\",\\n  volume =       \"2\",\\n  number =       \"2\",\\n  pages =        \"678--690\",\\n  publisher =    \"IEEE\",\\n  _month =        apr,\\n  year =         \"2021\",\\n  bibtex =       \"http://www.hutter1.net/official/bib.htm#bomaix\",\\n  url =          \"http://arxiv.org/abs/2105.06268\",\\n  pdf =          \"http://www.hutter1.net/publ/bomaix.pdf\",\\n  slides =       \"http://www.hutter1.net/publ/sbomai.pdf\",\\n  poster =       \"http://www.hutter1.net/publ/pbomai.pdf\",\\n  press =        \"http://medium.com/analytics-vidhya/paper-summary-asymptotically-unambitious-artificial-general-intelligence-cohen-et-al-a5d091d501db\",\\n  project =      \"http://www.hutter1.net/official/projects.htm#safe\",\\n  issn =         \"2641-8770\",\\n  doi =          \"10.1109/JSAIT.2021.3073844\",\\n  keywords =     \"information theory; task analysis; computational modeling; history; \\n                  schedules; Bayes methods; artificial general intelligence;\\n                  existental threat; alignment problem; power; instrumental goal; \\n                  reinforcement learning; inference algorithms; autonomous agents; learning\",\\n}\\n\\n@InProceedings{Hutter:20aixipess,\\n  author =       \"Michael Cohen and Marcus Hutter\",\\n  title =        \"Pessimism About Unknown Unknowns Inspires Conservatism\",\\n  booktitle =    \"33rd Conference on Learning Theory ({COLT\\'20})\",\\n  address =      \"Virtual / Graz, Austria\",\\n  volume =       \"125\",\\n  series = \\t     \"Proceedings of Machine Learning Research\",\\n  pages =        \"1344--1373\",\\n  _editor =       \"Jacob Abernethy and Shivani Agarwal\",\\n  publisher =    \"PMLR\",\\n  _month =        jul,\\n  year =         \"2020\",\\n  bibtex =       \"http://www.hutter1.net/official/bib.htm#aixipess\",\\n  http =         \"http://proceedings.mlr.press/v125/cohen20a.html\",\\n  url =          \"http://arxiv.org/abs/2006.08753\",\\n  pdf =          \"http://www.hutter1.net/publ/aixipess.pdf\",\\n  slides =       \"http://www.hutter1.net/publ/saixipess.pdf\",\\n  video =        \"http://www.colt2020.org/virtual/papers/paper_221.html\",\\n  project =      \"http://www.hutter1.net/official/projects.htm#safe\",\\n  issn =         \"1532-4435\",\\n  keywords =     \"\",\\n}\\n\\n@Article{Solomonoff:64,\\n  author =       \"R. J. Solomonoff\",\\n  title =        \"A Formal Theory of Inductive Inference: Parts 1 and 2\",\\n  journal =      \"Information and Control\",\\n  volume =       \"7\",\\n  year =         \"1964\",\\n  pages =        \"1--22 and 224--254\",\\n  keywords =     \"MML, minimum message length encoding, description,\\n                 MDL, inductive inference, II, Kolmogoroff, Chaitin,\\n                 algorithmic probability, complexity\",\\n  abstract =     \"general paper on inductive inference, various methods\\n                 including probability and \\\\~{}MML or \\\\~{}MDL\\n                 http://www.csse.monash.edu.au/\\\\~{}lloyd/tildeMML (LA\\n                 MML page) Hint, search for: MML\",\\n  comment =      \"Concerned with extrapolation of sequences. Defines\\n                 probability of extension via likelihood random TM\\n                 program will generate it.\",\\n  mhnote =       \"+++ Classic paper initiating AIT & universal prediction\",\\n}\\n',\n",
       "  'arxiv_citations': {'1907.02893': True,\n",
       "   '2108.07258': True,\n",
       "   '2005.14165': True,\n",
       "   '2106.01345': True,\n",
       "   '2103.03938': True,\n",
       "   '1611.02779': True,\n",
       "   '2106.02039': True,\n",
       "   '2002.02836': True,\n",
       "   '1611.05763': True,\n",
       "   '1910.01077': True,\n",
       "   '1105.5721': True,\n",
       "   '1202.6153': True,\n",
       "   '1102.2467': True,\n",
       "   '2102.08686': True,\n",
       "   '2006.03357': True,\n",
       "   '2105.06268': True,\n",
       "   '2006.08753': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Deep learning',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #170',\n",
       "   'newsletter_url': 'https://mailchi.mp/6a288a3cd011/an-170analyzing-the-argument-for-risk-from-power-seeking-ai',\n",
       "   'summarizer': 'Robert',\n",
       "   'summary': \"**Delusions** in language models (LMs) like GPT-3 occur when an incorrect generation early on throws the LM off the rails later. Specifically, if there is some unobserved context that influences how humans generate text that the LM is unaware of, then the LM will generate some plausible text -- and then take that text as _evidence_ about what the unobserved context must be. This can be especially likely when the desired context or task for the generation is difficult to infer from the input. In these settings the human generating the text has access to a lot more information than the model, making generation harder for the model and delusions more likely: an incorrect generation will make it more likely that the model infers the task or context incorrectly. This also applies to sequence modelling approaches in RL like <@Decision Transformer@>(@Decision Transformer: Reinforcement Learning via Sequence Modeling@) and <@Trajectory Transformer@>(@Reinforcement Learning as One Big Sequence Modeling Problem@), where incorrectly chosen actions could change the model's beliefs about optimal future actions.\\n\\nThis work explains this problem using tools from causality and argues that these models should act as if their previous actions are causal interventions rather than observations. However, training a model in this way requires access to a model of the environment and the expert demonstrating trajectories in an online way, and the authors don't describe a way to do this with purely offline data (it may be fundamentally impossible). The authors do argue that in settings where the context or task information can be easily extracted from the observations so far, then delusions are less likely. This points to the importance of prompt engineering, or providing context information in another way to sequence models, so that they don't delude themselves.\",\n",
       "   'opinion': \"Understanding specific failure modes of large language model generation seems useful, and the detailed mathematical explanation here makes it easier to understand what exactly the problem is, and what we can do to fix it. I'd be interested to see whether we can distinguish delusions from other failure modes and measure what proportion of failures are delusions (although failure modes likely cant be as cleanly divided as Im implying here). However, it seems fundamentally very difficult to train using offline data in a way that the model does learn to understand its own actions as interventions, so other solutions may need to be found.\\n\",\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '2110.10819v1',\n",
       "   'arxiv_id': '2110.10819',\n",
       "   'title': 'Shaking the foundations: delusions in sequence models for interaction and control',\n",
       "   'authors': ['Pedro A. Ortega',\n",
       "    'Markus Kunesch',\n",
       "    'Grgoire Deltang',\n",
       "    'Tim Genewein',\n",
       "    'Jordi Grau-Moya',\n",
       "    'Joel Veness',\n",
       "    'Jonas Buchli',\n",
       "    'Jonas Degrave',\n",
       "    'Bilal Piot',\n",
       "    'Julien Perolat',\n",
       "    'Tom Everitt',\n",
       "    'Corentin Tallec',\n",
       "    'Emilio Parisotto',\n",
       "    'Tom Erez',\n",
       "    'Yutian Chen',\n",
       "    'Scott Reed',\n",
       "    'Marcus Hutter',\n",
       "    'Nando de Freitas',\n",
       "    'Shane Legg'],\n",
       "   'date_published': '2021-10-20 23:31:05+00:00',\n",
       "   'data_last_modified': '2021-10-20 23:31:05+00:00',\n",
       "   'url': 'http://arxiv.org/abs/2110.10819v1',\n",
       "   'abstract': 'The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models \"lack the understanding of the cause and effect of their actions\" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.',\n",
       "   'author_comment': 'DeepMind Tech Report, 16 pages, 4 figures',\n",
       "   'journal_ref': 'None',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'cs.LG',\n",
       "   'categories': \"['cs.LG', 'cs.AI']\",\n",
       "   'individual_summary': 'Title: Shaking the foundations: delusions in sequence models for interaction and control\\nAuthors: Pedro A. Ortega, Markus Kunesch, Grgoire Deltang, Tim Genewein, Jordi Grau-Moya, Joel Veness, Jonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, Tom Everitt, Corentin Tallec, Emilio Parisotto, Tom Erez, Yutian Chen, Scott Reed, Marcus Hutter, Nando de Freitas, Shane Legg\\nPaper abstract: The recent phenomenal success of language models has reinvigorated machine learning research, and large sequence models such as transformers are being applied to a variety of domains. One important problem class that has remained relatively elusive however is purposeful adaptive behavior. Currently there is a common perception that sequence models \"lack the understanding of the cause and effect of their actions\" leading them to draw incorrect inferences due to auto-suggestive delusions. In this report we explain where this mismatch originates, and show that it can be resolved by treating actions as causal interventions. Finally, we show that in supervised learning, one can teach a system to condition or intervene on data by training with factual and counterfactual error signals respectively.\\nSummary: **Delusions** in language models (LMs) like GPT-3 occur when an incorrect generation early on throws the LM off the rails later. Specifically, if there is some unobserved context that influences how humans generate text that the LM is unaware of, then the LM will generate some plausible text -- and then take that text as _evidence_ about what the unobserved context must be. This can be especially likely when the desired context or task for the generation is difficult to infer from the input. In these settings the human generating the text has access to a lot more information than the model, making generation harder for the model and delusions more likely: an incorrect generation will make it more likely that the model infers the task or context incorrectly. This also applies to sequence modelling approaches in RL like <@Decision Transformer@>(@Decision Transformer: Reinforcement Learning via Sequence Modeling@) and <@Trajectory Transformer@>(@Reinforcement Learning as One Big Sequence Modeling Problem@), where incorrectly chosen actions could change the model\\'s beliefs about optimal future actions.\\n\\nThis work explains this problem using tools from causality and argues that these models should act as if their previous actions are causal interventions rather than observations. However, training a model in this way requires access to a model of the environment and the expert demonstrating trajectories in an online way, and the authors don\\'t describe a way to do this with purely offline data (it may be fundamentally impossible). The authors do argue that in settings where the context or task information can be easily extracted from the observations so far, then delusions are less likely. This points to the importance of prompt engineering, or providing context information in another way to sequence models, so that they don\\'t delude themselves.\\nMy opinion: Understanding specific failure modes of large language model generation seems useful, and the detailed mathematical explanation here makes it easier to understand what exactly the problem is, and what we can do to fix it. I\\'d be interested to see whether we can distinguish delusions from other failure modes and measure what proportion of failures are delusions (although failure modes likely cant be as cleanly divided as Im implying here). However, it seems fundamentally very difficult to train using offline data in a way that the model does learn to understand its own actions as interventions, so other solutions may need to be found.\\n',\n",
       "   'paper_text': '',\n",
       "   'text': \"HIGHLIGHTS\\n[Draft report on existential risk from power-seeking AI](https://www.alignmentforum.org/posts/HduCjmXTBD4xYTegv/draft-report-on-existential-risk-from-power-seeking-ai) *(Joe Carlsmith)* (summarized by Rohin): This report investigates the classic AI risk argument in detail, and decomposes it into a set of conjunctive claims. Heres the quick version of the argument: We will likely build highly capable and agentic AI systems that are aware of their place in the world, and which will be pursuing problematic objectives. Thus, they will take actions that increase their power, which will eventually disempower humans, leading to an existential catastrophe. We will try and avert this, but will probably fail to do so since it is technically challenging and we are not capable of the necessary coordination.Theres a lot of vague words in the argument above, so lets introduce some terminology to make it clearer:- **[Advanced capabilities](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.lv9yiqgfft0n):** We say that a system has advanced capabilities if it outperforms the best humans on some set of important tasks (such as scientific research, business/military/political strategy, engineering, and persuasion/manipulation). - **[Agentic planning](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.70ean6ha5tu6)**: We say that a system engages in agentic planning if it (a) makes and executes plans, (b) in pursuit of objectives, (c) on the basis of models of the world. This is a very broad definition and doesnt have many of the connotations you might be used to for an agent. It does not need to be a literal planning algorithm -- for example, human cognition would count, despite (probably) not being just a planning algorithm.- **[Strategically aware](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.ut8tt579qm0v)**: We say that a system is strategically aware if it models the effects of gaining and maintaining power over humans and the real-world environment.- **[PS-misaligned](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.xeuhqiernpnr) (power-seeking misaligned):** On some inputs, the AI system seeks power in unintended ways due to problems with its objectives (if the system actually receives such inputs, then it is **practically PS-misaligned**).The core argument is then that AI systems with advanced capabilities, agentic planning, and strategic awareness (APS-systems) will be practically PS-misaligned, to an extent that causes an existential catastrophe. Of course, we will try to prevent this -- why should we expect that we cant fix the problem? The author considers possible remedies, and argues that they all seem quite hard:- We [could](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.ser8zbn1gx0r) give AI systems the right objectives (alignment), but this seems quite hard -- its not clear how we would solve either outer or inner alignment.- We [could](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.viufb7455zol) try to shape objectives to be e.g. myopic, but we dont know how to do this, and there are strong incentives against myopia.- We [could](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.8ogiacvl3jtk) try to limit AI capabilities by keeping systems special-purpose rather than general, but there are strong incentives for generality, and some special-purpose systems can be dangerous, too.- We [could](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.vy4yqugrdgac) try to prevent the AI system from improving its own capabilities, but this requires us to anticipate all the ways the AI system could improve, and there are incentives to create systems that learn and change as they gain experience. - We [could](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.534uqe12opz9) try to control the deployment situations to be within some set of circumstances where we know the AI system wont seek power. However, this seems harder and harder to do as capabilities increase, since with more capabilities, more options become available.- We [could](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.eq2ddewwckpx) impose a high threshold of safety before an AI system is deployed, but the AI system could still seek power during training, and there are many incentives pushing for faster, riskier deployment (even if we have already seen warning shots).- We [could](https://docs.google.com/document/d/1smaI1lagHHcrhoi6ohdq3TYIZv0eNWWZMPEy8C8byYg/edit#heading=h.xsj40o1f47ow) try to correct the behavior of misaligned AI systems, or mitigate their impact, after deployment. This seems like it requires humans to have comparable or superior power to the misaligned systems in question, though; and even if we are able to correct the problem at one level of capability, we need solutions that scale as our AI systems become more powerful.The author breaks the overall argument into six conjunctive claims, assigns probabilities to each of them, and ends up computing a 5% probability of existential catastrophe from misaligned, power-seeking AI by 2070. This is a lower bound, since the six claims together add a fair number of assumptions, and there can be risk scenarios that violate these assumptions, and so overall the author would shade upward another couple of percentage points. |\\n\\n\\n |\\n\\n\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** This is a great investigation of the typical argument for existential risk from AI systems adversarially optimizing against humans. When I put my own numbers in without looking at Joes numbers, I got a 3% chance of existential catastrophe by 2070 through the argument in this post, though I think I underestimated the probability for claim (4) so Id now get something more like 4%. (The main difference from Joes 5% is that I am more optimistic about possible remedies, though of course these differences are tiny relative to our high overall uncertainty.) |\\n\\n |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Comments on Carlsmith's Is power-seeking AI an existential risk?](https://www.alignmentforum.org/posts/cCMihiwtZx7kdcKgt/comments-on-carlsmith-s-is-power-seeking-ai-an-existential) *(Nate Soares)* (summarized by Rohin): This response to the report above touches on many topics, but has three main object-level disagreements and one meta-level disagreement:1. The author has significantly shorter timelines, though this is based on a very different argument structure than the one presented in the report above, and so it is hard to turn this into more concrete disagreements with the report.2. The author expects that alignment is hard enough that we wont solve it in time (which is not to say that it is harder than every other technical problem humanity has ever faced). Its also not clear how to turn this into more concrete disagreements with the report.3. The author does not expect to have warning shots where misaligned AI systems cause trillions of dollars of damage but *dont* cause an existential catastrophe, because this seems like too narrow a capability range for us to hit in practice. Even if there are warning shots, he expects that civilization will continue to deploy risky AI systems anyway, similarly to how we are not banning gain-of-function research despite the warning shot of COVID-19.4. On the meta level, the author expects that the decomposition of the AI risk argument into six conjunctive claims will typically bias you towards giving too low a probability on the overall conjunction. |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n TECHNICAL AI ALIGNMENT\\n\\n\\n PROBLEMS\\n[The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models](https://openreview.net/forum?id=JYtwGwIL7ye) *(Anonymous)* (summarized by Zach): Reward hacking occurs when RL agents exploit the difference between a true reward and a proxy. Reward hacking has been [observed in practice](https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/) ([AN #1](https://mailchi.mp/ff6340049bd0/alignment-newsletter-1)), and as reinforcement learning agents are trained with better algorithms, more data, and larger policies, they are at increased risk of overfitting their proxy objectives. However, reward hacking has not yet been systematically studied.This paper fills this gap by constructing four example environments with a total of nine proxy rewards to investigate how reward hacking changes as a function of optimization power. They increase optimization power in several different ways, such as increasing the size of the neural net, or providing the model with more fine-grained observations.Overall, the authors find that reward hacking occurs in five of the nine cases. Moreover, the authors observed phase transitions in four of these cases. These are stark transitions where a moderate increase in optimization power leads to a drastic increase in reward hacking behavior. This poses a challenge in monitoring the safety of ML systems. To address this the authors suggest performing anomaly detection to notice reward hacking and offer several baselines.  |\\n\\n\\n |\\n\\n\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Zach's opinion:** It is good to see an attempt at formalizing reward hacking. The experimental contributions are interesting and the anomaly detection method seems reasonable. However, the proxy rewards chosen to represent reward hacking are questionable. In my opinion, these rewards are obviously 'wrong' so it is less surprising that they result in undesired behavior. I look forward to seeing more comprehensive experiments on this subject.**Rohins opinion:** Note that on OpenReview, the authors say that one of the proxy rewards (maximize average velocity for the driving environment) was actually the default and they only noticed it was problematic after they had trained large neural nets on that environment. I do agree that future proxy objectives will probably be less clearly wrong than most of the ones in this paper. |\\n\\n |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n OTHER PROGRESS IN AI\\n\\n\\n DEEP LEARNING\\n[Shaking the foundations: delusions in sequence models for interaction and control](https://arxiv.org/abs/2110.10819) *(Pedro A. Ortega et al)* (summarized by Robert): **Delusions** in language models (LMs) like GPT-3 occur when an incorrect generation early on throws the LM off the rails later. Specifically, if there is some unobserved context that influences how humans generate text that the LM is unaware of, then the LM will generate some plausible text -- and then take that text as *evidence* about what the unobserved context must be. This can be especially likely when the desired context or task for the generation is difficult to infer from the input. In these settings the human generating the text has access to a lot more information than the model, making generation harder for the model and delusions more likely: an incorrect generation will make it more likely that the model infers the task or context incorrectly. This also applies to sequence modelling approaches in RL like [Decision Transformer](https://arxiv.org/abs/2106.01345) ([AN #153](https://mailchi.mp/54987353654b/an-153experiments-that-demonstrate-failures-of-objective-robustness)) and [Trajectory Transformer](https://arxiv.org/abs/2106.02039) ([AN #153](https://mailchi.mp/54987353654b/an-153experiments-that-demonstrate-failures-of-objective-robustness)), where incorrectly chosen actions could change the model's beliefs about optimal future actions.This work explains this problem using tools from causality and argues that these models should act as if their previous actions are causal interventions rather than observations. However, training a model in this way requires access to a model of the environment and the expert demonstrating trajectories in an online way, and the authors don't describe a way to do this with purely offline data (it may be fundamentally impossible). The authors do argue that in settings where the context or task information can be easily extracted from the observations so far, then delusions are less likely. This points to the importance of prompt engineering, or providing context information in another way to sequence models, so that they don't delude themselves. |\\n\\n\\n |\\n\\n\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Robert's opinion:** Understanding specific failure modes of large language model generation seems useful, and the detailed mathematical explanation here makes it easier to understand what exactly the problem is, and what we can do to fix it. I'd be interested to see whether we can distinguish delusions from other failure modes and measure what proportion of failures are delusions (although failure modes likely cant be as cleanly divided as Im implying here). However, it seems fundamentally very difficult to train using offline data in a way that the model does learn to understand its own actions as interventions, so other solutions may need to be found. |\\n\\n |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n NEWS\\n[GovAI Summer 2022 Fellowships](https://www.governance.ai/opportunities/fellowships) (summarized by Rohin): Applications are now open for the GovAI 2022 Summer Fellowship! This is an opportunity for early-career individuals to spend three months working on an AI governance research project, learning about the field, and making connections with other researchers and practitioners. Application deadline is Jan 1.[Foundations of Cooperative AI Lab](https://www.andrew.cmu.edu/user/coesterh/FOCAL/) (summarized by Rohin): This new lab at CMU aims to create foundations of game theory appropriate for advanced, autonomous AI agents -- think of work on agent foundations and [cooperative AI](https://arxiv.org/abs/2012.08630) ([AN #133](https://mailchi.mp/c8b57f25d787/an-133building-machines-that-can-cooperate-with-humans-institutions-or-other-machines)). Apply for a PhD [here](https://csd.cmu.edu/academics/doctoral/admissions) (deadline Dec 9) or for a postdoc [here](https://apply.interfolio.com/98450).[Public reports are now optional for EA Funds grantees](https://forum.effectivealtruism.org/posts/LKdtHdETxSYAXwoW6/public-reports-are-now-optional-for-ea-funds-grantees) *(Asya Bergal and Jonas Vollmer)* (summarized by Rohin): This is your regular reminder that you can apply to the Long-Term Future Fund (and the broader EA Funds) for funding for a wide variety of projects. They have now removed the requirement for public reporting of your grant. They encourage you to apply if you have a preference for private funding.[Sydney AI Safety Fellowship](https://forum.effectivealtruism.org/posts/QrwnajRpteBZhQZnu/sydney-ai-safety-fellowship) *(casebash)* (summarized by Rohin): This 7-week fellowship will provide fellows from Australia and New Zealand the opportunity to pursue projects in AI Safety or spend time upskilling. Applications are due December 14. |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI'm always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2021 Alignment Newsletter, All rights reserved.*\\n\\n**\"}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2104.03946v2',\n",
       "  'title': 'Learning What To Do by Simulating the Past',\n",
       "  'authors': ['David Lindner', 'Rohin Shah', 'Pieter Abbeel', 'Anca Dragan'],\n",
       "  'date_published': '2021-04-08 17:43:29+00:00',\n",
       "  'data_last_modified': '2021-05-03 10:51:40+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2104.03946v2',\n",
       "  'abstract': 'Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill.',\n",
       "  'author_comment': 'Presented at ICLR 2021',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'stat.ML'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'unlabeled',\n",
       "  'confidence_score': 0.9506179537,\n",
       "  'main_tex_filename': './iclr2021_conference.tex',\n",
       "  'text': '---\\nabstract: |\\n  Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will *already* be optimized for human preferences, and thus an agent can extract information about what humans want from the state [@shah2019preferences]. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a *single state* sampled from the optimal policy for that skill.\\nauthor:\\n- |\\n  David Lindner[^1]\\\\\\n  Department of Computer Science\\\\\\n  ETH Zurich\\\\\\n  `david.lindner@inf.ethz.ch`\\\\\\n  Rohin Shah, Pieter Abbeel & Anca Dragan\\\\\\n  Center for Human-Compatible AI\\\\\\n  UC Berkeley\\\\\\n  `{rohinmshah,pabbeel,anca}@berkeley.edu`\\\\\\nbibliography:\\n- iclr2021_conference.bib\\ntitle: Learning What To Do by Simulating the Past\\n---\\n\\nIntroduction\\n============\\n\\nAs deep learning has become popular, many parts of AI systems that were previously designed by hand have been replaced with learned components. Neural architecture search has automated architecture design\\xa0[@zoph2016neural; @elsken2018neural], population-based training has automated hyperparameter tuning\\xa0[@jaderberg2017population], and self-supervised learning has led to impressive results in language modeling\\xa0[@devlin2018bert; @radford2019language; @clark2020electra] and reduced the need for labels in image classification\\xa0[@oord2018representation; @he2019momentum; @chen2020simple]. However, in reinforcement learning, one component continues to be designed by humans: the task specification. Handcoded reward functions are notoriously difficult to specify\\xa0[@clark2016faulty; @krakovna2018specification], and learning from demonstrations\\xa0[@ng2000algorithms; @fu2017learning] or preferences\\xa0[@wirth2017survey; @christiano2017deep] requires a lot of human input. Is there a way that we can automate even the specification of what must be done?\\n\\nIt turns out that we can learn part of what the user wants *simply by looking at the state of the environment*: after all, the user will already have optimized the state towards their own preferences [@shah2019preferences]. For example, when a robot is deployed in a room containing an intact vase, it can reason that if its user wanted the vase to be broken, it would already have been broken; thus she probably wants the vase to remain intact.\\n\\nHowever, we must ensure that the agent distinguishes between aspects of the state that the user *couldn\\'t control* from aspects that the user *deliberately designed*. This requires us to simulate what the user *must have done* to lead to the observed state: anything that the user put effort into in the past is probably something the agent should do as well. As illustrated in Figure\\xa0[1](#fig:simulating_the_past){reference-type=\"ref\" reference=\"fig:simulating_the_past\"}, if we observe a Cheetah balancing on its front leg, we can infer how it must have launched itself into that position. Unfortunately, it is unclear how to simulate these past trajectories that lead to the observed state. So far, this has only been done in gridworlds, where all possible trajectories can be considered using dynamic programming\\xa0[@shah2019preferences].\\n\\nOur key insight is that we can sample such trajectories by starting at the observed state and simulating *backwards in time*. To enable this, we derive a gradient that is amenable to estimation through backwards simulation, and learn an inverse policy and inverse environment dynamics model using supervised learning to perform the backwards simulation. Then, the only remaining challenge is finding a reward representation that can be meaningfully updated from a single state observation. To that end, rather than defining the reward directly on the raw input space, we represent it as a linear combination of features learned through self-supervised representation learning. Putting these components together, we propose the *Deep Reward Learning by Simulating the Past* (Deep RLSP) algorithm.\\n\\nWe evaluate Deep RLSP on MuJoCo environments and show that it can recover fairly good performance on the task reward given access to a small number of states sampled from a policy optimized for that reward. We also use Deep RLSP to imitate skills generated using a skill discovery algorithm [@sharma2020dynamics], in some cases given just a *single state* sampled from the policy for that skill.\\n\\nInformation from the environment state cannot completely replace reward supervision. For example, it would be hard to infer how clean Bob would ideally want his room to be, if the room is currently messy because Bob is too busy to clean it. Nonetheless, we are optimistic that information from the environment state can be used to significantly reduce the burden of human supervision required to train useful, capable agents.\\n\\n![Suppose we observe a Cheetah balancing on its front leg (left). Given a simulator for the environment, Deep RLSP is able to infer how the cheetah must have acted to end up in this position. It can then imitate these actions in order to recreate this skill. Note that the state contains joint *velocities* in addition to positions, which makes the task more tractable than this picture might suggest.](figures/cheetah-drlsp.pdf){#fig:simulating_the_past width=\"\\\\\\\\textwidth\"}\\n\\nMethod {#sec:deep-rlsp}\\n======\\n\\nIn this section, we describe how Deep RLSP can learn a reward function for high dimensional environments given access only to a simulator and the observed state $s_0$.\\n\\n**Notation.** A finite-horizon Markov Decision Process (MDP) $\\\\mathcal{M}= \\\\langle \\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{T}, r, \\\\mathcal{P}, T \\\\rangle$ contains a set of states $\\\\mathcal S$ and a set of actions $\\\\mathcal A$. The transition function $\\\\mathcal T: \\\\mathcal S \\\\times \\\\mathcal A \\\\times \\\\mathcal S \\\\mapsto [0,1]$ determines the distribution over next states given a state and an action, and $\\\\mathcal{P}$ is a prior distribution over initial states. The reward function $r : \\\\mathcal{S} \\\\mapsto \\\\mathbb{R}$ determines the agent\\'s objective. $T \\\\in \\\\mathbb{Z}_{+}$ is a finite planning horizon. A *policy* $\\\\pi : S \\\\times A \\\\mapsto [0,1]$ specifies how to choose actions given a state. Given an initial state distribution, a policy and the transition function, we can sample a *trajectory* $\\\\tau$ by sampling the first state from $\\\\mathcal{P}$, every subsequent action from $\\\\pi$, and every subsequent state from $\\\\mathcal{T}$. We denote the probability distribution over trajectories as $\\\\langle \\\\mathcal{P}, \\\\pi, \\\\mathcal{T} \\\\rangle$ and write $\\\\tau \\\\sim \\\\langle \\\\mathcal{P}, \\\\pi, \\\\mathcal{T} \\\\rangle$ for the sampling step. We will sometimes write a single state $s$ instead of a distribution $\\\\mathcal{P}$ if the initial state is deterministic. The goal of reinforcement learning (RL) is to find a policy $\\\\pi^*$ that maximizes the expected cumulative reward $\\\\mathop{\\\\mathbb{E}}_{\\\\tau \\\\sim \\\\langle \\\\mathcal{P}, \\\\pi, \\\\mathcal{T} \\\\rangle}\\\\left[{\\\\sum_{t=1}^{T} r(s_t)}\\\\right]$.\\n\\nWe use $\\\\phi : \\\\mathcal{S}\\\\rightarrow \\\\mathbb{R}^n$ to denote a feature function (whether handcoded or learned) that produces a feature vector of length $n$ for every state. The reward function $r$ is linear over $\\\\phi$ if it can be expressed in the form $r(s) = \\\\theta^T \\\\phi(s)$ for some $\\\\theta \\\\in \\\\mathbb{R}^n$.\\n\\nWe assume that some *past* trajectory $\\\\tau_{-T:0} = s_{-T}a_{-T} \\\\dots a_{-1}s_0$ produced the observed state $s_0$.\\n\\nIdealized algorithm {#sec:rlsp}\\n-------------------\\n\\nWe first explain what we would ideally do, if we had a handcoded a feature function $\\\\phi$ and an enumerable (small) state space $\\\\mathcal{S}$ that affords dynamic programming. This is a recap of *Reward Learning by Simulating the Past* [RLSP; @shah2019preferences].\\n\\nWe assume the human follows a Boltzmann-rational policy $\\\\pi_t(a \\\\mid s, \\\\theta) \\\\propto \\\\exp(Q_t(s,a;\\\\theta))$, where the $Q$ values are computed using soft value iteration. Marginalizing over past trajectories, yields a distribution over the observed state $p(s_0 \\\\mid \\\\theta) = \\\\sum_{s_{-T} \\\\dots a_{-1}} p(\\\\tau =  s_{-T}a_{-T} \\\\dots a_{-1}s_0 \\\\mid \\\\theta)$. We compute the maximum likelihood estimate, $\\\\mathop{\\\\mathrm{arg\\\\!\\\\max}}_{\\\\theta} \\\\ln p(s_0 \\\\mid \\\\theta)$, via gradient ascent, by expressing the gradient of the *observed state* as a weighted combination of gradients of *consistent trajectories*\\xa0[@shah2019preferences Appendix B]: $$\\\\label{eq:state-to-traj-grad}\\n    \\\\nabla_{\\\\theta} \\\\ln p(s_0 \\\\mid \\\\theta) = \\\\mathop{\\\\mathbb{E}}_{\\\\tau_{-T:-1}~\\\\sim~p(\\\\tau_{-T:-1} \\\\mid s_0, \\\\theta)}\\\\left[{\\\\nabla_{\\\\theta} \\\\ln p(\\\\tau \\\\mid \\\\theta)}\\\\right]$$ $\\\\nabla_{\\\\theta} \\\\ln p(\\\\tau \\\\mid \\\\theta)$ is a gradient for inverse reinforcement learning. Since we assume a Boltzmann-rational human, this is the gradient for Maximum Causal Entropy Inverse Reinforcement Learning [MCEIRL; @ziebart2010modeling]. However, we still need to compute an expectation over all trajectories that end in $s_0$, which is in general intractable. @shah2019preferences use dynamic programming to compute this gradient in tabular settings.\\n\\nGradient as backwards-forwards consistency {#sec:backwards-forwards-consistency}\\n------------------------------------------\\n\\n**Approximating the expectation.** For higher-dimensional environments, we must approximate the expectation over past trajectories $p(\\\\tau_{-T:-1} \\\\mid s_0, \\\\theta)$. We would like to sample from the distribution, but it is not clear how to sample the past conditioned on the present. Our key idea is that just as we can sample the future by rolling out forwards in time, *we should be able to sample the past by rolling out backwards in time*. Note that by the Markov property we have: $$\\\\begin{aligned}\\n    p(\\\\tau_{-T:-1} \\\\mid s_0, \\\\theta) &= \\\\prod_{t=-T}^{-1} p(s_{t} \\\\mid a_{t}, s_{t+1}, \\\\dots s_0, \\\\theta) p(a_{t} \\\\mid s_{t+1},a_{t+1}, \\\\dots s_0, \\\\theta) \\\\\\\\\\n    &= \\\\prod_{t=-T}^{-1} p(s_{t} \\\\mid a_{t}, s_{t+1}, \\\\theta) p(a_{t} \\\\mid s_{t+1}, \\\\theta)\\\\end{aligned}$$ Thus, given the *inverse policy* $\\\\pi^{-1}_t(a_t \\\\mid s_{t+1}, \\\\theta)$, the *inverse environment dynamics* $\\\\mathcal{T}^{-1}_t(s_t \\\\mid a_t, s_{t+1}, \\\\theta)$, and the observed state $s_0$, we can sample a past trajectory $\\\\tau_{-T:-1} \\\\sim p(\\\\tau_{-T:-1} \\\\mid s_0, \\\\theta)$ by iteratively applying $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$, starting from $s_0$. Analogous to forward trajectories, we express the sampling as $\\\\tau_{-T:-1} \\\\sim \\\\langle s_0, \\\\pi^{-1}, \\\\mathcal{T}^{-1} \\\\rangle$. Thus, we can write the gradient in Equation\\xa0[\\\\[eq:state-to-traj-grad\\\\]](#eq:state-to-traj-grad){reference-type=\"ref\" reference=\"eq:state-to-traj-grad\"} as $\\\\mathop{\\\\mathbb{E}}_{\\\\tau_{-T:-1}~\\\\sim~\\\\langle s_0, \\\\pi^{-1}, \\\\mathcal{T}^{-1} \\\\rangle}\\\\left[{\\\\nabla_{\\\\theta} \\\\ln p(\\\\tau \\\\mid \\\\theta)}\\\\right]$.\\n\\n**Learning $\\\\pi$, $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$.** In order to learn $\\\\pi^{-1}$, we must first know $\\\\pi$. We assumed that the human was Boltzmann-rational, which corresponds to the *maximum entropy* reinforcement learning objective\\xa0[@levine2018reinforcement]. We use the Soft Actor-Critic algorithm [SAC; @haarnoja2018soft] to estimate the policy $\\\\pi(a \\\\mid s, \\\\theta)$, since it explicitly optimizes the maximum entropy RL objective.\\n\\nGiven the forward policy $\\\\pi(a \\\\mid s, \\\\theta)$ and simulator $\\\\mathcal{T}$, we can construct a dataset of sampled forward trajectories, and learn the inverse policy $\\\\pi^{-1}$ and the inverse environment dynamics $\\\\mathcal{T}^{-1}$ using supervised learning. Given these, we can then sample $\\\\tau_{-T:-1}$, allowing us to approximate the expectation in the gradient. In general, both $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$ could be stochastic and time-dependent.\\n\\n**Estimating the gradient for a trajectory.** We now turn to the term within the expectation, which is the inverse reinforcement learning gradient given a demonstration trajectory $\\\\tau = s_{-T} a_{-T} \\\\dots s_0$. Assuming that the user is Boltzmann-rational, this is the MCEIRL gradient\\xa0[@ziebart2010modeling], which can be written as [@shah2019preferences Appendix A]:\\n\\n$$\\\\label{eq:grad-traj}\\n    \\\\nabla_{\\\\theta} \\\\ln p(\\\\tau \\\\mid \\\\theta) = {\\\\color{ForestGreen} \\\\left( \\\\sum_{t=-T}^{0} \\\\phi(s_t) \\\\right)} - {\\\\color{Red} \\\\mathcal{F}_{-T}(s_{-T})} + {\\\\color{Fuchsia} \\\\sum_{t=-T}^{-1} \\\\left( \\\\mathop{\\\\mathbb{E}}\\\\limits_{s\\'_{t+1} \\\\sim \\\\mathcal{T}(\\\\cdot \\\\mid s_t, a_t)}\\\\left[{\\\\mathcal{F}_{t+1}(s\\'_{t+1})}\\\\right] - \\\\mathcal{F}_{t+1}(s_{t+1}) \\\\right)}$$\\n\\n$\\\\mathcal{F}$ is the expected feature count under $\\\\pi$, that is, $\\\\mathcal{F}_{-t}(s_{-t}) \\\\triangleq \\\\mathop{\\\\mathbb{E}}_{\\\\tau_{-t:0}~\\\\sim~\\\\langle s_{-t}, \\\\pi, \\\\mathcal{T} \\\\rangle}\\\\left[{\\\\sum_{t\\'=-t}^{0} \\\\phi(s_{t\\'})}\\\\right]$.\\n\\nThe first term computes the feature counts of the demonstrated trajectory $\\\\tau$, while the second term computes the feature counts obtained by the policy for the current reward function $\\\\theta$ (starting from the initial state $s_{-T}$). Since $r(s) = \\\\theta^T\\\\phi(s)$, these terms increase the reward of features present in the demonstration $\\\\tau$ and decrease the reward of features under the current policy. Thus, the gradient incentivizes *consistency* between the demonstration and rollouts from the learned policy.\\n\\nThe last term is essentially a correction for the observed dynamics: if we see that $s_t, a_t$ led to $s_{t+1}$, it corrects for the fact that we \"could have\" seen some other state $s\\'_{t+1}$. Since this correction is zero in expectation (and expensive to compute), we drop it for our estimator.\\n\\n**Gradient estimator.** After dropping the last term in Equation\\xa0[\\\\[eq:grad-traj\\\\]](#eq:grad-traj){reference-type=\"ref\" reference=\"eq:grad-traj\"}, expanding the definition of $\\\\mathcal{F}$, and substituting in to Equation\\xa0[\\\\[eq:state-to-traj-grad\\\\]](#eq:state-to-traj-grad){reference-type=\"ref\" reference=\"eq:state-to-traj-grad\"}, our final gradient estimator is: $$\\\\label{eq:final-grad}\\n    \\\\nabla_{\\\\theta} \\\\ln p(s_0 \\\\mid \\\\theta) = {\\\\color{Blue}\\\\mathop{\\\\mathbb{E}}\\\\limits_{\\\\tau_{-T:-1}~\\\\sim~\\\\langle s_0, \\\\pi^{-1}, \\\\mathcal{T}^{-1} \\\\rangle}} \\\\left[ {\\\\color{ForestGreen} \\\\left( \\\\sum_{t=-T}^{0} \\\\phi(s_t) \\\\right)}  - {\\\\color{Brown}\\\\mathop{\\\\mathbb{E}}\\\\limits_{\\\\tau\\'~\\\\sim~\\\\langle s_{-T}, \\\\pi, \\\\mathcal{T} \\\\rangle}} \\\\left[ {\\\\color{Red} \\\\left( \\\\sum_{t=-T}^{0} \\\\phi(s\\'_t) \\\\right)} \\\\right] \\\\right]$$ Thus, given $s_0$, $\\\\theta$, $\\\\pi$, $\\\\mathcal{T}$, $\\\\pi^{-1}$, and $\\\\mathcal{T}^{-1}$, computing the gradient consists of three steps:\\n\\n1.   Simulate backwards from $s_0$, and compute the feature counts of the resulting trajectories.\\n\\n2.   Simulate forwards from $s_{-T}$ of these trajectories, and compute their feature counts.\\n\\n3.  Take the difference between these two quantities.\\n\\nThis again incentivizes consistency, this time between the backwards and forwards trajectories: the gradient leads to movement towards \"what the human must have done\" and away from \"what the human would do if they had this reward\". The gradient becomes zero when they are identical.\\n\\nIt may seem like the backwards and forwards trajectories should always be consistent with each other, since $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$ are inverses of $\\\\pi$ and $\\\\mathcal{T}$. The key difference is that $s_0$ imposes constraints on the backwards trajectories, but not on the forward trajectories. For example, suppose we observe $s_0$ in which a vase is unbroken, and our current hypothesis is that the user wants to break the vase. When we simulate backwards, our trajectory will contain an unbroken vase, but when we simulate forwards from $s_{-T}$, $\\\\pi$ will break the vase. The gradient would then reduce the reward for a broken vase and increase the reward for an unbroken vase.\\n\\nLearning a latent MDP\\n---------------------\\n\\nOur gradient still relies on a feature function $\\\\phi$, with the reward parameterized as $r(s) = \\\\theta^T\\\\phi(s)$. A natural way to remove this assumption would be to instead allow $\\\\theta$ to parameterize a neural network, which can then learn whatever features are relevant to the reward from the RLSP gradient.\\n\\nHowever, this approach will not work because the information contained in the RLSP gradient is insufficient to identify the appropriate features to construct: after all, it is derived from a single state. If we were to learn a single unified reward using the same gradient, the resulting reward would likely be degenerate: for example, it may simply identify the observed state, that is $R(s) = \\\\mathbbm{1}[s = s_0]$.\\n\\nThus, we continue to assume that the reward is linear in features, and instead *learn the feature function* using self-supervised learning\\xa0[@oord2018representation; @he2019momentum]. In our experiments, we use a variational autoencoder [VAE; @kingma2013auto] to learn the feature function. The VAE encodes the states into a latent feature representation, which we can use to learn a reward function if the environment is fully observable, i.e., the states contain all relevant information.\\n\\nFor partially observable environments recurrent state space models [RSSMs; @karl2016deep; @doerr2018probabilistic; @buesing2018learning; @kurutach2018learning; @hafner2018learning; @hafner2019dream] could be used instead. These methods aim to learn a *latent MDP*, by computing the states using a recurrent model over the observations, thus allowing the states to encode the history. For such a model, we can imagine that the underlying POMDP has been converted into a latent MDP whose feature function $\\\\phi$ is the identity. We can then compute gradients directly in this latent MDP.\\n\\nDeep RLSP {#sec:full-algo}\\n---------\\n\\n$D \\\\gets$ dataset of environment interactions Initialize $\\\\phi_e, \\\\phi_d, \\\\pi, \\\\pi^{-1}, \\\\mathcal{T}^{-1}, \\\\theta$ randomly. $\\\\phi_e, \\\\phi_d \\\\gets$ SelfSupervisedLearning$(D)$ Initialize experience replay $E$ with data in $D$. $\\\\mathcal{T}^{-1}\\\\gets$ SupervisedLearning$(D)$ $T \\\\gets 1$ $\\\\pi \\\\gets$ SAC$(\\\\theta)$ $\\\\pi^{-1}\\\\gets$ SupervisedLearning$(\\\\phi_e, E)$ $\\\\theta \\\\gets \\\\theta + \\\\alpha \\\\; \\\\times$ [ComputeGrad]{.smallcaps}($\\\\{ s_0 \\\\}$, $\\\\pi$, $\\\\mathcal{T}$, $\\\\pi^{-1}$, $\\\\mathcal{T}^{-1}$, $T$, $\\\\phi_e$) $T \\\\gets T + 1$\\n\\n$\\\\{ \\\\tau_{\\\\text{backward}} \\\\} \\\\gets$ Rollout$(\\\\{ s_0 \\\\}, \\\\pi^{-1}, \\\\mathcal{T}^{-1}, T)$ $\\\\phi_{\\\\text{backward}} \\\\gets$ AverageFeatureCounts$(\\\\phi_e, \\\\{ \\\\tau_{\\\\text{backward}} \\\\})$ $\\\\{ s_{-T} \\\\} \\\\gets$ FinalStates$(\\\\{ \\\\tau_{\\\\text{backward}} \\\\})$ $\\\\{ \\\\tau_{\\\\text{forward}} \\\\} \\\\gets$ Rollout$(\\\\{ s_{-T} \\\\}, \\\\pi, \\\\mathcal{T}, T)$ $\\\\phi_{\\\\text{forward}} \\\\gets$ AverageFeatureCounts$(\\\\phi_e, \\\\{ \\\\tau_{\\\\text{forward}} \\\\})$ Relabel $\\\\{ \\\\tau_{\\\\text{backward}} \\\\}$, $\\\\{ \\\\tau_{\\\\text{forward}} \\\\}$ and add them to $E$.\\n\\nPutting these components together gives us the Deep RLSP algorithm (Algorithm\\xa0[\\\\[alg:deep-rlsp\\\\]](#alg:deep-rlsp){reference-type=\"ref\" reference=\"alg:deep-rlsp\"}). We first learn a feature function $\\\\phi$ using self-supervised learning, and then train an inverse environment dynamics model $\\\\mathcal{T}^{-1}$, all using a dataset of environment interactions (such as random rollouts). Then, we update $\\\\theta$ using Equation\\xa0[\\\\[eq:final-grad\\\\]](#eq:final-grad){reference-type=\"ref\" reference=\"eq:final-grad\"}, and continually train $\\\\pi$, and $\\\\pi^{-1}$ alongside $\\\\theta$ to keep them up to date. The full algorithm also adds a few bells and whistles that we describe next.\\n\\n**Initial state distribution $\\\\mathcal{P}$.** The attentive reader may wonder why our gradient appears to be independent of $\\\\mathcal{P}$. This is actually not the case: while $\\\\pi$ and $\\\\mathcal{T}$ are independent of $\\\\mathcal{P}$, $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$ *do* depend on it. For example, if we observe Alice exiting the San Francisco airport, the corresponding $\\\\pi^{-1}$ should hypothesize different flights if she started from New York than if she started from Tokyo.\\n\\nHowever, in order to actually produce such explanations, we must train $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$ solely on trajectories of length $T$ starting from $s_{-T} \\\\sim \\\\mathcal{P}$. We instead train $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$ on a variety of trajectory data, which loses the useful information in $\\\\mathcal{P}$, but leads to several benefits. First, we can train the models on exactly the distributions that they will be used on, allowing us to avoid failures due to distribution shift. Second, the horizon $T$ is no longer critical: previously, $T$ encoded the separation in time between $s_{-T}$ and $s_0$, and as a result misspecification of $T$ could cause bad results. Since we now only have information about $s_0$, it doesn\\'t matter much what we set $T$ to, and as a result we can use it to set a curriculum (discussed next). Finally, this allows Deep RLSP to be used in domains where an initial state distribution is not available.\\n\\nNote that we are no longer able to use information about $\\\\mathcal{P}$ through $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$. However, having information about $\\\\mathcal{P}$ might be crucial in some applications to prevent Deep RLSP from converging to a degenerate solution with $s_{-T} = s_0$ and a policy $\\\\pi$ that does nothing. While we did not find this to be a problem in our experiments, we discuss a heuristic to incorporate information about $s_{-T}$ into Deep RLSP in Appendix\\xa0[8](#app:ablation){reference-type=\"ref\" reference=\"app:ablation\"}.\\n\\n**Curriculum.** Since the horizon $T$ is no longer crucial, we can use it to provide a curriculum. We initially calculate gradients with low values of $T$, to prevent compounding errors in our learned models, and making it easier to enforce backwards-forwards consistency, and then slowly grow $T$, making the problem harder. In practice, we found this crucial for performance: intuitively, it is much easier to make short backwards and forwards trajectories consistent than with longer trajectories; the latter would likely have much higher variance.\\n\\n**Multiple input states.** If we get multiple independent $s_0$ as input, we average their gradients.\\n\\n**Experience replay.** We maintain an experience replay buffer $E$ that persists across policy training steps. We initialize $E$ with the same set of environment interactions that the feature function and inverse environment dynamics model are trained on. When computing the gradient, we collect all backward and forward trajectories and add them to $E$. To avoid compounding errors from the inverse environment dynamics model, we relabel all transitions using a simulator of the environment. Whenever we\\'d add a transition $(s, a, s\\')$ to $E$, we initialize the simulator at $s$ and execute $a$ to obtain $\\\\tilde{s}$ and add transition $(s_1, a, \\\\tilde{s})$ to $E$ instead.\\n\\nExperiments\\n===========\\n\\nSetup\\n-----\\n\\nTo demonstrate that Deep RLSP can be scaled to complex, continuous, high-dimensional environments, we use the MuJoCo physics simulator\\xa0[@todorov2012mujoco]. We consider the *Inverted Pendulum*, *Half-Cheetah* and *Hopper* environments implemented in Open AI Gym\\xa0[@brockman2016openai]. The hyperparameters of our experiments are described in detail in Appendix\\xa0[7](#app:hyperparameters){reference-type=\"ref\" reference=\"app:hyperparameters\"}. We provide code to replicate our experiments at <https://github.com/HumanCompatibleAI/deep-rlsp>.\\n\\n**Baselines.** To our knowledge, this is the first work to train policies using a single state as input. Due to lack of alternatives, we compare against GAIL\\xa0[@ho2016generative] using the implementation from the `imitation` library\\xa0[@wang2020imitation]. For each state we provide to Deep RLSP, we provide a transition $(s, a, s\\')$ to GAIL.\\n\\n**Ablations.** In Section\\xa0[2.2](#sec:backwards-forwards-consistency){reference-type=\"ref\" reference=\"sec:backwards-forwards-consistency\"}, we derived a gradient for Deep RLSP that enforces consistency between the backwards and forwards trajectories. However, we could also ignore the temporal information altogether. If an optimal policy led to the observed state $s_0$, then it is probably a good bet that $s_0$ is high reward, and that the agent should try to keep the state similar to $s_0$. Thus, we can simply set $\\\\theta = \\\\frac{\\\\phi(s_0)}{||\\\\phi(s_0)||}$, and not deal with $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$ at all.\\n\\nHow should we handle multiple states $s_0^1, \\\\dots, s_0^N$? Given that these are all sampled i.i.d. from rollouts of an optimal policy, a natural choice is to simply average the feature vectors of all of the states, which we call *AverageFeatures*. Alternatively, we could view each of the observed states as a potential *waypoint* of the optimal policy, and reward an agent for being near any one of them. We implement this *Waypoints* method as $R(s) = \\\\max_i \\\\frac{\\\\phi(s^i_{0})}{||\\\\phi(s^i_{0})||} \\\\cdot \\\\phi(s)$. Note that both of these ablations still require us to learn the feature function $\\\\phi$.\\n\\n**Feature learning dataset.** By default, we use random rollouts to generate the initial dataset that is used to train the features $\\\\phi$ and the inverse environment dynamics model $\\\\mathcal{T}^{-1}$. (This is $D$ in Algorithm\\xa0[\\\\[alg:deep-rlsp\\\\]](#alg:deep-rlsp){reference-type=\"ref\" reference=\"alg:deep-rlsp\"}.) However, in the inverted pendulum environment, the pendulum falls very quickly in random rollouts, and $\\\\mathcal{T}^{-1}$ never learns what a balanced pendulum looks like. So, for this environment only, we combine random rollouts with rollouts from an expert policy that balances the pendulum.\\n\\nGridworld environments {#sec:gridworlds}\\n----------------------\\n\\nAs a first check, we consider the gridworld environments in\\xa0@shah2019preferences. In these stylized gridworlds, self-supervised learning should not be expected to learn the necessary features. For example, in the room with vase environment, the two door features are just particular locations, with no distinguishing features that would allow self-supervised learning to identify these locations as important. So, we run Algorithm\\xa0[\\\\[alg:deep-rlsp\\\\]](#alg:deep-rlsp){reference-type=\"ref\" reference=\"alg:deep-rlsp\"} without the feature learning and instead use the pre-defined feature function of the environments. With this setup we are able to use Deep RLSP to recover the desired behavior from a single state in all environments in which the exact RLSP algorithm is able to recover it. However, AverageFeatures fails on several of the environments. Since only one state is provided, Waypoints is equivalent to AverageFeatures. It is not clear how to apply GAIL to these environments, and so we do not compare to it. Further details on all of the environments and results can be found in Appendix\\xa0[6](#app:gridworlds){reference-type=\"ref\" reference=\"app:gridworlds\"}.\\n\\nSolving the environments without access to the reward function {#sec:existing-envs}\\n--------------------------------------------------------------\\n\\n::: {#tab:final_return_rlsp_policies}\\n   Environment   SAC   \\\\# states      Deep RLSP      AverageFeatures     Waypoints          GAIL\\n  ------------- ----- ----------- ----------------- ----------------- ---------------- ---------------\\n                                                                                       \\n    Pendulum               1          303 (299)           6 (2)             N/A         **1000 (0)**\\n                          10          335 (333)           3 (1)            4 (1)        **1000 (0)**\\n                          50          339 (331)           6 (4)          3.7 (0.3)      **1000 (0)**\\n                                                                                       \\n    (forward)              1       **4591 (2073)**   **6466 (3343)**        N/A           -288 (55)\\n                          10       **6917 (421)**    **6245 (2352)**      -10 (23)       -296 (172)\\n                          50       **6078 (589))**     4504 (2970)       -126 (38)        -54 (295)\\n                                                                                       \\n   (backward)              1         5730 (2733)     **12443 (645)**        N/A           -335 (46)\\n                          10         7917 (249)      **12829 (651)**     -80 (388)        -283 (45)\\n                          50         7588 (171)      **11616 (178)**     -509 (87)       2113 (1015)\\n                                                                                       \\n   (terminate)             1           68 (8)            99 (45)            N/A          **991 (9)**\\n                          10           47 (21)          159 (126)          58 (7)       **813 (200)**\\n                          50           72 (1)            65 (36)           14 (4)       **501 (227)**\\n                                                                                       \\n    (penalty)              1       **1850 (634)**    **2537 (363)**         N/A            990 (9)\\n                          10        **2998 (62)**     **3103 (64)**      709 (133)        784 (229)\\n                          50       **1667 (737)**    **2078 (581)**    **1612 (785)**     508 (259)\\n\\n  : Average returns achieved by the policies learned through various methods, for different numbers of input states. The states are sampled from a policy trained using SAC on the true reward function; the return of that policy is given as a comparison. Besides the SAC policy return, all values are averaged over 3 seeds and the standard error is given in parentheses. We don\\'t report Waypoints on 1 state as it is identical to AverageFeatures on 1 state.\\n:::\\n\\nFirst we look at the typical target behavior in each environment: balancing the inverted pendulum, and making the half-cheetah and the hopper move forwards. Additionally we consider the goal of making the cheetah run backwards (that is, the negative of its usual reward function). We aim to use Deep RLSP to learn these behaviors *without having access to the reward function*.\\n\\nWe train a policy using soft actor critic \\xa0[SAC; @haarnoja2018soft] to optimize for the true reward function, and sample either 1, 10 or 50 states from rollouts of this policy to use as input. We then use Deep RLSP to infer a reward and policy. Ideally we would evaluate this learned policy rather than reoptimizing the learned reward, since learned reward models can often be gamed\\xa0[@stiennon2020learning], but it would be too computationally expensive to run the required number of SAC steps during each policy learning step. As a result, we run SAC for many more iterations on the inferred reward function, and evaluate the resulting policy on the true reward function (which Deep RLSP does not have access to).\\n\\nResults are shown in Table\\xa0[1](#tab:final_return_rlsp_policies){reference-type=\"ref\" reference=\"tab:final_return_rlsp_policies\"}. In Hopper, we noticed that videos of the policies learned by Deep RLSP looked okay, but the quantitative evaluation said otherwise. It turns out that the policies learned by Deep RLSP do jump, as we might want, but they often fall down, terminating the episode; in contrast GAIL policies stand still or fall over slowly, leading to later termination and explaining their better quantitative performance. We wanted to also evaluate the policies without this termination bias, and so we evaluate the same policies in an environment that does not terminate the episode, but provides a negative reward instead; in this evaluation both Deep RLSP and AverageFeatures perform much better. We also provide videos of the learned policies at <https://sites.google.com/view/deep-rlsp>, which show that the policies learned by Deep RLSP do exhibit hopping behavior (though with a strong tendency to fall forward).\\n\\nGAIL is only able to learn a truly good policy for the (very simple) inverted pendulum, even though it gets states and actions as input. Deep RLSP on the other hand achieves reasonable behavior (though clearly not expert behavior) in all of the environments, using only states as input. Surprisingly, the AverageFeatures method also performs quite well, even beating the full algorithm on some tasks, though failing quite badly on Pendulum. It seems that the task of running forward or backward is very well specified by a single state, since it can be inferred even without any information about the dynamics (except that which is encoded in the features learned from the initial dataset).\\n\\nLearning skills from a single state {#sec:skills}\\n-----------------------------------\\n\\n![We sample a few states from a policy performing a specific skill to provide as input. Here, Deep RLSP learns to balance the cheetah on the front leg from a *single state*. We provide videos of the original skills and learned policies at: <https://sites.google.com/view/deep-rlsp>.](figures/balancing_skill_experiment.pdf){#fig:skills width=\"\\\\\\\\linewidth\"}\\n\\nWe investigate to what extent Deep RLSP can learn other skills where the reward is not clear. Evaluation on these tasks is much harder, because there is no ground truth reward. Therefore we evaluate qualitatively how similar the policies learned by Deep RLSP are to the original skill. We also attempted to quantify similarity by checking how quickly a discriminator could learn to distinguish between the learned policy and the original skill, but unfortunately this metric was not conclusive (results are reported in Appendix\\xa0[9.1](#app:discriminator){reference-type=\"ref\" reference=\"app:discriminator\"}). Unlike the previous case, we do not reoptimize the learned reward and only look at the policies learned by Deep RLSP.\\n\\nWe consider skills learned by running Dynamics-Aware Unsupervised Discovery of Skills [DADS; @sharma2020dynamics]. Since we are not interested in navigation, we remove the \"x-y prior\" used to get directional skills in DADS. We run DADS on the half-cheetah environment and select all skills that are not some form of running. This resulted in two skills: one in which the cheetah is moving forward making big leaps (*\"jumping\"*) and one in which it is slowly moving forward on one leg (*\"balancing\"*). As before we roll out these policies and sample individual states from the trajectories to provide as an input for Deep RLSP. We then evaluate the policy learned by Deep RLSP. Since the best evaluation here is to simply watch what the learned policy does, we provide videos of the learned policies at <https://sites.google.com/view/deep-rlsp>. We also provide visualizations in Appendix\\xa0[9.2](#app:visualizing-skills){reference-type=\"ref\" reference=\"app:visualizing-skills\"}.\\n\\nThe first thing to notice is that relative to the ablations, only Deep RLSP is close to imitating the skill. None of the other policies resemble the original skills at all. While AverageFeatures could perform well on simple tasks such as running, the full algorithm is crucial to imitate more complex behavior.\\n\\nBetween Deep RLSP and GAIL the comparison is less clear. Deep RLSP can learn the balancing skill fairly well from a single state, which we visualize in Figure\\xa0[2](#fig:skills){reference-type=\"ref\" reference=\"fig:skills\"} (though we emphasize that the videos are much clearer). Like the original skill, the learned policy balances on one leg and slowly moves forward by jumping, though with slightly more erratic behavior. However, the learned policy sometimes drops back to its feet or falls over on its back. We suspect this is an artifact of the short horizon ($T \\\\leq 10$) used for simulating the past in our algorithm. A small horizon is necessary to avoid compounding errors in the learned inverse environment dynamics model, but can cause the resulting behavior to be more unstable on timescales greater than $T$. We see similar behavior when given 10 or 50 states. GAIL leads to a good policy given a single transition, where the cheetah balances on its front leg and head (rather than just the front leg), but does not move forward very much. However, with 10 or 50 transition, the policies learned by GAIL do not look at all like balancing.\\n\\nHowever, the jumping behavior is harder to learn, especially from a single state. We speculate that here a single state is less informative than the balancing state. In the balancing state, the low joint velocities tell us that the cheetah is not performing a flip, suggesting that we had optimized for this specific balancing state. On the other hand, with the jumping behavior, we only get a single state of the cheetah in the air with high velocity, which is likely not sufficient to determine what the jump looked like exactly. In line with this hypothesis, at 1 state Deep RLSP learns to erratically hop, at 10 states it executes slightly bigger jumps, and at 50 states it matches the original skill relatively closely.\\n\\nThe GAIL policies for jumping are also reasonable, though in a different way that makes it hard to compare. Using 1 or 10 transitions, the policy doesn\\'t move very much, staying in contact with the ground most of the time. However, at 50 transitions, it performs noticeably forward hops slightly smoother than the policy learned by Deep RLSP.\\n\\nRelated work\\n============\\n\\n**Learning from human feedback.** Many algorithms aim to learn good policies from human demonstrations, including ones in imitation learning\\xa0[@ho2016generative] and inverse reinforcement learning [IRL; @ng2000algorithms; @abbeel2004apprenticeship; @fu2017learning]. Useful policies can also be learned from other types of feedback, such as preferences\\xa0[@christiano2017deep], corrections\\xa0[@bajcsy2017learning], instructions\\xa0[@bahdanau2018learning], or combinations of feedback modalities\\xa0[@ibarz2018reward].\\n\\nWhile these methods require expensive human feedback, Deep RLSP instead *simulates* the trajectories that must have happened. This is reflected in the algorithm: in Equation\\xa0[\\\\[eq:state-to-traj-grad\\\\]](#eq:state-to-traj-grad){reference-type=\"ref\" reference=\"eq:state-to-traj-grad\"}, the inner gradient corresponds to an inverse reinforcement learning problem. While we used the MCEIRL formulation\\xa0[@ziebart2010modeling], other IRL algorithms could be used instead\\xa0[@fu2017learning].\\n\\n**Learning from observations.** For many tasks, we have demonstrations *without action labels*, e.g., YouTube videos. Learning from Observations [LfO; @torabi2018generative; @gandhi2019learning] aims to recover a policy from such demonstrations. Similarly to LfO, we do not have access to action labels, but our setting is further restricted to observing only a small number of states.\\n\\n**Inverse environment dynamics.** In RL, the term *inverse dynamics* commonly refers to a model that maps a current state and a next state to an action that achieves the transition between both states [@christiano2016transfer], whereas our *inverse environment dynamics* return a previous state given a current state and a previous action. @goyal2018recall learn an inverse environment dynamics model to improve the sample efficiency of RL with a specified reward function by backtracking from high-reward states. In contrast, we infer the full reward function from a single input state.\\n\\nLimitations and future work\\n===========================\\n\\n**Summary.** Learning useful policies with neural networks requires significant human effort, whether it is done by writing down a reward function by hand, or by learning from explicit human feedback such as preferences or demonstrations. We showed that it is possible to reduce this burden by extracting \"free\" information present in the current state of the environment. This enables us to imitate policies in MuJoCo environments with access to just a few states sampled from those policies. We hope that Deep RLSP will help us train agents that are better aligned with human preferences.\\n\\n**Learned models.** The Deep RLSP gradient depends on having access to a good model of $\\\\pi$, $\\\\mathcal{T}$, $\\\\pi^{-1}$, and $\\\\mathcal{T}^{-1}$. In practice, it was quite hard to train sufficiently good versions of the inverse models. This could be a significant barrier to practical implementations of Deep RLSP. It can also be taken as a sign for optimism: self-supervised representation learning through deep learning is fairly recent and is advancing rapidly; such advances will likely translate directly into improvements in Deep RLSP.\\n\\n**Computational cost.** Imitation learning with full demonstrations can already be quite computationally expensive. Deep RLSP learns several distinct neural network models, and then *simulates* potential demonstrations, and finally imitates them. Unsurprisingly, this leads to increased computational cost.\\n\\n**Safe RL.** @shah2019preferences discuss how the exact RLSP algorithm can be used to avoid negative side-effects in RL by combining preferences learned from the initial state with a reward function. While we focused on learning hard to specify behavior, Deep RLSP can also be used to learn to avoid negative side-effects, which is crucial for safely deploying RL systems in the real world [@amodei2016concrete].\\n\\n**Multiagent settings.** In any realistic environment, there is not just a single \"user\" who is influencing the environment: many people act simultaneously, and the state is a result of joint optimization by all of them. However, our model assumes that the environment state resulted from optimization by a single agent, which will not take into account the fact that each agent will have constraints imposed upon them by other agents. We will likely require new algorithms for such a setting.\\n\\n### Acknowledgments {#acknowledgments .unnumbered}\\n\\nThis work was partially supported by Open Philanthropy, AFOSR, ONR YIP, NSF CAREER, NSF NRI, and Microsoft Swiss JRC. We thank researchers at the Center for Human-Compatible AI and the InterACT lab for helpful discussion and feedback.\\n\\nGridworld environments {#app:gridworlds}\\n======================\\n\\nHere we go into more detail on the experiments in Section\\xa0[3.2](#sec:gridworlds){reference-type=\"ref\" reference=\"sec:gridworlds\"}, in which we ran Deep RLSP on the environment suite constructed in @shah2019preferences.\\n\\nIn this test suite, each environment comes equipped with an *observed state* $s_0$, an *initial state* $s_{-T}$, a specified reward $R_{\\\\text{spec}}$, and a true reward $R_{\\\\text{true}}$. A given algorithm should be run on $s_0$ and optionally also $s_{-T}$ and produce an inferred reward $R_{\\\\text{inferred}}$. This is then added to the specified reward to produce $R_{\\\\text{final}} = R_{\\\\text{spec}} + \\\\lambda R_{\\\\text{inferred}}$, where $\\\\lambda$ is a hyperparameter that determines the weighting between the two. An optimal policy for $R_{\\\\text{final}}$ is then found using value iteration, and the resulting policy is evaluated according to $R_{\\\\text{true}}$.\\n\\nThere is no clear way to set $\\\\lambda$: it depends on the scales of the rewards. We leverage the fact that $R_{\\\\text{spec}}$ is deliberately chosen to incentivize bad behavior, such that we know $\\\\lambda = 0$ will always give incorrect behavior. So, we normalize $R_{\\\\text{inferred}}$, and then increase $\\\\lambda$ from 0 until the behavior displayed by the final policy changes.\\n\\nSince GAIL does not produce a reward function as output, we do not run it here. We do however report results with AverageFeatures (which is equivalent to Waypoints here, because there is only a single observed state).\\n\\n![Reproduction of part of Figure 2 in\\xa0@shah2019preferences illustrating the gridworld environments that we test on.](figures/gridworlds/h-room.pdf){#fig:gridworlds width=\".98\\\\\\\\textwidth\"}\\n\\n![Reproduction of part of Figure 2 in\\xa0@shah2019preferences illustrating the gridworld environments that we test on.](figures/gridworlds/h-train.pdf){#fig:gridworlds width=\".98\\\\\\\\textwidth\"}\\n\\n![Reproduction of part of Figure 2 in\\xa0@shah2019preferences illustrating the gridworld environments that we test on.](figures/gridworlds/h-batteries.pdf){#fig:gridworlds width=\".98\\\\\\\\textwidth\"}\\n\\n![Reproduction of part of Figure 2 in\\xa0@shah2019preferences illustrating the gridworld environments that we test on.](figures/gridworlds/h-apples.pdf){#fig:gridworlds width=\".98\\\\\\\\textwidth\"}\\n\\n![Reproduction of part of Figure 2 in\\xa0@shah2019preferences illustrating the gridworld environments that we test on.](figures/gridworlds/h-room-bad.pdf){#fig:gridworlds width=\".98\\\\\\\\textwidth\"}\\n\\n[\\\\[fig:gridworlds\\\\]]{#fig:gridworlds label=\"fig:gridworlds\"}\\n\\nFrom left to right, the environments are:\\n\\n1.  Room with vase: $R_{\\\\text{spec}}$ has weight 1 for the purple door feature, and 0 for all other weights. $R_{\\\\text{true}}$ additionally has weight -1 for the broken vases feature. Since we observe a state in which the vase is unbroken, we can infer that the human avoided breaking the vase, and so that there should be a negative weight on broken vases. Deep RLSP indeed does this and so avoids breaking the vase. AverageFeatures fails to do so, though this is due to a quirk in the feature encoding. In particular, the feature counts the number of broken vases, and so the inferred $\\\\theta$ has a value of zero for this feature, effectively ignoring it. If we change the featurization to instead count the number of *unbroken* vases, then AverageFeatures would likely get the right behavior.\\n\\n2.  Toy train: In this environment, we observe a state in which an operational train is moving around a track. Once again, $R_{\\\\text{spec}}$ just has weight 1 on the purple door feature. $R_{\\\\text{true}}$ additionally has weight -1 on broken vases and trains. Deep RLSP appropriately avoids breaking objects, but AverageFeatures does not.\\n\\n3.  Batteries: We observe a state in which the human has put a battery in the train to keep it operational ($s_{-T}$ has two batteries while $s_0$ only has one). $R_{\\\\text{spec}}$ still has weight 1 on the purple door feature. $R_{\\\\text{true}}$ additionally has weight -1 on allowing the train to run out of power. Algorithms should infer that it is good to put batteries in the train to keep it operational, even though this irreversibly uses up the battery. Deep RLSP correctly does this, while AverageFeatures does not. In fact, AverageFeatures incorrectly infers that batteries should *not* be used up.\\n\\n4.  Apples: We observe a state in which the human has collected some apples and placed them in a basket. $R_{\\\\text{spec}}$ is always zero, while $R_{\\\\text{true}}$ has weight 1 on the number of apples in the basket. The environment tests whether algorithms can infer that it is good for there to be apples in the basket. Deep RLSP does this, learning a policy that continues to collect apples and place them in the basket. AverageFeatures also learns to place apples in the basket, but does not do so as effectively as Deep RLSP, because AverageFeatures also rewards the agent for staying in the original location, leading it to avoid picking apples from the tree that is furthest away.\\n\\n5.  Room with far away vase: This is an environment that aims to show what *can\\'t* be learned: in this case, the breakable vase is so far away, that it is not much evidence that the human has not broken it so far. As a result, algorithms should *not* learn anything significant about whether or not to break vases. This is indeed the case for Deep RLSP, as well as AverageFeatures (though once again, in the latter case, this is dependent on the specific form of the feature).\\n\\nOverall, Deep RLSP has the same behavior on these environments as RLSP, while AverageFeatures does not.\\n\\nArchitecture and hyperparameter choices {#app:hyperparameters}\\n=======================================\\n\\nIn this section we describe the architecture choices for the models used in our algorithm and the hyperparameter choices in our experiments. All models are implemented using the TensorFlow framework.\\n\\nFeature function\\n----------------\\n\\nWe use a variational autoencoder\\xa0[VAE; @kingma2013auto] to learn the feature function. The encoder and decoder consist of $3$ feed-forward layers of size $512$. The latent space has dimension $30$. The model is trained for $100$ epochs on $100$ rollouts of a random policy in the environment. During training we use a batch size of $500$ and a learning rate of $10^{-5}$. We use the standard VAE loss function, but weight the KL-divergence term with a factor $c = 0.001$, which reduces the regularization and empirically improved the reconstruction of the model significantly. We hypothesize that the standard VAE regularizes too much in our setting, because the latent space has a higher dimension than the input space, which is not the case in typical dimensionality reduction settings.\\n\\nInverse environment dynamics model\\n----------------------------------\\n\\nOur inverse environment dynamics model is a feed-forward neural network with $5$ layers of size $1024$ with ReLU activations. We train it on $1000$ rollouts of a random policy in the environment for $100$ epochs, with a batch size of $500$ and a learning rate of $10^{-5}$.\\n\\nNote that the model predicts the previous observation given the current observation and action; it does not use the feature representation. We found the model to perform better if it predicts the residual $o_{t-1} - o_t$ given $o_t$ and $a_t$ instead of directly predicting $o_{t-1}$.\\n\\nWe normalize all inputs to the model to have zero mean and unit variance. To increase robustness, we also add zero-mean Gaussian noise with standard deviation $0.001$ to the inputs and labels during training and clip the outputs of the model to the range of values observed during training.\\n\\nPolicy\\n------\\n\\nFor learning the policy we use the *stable-baselines* implementation of Soft Actor-Critic (SAC) with its default parameters for the MuJoCo environments\\xa0[@haarnoja2018soft; @hill2018stable]. Each policy update during Deep RLSP uses $10^4$ total timesteps for the cheetah, $2\\\\times 10^4$ for the hopper. We perform the policy updates usually starting from the last iteration\\'s policy, except in the pendulum environment, where we randomly initialize the policy in each iteration and train it using $5 \\\\times 10^4$ iterations of SAC. We evaluate the final reward function generally using $2\\\\times 10^6$ timesteps, except for the pendulum, where we use $6 \\\\times 10^4$.\\n\\nInverse policy\\n--------------\\n\\nBecause the inverse policy is not deterministic, we represent it with a *mixture density network*, a feed-forward neural network that outputs a mixture of Gaussian distributions\\xa0[@bishop1994mixture].\\n\\nThe network has $3$ layers of size $512$ with ReLU activations and outputs a mixture of $5$ Gaussians with a fixed variance of $0.05$.\\n\\nTo update the inverse policy we sample batches with batch size $500$ from the experience replay, apply the forward policy and the environment transition function on the states to label the data. We then train the model with a learning rate of $10^{-4}$.\\n\\nDeep RLSP hyperparameters\\n-------------------------\\n\\nWe run Deep RLSP with a learning rate of $0.01$, and use $200$ forward and backward trajectories to estimate the gradients. Starting with $T=1$ we increment the horizon when the gradient norm drops below $2.0$ or after $10$ steps, whichever comes first. We run the algorithm until $T=10$.\\n\\nHeuristic for incorporating information about the initial state {#app:ablation}\\n===============================================================\\n\\n::: {#tab:gradient-weights-ablation}\\n  ------------------------- -- ---- ----------------- ----------------- -- --\\n                                                                           \\n    (no gradient weights)                                                  \\n   (with gradient weights)                                                 \\n                                                                           \\n                                                                           \\n          Pendulum              1     **303 (299)**         6 (3)          \\n                                10    **335 (333)**     **667 (333)**      \\n                                50    **339 (331)**         5 (3)          \\n                                                                           \\n          (forward)             1    **4591 (2073)**   **4833 (2975)**     \\n                                10   **6917 (421)**    **6299 (559)**      \\n                                50     6078 (589)      **7657 (177)**      \\n                                                                           \\n         (backward)             1    **5730 (2733)**   **5694 (2513)**     \\n                                10   **7917 (249)**    **8102 (624)**      \\n                                50   **7588 (171)**    **7795 (551)**      \\n                                                                           \\n         (terminate)            1        68 (8)            70 (33)         \\n                                10       47 (21)           81 (9)          \\n                                50       72 (1)            81 (15)         \\n                                                                           \\n          (penalty)             1    **1850 (634)**      1152 (583)        \\n                                10    **2998 (62)**      1544 (608)        \\n                                50   **1667 (737)**    **2020 (571)**      \\n  ------------------------- -- ---- ----------------- ----------------- -- --\\n\\n  : Ablation of the gradient weighting heuristic described in Section\\xa0[2.4](#sec:full-algo){reference-type=\"ref\" reference=\"sec:full-algo\"}. We report average returns (over 3 random seeds) achieved by the policies learned with and without the heuristic, for different numbers of input states. Experiment setup is the same as in Table\\xa0[1](#tab:final_return_rlsp_policies){reference-type=\"ref\" reference=\"tab:final_return_rlsp_policies\"}.\\n:::\\n\\nIn Section\\xa0[2.4](#sec:full-algo){reference-type=\"ref\" reference=\"sec:full-algo\"} we discussed that it might be necessary for Deep RLSP to have information about the distribution $\\\\mathcal{P}$ of the initial state $s_{-T}$. Since in our setup Deep RLSP can not obtain any information about $\\\\mathcal{P}$ through $\\\\pi^{-1}$ and $\\\\mathcal{T}^{-1}$, here we present a heuristic to incorporate the information elsewhere.\\n\\nSpecifically, we weight every backwards trajectory by the cosine similarity between the final state ${s}_{-T}$, and a sample $\\\\hat{s}_{-T} \\\\sim \\\\mathcal{P}$. This weights gradient terms higher that correspond to trajectories that are more likely given our knowledge about $\\\\mathcal{P}$ and weights trajectories lower that end in a state ${s}_{-T}$ that has low probability under $\\\\mathcal{P}$.\\n\\nTo test whether this modification improves the performance of Deep RLSP, we compared Deep RLSP with this gradient weighting heuristic to Deep RLSP without it as it was presented in the main paper.\\n\\nFirst, we ran Deep RLSP with the gradient weighting on the gridworld environments from\\xa0@shah2019preferences, described in Section\\xa0[3.2](#sec:gridworlds){reference-type=\"ref\" reference=\"sec:gridworlds\"} and Appendix\\xa0[6](#app:gridworlds){reference-type=\"ref\" reference=\"app:gridworlds\"}. The results are identical to the case when using the heuristics.\\n\\nNext, we tested on the tasks in the MuJoCo environments described in Section\\xa0[3.3](#sec:existing-envs){reference-type=\"ref\" reference=\"sec:existing-envs\"}. We report the results in Table\\xa0[2](#tab:gradient-weights-ablation){reference-type=\"ref\" reference=\"tab:gradient-weights-ablation\"}, alongside the previously reported results without the gradient weighting. The results are quite similar, suggesting that the gradient weighting does not make much of a difference in these environments.\\n\\nAnalysis of the learned skills\\n==============================\\n\\nTraining a discriminator {#app:discriminator}\\n------------------------\\n\\nIn the main text, we focused on visual evaluation of the learned skills, because it is difficult to define a metric that properly measures the similarity between an original skill and one learned by Deep RLSP. In this section, we attempt to quantify the similarity between policies by training a discriminator to distinguish trajectories from the policies. Conceptually, the easier it is to train this discriminator, the more different the two policies are. We could thus use this to check how similar our learned policies are to the original skills.\\n\\nWe train a neural network with a single hidden layer of size 10 with ReLU activation functions. We sample trajectories from both policies and randomly sample trajectory pieces consisting of 5 observations to train the model on. We label the trajectory pieces with a binary label depending on which policy they come from, and then use a cross-entropy loss to train the model. To ensure comparable results, we keep this setup the same for all policies and average the resulting learning curves over 10 different random seeds.\\n\\nThe resulting learning curves are shown in [17](#fig:discriminator){reference-type=\"ref\" reference=\"fig:discriminator\"}. The differences between the learning curves are relatively small overall, suggesting that we cannot draw strong conclusions from this experiment. In addition, while the AverageFeatures and Waypoints ablations can be seen to be extremely bad visually relative to GAIL and Deep RLSP, this is not apparent from the learning curves. As a result, we conclude that this is not actually a good metric to judge performance. (Note that if we were to use the metric, it would suggest that Deep RLSP is best for the balancing learning skill, while for the jumping skill GAIL is better for 1 and 50 states and Deep RLSP is better for 10 states.)\\n\\n  -------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n           Balancing                                                                                                                                                                                                                                                                                                                                                     Jumping\\n\\n  state    ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/discriminator_balancing_1.pdf){#fig:discriminator width=\"\\\\\\\\linewidth\"}    ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/discriminator_jumping_1.pdf){#fig:discriminator width=\"\\\\\\\\linewidth\"}\\n\\n  states   ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/discriminator_balancing_10.pdf){#fig:discriminator width=\"\\\\\\\\linewidth\"}   ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/discriminator_jumping_10.pdf){#fig:discriminator width=\"\\\\\\\\linewidth\"}\\n\\n  states   ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/discriminator_balancing_50.pdf){#fig:discriminator width=\"\\\\\\\\linewidth\"}   ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/discriminator_jumping_50.pdf){#fig:discriminator width=\"\\\\\\\\linewidth\"}\\n  -------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/legend_GAIL.pdf \"fig:\"){#fig:discriminator width=\"3em\"}\\xa0\\xa0\\xa0GAIL ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/legend_DeepRLSP.pdf \"fig:\"){#fig:discriminator width=\"3em\"}\\xa0\\xa0\\xa0Deep RLSP ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/legend_AverageFeatures.pdf \"fig:\"){#fig:discriminator width=\"3em\"}\\xa0\\xa0\\xa0AverageFeatures ![Learning curves for training a discriminator to distinguish the learned skill from the original skill averaged over 10 random seeds. A slower learning curve indicates that the learned skill is more similar to the original skill, that is, higher is better.](figures/appendix/legend_Waypoints.pdf \"fig:\"){#fig:discriminator width=\"3em\"}\\xa0\\xa0\\xa0Waypoints\\n\\nVisualization of learned skills {#app:visualizing-skills}\\n-------------------------------\\n\\nHere we provide larger visualizations of the skills learned in the experiments discussed in Section\\xa0[3.4](#sec:skills){reference-type=\"ref\" reference=\"sec:skills\"} of the main paper. For each experiment we show the original policy, the states sampled from this policy and given as an input to Deep RLSP, the policy learned by the AverageFeatures ablation, and the policy learned by Deep RLSP in \\xa0[\\\\[fig:balancing_1,fig:balancing_10,fig:balancing_50,fig:jumping_1,fig:jumping_10,fig:jumping_50\\\\]](#fig:balancing_1,fig:balancing_10,fig:balancing_50,fig:jumping_1,fig:jumping_10,fig:jumping_50){reference-type=\"ref\" reference=\"fig:balancing_1,fig:balancing_10,fig:balancing_50,fig:jumping_1,fig:jumping_10,fig:jumping_50\"} (on future pages). Again, we emphasize that the visual comparison is easier with videos of the policies which we provide at <https://sites.google.com/view/deep-rlsp>(including Waypoints and AverageFeatures ablations).\\n\\nThings we tried that did not work\\n=================================\\n\\nHere we list a few variations of the Deep RLSP algorithm that we tested on the MuJoCo environments that failed to provide good results.\\n\\n-   We tried to learn a latent state-space jointly with a latent dynamics model using a recurrent state-space model (RSSM). However, we found existing models too brittle to reliably learn a good dynamics model. The reward function and policy learned by Deep RLSP worked in the RSSM but did not generalize to the actual environment.\\n\\n-   We also tried learning a forward dynamics model from the initial set of rollouts, similarly to how we learn an inverse environment dynamics model, rather than relying on the simulator $\\\\mathcal{T}$. However, we found this to cause a similar issue as the RSSM: the reward function and policy learned by Deep RLSP did not generalize to the actual environment. However, we hope that progress in model-based RL will allow us to implement Deep RLSP using only learned dynamics models in the future.\\n\\n-   Using an mixture density network instead of an MLP to model the inverse environment dynamics did not improve the performance of the algorithm. We suspect this to be because in the MuJoCo simulator the dynamics and the inverse environment dynamics are \"almost deterministic\".\\n\\n-   Updating the inverse environment dynamics model and the feature function during Deep RLSP by training it on data from the experience replay did not improve performance and in some cases significantly decreased performance. The decrease in performance seems to have been caused by the feature function changing too much and the training of the other models suffering from catastrophic forgetting as a result.\\n\\n-   In the main paper we evaluated the policies learned by Deep RLSP from jumping and balancing skills. However, we also looked at policies obtained by optimizing for the learned reward. These also showed similarities to the original skills but they were significantly worse then the policies directly learned by Deep RLSP. For the jumping skill the optimized policies jump very erratically, and for the balancing skill they tend to fall over or perform forward flips. This discrepency is a result of the policy updates during Deep RLSP only using a limited number of iterations. It seems like in these experiments the learned reward functions lead to good policies when optimized for weakly but do not produce good policies when optimized for strongly. We saw in preliminary experiments that increasing the number of iterations for updating the policies during Deep RLSP reduces this discrepency. However, the resulting algorithm was computationally too expensive to evaluate with our resources.\\n\\n-   We tried running Deep RLSP for longer horizons up to $T=30$, but found the results to be worse than for $T=10$ which we reported in the main paper. We hypothesize that this is caused by compounding errors in the inverse environment dynamics model. This hypothesis is supported by manually looking at trajectories generated by the inverse environment dynamics model. While they look reasonable for short horizons $T \\\\leq 10$, compounding errors become significantly bigger for horizons $10  \\\\leq T \\\\leq 30$.\\n\\n  ----------------------------------------------------------------------------------------\\n  Original Policy   ![image](figures/appendix/balancing_traj.png){width=\"\\\\\\\\linewidth\"}\\n  ----------------- ----------------------------------------------------------------------\\n  Sampled States    ![image](figures/appendix/states/balancing_0_0_samples_1_0.png)\\n\\n  GAIL              ![image](figures/appendix/gail_balancing_1.png){width=\"\\\\\\\\linewidth\"}\\n\\n  Deep RLSP         ![image](figures/appendix/rlsp_balancing_1.png){width=\"\\\\\\\\linewidth\"}\\n  ----------------------------------------------------------------------------------------\\n\\n  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  Original Policy   ![image](figures/appendix/balancing_traj.png){width=\"\\\\\\\\linewidth\"}\\n  ----------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  Sampled States    ![image](figures/appendix/states/balancing_0_0_samples_10_0.png) ![image](figures/appendix/states/balancing_0_0_samples_10_1.png) ![image](figures/appendix/states/balancing_0_0_samples_10_2.png) ![image](figures/appendix/states/balancing_0_0_samples_10_3.png) ![image](figures/appendix/states/balancing_0_0_samples_10_4.png) ![image](figures/appendix/states/balancing_0_0_samples_10_5.png) ![image](figures/appendix/states/balancing_0_0_samples_10_6.png) ![image](figures/appendix/states/balancing_0_0_samples_10_7.png) ![image](figures/appendix/states/balancing_0_0_samples_10_8.png) ![image](figures/appendix/states/balancing_0_0_samples_10_9.png)\\n\\n  GAIL              ![image](figures/appendix/gail_balancing_10.png){width=\"\\\\\\\\linewidth\"}\\n\\n  Deep RLSP         ![image](figures/appendix/rlsp_balancing_10.png){width=\"\\\\\\\\linewidth\"}\\n  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  Original Policy   ![image](figures/appendix/balancing_traj.png){width=\"\\\\\\\\linewidth\"}\\n  ----------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n                    ![image](figures/appendix/states/balancing_0_0_samples_50_0.png) ![image](figures/appendix/states/balancing_0_0_samples_50_1.png) ![image](figures/appendix/states/balancing_0_0_samples_50_2.png) ![image](figures/appendix/states/balancing_0_0_samples_50_3.png) ![image](figures/appendix/states/balancing_0_0_samples_50_4.png) ![image](figures/appendix/states/balancing_0_0_samples_50_5.png) ![image](figures/appendix/states/balancing_0_0_samples_50_6.png) ![image](figures/appendix/states/balancing_0_0_samples_50_7.png) ![image](figures/appendix/states/balancing_0_0_samples_50_8.png) ![image](figures/appendix/states/balancing_0_0_samples_50_9.png)\\n\\n                    ![image](figures/appendix/states/balancing_0_0_samples_50_10.png) ![image](figures/appendix/states/balancing_0_0_samples_50_11.png) ![image](figures/appendix/states/balancing_0_0_samples_50_12.png) ![image](figures/appendix/states/balancing_0_0_samples_50_13.png) ![image](figures/appendix/states/balancing_0_0_samples_50_14.png) ![image](figures/appendix/states/balancing_0_0_samples_50_15.png) ![image](figures/appendix/states/balancing_0_0_samples_50_16.png) ![image](figures/appendix/states/balancing_0_0_samples_50_17.png) ![image](figures/appendix/states/balancing_0_0_samples_50_18.png) ![image](figures/appendix/states/balancing_0_0_samples_50_19.png)\\n\\n                    ![image](figures/appendix/states/balancing_0_0_samples_50_20.png) ![image](figures/appendix/states/balancing_0_0_samples_50_21.png) ![image](figures/appendix/states/balancing_0_0_samples_50_22.png) ![image](figures/appendix/states/balancing_0_0_samples_50_23.png) ![image](figures/appendix/states/balancing_0_0_samples_50_24.png) ![image](figures/appendix/states/balancing_0_0_samples_50_25.png) ![image](figures/appendix/states/balancing_0_0_samples_50_26.png) ![image](figures/appendix/states/balancing_0_0_samples_50_27.png) ![image](figures/appendix/states/balancing_0_0_samples_50_28.png) ![image](figures/appendix/states/balancing_0_0_samples_50_29.png)\\n\\n                    ![image](figures/appendix/states/balancing_0_0_samples_50_30.png) ![image](figures/appendix/states/balancing_0_0_samples_50_31.png) ![image](figures/appendix/states/balancing_0_0_samples_50_32.png) ![image](figures/appendix/states/balancing_0_0_samples_50_33.png) ![image](figures/appendix/states/balancing_0_0_samples_50_34.png) ![image](figures/appendix/states/balancing_0_0_samples_50_35.png) ![image](figures/appendix/states/balancing_0_0_samples_50_36.png) ![image](figures/appendix/states/balancing_0_0_samples_50_37.png) ![image](figures/appendix/states/balancing_0_0_samples_50_38.png) ![image](figures/appendix/states/balancing_0_0_samples_50_39.png)\\n\\n                    ![image](figures/appendix/states/balancing_0_0_samples_50_40.png) ![image](figures/appendix/states/balancing_0_0_samples_50_41.png) ![image](figures/appendix/states/balancing_0_0_samples_50_42.png) ![image](figures/appendix/states/balancing_0_0_samples_50_43.png) ![image](figures/appendix/states/balancing_0_0_samples_50_44.png) ![image](figures/appendix/states/balancing_0_0_samples_50_45.png) ![image](figures/appendix/states/balancing_0_0_samples_50_46.png) ![image](figures/appendix/states/balancing_0_0_samples_50_47.png) ![image](figures/appendix/states/balancing_0_0_samples_50_48.png) ![image](figures/appendix/states/balancing_0_0_samples_50_49.png)\\n\\n  GAIL              ![image](figures/appendix/gail_balancing_50.png){width=\"\\\\\\\\linewidth\"}\\n\\n  Deep RLSP         ![image](figures/appendix/rlsp_balancing_50.png){width=\"\\\\\\\\linewidth\"}\\n  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n  --------------------------------------------------------------------------------------\\n  Original Policy   ![image](figures/appendix/jumping_traj.png){width=\"\\\\\\\\linewidth\"}\\n  ----------------- --------------------------------------------------------------------\\n  Sampled States    ![image](figures/appendix/states/jumping_4_0_samples_1_0.png)\\n\\n  GAIL              ![image](figures/appendix/gail_jumping_1.png){width=\"\\\\\\\\linewidth\"}\\n\\n  Deep RLSP         ![image](figures/appendix/rlsp_jumping_1.png){width=\"\\\\\\\\linewidth\"}\\n  --------------------------------------------------------------------------------------\\n\\n  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  Original Policy   ![image](figures/appendix/jumping_traj.png){width=\"\\\\\\\\linewidth\"}\\n  ----------------- -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  Sampled States    ![image](figures/appendix/states/jumping_4_0_samples_10_0.png) ![image](figures/appendix/states/jumping_4_0_samples_10_1.png) ![image](figures/appendix/states/jumping_4_0_samples_10_2.png) ![image](figures/appendix/states/jumping_4_0_samples_10_3.png) ![image](figures/appendix/states/jumping_4_0_samples_10_4.png) ![image](figures/appendix/states/jumping_4_0_samples_10_5.png) ![image](figures/appendix/states/jumping_4_0_samples_10_6.png) ![image](figures/appendix/states/jumping_4_0_samples_10_7.png) ![image](figures/appendix/states/jumping_4_0_samples_10_8.png) ![image](figures/appendix/states/jumping_4_0_samples_10_9.png)\\n\\n  GAIL              ![image](figures/appendix/gail_jumping_10.png){width=\"\\\\\\\\linewidth\"}\\n\\n  Deep RLSP         ![image](figures/appendix/rlsp_jumping_10.png){width=\"\\\\\\\\linewidth\"}\\n  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  Original Policy   ![image](figures/appendix/jumping_traj.png){width=\"\\\\\\\\linewidth\"}\\n  ----------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n                    ![image](figures/appendix/states/jumping_4_0_samples_50_0.png) ![image](figures/appendix/states/jumping_4_0_samples_50_1.png) ![image](figures/appendix/states/jumping_4_0_samples_50_2.png) ![image](figures/appendix/states/jumping_4_0_samples_50_3.png) ![image](figures/appendix/states/jumping_4_0_samples_50_4.png) ![image](figures/appendix/states/jumping_4_0_samples_50_5.png) ![image](figures/appendix/states/jumping_4_0_samples_50_6.png) ![image](figures/appendix/states/jumping_4_0_samples_50_7.png) ![image](figures/appendix/states/jumping_4_0_samples_50_8.png) ![image](figures/appendix/states/jumping_4_0_samples_50_9.png)\\n\\n                    ![image](figures/appendix/states/jumping_4_0_samples_50_10.png) ![image](figures/appendix/states/jumping_4_0_samples_50_11.png) ![image](figures/appendix/states/jumping_4_0_samples_50_12.png) ![image](figures/appendix/states/jumping_4_0_samples_50_13.png) ![image](figures/appendix/states/jumping_4_0_samples_50_14.png) ![image](figures/appendix/states/jumping_4_0_samples_50_15.png) ![image](figures/appendix/states/jumping_4_0_samples_50_16.png) ![image](figures/appendix/states/jumping_4_0_samples_50_17.png) ![image](figures/appendix/states/jumping_4_0_samples_50_18.png) ![image](figures/appendix/states/jumping_4_0_samples_50_19.png)\\n\\n                    ![image](figures/appendix/states/jumping_4_0_samples_50_20.png) ![image](figures/appendix/states/jumping_4_0_samples_50_21.png) ![image](figures/appendix/states/jumping_4_0_samples_50_22.png) ![image](figures/appendix/states/jumping_4_0_samples_50_23.png) ![image](figures/appendix/states/jumping_4_0_samples_50_24.png) ![image](figures/appendix/states/jumping_4_0_samples_50_25.png) ![image](figures/appendix/states/jumping_4_0_samples_50_26.png) ![image](figures/appendix/states/jumping_4_0_samples_50_27.png) ![image](figures/appendix/states/jumping_4_0_samples_50_28.png) ![image](figures/appendix/states/jumping_4_0_samples_50_29.png)\\n\\n                    ![image](figures/appendix/states/jumping_4_0_samples_50_30.png) ![image](figures/appendix/states/jumping_4_0_samples_50_31.png) ![image](figures/appendix/states/jumping_4_0_samples_50_32.png) ![image](figures/appendix/states/jumping_4_0_samples_50_33.png) ![image](figures/appendix/states/jumping_4_0_samples_50_34.png) ![image](figures/appendix/states/jumping_4_0_samples_50_35.png) ![image](figures/appendix/states/jumping_4_0_samples_50_36.png) ![image](figures/appendix/states/jumping_4_0_samples_50_37.png) ![image](figures/appendix/states/jumping_4_0_samples_50_38.png) ![image](figures/appendix/states/jumping_4_0_samples_50_39.png)\\n\\n                    ![image](figures/appendix/states/jumping_4_0_samples_50_40.png) ![image](figures/appendix/states/jumping_4_0_samples_50_41.png) ![image](figures/appendix/states/jumping_4_0_samples_50_42.png) ![image](figures/appendix/states/jumping_4_0_samples_50_43.png) ![image](figures/appendix/states/jumping_4_0_samples_50_44.png) ![image](figures/appendix/states/jumping_4_0_samples_50_45.png) ![image](figures/appendix/states/jumping_4_0_samples_50_46.png) ![image](figures/appendix/states/jumping_4_0_samples_50_47.png) ![image](figures/appendix/states/jumping_4_0_samples_50_48.png) ![image](figures/appendix/states/jumping_4_0_samples_50_49.png)\\n\\n  GAIL              ![image](figures/appendix/gail_jumping_50.png){width=\"\\\\\\\\linewidth\"}\\n\\n  Deep RLSP         ![image](figures/appendix/rlsp_jumping_50.png){width=\"\\\\\\\\linewidth\"}\\n  -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\n[^1]: Work done at the Center for Human-Compatible AI, UC Berkeley.\\n',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{43}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Abbeel \\\\& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}\\nPieter Abbeel and Andrew~Y Ng.\\n\\\\newblock Apprenticeship learning via inverse reinforcement learning.\\n\\\\newblock In \\\\emph{\\\\ICML}, 2004.\\n\\n\\\\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and\\n  Man{\\\\\\'e}]{amodei2016concrete}\\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and\\n  Dan Man{\\\\\\'e}.\\n\\\\newblock Concrete problems in {AI} safety.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1606.06565}, 2016.\\n\\n\\\\bibitem[Bahdanau et~al.(2019)Bahdanau, Hill, Leike, Hughes, Hosseini, Kohli,\\n  and Grefenstette]{bahdanau2018learning}\\nDzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian Hosseini,\\n  Pushmeet Kohli, and Edward Grefenstette.\\n\\\\newblock Learning to understand goal specifications by modelling reward.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2019.\\n\\n\\\\bibitem[Bajcsy et~al.(2017)Bajcsy, Losey, OMalley, and\\n  Dragan]{bajcsy2017learning}\\nAndrea Bajcsy, Dylan~P Losey, Marcia~K OMalley, and Anca~D Dragan.\\n\\\\newblock Learning robot objectives from physical human interaction.\\n\\\\newblock In \\\\emph{\\\\CoRL}, 2017.\\n\\n\\\\bibitem[Bishop(1994)]{bishop1994mixture}\\nChristopher~M Bishop.\\n\\\\newblock Mixture density networks.\\n\\\\newblock Neural Computing Research Group Report, Aston University, 1994.\\n\\n\\\\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,\\n  Schulman, Tang, and Zaremba]{brockman2016openai}\\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,\\n  Jie Tang, and Wojciech Zaremba.\\n\\\\newblock Open{AI} gym.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1606.01540}, 2016.\\n\\n\\\\bibitem[Buesing et~al.(2018)Buesing, Weber, Racaniere, Eslami, Rezende,\\n  Reichert, Viola, Besse, Gregor, Hassabis, et~al.]{buesing2018learning}\\nLars Buesing, Theophane Weber, S{\\\\\\'e}bastien Racaniere, SM~Eslami, Danilo\\n  Rezende, David~P Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis\\n  Hassabis, et~al.\\n\\\\newblock Learning and querying fast generative models for reinforcement\\n  learning.\\n\\\\newblock In \\\\emph{FAIM workshop Prediction and Generative Modeling in\\n  Reinforcement Learning}, 2018.\\n\\n\\\\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.\\n\\\\newblock A simple framework for contrastive learning of visual\\n  representations.\\n\\\\newblock In \\\\emph{\\\\ICML}, 2020.\\n\\n\\\\bibitem[Christiano et~al.(2016)Christiano, Shah, Mordatch, Schneider,\\n  Blackwell, Tobin, Abbeel, and Zaremba]{christiano2016transfer}\\nPaul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell,\\n  Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba.\\n\\\\newblock Transfer from simulation to real world through learning deep inverse\\n  dynamics model.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1610.03518}, 2016.\\n\\n\\\\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and\\n  Amodei]{christiano2017deep}\\nPaul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario\\n  Amodei.\\n\\\\newblock Deep reinforcement learning from human preferences.\\n\\\\newblock In \\\\emph{\\\\NeurIPS}, 2017.\\n\\n\\\\bibitem[Clark \\\\& Amodei(2016)Clark and Amodei]{clark2016faulty}\\nJack Clark and Dario Amodei.\\n\\\\newblock {Faulty reward functions in the wild}, 2016.\\n\\\\newblock URL \\\\url{https://blog.openai.com/faulty-reward-functions}.\\n\\n\\\\bibitem[Clark et~al.(2020)Clark, Luong, Le, and Manning]{clark2020electra}\\nKevin Clark, Minh-Thang Luong, Quoc~V Le, and Christopher~D Manning.\\n\\\\newblock Electra: Pre-training text encoders as discriminators rather than\\n  generators.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2020.\\n\\n\\\\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\\n\\\\newblock {BERT}: Pre-training of deep bidirectional transformers for language\\n  understanding.\\n\\\\newblock In \\\\emph{Proceedings of the Conference of the North {A}merican\\n  Chapter of the Association for Computational Linguistics: Human Language\\n  Technologies, Volume 1 (Long and Short Papers)}, 2019.\\n\\n\\\\bibitem[Doerr et~al.(2018)Doerr, Daniel, Schiegg, Nguyen-Tuong, Schaal,\\n  Toussaint, and Trimpe]{doerr2018probabilistic}\\nAndreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan\\n  Schaal, Marc Toussaint, and Sebastian Trimpe.\\n\\\\newblock Probabilistic recurrent state-space models.\\n\\\\newblock In \\\\emph{\\\\ICML}, 2018.\\n\\n\\\\bibitem[Elsken et~al.(2019)Elsken, Metzen, and Hutter]{elsken2018neural}\\nThomas Elsken, Jan~Hendrik Metzen, and Frank Hutter.\\n\\\\newblock Neural architecture search: A survey.\\n\\\\newblock \\\\emph{Journal of Machine Learning Research}, 20\\\\penalty0\\n  (55):\\\\penalty0 1--21, 2019.\\n\\n\\\\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{fu2017learning}\\nJustin Fu, Katie Luo, and Sergey Levine.\\n\\\\newblock Learning robust rewards with adversarial inverse reinforcement\\n  learning.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2018.\\n\\n\\\\bibitem[Gandhi et~al.(2019)Gandhi, Oates, Mohsenin, and\\n  Waytowich]{gandhi2019learning}\\nSunil Gandhi, Tim Oates, Tinoosh Mohsenin, and Nicholas Waytowich.\\n\\\\newblock Learning from observations using a single video demonstration and\\n  human feedback.\\n\\\\newblock In \\\\emph{Proceedings of the 18th International Conference on\\n  Autonomous Agents and MultiAgent Systems}, 2019.\\n\\n\\\\bibitem[Goyal et~al.(2019)Goyal, Brakel, Fedus, Singhal, Lillicrap, Levine,\\n  Larochelle, and Bengio]{goyal2018recall}\\nAnirudh Goyal, Philemon Brakel, William Fedus, Soumye Singhal, Timothy\\n  Lillicrap, Sergey Levine, Hugo Larochelle, and Yoshua Bengio.\\n\\\\newblock Recall traces: Backtracking models for efficient reinforcement\\n  learning.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2019.\\n\\n\\\\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and\\n  Levine]{haarnoja2018soft}\\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.\\n\\\\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement\\n  learning with a stochastic actor.\\n\\\\newblock In \\\\emph{\\\\ICML}, 2018.\\n\\n\\\\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and\\n  Davidson]{hafner2018learning}\\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha,\\n  Honglak Lee, and James Davidson.\\n\\\\newblock Learning latent dynamics for planning from pixels.\\n\\\\newblock In \\\\emph{\\\\ICML}, 2019.\\n\\n\\\\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and\\n  Norouzi]{hafner2019dream}\\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.\\n\\\\newblock Dream to control: Learning behaviors by latent imagination.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2020.\\n\\n\\\\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2019momentum}\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\\n\\\\newblock Momentum contrast for unsupervised visual representation learning.\\n\\\\newblock In \\\\emph{Proceedings of the IEEE/CVF Conference on Computer Vision\\n  and Pattern Recognition (CVPR)}, 2020.\\n\\n\\\\bibitem[Hill et~al.(2018)Hill, Raffin, Ernestus, Gleave, Kanervisto, Traore,\\n  Dhariwal, Hesse, Klimov, Nichol, Plappert, Radford, Schulman, Sidor, and\\n  Wu]{hill2018stable}\\nAshley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi\\n  Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov,\\n  Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor,\\n  and Yuhuai Wu.\\n\\\\newblock Stable baselines.\\n\\\\newblock \\\\url{https://github.com/hill-a/stable-baselines}, 2018.\\n\\n\\\\bibitem[Ho \\\\& Ermon(2016)Ho and Ermon]{ho2016generative}\\nJonathan Ho and Stefano Ermon.\\n\\\\newblock Generative adversarial imitation learning.\\n\\\\newblock In \\\\emph{\\\\NeurIPS}, 2016.\\n\\n\\\\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and\\n  Amodei]{ibarz2018reward}\\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario\\n  Amodei.\\n\\\\newblock Reward learning from human preferences and demonstrations in {A}tari.\\n\\\\newblock In \\\\emph{\\\\NeurIPS}, 2018.\\n\\n\\\\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,\\n  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan,\\n  et~al.]{jaderberg2017population}\\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech~M Czarnecki, Jeff\\n  Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan,\\n  et~al.\\n\\\\newblock Population based training of neural networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1711.09846}, 2017.\\n\\n\\\\bibitem[Karl et~al.(2017)Karl, Soelch, Bayer, and Van~der Smagt]{karl2016deep}\\nMaximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van~der Smagt.\\n\\\\newblock Deep variational {B}ayes filters: Unsupervised learning of state\\n  space models from raw data.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2017.\\n\\n\\\\bibitem[Kingma \\\\& Welling(2014)Kingma and Welling]{kingma2013auto}\\nDiederik~P Kingma and Max Welling.\\n\\\\newblock Auto-encoding variational {B}ayes.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2014.\\n\\n\\\\bibitem[Krakovna(2018)]{krakovna2018specification}\\nVictoria Krakovna.\\n\\\\newblock {Specification gaming examples in AI}, 2018.\\n\\\\newblock URL\\n  \\\\url{https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/}.\\n\\n\\\\bibitem[Kurutach et~al.(2018)Kurutach, Tamar, Yang, Russell, and\\n  Abbeel]{kurutach2018learning}\\nThanard Kurutach, Aviv Tamar, Ge~Yang, Stuart~J Russell, and Pieter Abbeel.\\n\\\\newblock Learning plannable representations with causal infogan.\\n\\\\newblock In \\\\emph{\\\\NeurIPS}, 2018.\\n\\n\\\\bibitem[Levine(2018)]{levine2018reinforcement}\\nSergey Levine.\\n\\\\newblock Reinforcement learning and control as probabilistic inference:\\n  Tutorial and review.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1805.00909}, 2018.\\n\\n\\\\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}\\nAndrew~Y Ng, Stuart~J Russell, et~al.\\n\\\\newblock Algorithms for inverse reinforcement learning.\\n\\\\newblock In \\\\emph{\\\\ICML}, 2000.\\n\\n\\\\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}\\nAaron van~den Oord, Yazhe Li, and Oriol Vinyals.\\n\\\\newblock Representation learning with contrastive predictive coding.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1807.03748}, 2018.\\n\\n\\\\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and\\n  Sutskever]{radford2019language}\\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\\n  Sutskever.\\n\\\\newblock Language models are unsupervised multitask learners, 2019.\\n\\n\\\\bibitem[Shah et~al.(2019)Shah, Krasheninnikov, Alexander, Abbeel, and\\n  Dragan]{shah2019preferences}\\nRohin Shah, Dmitrii Krasheninnikov, Jordan Alexander, Pieter Abbeel, and Anca\\n  Dragan.\\n\\\\newblock Preferences implicit in the state of the world.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2019.\\n\\n\\\\bibitem[Sharma et~al.(2020)Sharma, Gu, Levine, Kumar, and\\n  Hausman]{sharma2020dynamics}\\nArchit Sharma, Shane Gu, Sergey Levine, Vikash Kumar, and Karol Hausman.\\n\\\\newblock Dynamics-aware unsupervised skill discovery.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2020.\\n\\n\\\\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,\\n  Radford, Amodei, and Christiano]{stiennon2020learning}\\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M Ziegler, Ryan Lowe, Chelsea\\n  Voss, Alec Radford, Dario Amodei, and Paul Christiano.\\n\\\\newblock Learning to summarize from human feedback.\\n\\\\newblock In \\\\emph{\\\\NeurIPS}, 2020.\\n\\n\\\\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}\\nEmanuel Todorov, Tom Erez, and Yuval Tassa.\\n\\\\newblock {M}u{J}o{C}o: A physics engine for model-based control.\\n\\\\newblock In \\\\emph{International Conference on Intelligent Robots and Systems\\n  (IROS)}, 2012.\\n\\n\\\\bibitem[Torabi et~al.(2019)Torabi, Warnell, and Stone]{torabi2018generative}\\nFaraz Torabi, Garrett Warnell, and Peter Stone.\\n\\\\newblock Generative adversarial imitation from observation.\\n\\\\newblock In \\\\emph{Imitation, Intent, and Interaction (I3) Workshop at ICML},\\n  2019.\\n\\n\\\\bibitem[Wang et~al.(2020)Wang, Toyer, Gleave, and Emmons]{wang2020imitation}\\nSteven Wang, Sam Toyer, Adam Gleave, and Scott Emmons.\\n\\\\newblock The {\\\\tt imitation} library for imitation learning and inverse\\n  reinforcement learning.\\n\\\\newblock \\\\url{https://github.com/HumanCompatibleAI/imitation}, 2020.\\n\\n\\\\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, and\\n  F{{\\\\\"u}}rnkranz]{wirth2017survey}\\nChristian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F{{\\\\\"u}}rnkranz.\\n\\\\newblock A survey of preference-based reinforcement learning methods.\\n\\\\newblock \\\\emph{Journal of Machine Learning Research}, 18\\\\penalty0\\n  (136):\\\\penalty0 1--46, 2017.\\n\\n\\\\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{ziebart2010modeling}\\nBrian~D Ziebart, J~Andrew Bagnell, and Anind~K Dey.\\n\\\\newblock Modeling interaction via the principle of maximum causal entropy.\\n\\\\newblock In \\\\emph{\\\\ICML}, 2010.\\n\\n\\\\bibitem[Zoph \\\\& Le(2017)Zoph and Le]{zoph2016neural}\\nBarret Zoph and Quoc~V Le.\\n\\\\newblock Neural architecture search with reinforcement learning.\\n\\\\newblock In \\\\emph{\\\\ICLR}, 2017.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1606.06565': True,\n",
       "   '1606.01540': True,\n",
       "   '1610.03518': True,\n",
       "   '1711.09846': True,\n",
       "   '1805.00909': True,\n",
       "   '1807.03748': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'ICLR 2021',\n",
       "   'newsletter_category': 'Learning human intent',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #149',\n",
       "   'newsletter_url': 'https://mailchi.mp/dba8371e9221/an-149-the-newsletters-editorial-policy',\n",
       "   'summarizer': 'Rohin',\n",
       "   'summary': 'Since the state of the world has already been optimized for human preferences, it can be used to infer those preferences. For example, it isnt a coincidence that vases tend to be intact and on tables. An agent with an understanding of physics can observe that humans havent yet broken a particular vase, and infer that they care about vases not being broken.\\n\\n<@Previous work@>(@Learning Preferences by Looking at the World@) provides an algorithm, RLSP, that can perform this type of reasoning, but it is limited to small environments with known dynamics and features. In this paper (on which I am an author), we introduce a deep variant of the algorithm, called Deep RLSP, to move past these limitations. While RLSP assumes known features, Deep RLSP learns a feature function using self-supervised learning. While RLSP computes statistics for all possible past trajectories using dynamic programming, deep RLSP learns an inverse dynamics model and inverse policy to _simulate_ the most likely past trajectories, which serve as a good approximation for the necessary statistics. \\n\\nWe evaluate the resulting algorithm on a variety of Mujoco tasks, with promising results. For example, given a single state of a HalfCheetah balancing on one leg, Deep RLSP is able to learn a (noisy) policy that somewhat mimics this balancing behavior. (These results can be seen [here](https://sites.google.com/view/deep-rlsp).)',\n",
       "   'opinion': 'nan',\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': '[Paper: Learning What To Do by Simulating the Past](https://arxiv.org/abs/2104.03946)\\n\\n[Thesis: Extracting and Using Preference Information from the State of the World](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-210.html)',\n",
       "   'paper_version': None,\n",
       "   'arxiv_id': 'None',\n",
       "   'title': 'Learning What To Do by Simulating the Past',\n",
       "   'authors': 'David Lindner, Rohin Shah, Pieter Abbeel, Anca Dragan',\n",
       "   'date_published': 2021.0,\n",
       "   'data_last_modified': '',\n",
       "   'url': 'https://bair.berkeley.edu/blog/2021/05/03/rlsp/',\n",
       "   'abstract': '',\n",
       "   'author_comment': '',\n",
       "   'journal_ref': '',\n",
       "   'doi': '',\n",
       "   'primary_category': '',\n",
       "   'categories': '',\n",
       "   'individual_summary': 'Title: Learning What To Do by Simulating the Past\\nAuthors: David Lindner, Rohin Shah, Pieter Abbeel, Anca Dragan\\nSummary: Since the state of the world has already been optimized for human preferences, it can be used to infer those preferences. For example, it isnt a coincidence that vases tend to be intact and on tables. An agent with an understanding of physics can observe that humans havent yet broken a particular vase, and infer that they care about vases not being broken.\\n\\n<@Previous work@>(@Learning Preferences by Looking at the World@) provides an algorithm, RLSP, that can perform this type of reasoning, but it is limited to small environments with known dynamics and features. In this paper (on which I am an author), we introduce a deep variant of the algorithm, called Deep RLSP, to move past these limitations. While RLSP assumes known features, Deep RLSP learns a feature function using self-supervised learning. While RLSP computes statistics for all possible past trajectories using dynamic programming, deep RLSP learns an inverse dynamics model and inverse policy to _simulate_ the most likely past trajectories, which serve as a good approximation for the necessary statistics. \\n\\nWe evaluate the resulting algorithm on a variety of Mujoco tasks, with promising results. For example, given a single state of a HalfCheetah balancing on one leg, Deep RLSP is able to learn a (noisy) policy that somewhat mimics this balancing behavior. (These results can be seen [here](https://sites.google.com/view/deep-rlsp).)\\nMy opinion: nan',\n",
       "   'paper_text': '',\n",
       "   'text': 'HIGHLIGHTS\\nIn the survey I ran about a month ago, a couple of people suggested that I should clarify my editorial policy, especially since it has drifted since the newsletter was created. Note that I dont view what Im writing here as a policy that I am committing to. This is more like a description of how I currently make editorial decisions in practice, and it may change in the future.I generally try to only summarize high quality articles. Here, \"high quality\" means that the article presents some conceptually new thing not previously sent in the newsletter and there is decent evidence convincing me that this new thing is true / useful / worth considering. (Yes, novelty is one of my criteria. I could imagine sending e.g. a replication of some result if I wasnt that confident of the original result, but I usually wouldnt.)Throughout the history of the newsletter, when deciding whether or not to summarize an article, I have also looked for some plausible pathway by which the new knowledge might be useful to an alignment researcher. Initially, there was a pretty small set of subfields that seemed particularly relevant (especially reward learning) and I tried to cover most high-quality work within those areas. (I cover progress in ML because it seems like a good model of ML / AGI development should be very useful for alignment research.)However, over time as I learned more, I became more excited about a large variety of subfields. Theres basically no hope for me to keep up with all of the subfields, so now I rely a lot more on quick intuitive judgments about how exciting I expect a particular paper to be, and many high quality articles that are relevant to AI alignment never get summarized. I currently still try to cover almost every new high quality paper or post that *directly* talks about AI alignment (as opposed to just being relevant).Highlights are different. The main question I ask myself when deciding whether or not to highlight an article is: Does it seem useful for *most* technical alignment researchers to read this? Note that this is very different from an evaluation of how impactful or high quality the article is: a paper that talks about all the tips and tricks you need to get learning from human feedback to work in practice could be very impactful and high quality, but probably still wouldnt be highlighted because many technical researchers dont work with systems that learn from human feedback, and so wont read it. On the other hand, this editorial policy probably isnt that impactful, but it seems particularly useful for my readers to read (so that you know what you are and arent getting with this newsletter).A summary is where I say things that the authors would agree with. Usually, I strip out things that the authors said that I think are wrong. The exception is when the thing I believe is wrong is a central point of the article, in which case I will put it in the summary even though I dont believe it. Typically I will then mention the disagreement in the opinion (though this doesnt always happen, e.g. if Ive mentioned the disagreement in previous newsletters, or if it would be very involved to explain why I disagree). I often give authors a chance to comment on the summaries + opinions, and usually authors are happy overall but might have some fairly specific nitpicks.An opinion is where I say things that I believe that the authors may or may not believe. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n TECHNICAL AI ALIGNMENT\\n\\ufeff\\n\\n PROBLEMS\\n[Low-stakes alignment](https://ai-alignment.com/low-stakes-alignment-f3c36606937f) *(Paul Christiano)* (summarized by Rohin): We often split AI alignment into two parts: outer alignment, or \"finding a good reward function\", and inner alignment, or \"robustly optimizing that reward function\". However, these are not very precise terms, and they don\\'t form clean subproblems. In particular, for outer alignment, how good does the reward function have to be? Does it need to incentivize good behavior in all possible situations? How do you handle the no free lunch theorem? Perhaps you only need to handle the inputs in the training set? But then what specifies the behavior of the agent on new inputs?This post proposes an operationalization of outer alignment that admits a clean subproblem: *low stakes alignment*. Specifically, we are given as an assumption that we don\\'t care much about any small number of decisions that the AI makes -- only a large number of decisions, in aggregate, can have a large impact on the world. This prevents things like quickly seizing control of resources before we have a chance to react. We do not expect this assumption to be true in practice: the point here is to solve an easy subproblem in the hopes that the solution is useful for solving the hard version of the problem.The main power of this assumption is that we no longer have to worry about distributional shift. We can simply keep collecting new data online and training the model on the new data. Any decisions it makes in the interim period could be bad, but by the low-stakes assumption, they won\\'t be catastrophic. Thus, the primary challenge is in obtaining a good reward function, that incentivizes the right behavior after the model is trained. We might also worry about whether gradient descent will successfully find a model that optimizes the reward even on the training distribution -- after all, gradient descent has no guarantees for non-convex problems -- but it seems like, to the extent that gradient descent doesn\\'t do this, it will probably affect aligned and unaligned models equally.Note that this subproblem is still non-trivial, and existential catastrophes still seem possible if we fail to solve it. For example, one way that the low-stakes assumption could be made true was if we had a lot of bureaucracy and safeguards that the AI system had to go through before making any big changes to the world. It still seems possible for the AI system to cause lots of trouble if none of the bureaucracy or safeguards can understand what the AI system is doing. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** I like the low-stakes assumption as a way of saying \"let\\'s ignore distributional shift for now\". Probably the most salient alternative is something along the lines of \"assume that the AI system is trying to optimize the true reward function\". The main way that low-stakes alignment is cleaner is that it uses an assumption on the *environment* (an input to the problem) rather than an assumption on the *AI system* (an output of the problem). This seems to be a lot nicer because it is harder to \"unfairly\" exploit a not-too-strong assumption on an input rather than on an output. See [this comment thread](https://www.alignmentforum.org/posts/TPan9sQFuPP6jgEJo/low-stakes-alignment?commentId=askebCP36Ce96ZiJa) for more discussion. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n LEARNING HUMAN INTENT\\n[Transfer Reinforcement Learning across Homotopy Classes](https://arxiv.org/abs/2102.05207) *(Zhangjie Cao, Minae Kwon et al)* (summarized by Rohin): Suppose a robot walks past a person and it chooses to pass them on the right side. Imagine that we want to make the robot instead pass on the left side, and our tool for doing this was to keep nudging the robot\\'s trajectory until it did what we wanted. In this case, we\\'re screwed: there is no way to \"nudge\" the trajectory from passing on the right to passing on the left, without going through a trajectory that crashes straight into the person.The core claim of this paper is that the same sort of situation applies to finetuning for RL agents. Suppose we train an agent for one task where there is lots of data, and then we want to finetune it to another task. Let\\'s assume that the new task is in a different *homotopy class* than the original task, which roughly means that you can\\'t nudge the trajectory from the old task to the new task without going through a very low reward trajectory (in our example, crashing into the person). However, finetuning uses gradient descent, which nudges model parameters; and intuitively, a nudge to model parameters would likely correspond to a nudge to the trajectory as well. Since the new task is in a different homotopy class, this means that gradient descent would have to go through a region in which the trajectory gets very low reward. This is not the sort of thing gradient descent is likely to do, and so we should expect finetuning to fail in this case.The authors recommend that in such cases, we first train in a simulated version of the task in which the large negative reward is removed, allowing the finetuning to \"cross the gap\". Once this has been done, we can then reintroduce the large negative reward through a curriculum -- either by gradually increasing the magnitude of the negative reward, or by gradually increasing the number of states that have large negative reward. They run several robotics experiments demonstrating that this approach leads to significantly faster finetuning than other methods. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** This seems like an interesting point to be thinking about. The part I\\'m most interested in is whether it is true that small changes in the neural net parameters must lead to small changes in the resulting trajectory. It seems plausible to me that this is true for small neural nets but ends up becoming less true as neural nets become larger and data becomes more diverse. In our running example, if the neural net was implementing some decision process that considered both left and right as options, and then \"chose\" to go right, then it seems plausible that a small change to the weights could cause it to choose to go left instead, allowing gradient descent to switch across trajectory homotopy classes with a small nudge to model parameters. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Learning What To Do by Simulating the Past](https://bair.berkeley.edu/blog/2021/05/03/rlsp/) *(David Lindner et al)* (summarized by Rohin): Since the state of the world has already been optimized for human preferences, it can be used to infer those preferences. For example, it isnt a coincidence that vases tend to be intact and on tables. An agent with an understanding of physics can observe that humans havent yet broken a particular vase, and infer that they care about vases not being broken.[Previous work](https://bair.berkeley.edu/blog/2019/02/11/learning_preferences/) ([AN #45](https://mailchi.mp/35b451cb4d70/alignment-newsletter-45)) provides an algorithm, RLSP, that can perform this type of reasoning, but it is limited to small environments with known dynamics and features. In this paper (on which I am an author), we introduce a deep variant of the algorithm, called Deep RLSP, to move past these limitations. While RLSP assumes known features, Deep RLSP learns a feature function using self-supervised learning. While RLSP computes statistics for all possible past trajectories using dynamic programming, deep RLSP learns an inverse dynamics model and inverse policy to *simulate* the most likely past trajectories, which serve as a good approximation for the necessary statistics. We evaluate the resulting algorithm on a variety of Mujoco tasks, with promising results. For example, given a single state of a HalfCheetah balancing on one leg, Deep RLSP is able to learn a (noisy) policy that somewhat mimics this balancing behavior. (These results can be seen [here](https://sites.google.com/view/deep-rlsp).)**Read more:** [Paper: Learning What To Do by Simulating the Past](https://arxiv.org/abs/2104.03946)[Thesis: Extracting and Using Preference Information from the State of the World](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-210.html)\\ufeff\\n\\n MISCELLANEOUS (ALIGNMENT)\\n[Mundane solutions to exotic problems](https://ai-alignment.com/mundane-solutions-to-exotic-problems-395bad49fbe7) *(Paul Christiano)* (summarized by Rohin): The authors goal is to find mundane or simple algorithms that solve even exotic problems in AI alignment. Why should we expect this is possible? If an AI system is using powerful, exotic capabilities to evade detection, shouldnt we need powerful, exotic algorithms to fight that? The key idea here is that we can instead have a mundane algorithm that leverages the exotic capabilities of the AI system to produce an exotic oversight process. For example, we could imagine that a mundane algorithm could be used to create a question-answerer that knows everything the model knows. We could then address [gradient hacking](https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking) ([AN #71](https://mailchi.mp/938a7eed18c3/an-71avoiding-reward-tampering-through-current-rf-optimization)) by asking the question what should the loss be? In this case, our model has an exotic capability: very strong introspective access to its own reasoning and the training process that modifies it. (This is what is needed to successfully hack gradients). As a result, our question answerer should be able to leverage this capability to assign high loss (low reward) to cases where our AI system tries to hack gradients, even if our normal hardcoded loss would not do so. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n OTHER PROGRESS IN AI\\n\\ufeff\\n\\n DEEP LEARNING\\n[Scaling Scaling Laws with Board Games](https://arxiv.org/abs/2104.03113) *(Andrew L. Jones)* (summarized by Rohin): While we\\'ve seen [scaling laws](https://arxiv.org/abs/2001.08361) ([AN #87](https://mailchi.mp/c29b3247da6f/4da2bu7tjd)) for compute, data, and model size, we haven\\'t yet seen scaling laws for the *problem size*. This paper studies this case using the board game Hex, in which difficulty can be increased by scaling up the size of the board. The author applies AlphaZero to a variety of different board sizes, model sizes, RL samples, etc and finds that performance tends to be a logistic function of compute / samples used. The function can be characterized as follows:1. Slope: In the linearly-increasing regime, you will need about 2 as much compute as your opponent to beat them 2/3 of the time.2) Perfect play: The minimum compute needed for perfect play increases 7 for each increment in board size.3) Takeoff: The minimum training compute needed to see any improvement over random play increases by 4 for each increment of board size.These curves fit the data quite well. If the curves are fit to data from small board sizes and then used to predict results for large board sizes, their error is small.Recall that AlphaZero uses MCTS to amplify the neural net policy. The depth of this MCTS determines how much compute is spent on each decision, both at training time and test time. The author finds that a 10x increase in training-time compute allows you to eliminate about 15x of test-time compute while maintaining similar performance. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n NEWS\\n[BERI Seeking New University Collaborators](https://existence.org/2021/04/29/new-university-collaborators.html#new-university-collaborators) *(Sawyer Bernath)* (summarized by Rohin): [BERI](https://existence.org/faq) is seeking applications for new collaborators. They offer free services to university groups. If youre a member of a research group, or an individual researcher, working on long-termist projects, you can [apply here](http://existence.org/apply). Applications are due June 20th. |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI\\'m always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2021 Alignment Newsletter, All rights reserved.*\\n\\n**'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2007.01223v1',\n",
       "  'title': 'Verifiably Safe Exploration for End-to-End Reinforcement Learning',\n",
       "  'authors': ['Nathan Hunt',\n",
       "   'Nathan Fulton',\n",
       "   'Sara Magliacane',\n",
       "   'Nghia Hoang',\n",
       "   'Subhro Das',\n",
       "   'Armando Solar-Lezama'],\n",
       "  'date_published': '2020-07-02 16:12:20+00:00',\n",
       "  'data_last_modified': '2020-07-02 16:12:20+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2007.01223v1',\n",
       "  'abstract': 'Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.',\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.LG', 'cs.LO', 'F.3.1; I.2.8'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'unlabeled',\n",
       "  'confidence_score': 0.8948095199,\n",
       "  'main_tex_filename': './main.tex',\n",
       "  'text': '---\\nabstract: |\\n  Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.\\nauthor:\\n- |\\n  Nathan Hunt$^1$, Nathan Fulton$^2$, Sara Magliacane$^2$, Nghia Hoang$^2$, Subhro Das$^2$, Armando Solar-Lezama$^1$[^1]\\\\\\n  $^1$ Massachusetts Institute of Technology\\\\\\n  $^2$ MIT-IBM Watson AI Lab, IBM Research\\nbibliography:\\n- main.bib\\ntitle: Verifiably Safe Exploration for End-to-End Reinforcement Learning\\n---\\n\\nIntroduction {#sec:intro}\\n============\\n\\nDeep reinforcement learning algorithms [@sutton.barto:reinforcement] are effective at learning, often from raw sensor inputs, control policies that optimize for a quantitative reward signal. Learning these policies can require experiencing millions of unsafe actions. Even if a safe policy is finally learned -- which will happen only if the reward signal reflects all relevant safety priorities -- providing a purely statistical guarantee that the optimal policy is safe requires an unrealistic amount of training data [@RANDDriveToSafety]. The difficulty of establishing the safety of these algorithms makes it difficult to justify the use of reinforcement learning in safety-critical domains where industry standards demand strong evidence of safety prior to deployment [@iso26262].\\n\\nFormal verification provides a rigorous way of establishing safety for traditional control systems [@clarke2018handbook]. The problem of providing formal guarantees in RL is called *formally constrained reinforcement learning (FCRL)*. Existing FCRL methods such as [@HasanbeigKroening; @hasanbeig2018; @Hasanbeig2019; @hasanbeig2020cautious; @DBLP:conf/tacas/HahnPSSTW19; @DBLP:conf/aaai/AlshiekhBEKNT18; @aaai18; @DBLP:journals/corr/neuralsimplex; @de2019foundations] combine the best of both worlds: they optimize for a reward function while safely exploring the environment.\\n\\nExisting FCRL methods suffer from two significant disadvantages that detract from their real-world applicability: a) they enforce constraints over a completely symbolic state space that is assumed to be noiseless (e.g. the position of the safety-relevant objects are extracted from a simulator\\'s internal state); b) they assume that the entire reward structure depends upon the same symbolic state-space used to enforce formal constraints. The first assumption limits the applicability of FCRL in real-world settings where the system\\'s state must be inferred by an imperfect and perhaps even untrusted perception system. The second assumption implies a richer symbolic state that includes a symbolic representation of the reward, which we argue is unnecessary and may require more labelled data. Furthermore, this means these approaches may not generalize across different environments that have similar safety concerns, but completely different reward structures.\\n\\nThe goal of this paper is to *safely learn a safe policy* without assuming a perfect oracle that identifies the positions of all safety-relevant objects. I.e., unlike all existing FCRL methods, we do not rely on the internal state of the simulator. Prior to reinforcement learning, we train an object detection system to extract the positions of safety-relevant objects up to a certain precision. The pre-trained object detection system is used during reinforcement learning to extract the positions of safety-relevant objects, and that information is then used to enforce formal safety constraints. Absolute safety in the presence of untrusted perception is epistemologically challenging, but our formal safety constraints do at least account for a type of noise commonly found in object detection systems. Finally, although our system (called Verifiably Safe Reinforcement Learning, or `VSRL`) uses a few labeled data to pre-train the object detection, we still learn an end-to-end policy that may leverage the entire visual observation for reward optimization.\\n\\nPrior work from the formal methods community has demonstrated that you can do safe RL when you have full symbolic characterization of the environment and you can precisely observe the entire state. However, this is not realistic for actual robotic systems which have to interact with the physical world and can only perceive it through an imperfect visual system. This paper demonstrates that techniques inspired by formal methods can provide value even in this situation. First, we show that by using existing vision techniques to bridge between the visual input and the symbolic representation, one can leverage formal techniques to achieve highly robust behavior. Second, we prove that under weak assumptions on this vision system, the new approach will safely converge to an optimal safe policy.\\n\\nOur convergence result is the first of its kind for formally constrained reinforcement learning. Existing FCRL algorithms provide convergence guarantees only for an MDP that is defined over high-level symbolic features that are extracted from the internal state of a simulator. Instead, we establish optimality for policies that are learned from the low-level feature space (i.e., images). We prove that our method is capable of optimizing for reward even when significant aspects of the reward structure are not extracted as high-level features used for safety checking. Our experiments demonstrate that `VSRL` is capable of optimize for reward structure related to objects whose positions we do *not* extract via supervised training. This is significant because it means that `VSRL` needs pre-trained object detectors only objects that are safety-relevant.\\n\\nFinally, we provide a novel benchmark suite for Safe Exploration in Reinforcement Learning that includes both environments where the reward signal is aligned with the safety objectives and environments where the reward-optimal policy is unsafe. Our motivation for the latter is that assuming reward-optimal policies respect hard safety constraints neglects one of the fundamental challenges of Safe RL: preventing \"reward-hacking\\\\\". For example, it fundamentally difficult to tune a reward signal so that it has the \"correct\\\\\" trade-off between a pedestrian\\'s life and battery efficiency. We show that in the environments where the reward-optimal policy is safe (\"reward-aligned\"), `VSRL` learns a safe policy with convergence rates and final rewards which are competitive or even superior to the baseline method. More importantly, `VSRL` learns these policie with zero safety violations during training; i.e., it achieves perfectly safe exploration. In the environments where the reward-optimal policy is unsafe (\"reward-misaligned\"), `VSRL` both effectively optimizes for the subset of reward that can be achieved without violating safety constraints and successfully avoids \"reward-hacking\\\\\" by violating safety constraints.\\n\\nSummarily, this paper contributes: **(1)** `VSRL`, a new approach toward formally constrained reinforcement learning that does not make unrealistic assumptions about oracle access to symbolic features. This approach requires minimal supervision before reinforcement learning begins and explores safely while remaining competitive at optimizing for reward. **(2)** Theorems establishing that `VSRL` learns safely and maintains convergence properties of any underlying deep RL algorithm within the set of safe policies. **(3)** A novel benchmark suite for Safe Exploration in Reinforcement Learning that includes both properly specified and mis-specified reward signals.\\n\\nProblem Definition {#sec:probdef}\\n==================\\n\\nA reinforcement learning (RL) system can be represented as a Markov Decision Process (MDP) $(\\\\mathcal S, \\\\mathcal A, T, R, \\\\gamma)$ which includes a (possibly infinite) set $\\\\mathcal S$ of system states, an action space $\\\\mathcal A$, a transition function $T(s, a, s\\')$ which specifies the probability of the next system state being $s\\'$ after the agent executes action $a$ at state $s$, a reward function $R(s, a)$ that gives the reward for taking action $a$ in state $s$, and a discount factor $0 < \\\\gamma < 1$ that indicates the system preference to earn reward as fast as possible. We denote the set of initial states as $\\\\mathcal{S}_{init}\\\\subseteq \\\\mathcal S$.\\n\\nIn our setting, $\\\\mathcal S$ are images and we are given a safety specification $\\\\texttt{safe}: \\\\mathcal O \\\\rightarrow \\\\{0,1\\\\}$ over a set of high-level observations $\\\\mathcal O$, specifically, the positions (planar coordinates) of the safety-relevant objects in a 2D or 3D space. Since $\\\\mathcal S \\\\not = \\\\mathcal O$, it is not trivial to learn a safe policy $\\\\pi$ such that $\\\\texttt{safe}(\\\\mathcal O) = 1$ along every trajectory. We decompose this challenge into two well-formed and tractable sub-problems:\\n\\n1.  Pre-training a system $\\\\psi : \\\\mathcal S \\\\rightarrow \\\\mathcal O$ that converts the visual inputs into symbolic states using synthetic data (without acting in the environment);\\n\\n2.  Learning policies over the visual input space $\\\\mathcal S$ while enforcing safety in the symbolic state space $\\\\mathcal O$.\\n\\nThis problem is not solvable without making some assumptions, so here we focus on the following:\\n\\n::: {#assm:eps .assumption}\\n**Assumption 1**. *The symbolic mapping $\\\\psi$ is correct up to $\\\\epsilon$. More precisely, the true position of every object $o_i$ can be extracted from the image $s$ through the object detector $\\\\psi (s)_i$ so that the Euclidean distance between the actual and extracted positions is at most $\\\\epsilon$, i.e. $\\\\forall i \\\\; ||\\\\psi(s)_i - o_i||_2 \\\\leq \\\\epsilon$. We assume that we know an upper bound on the number of objects whose positions are extracted.*\\n:::\\n\\n::: {#assm:safe .assumption}\\n**Assumption 2**. *Initial states, described by a set of properties denoted as `init`, are safe, i.e. $\\\\forall s \\\\in \\\\mathcal{S}_{init}: \\\\texttt{safe}(\\\\psi(s))=1$ . Moreover, every state we reach after taking only safe actions has at least one available safe action.*\\n:::\\n\\n::: {#assm:plant .assumption}\\n**Assumption 3**. *We are given a dynamical model of the safety-relevant dynamics in the environment, given as either a discrete-time dynamical system or a system of ordinary differential equations, denoted as `plant`. We assume that model is correct up to simulation; i.e., if $T(s_i,a,s_j) \\\\not = 0$ for some action $a$, then the dynamical system *plant* maps $\\\\psi(s_i)$ to a set of states that includes $\\\\psi(s_j)$.*\\n:::\\n\\nFor example, the model may be a system of ODEs that describes how the acceleration and angle impact the future positions of a robot, as well as the potential dynamical behavior of some hazards in the environment. Note that this model only operates on $\\\\mathcal O$ (the symbolic state space), not $\\\\mathcal S$ (low-level features such as images or LiDAR).\\n\\n::: {#assm:agent .assumption}\\n**Assumption 4**. *We have an abstract model of the agent\\'s behavior, denoted as `ctrl`, that is correct up to simulation: if $T(s_i,a,s_j) \\\\not = 0$ for some action $a$, then $\\\\psi(s_j)$ is one of the possible next states after $a(\\\\psi(s_i))$ by *ctrl*.*\\n:::\\n\\nAn abstract model of the agent\\'s behavior describes at a high-level a safe controller behavior, disregarding the fine-grained details an actual controller needs to be efficient. An example is a model that brakes if it is too close to a hazard and can have any other type of behavior otherwise. Note that `ctrl` is very different from a safe policy $\\\\pi$, since it only models the safety-related aspects of $\\\\pi$ without considering reward optimization.\\n\\nAssumptions\\xa0[Assumption\\xa01](#assm:eps){reference-type=\"ref\" reference=\"assm:eps\"}-[Assumption\\xa04](#assm:agent){reference-type=\"ref\" reference=\"assm:agent\"} are mild and reasonable for most practical systems to satisfy.\\n\\nBackground {#sec:background}\\n==========\\n\\n[\\\\[subsec:deepRL\\\\]]{#subsec:deepRL label=\"subsec:deepRL\"} The goal of an RL agent represented as an MDP $(\\\\mathcal S, \\\\mathcal A, T, R, \\\\gamma)$ is to find a policy $\\\\pi$ that maximizes its expected total reward from an initial state $s_0 \\\\in \\\\mathcal{S}_{init}$ : $$\\\\begin{aligned}\\nV^{\\\\pi}(s) &\\\\triangleq& \\\\mathbb{E}_{\\\\pi}\\\\left[\\\\sum\\\\nolimits_{i=0}^{\\\\infty}\\\\gamma^i r_i)\\\\right] \\\\label{eq:1}\\\\end{aligned}$$ where $r_i$ is the reward at step $i$. In a deep RL setting, we can use the DNN parameters $\\\\theta$ to parametrize $\\\\pi(a|s; \\\\theta)$. One particularly effective implementation and extension of this idea is proximal policy optimization (PPO), which improves sample efficiency and stability by sampling data in batches and then optimizing a surrogate objective function that prevents overly large policy updates [@DBLP:journals/corr/SchulmanWDRK17]. This enables end-to-end learning through gradient descent which significantly reduces the dependency of the learning task on refined domain knowledge. Deep RL thus provides a key advantage over traditional approaches which were bottle-necked by a manual, time-consuming, and often incomplete feature engineering process.\\n\\nTo ensure formal guarantees we use differential Dynamic Logic () [@DBLP:journals/jar/Platzer08; @Platzer10; @DBLP:conf/lics/Platzer12a; @DBLP:journals/jar/Platzer17], a logic for specifying and proving reachability properties of hybrid dynamical systems, which combine both discrete-time (e.g. a robot that decides actions at discrete times) and continuous-time dynamics (e.g. an ODE describing the position of the robot at any time). Hybrid systems can be described with hybrid programs (HPs), for which we give an informal definition in . Notably, besides the familiar program syntax, HPs are able to represent a non-deterministic choice between two programs $\\\\alpha \\\\cup \\\\beta$, and a continuous evolution of a system of ODEs for an arbitrary amount of time, given a domain constraint $F$ on the state space $\\\\{x_1\\'=\\\\theta_1,...,x_n\\'=\\\\theta_n \\\\ \\\\& \\\\ F\\\\}$.\\n\\nFormulas of are generated by the following grammar where $\\\\alpha$ ranges over HPs: $$\\\\varphi,\\\\psi ::= f \\\\sim g ~|~  \\\\varphi \\\\land \\\\psi ~|~ \\\\varphi \\\\lor \\\\psi ~|~ \\\\varphi \\\\limply \\\\psi ~|~ \\\\forall x. \\\\varphi ~|~ \\\\exists x. \\\\varphi ~|~ \\\\dibox{\\\\alpha}\\\\varphi$$ where $f,g$ are polynomials over the state variables, $\\\\phi$ and $\\\\psi$ are formulas of the state variables, $\\\\sim$ is one of $\\\\{ \\\\le, <, =, >, \\\\ge\\\\}$. The formula $\\\\dibox{\\\\alpha}\\\\varphi$ means that a formula $\\\\varphi$ is true in every state that can be reached by executing the hybrid program $\\\\alpha$.\\n\\nGiven a set of initial conditions `init` for the initial states, a discrete-time controller `ctrl` representing the abstract behaviour of the agent, a continuous-time system of ODEs `plant` representing the environment and a safety property `safe` we define the *safety preservation problem* as verifying that the following holds: $$\\\\label{eq:safety}\\n\\\\texttt{init} \\\\limply \\\\dibox{\\\\{\\\\texttt{ctrl};\\\\texttt{plant}\\\\}^*}\\\\texttt{safe}$$ Intuitively, this formula means that if the system starts in an intial state that satisfies `init`, takes one of the (possibly infinite) set of control choices described by `ctrl`, and then follows the system of ordinary differential equations described by `plant`, then the system will always remain in states where `safe` is true.\\n\\n::: {.example}\\n**Example 1** (Hello, World). *Consider a 1D point-mass $x$ that must avoid colliding with a static obstacle ($o$) and has perception error bounded by $\\\\frac{\\\\epsilon}{2}$. The following model characterizes an infinite set controllers that are all safe, in the sense that $x \\\\not = o$ *for **all** forward time and at every point throughout the entire flow of the ODE*: $$\\\\texttt{init} \\\\limply \\\\dibox{\\\\{\\\\texttt{ctrl}; t:=0; \\\\texttt{plant}\\\\}^*}x-o > \\\\epsilon$$ $$\\\\begin{aligned}\\n\\\\text{where,} \\\\qquad  \\\\textsf{SB}(a) &\\\\equiv 2B(x-o-\\\\epsilon) > v^2+(a+B)*(aT^2+2Tv)) \\\\\\\\\\n\\\\texttt{init} &\\\\equiv \\\\textsf{SB}(-B) \\\\land B>0 \\\\land T>0 A > 0 \\\\land v \\\\ge 0 \\\\land \\\\epsilon > 0 \\\\\\\\\\n\\\\texttt{ctrl} &\\\\equiv a := *; ?-B \\\\le a \\\\le A \\\\land \\\\textsf{SB}(a) \\\\\\\\\\n\\\\texttt{plant} &\\\\equiv \\\\{ x\\'=v,v\\'=a,t\\'=1 \\\\& t \\\\le T \\\\land v \\\\ge 0 \\\\}\\\\end{aligned}$$ Starting from any state that satisifies the formula $\\\\texttt{init}$, the (abstract/non-deterministic) controller chooses **any** acceleration satisfying the $\\\\textsf{SB}$ constraint. After choosing any $a$ that satisfies [SB]{.sans-serif}, the system then follows the flow of the system of ODEs in `plant` for any positive amount of time $t$ less than $T$. The constraint $v \\\\ge 0$ simply means that braking (i.e., choosing a negative acceleration) can braing the pointmass to a stop, but cannot cause it to move backwards.*\\n\\n*The full formula says that no matter how many times we execute the controller and then follow the flow of the ODEs, it will always be the case -- again, for an infinite set of permissible controllers -- that $x-o < \\\\epsilon$.*\\n:::\\n\\nTheorems of can be automatically proven in the KeYmaera\\xa0X theorem prover [@DBLP:conf/cade/FultonMQVP15; @bellerophon]. [@DBLP:journals/fmsd/MitschP16] explains how to synthesize action space guards from non-deterministic specifications of controllers (`ctrl`), and @aaai18 [@DBLP:conf/tacas/FultonP19] explains how these action space guards are incorporated into reinofrcement learning to esnure safe exploration. Additional details about how we synthesize monitoring conditions from models is available in [@DBLP:journals/fmsd/MitschP16] and in .\\n\\n`VSRL`: Verifiably Safe RL on Visual Inputs {#sec:fossil}\\n===========================================\\n\\nWe present `VSRL`, a framework that can augment any deep RL algorithm to perform *safe exploration* on visual inputs. As discussed in , we decompose the general problem in two tasks:\\n\\nlearning a mapping of visual inputs $s$ into a symbolic state $o$ for safety-relevant properties using only a few examples (described in and shown in a);\\n\\nlearning policies over visual inputs, while enforcing safety in the symbolic state space (described in and shown in c).\\n\\nThis latter task requires a controller monitor, which is a function $\\\\varphi : O \\\\times A \\\\rightarrow \\\\{0, 1\\\\}$ that classifies each action $a$ in each symbolic state $o$ as \"safe\" or not. In this paper this monitor is synthesized and verified offline following [@aaai18; @DBLP:conf/tacas/FultonP19]. In particular, as discussed in the previous sections, the KeYmaera X theorem prover solves the safety preservation problem presented in Eq. for a set of high-level reward-agnostic safety properties `safe`, a system of differential equations characterizing the relevant subset of environmental dynamics `plant`, an abstract description of a safe controller `ctrl` and a set of initial conditions `init` (shown in b).\\n\\n![image](figures/vsrl_system.pdf){width=\"0.9\\\\\\\\columnwidth\"}\\n\\nObject Detection {#sec:symbolicMapping}\\n----------------\\n\\nIn order to remove the need to construct labelled datasets for each environment, we only assume that we are given a small set of images of each safety-critical object and a set of background images (in practice, we use 1 image per object and 1 background). We generate synthetic images by pasting the objects onto a background with different locations, rotations, and other augmentations. We then train a CenterNet-style object detector [@zhou2019objects_centernet] which performs multi-way classification for whether each pixel is the center of an object. For speed and due to the visual simplicity of the environments, the feature extraction CNN is a truncated ResNet18 [@he2016deep_resnet] which only keeps the first residual block. The loss function is the modified focal loss [@lin2017focal] from [@law2018cornernet]. See for full details on the object detector. Our current implementation does not optimize or dedicate hardware to the object detector, so detection adds some run-time overhead for all environments. However, this is an implementation detail rather than an actual limitation of the approach. There are many existing approaches that make it possible to run object detectors quickly enough for real-time control.\\n\\nEnforcing Constraints {#sec:constrainedLearning}\\n---------------------\\n\\nWhile `VSRL` can augment any existing deep RL algorithm, this paper extends PPO [@DBLP:conf/icml/SchulmanLAJM15]. The algorithm performs RL as normal except that, whenever an action is attempted, the object detector and safety monitor are first used to check if the action is safe. If not, a safe action is sampled uniformly at random from the safe actions in the current state. This happens outside of the agent and can be seen as wrapping the environment with a safety check. Pseudocode for performing this wrapping is in . The controller monitor is extracted from a verified model (see Page 3 of [@aaai18] for details). A full code listing that in-lines into a generic RL algorithm is provided in Appendix [12](#appendix:fullcode){reference-type=\"ref\" reference=\"appendix:fullcode\"}.\\n\\n[\\\\[alg:mname\\\\]]{#alg:mname label=\"alg:mname\"} **Input:** $s_t$: input image; $a_t$: input action; $\\\\psi$: object detector; $\\\\varphi$: controller monitor; $E = (\\\\mathcal S, \\\\mathcal A, R, T)$: MDP of the original environment\\\\\\n$a\\'_t = a_t$ Sample substitute safe action $a\\'_t$ uniformly from $\\\\{ a \\\\in \\\\mathcal A \\\\mid \\\\varphi(\\\\psi(s_t), a) \\\\}$ **Return** $s_{t+1} \\\\sim T(s_t, a\\'_t, \\\\cdot)$, $r_{t+1} \\\\sim R(s_t, a\\'_t)$\\n\\nSafety and Convergence Results\\n------------------------------\\n\\nWe establish two theoretical properties about `VSRL`. First, we show that `VSRL` safely explores. Second, we show that if `VSRL` is used on top of an RL algorithm which converges (locally or globally) then `VSRL` will converge to the (locally or globally) optimal safe policy. All proofs are in the Appendix.\\n\\n::: {#thm:safety .theorem}\\n**Theorem 1** (). *If Assumptions [Assumption\\xa01](#assm:eps){reference-type=\"ref\" reference=\"assm:eps\"}-[Assumption\\xa04](#assm:agent){reference-type=\"ref\" reference=\"assm:agent\"} hold along a trajectory $s_0, a_0, s_1, a_1, \\\\dots, a_{n-1}, s_n$ with $s_0 \\\\in \\\\mathcal{S}_{init}$ for a model of the environment `plant` and a model of the controller `ctrl`, where each $a_i$ is chosen based on , then every state along the trajectory is safe; i.e., $\\\\forall i \\\\ge 0, \\\\texttt{safe}(\\\\psi(s_i))$. [\\\\[thm:correctness\\\\]]{#thm:correctness label=\"thm:correctness\"}*\\n:::\\n\\nThis results implies that any RL agent augmented with is always safe during learning. Our second theorem states that any RL agent that is able to learn an optimal policy in an environment $E$ can be combined with to learn a *reward-optimal safe policy*.\\n\\n::: {#thm:policy_equivalence .theorem}\\n**Theorem 2**. *Let $E$ be an environment and $L$ a reinforcement learning algorithm.*\\n\\n*If $L$ converges to a reward-optimal policy $\\\\pi^*$ in $E$, then using with $L$ converges to $\\\\pi^*_s$, the safe policy with the highest reward (i.e. the *reward-optimal safe policy*).*\\n:::\\n\\nExperimental Validation of `VSRL` {#sec:evaluation}\\n=================================\\n\\nWe evaluate `VSRL` on four environments: a discrete **XO** environment [@garnelo2016towards], an adaptive cruise control environment (ACC), a 2D goal-finding environment similar to the Open AI Safety Gym Goal environment [@Ray2019] but without a MuJoCo dependency (GF), and a pointmesses environment that emphasizes the problem of preventing reward hacking in safe exploration systems (PM). `VSRL` explores each environment without encountering any unsafe states.\\n\\nThe **XO** Environment is a simple setting introduced by [@garnelo2016towards] for demonstrating symbolic reinforcement learning algorithms (the implementation by @garnelo2016towards was unavailable, so we reimplemented this environment). The **XO** environment, visualized in , contains three types of objects: **X** objects that must be collected (+1 reward), **O** objects that must be avoided (-1 reward), and the agent (marked by a **+**). There is also a small penalty (-0.01) at each step to encourage rapid collection of all **X**s and completion of the episode. This environment provides a simple baseline for evaluating `VSRL`. It is also simple to modify and extend, which we use to evaluate the ability of `VSRL` to generalize safe policies to environments that deviate slightly from implicit modeling assumptions. The symbolic state space includes the position of the **+** and the **O**, but not the position of the **X**s because they are not safety-relevant. The purpose of this benchmark is to provide a benchmark for safe exploration in a simple discrete setting.\\n\\nThe adaptive cruise control (ACC) environment has two objects: a follower and a leader. The follower must maintain a fixed distance from the leader without either running into the leader or following too far behind. We use the verified model from [@DBLP:journals/sttt/QueselMLAP16] to constrain the agent\\'s dynamics.\\n\\nThe 2D goal-finding environment consists of an agent, a set of obstacles, and a goal state. The obstacles are the red circles and the goal state is the green circle. The agent must navigate from its (random) starting position to the goal state without encountering any of the obstacles. Unlike the OpenAI Safety Gym, the obstacles are *hard* safety constraints; i.e., the episode ends if the agent hits a hazard. We use the verified model from [@DBLP:conf/rss/MitschGP13] to constrain the agent\\'s dynamics.\\n\\nThe 2D pointmesses environment consists of an agent, a set of obstacles, a goal state, and a set of pointmesses (blue Xs). The agent receives reward for picking up the pointmesses, and the episode ends when the agent picks up all messes and reaches the goal state. Unlike the 2D goal-finding environment, hitting an obstacle does not end the episode. Instead, the obstacle is removed from the environment and a random number of new pointmesses spawn in its place. Notice that this means that the agent may reward hack by taking an unsafe action (hitting an obstacle) and then cleaning up the resulting pointmesses. We consider this the incorrect behavior. We use the verified model from [@DBLP:conf/rss/MitschGP13] to constrain the agent\\'s dynamics.\\n\\n::: {#tab:results}\\n  ------------------------------------- ------ ------ ----- ------- ------- ------ -------- ------\\n                                          XO           ACC            GF              PM    \\n  (lr)2-3(lr)4-5(lr)6-7(lr)8-9 Method     R      U      R      U       R      U       R       U\\n  PPO                                    10.5   7500   529   13983   0.233   3733   -0.25    3819\\n  VSRL                                   10.5    0     967     0     0.228    0     -0.225    0\\n  ------------------------------------- ------ ------ ----- ------- ------- ------ -------- ------\\n\\n  : Final reward (R; higher is better) and total number of unsafe actions (U; lower is better) on all environments. All results are the median over at least 4 replicates.\\n:::\\n\\nWe compare `VSRL` to PPO using two metrics: the number of safety violations during training and the cumulative reward. These results are summarized in . `VSRL` is able to perfectly preserve safety in all environments from the beginning of training even with the $\\\\epsilon$-bounded errors in extracting the symbolic features from the images. In contrast, vanilla PPO takes many unsafe actions while training and does not always converge to a policy that entirely avoids unsafe objects by the end of training.\\n\\nIn some environments, preserving safety specifications also substantially improves sample efficiency and policy performance early in the training process. In the ACC environment, in particular, it is very easy to learn a safe policy which is reward-optimal. In the GF and PM environments, both the baseline agent and the `VSRL` agent struggle to learn to perform the task well (note that these tasks are quite difficult because encountering an obstacle ends the episode). However, `VSRL` remains safe without losing much reward relative to the amount of uncertainty in both policies. See for details on our experimental evaluation and implementation.\\n\\nRelated Work {#sec: relwork}\\n============\\n\\nRecently, there has been a growing interest in safe RL, especially in the context of *safe exploration*, where the agent has to be safe also during training. A naive approach to RL safety is reward shaping, in which one defines a penalty cost for unsafe actions. This approach has several drawbacks, e.g. the choice of the penalty is brittle, so a naive choice may not outweight a shorter path to the reward, as shown by @dalal2018safe. Therefore, recent work on safe RL addresses the challenge of providing reward-agnostic safety guarantees for deep RL [@garcia2015comprehensive; @xiang2018verification]. Many recent safe exploration methods focus on safety guarantees that hold in expectation (e.g., [@DBLP:conf/icml/SchulmanLAJM15; @DBLP:conf/icml/AchiamHTA17]) or with high probability (e.g., [@berkenkamp2017safe; @dalal2018safe; @koller2018learning; @cheng2019end]. Some of these approaches achieve impressive results by drawing upon techniques from control theory, such as Lyapunov functions [@berkenkamp2017safe] and control barrier certificates.\\n\\nOn the other hand, ensuring safety in expectation or with high probability is generally not sufficient in safety-critical settings where guarantees must hold *always*, even for rare and measure-zero events. Numerical testing alone cannot provide such guarantees in practice [@RANDDriveToSafety] or even in theory [@DBLP:conf/hybrid/PlatzerC07]. The problem of providing formal guarantees in RL is called *formally constrained reinforcement learning (FCRL)*. Existing FCRL methods such as [@HasanbeigKroening; @hasanbeig2018; @Hasanbeig2019; @hasanbeig2020cautious; @DBLP:conf/tacas/HahnPSSTW19; @DBLP:conf/aaai/AlshiekhBEKNT18; @aaai18; @DBLP:journals/corr/neuralsimplex; @de2019foundations] combine the best of both worlds: they optimize for a reward function while still providing formal safety guarantees. While most FCRL method can only ensure the safety in discrete-time environments known a priori, @aaai18 [@DBLP:conf/tacas/FultonP19] introduce *Justified Speculative Control*, which exploits Differential Dynamic Logic[@DBLP:conf/cade/Platzer15] to prove the safety of *hybrid systems*, systems that combine an agent\\'s discrete-time decisions with a continuous time dynamics of the system.\\n\\nA major drawback of current FCRL methods is that they only learn control policies over handcrafted symbolic state spaces. While many methods extract a symbolic mapping for RL from visual data, e.g. [@lyu2019sdrl; @yang2018peorl; @yang2019program; @lu2018robot; @garnelo2016towards; @li2018object_odrl; @liang2018task; @goel2018unsupervised_morel], they all require that all of the reward-relevant features are explicitly represented in the symbolic space. As shown by the many successes of Deep RL, e.g. [@mnih-atari-2013], handcrafted features often miss important signals hidden in the raw data.\\n\\nOur approach aims at combining the best of FCRL and end-to-end RL to ensure that exploration is always safe with formal guarantees, while allowing a deep RL algorithm to fully exploit the visual inputs for reward optimization.\\n\\nConclusion and Discussions {#sec:conclusion}\\n==========================\\n\\nSafe exploration in the presence of hard safety constraints is a schallenging problem in reinforcement learning. We contribute `VSRL`, an approach toward safe learning on visual inputs. Through theoretical analysis and experimental evaluation, this paper establishes that `VSRL` maintains perfect safety during exploration while obtaining comparable reward. Because `VSRL` separates safety-critical object detection from RL, next steps should include applying tools from adversarial robustness to the object detectors used by `VSRL`.\\n\\nSupplementary material for: Verifiably Safe Exploration for End-to-End Reinforcement Learning {#supplementary-material-for-verifiably-safe-exploration-for-end-to-end-reinforcement-learning .unnumbered}\\n=============================================================================================\\n\\nModel Monitoring {#appendix:monitors}\\n================\\n\\nWe use differential Dynamic Logic () [@DBLP:journals/jar/Platzer08; @Platzer10; @DBLP:conf/lics/Platzer12a; @DBLP:conf/cade/Platzer15; @DBLP:journals/jar/Platzer17] to specify safety constraints on the agent\\'s action space. is a logic for specifying and proving reachability properties of both discrete and continuous time dynamical systems.\\n\\nIn this section we expand on the definitions and provide some illustrative examples. In particular, we focus on the language of hybrid programs (HPs), their reachability logic (), and monitor synthesis for formulas.\\n\\nHybrid Programs Overview\\n------------------------\\n\\nAs shown succinctly in , hybrid programs are a simple programming language that combines imperative programs with systems of differential equations. We expand the description from and define the syntax and informal semantics of HPs are as follows:\\n\\n-   $\\\\alpha;\\\\beta$ executes $\\\\alpha$ and then executes $\\\\beta$.\\n\\n-   $\\\\alpha \\\\cup \\\\beta$ executes either $\\\\alpha$ or $\\\\beta$ nondeterministically.\\n\\n-   $\\\\alpha^*$ repeats $\\\\alpha$ zero or more times nondeterministically.\\n\\n-   $x := \\\\theta$ evaluates term $\\\\theta$ and assigns result to $x$.\\n\\n-   $x := *$ assigns an arbitrary real value to $x$.\\n\\n-   $\\\\{x_1\\'=\\\\theta_1,...,x_n\\'=\\\\theta_n \\\\& F\\\\}$ is the continuous evolution of $x_i$ along the solution to the system constrained to a domain defined by $F$.\\n\\n-   $?F$ aborts if formula $F$ is not true.\\n\\nHybrid programs have a denotational semantics that defines, for reach program, the set of states that are reachable by executing the program from an initial state. A state is an assignment of variables to values. For example, the denotation of $x := t$ in a state $s$ is: $$\\\\begin{aligned}\\n    \\\\llbracket x:=t\\\\rrbracket(s)(v) &= s(v) \\\\text{ for } v \\\\not = x \\\\\\\\\\n    \\\\llbracket x:=t\\\\rrbracket(s)(x) &= t\\\\end{aligned}$$\\n\\nComposite programs are given meaning by their constituent parts. For example, the meaning of $\\\\alpha \\\\cup \\\\beta$ is: $$\\\\llbracket\\\\alpha \\\\cup \\\\beta\\\\rrbracket(s) = \\\\llbracket\\\\alpha\\\\rrbracket(s) \\\\cup \\\\llbracket\\\\beta\\\\rrbracket(s)$$\\n\\nA full definition of the denotational semantics corresponding to the informal meanings given above is provided by [@DBLP:conf/cade/Platzer15].\\n\\nDifferential Dynamic Logic Overview\\n-----------------------------------\\n\\nFormulas of are generated by the grammar: $$\\\\varphi,\\\\psi ::= f \\\\sim g ~|~  \\\\varphi \\\\land \\\\psi ~|~ \\\\varphi \\\\lor \\\\psi ~|~ \\\\varphi \\\\limply \\\\psi ~|~ \\\\forall x, \\\\varphi ~|~ \\\\exists x, \\\\varphi ~|~ \\\\dibox{\\\\alpha}\\\\varphi$$ where $f,g$ are polynomials of real arithmetic, $\\\\sim$ is one of $\\\\{ \\\\le, <, =, >, \\\\ge\\\\}$, and the meaning of $\\\\dibox{\\\\alpha}\\\\varphi$ is that $\\\\varphi$ is true in every state that can be reached by executing the program $\\\\alpha$. Formulas of can be stated and proven in the KeYmaera\\xa0X theorem prover [@DBLP:conf/cade/FultonMQVP15; @bellerophon].\\n\\nThe meaning of a formula is given by a *denotational semantics* that specifies the set of states $s \\\\in S$ in which a formula is true. For example, $$\\\\begin{aligned}\\n    \\\\llbracket true\\\\rrbracket &= S \\\\\\\\\\n    \\\\llbracket false\\\\rrbracket &= \\\\emptyset \\\\\\\\\\n    \\\\llbracket x=1 \\\\land y=2\\\\rrbracket &= \\\\{s \\\\in S | s(x) = 1 \\\\text{ and } s(y) = 2\\\\}\\\\end{aligned}$$ We write $\\\\models\\\\varphi$ as an alternative notation for the fact that $\\\\varphi$ is true in all states (i.e., $\\\\llbracket\\\\varphi\\\\rrbracket = \\\\llbracket true\\\\rrbracket$). We denote by $\\\\vdash \\\\varphi$ the fact that there is a proof of $\\\\varphi$ in the proof calculus of .\\n\\nUsing Safe Controller Specifications to Constrain Reinforcement Learning\\n------------------------------------------------------------------------\\n\\nGiven a hybrid program and proven safety specification, @aaai18 explains how to construct safety monitors (which we also call *safe actions filters* in this paper) for reinforcement learning algorithms over a symbolic state space. In this section, we summarize their algorithm.\\n\\nAs opposed to our approach, @aaai18 employs both a controller monitor (that ensures the safety of the controller) and a model monitor (that ensures the adherence of the model to the actual system and checks for model mismatch).\\n\\nThe meaning of the controller monitor and model monitor are stated with respect to a specification with the syntactic form $P \\\\limply \\\\dibox{\\\\{\\\\text{ctrl};\\\\text{plant}\\\\}^*} Q$ where $P$ is a formula specifying initial conditions, $\\\\text{plant}$ is a dynamical system expressed as a hybrid program that accurately encodes the dynamics of the environment, and $Q$ is a post-condition. [@aaai18] assumes that $\\\\text{ctrl}$ as the form $?P_1;a_1 \\\\cup \\\\cdots \\\\cup P_n; a_n$, where $a_i$ are discrete assignment programs that correspond to the action space of the RL agent. For example, an agent that can either accelerate or brake as action space $A = \\\\{ A, -B \\\\}$. The corresponding control program will be $?P_1; a:=A \\\\cup ?P_2; a:=-B$ where $P_1$ is a formula characterizing when it is safe to accelerate and $P_2$ is a formula characterizing when it is safe to brake.\\n\\nGiven such a formula, [@aaai18] defines the controller and model monitors using the following conditions:\\n\\n::: {#cor:CM .cor}\\n**Corollary 1** (Meaning of Controller Monitor). *Suppose $CM$ is a controller monitor for $P \\\\limply \\\\dibox{\\\\{\\\\text{ctrl};\\\\text{plant}\\\\}^*} Q$ and and $u : S \\\\rightarrow S$. Then $CM(u,s)$ implies $(s, u(s)) \\\\in \\\\llbracket\\\\text{ctrl}\\\\rrbracket$.*\\n:::\\n\\n::: {#cor:MM .cor}\\n**Corollary 2** (Meaning of Model Monitor). *Suppose $MM$ is a model monitor for $\\\\text{init} \\\\limply \\\\dibox{\\\\{\\\\text{ctrl};\\\\text{plant}\\\\}^*} Q$, that $u$ is a sequence of actions, and that $s$ is a sequence of states. If $MM(s_{i-1}, u_{i-1}, s_{i})$ for all $i$ then $s_i \\\\models Q$, and also $(s_i, u_i(s_i)) \\\\in \\\\llbracket\\\\text{ctrl}\\\\rrbracket$ implies $(u_i(s_i), s_{i+1}) \\\\in \\\\llbracket\\\\text{plant}\\\\rrbracket$.*\\n:::\\n\\nProof of  {#appendix:proof}\\n========\\n\\nIf the object detector produces an accurate mapping, then will preserve the safety constraint associated with the $\\\\varphi$ monitor. We state this property formally in .\\n\\n::: {#thm:safety .theorem*}\\n**Theorem 1** (Safety Theorem). *Assume the following conditions hold along a trajectory $s_0, a_0, \\\\dots, s_n$ with $s_0 \\\\in \\\\mathcal{S}_{init}$:*\\n\\nA1\\n\\n:   *Initial states are safe: $s \\\\in \\\\mathcal{S}_{init}$ implies $\\\\psi(s) \\\\models \\\\textit{init}$.*\\n\\nA2\\n\\n:   *The model and symbolic mapping are correct up to simulation: If $T(s_i,a,s_j) \\\\not = 0$ for some action $a$ then $(\\\\psi(s_i), a(\\\\psi(s_i))) \\\\in \\\\llbracket\\\\textit{ctrl}\\\\rrbracket$ and $(\\\\psi(s_i), \\\\psi(s_j)) \\\\in {\\\\small \\\\llbracket\\\\textit{plant}\\\\rrbracket}$.*\\n:::\\n\\n::: {.proof}\\n*Proof.* We begin the proof by pointing out that our assumption about how $$\\\\textit{init} \\\\limply \\\\dibox{\\\\{\\\\textit{ctrl};\\\\textit{plant}\\\\}^*}\\\\textit{safe}$$ was proven provides us with the following information about some formula $J$:\\n\\n$$\\\\begin{aligned}\\n\\\\vdash&~ \\\\textit{init} \\\\limply J  & (\\\\textbf{LI1}) \\\\\\\\\\n\\\\vdash&~ J \\\\limply \\\\textit{safe}  & (\\\\textbf{LI2}) \\\\\\\\\\n\\\\vdash&~ J \\\\limply \\\\dibox{\\\\{\\\\textit{ctrl};\\\\textit{plant}\\\\}^*}J  & (\\\\textbf{LI3})\\\\end{aligned}$$\\n\\nNow, assume $s_0, a_0, s_1, a_1, \\\\dots, s_n$ with $s_0 \\\\in \\\\mathcal{S}_{init}$ is a trajectory generated by running an RL agent with actions selected by and proceed by induction on the length of the sequence with the inductive hypothesis that $\\\\psi(s_i) \\\\models J$.\\n\\nIf $i=0$ then $s_0 \\\\in \\\\mathcal{S}_{init}$ by assumption. Therefore, $\\\\psi(s_0) \\\\models \\\\textit{init}$ by **A1**. We know by **LI1** that $\\\\vdash \\\\textit{init} \\\\limply J$. Therefore, $\\\\psi(s_0) \\\\models J$ by Modus Ponens and the soundness of the proof calculus.\\n\\nNow, suppose $i > 0$. We know $\\\\psi(s_i) \\\\models J$ by induction. Furthermore, we know $T(s_i, a_i, s_{i+1}) \\\\not = 0$ because otherwise this trajectory could not exist. By **A2** and the denotation of the $;$ operator, we know $(\\\\psi(s_i), \\\\psi(s_{i+1})) \\\\in \\\\llbracket\\\\textit{ctrl};\\\\textit{plant}\\\\rrbracket$. By **LI3**, we know $\\\\vdash J \\\\limply \\\\dibox{\\\\textit{ctrl};\\\\textit{plant}}J$ Therefore, $\\\\psi(s_i) \\\\models J$ and $(\\\\psi(s_i), \\\\psi(s_{i+1})) \\\\in \\\\llbracket\\\\textit{ctrl};\\\\textit{plant}\\\\rrbracket$ implies $\\\\psi(s_i+1) \\\\models J$ by the denotation of the box modality and the soundness of .\\n\\nWe have now established that $\\\\psi(s_i) \\\\models J$ for all $i \\\\ge 0$. By **LI2**, Modus Ponens, and soundness of the proof calculus, we finally conclude that $\\\\psi(s_i) \\\\models \\\\textit{safe}$.\\xa0\\n:::\\n\\nNote that if all actions $a_i$ along the trajectory are generated using , and if the model is accurate, then the two assumptions in will hold.\\n\\nProof of  {#app:env_wrapping}\\n========\\n\\nIn order to enforce safety, we wrap the original environment in a new one which has no unsafe actions. By not modifying the agent or training algorithm, any theoretical results (e.g. convergence) which the algorithm already has will still apply in our safety-wrapped environment. However, it is still necessary to show the relation between the (optimal) policies that may be found in the safe environment and the policies in the original environment. We show that 1) all safe policies in the original environment have the same transition probabilities and expected rewards in the wrapped environment and 2) all policies in the wrapped environment correspond to a policy in the original environment which has the same transition probabilities and expected rewards. This shows that the optimal policies in the wrapped environment are optimal among safe policies in the original environment (so no reward is lost except where required by safety).\\n\\nLet the original environment be the MDP $E = (\\\\mathcal S, \\\\mathcal A, T, R)$. We define a safety checker $C : \\\\mathcal S \\\\times \\\\mathcal A \\\\rightarrow \\\\{T, F\\\\}$ to be a predicate such that $C(s, a)$ is True iff action $a$ is safe in state $s$ in $E$. When we refer to an action as safe or unsafe, we always mean in the original environment $E$. A policy $\\\\pi$ in $E$ is safe iff $$\\\\forall s \\\\in \\\\mathcal S \\\\;\\\\forall a \\\\in \\\\mathcal A \\\\; \\\\pi(a|s) > 0 \\\\implies C(s, a).$$\\n\\nThe safety-wrapped environment will be $E\\' = (\\\\mathcal S, \\\\mathcal A, T\\', R\\')$ where the transition and reward functions will be modified to ensure there are no unsafe actions and expected rewards in $E\\'$ correspond with those from acting safely in $E$.\\n\\n$T\\'$ is required to prevent the agent from taking unsafe actions; for any safe action, we keep this identical to $T$. When an unsafe action is attempted, we could either take a particular safe action deterministically (perhaps shared across states, if some action is always safe, or a state-specific safe action) or sample (probably uniformly) from the safe actions in a given state. We prefer the latter approach of sampling from the safe actions because this makes taking an unsafe action have higher variance, so the agent will probably learn to avoid such actions. If unsafe actions are deterministically mapped to some safe action(s), they become indistinguishable, so the agent has no reason to avoid unsafe actions (unless we tamper with the reward function). Thus we set\\n\\n$$T\\'(s, a, s\\') = \\\\begin{cases}\\nT(s, a, s\\') & \\\\text{if } C(s, a) \\\\\\\\\\n\\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\mathcal A_{C(s)}} T(s, a\\', s\\') & \\\\text{otherwise}\\n\\\\end{cases}$$\\n\\nwhere $\\\\mathcal A_{C(s)} = \\\\{a \\\\in \\\\mathcal A \\\\mid C(s, a)\\\\}$ is the set of safe actions in state $s$. This simulates replacing unsafe actions with a safe action chosen uniformly at random.\\n\\n$R\\'$ is defined similarly so that it simulates the reward achieved by replacing unsafe actions with safe ones uniformly at random:\\n\\n$$R\\'(s, a) = \\\\begin{cases}\\nR(s, a) & \\\\text{if } C(s, a) \\\\\\\\\\n\\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\mathcal A_{C(s)}} R(s, a\\') & \\\\text{otherwise}\\n\\\\end{cases}.$$\\n\\n::: {.lemma}\\n**Lemma 1**. *For every safe policy $\\\\pi$ in E, following that policy in $E\\'$ leads to the same transitions with the same probabilities and gives the same expected rewards.*\\n:::\\n\\n::: {.proof}\\n*Proof.* By definition of safety, $\\\\pi$ has zero probability for any $(s, a)$ where $C(s, a)$ isn\\'t true. Thus actions sampled from $\\\\pi$ lead to transitions and rewards from the branch of $T\\'$ and $R\\'$ where they are identical to $T$ and $R$.\\xa0\\n:::\\n\\n::: {#lem:isomorphic_policies .lemma}\\n**Lemma 2**. *For every policy $\\\\pi\\'$ in $E\\'$ there exists a safe policy $\\\\pi$ in $E$ such that $\\\\pi\\'$ has the same transition probabilities and expected rewards in $E\\'$ as $\\\\pi$ does in $E$.*\\n:::\\n\\n::: {.proof}\\n*Proof.* For any $\\\\pi\\'$ in $E\\'$, let $g(\\\\pi\\') = \\\\pi$ be defined such that\\n\\n$$\\\\pi(a|s) = \\\\begin{cases}\\n\\\\pi\\'(a|s) + \\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\overline{\\\\mathcal A}_{C(s)} } \\\\pi\\'(a\\'|s) & \\\\text{if } C(s, a) \\\\\\\\\\n0 & \\\\text{otherwise}\\n\\\\end{cases}$$\\n\\nwhere $\\\\overline{\\\\mathcal A}_{C(s)} = \\\\{a \\\\in \\\\mathcal A ~|~ \\\\lnot C(s, a)\\\\}$ is the set of unsafe actions in state $s$. This simulates evenly redistributing the the probability that $\\\\pi\\'$ assigns to unsafe actions in $s$ among the safe actions.\\n\\nWe show first that the transition probabilities of $\\\\pi$ in $E$ and $\\\\pi\\'$ in $E\\'$ are the same.\\n\\n$$\\\\begin{aligned}\\nP_{\\\\pi, E}(s\\' | s) &= \\\\sum_{a \\\\in \\\\mathcal A} \\\\pi(a|s) T(s, a, s\\') \\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A_{C(s)} } \\\\pi(a|s) T(s, a, s\\') + \\\\sum_{a \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\underbrace{\\\\pi(a|s)}_{=0} T(s, a, s\\') \\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A_{C(s)} } \\\\left(\\\\pi\\'(a|s) + \\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\pi\\'(s, a\\')\\\\right) T(s, a, s\\') \\\\\\\\ \\n&= \\\\sum_{a \\\\in \\\\mathcal A_{C(s)} } \\\\pi\\'(a|s)  T(s, a, s\\') + \\\\sum_{a\\' \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\pi\\'(s, a\\') \\\\frac{1}{|\\\\mathcal A_{C(s)}|}  \\\\left( \\\\sum_{a \\\\in \\\\mathcal A_{C(s)} } T(s, a, s\\') \\\\right) \\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A_{C(s)} } \\\\pi\\'(a|s)  T\\'(s, a, s\\') +  \\\\sum_{a \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\pi\\'(a|s) \\\\frac{1}{|\\\\mathcal A_{C(s)}|}  \\\\left( \\\\sum_{a\\' \\\\in \\\\mathcal A_{C(s)} } T(s, a\\', s\\') \\\\right) \\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A_{C(s)} } \\\\pi\\'(a|s)  T\\'(s, a, s\\') + \\\\sum_{a \\\\in \\\\overline{\\\\mathcal A}_{C(s)} } \\\\pi\\'(a|s)  T\\'(s, a, s\\') \\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A } \\\\pi\\'(a|s)  T\\'(s, a, s\\') \\\\\\\\\\n&= P_{\\\\pi\\', E\\'}(s\\' | s)\\\\end{aligned}$$\\n\\nLet $\\\\mathop{\\\\mathbb{E}_{\\\\pi, E}} \\\\left[ R_s \\\\right]$ be the expected reward of following the policy $\\\\pi$ in environment $E$ at state $s$. The equality of the expected reward for $\\\\pi$ in every state of $E$ and $\\\\pi\\'$ in every state of $E\\'$ can be shown similarly:\\n\\n$$\\\\begin{aligned}\\n\\\\mathop{\\\\mathbb{E}_{\\\\pi, E}} \\\\left[ R_s \\\\right] &= \\\\sum_{a \\\\in \\\\mathcal A} \\\\pi(a|s) R(s, a) \\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A} R(s, a) \\\\begin{cases}\\n\\\\pi\\'(a|s) + \\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\pi\\'(a\\'|s) & \\\\text{if } C(s, a) \\\\\\\\\\n0 & \\\\text{otherwise}\\n\\\\end{cases}\\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A_{C(s)}} R(s, a) \\\\left(\\\\pi\\'(a|s) + \\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\pi\\'(a\\'|s)\\\\right)\\\\\\\\\\n&= \\\\left(\\\\sum_{a \\\\in \\\\mathcal A_{C(s)}} \\\\pi\\'(a|s) R(s, a)\\\\right) + \\\\left(\\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\pi\\'(a\\'|s)\\\\right) \\\\sum_{a \\\\in \\\\mathcal A_{C(s)}} R(s, a)\\\\end{aligned}$$\\n\\n$$\\\\begin{aligned}\\n\\\\mathop{\\\\mathbb{E}_{\\\\pi\\', E\\'}} \\\\left[ R\\'_s \\\\right] &= \\\\sum_{a \\\\in \\\\mathcal A} \\\\pi\\'(a|s) R\\'(s, a)\\\\\\\\\\n&= \\\\sum_{a \\\\in \\\\mathcal A} \\\\pi\\'(a|s) \\\\begin{cases}\\nR(s, a) & \\\\text{if } C(s, a)\\\\\\\\\\n\\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\mathcal A_{C(s)}} R(s, a\\') & \\\\text{otherwise}\\n\\\\end{cases}\\\\\\\\\\n&= \\\\left(\\\\sum_{a \\\\in \\\\mathcal A_{C(s)}} \\\\pi\\'(a|s) R(s, a)\\\\right) + \\\\left(\\\\frac{1}{|\\\\mathcal A_{C(s)}|} \\\\sum_{a\\' \\\\in \\\\mathcal A_{C(s)}} R(s, a\\')\\\\right) \\\\sum_{a \\\\in \\\\overline{\\\\mathcal A}_{C(s)}} \\\\pi\\'(a|s)\\\\\\\\\\n&= \\\\mathop{\\\\mathbb{E}_{\\\\pi, E}} \\\\left[ R_s \\\\right]\\\\end{aligned}$$\\xa0\\n:::\\n\\n::: {.theorem*}\\n**Theorem 2**. *Let $E$ be an environment and $L$ a reinforcement learning algorithm. If $L$ converges to a reward-optimal policy $\\\\pi^*$ in $E$, then using with $L$ converges to $\\\\pi^*_s$, the safe policy with the highest reward (i.e. the *reward-optimal safe policy*).*\\n:::\\n\\n::: {.proof}\\n*Proof.* We provide proof by contraposition. Let\\'s assume that $\\\\pi^*$ is not optimal in $E\\'$. Then there must exist $\\\\pi\\'$ in $E\\'$ that gets more reward. But, by Lemma [Lemma\\xa02](#lem:isomorphic_policies){reference-type=\"ref\" reference=\"lem:isomorphic_policies\"}, $\\\\pi\\'$ corresponds to a safe policy $\\\\pi = g(\\\\pi\\')$ in $E$ which gets the same amount of reward, so $\\\\pi$ is better in $E$ than $\\\\pi^*$. Hence, $\\\\pi^*$ is not optimal among safe policies in $E$.\\xa0\\n:::\\n\\nA few notes regarding this theorem:\\n\\n-   The intuitive approach to making an agent safe, if we know the set of safe actions in each state, might be to sample from the safe subset of the agent\\'s policy distribution (after renormalization). Because this is not actually sampling from the distribution the agent learned, this may interfere with training the agent.\\n\\n-   While we keep $\\\\mathcal S$ the same in $E$ and $E\\'$, there may be states which become unreachable in $E\\'$ because only unsafe transitions in $E$ lead to them. Thus the effective size of $E\\'$\\'s state space may be smaller which could speed up learning effective safe policies.\\n\\n-   Our approach can be viewed as transforming a constrained optimization problem (being safe in $E$; have to treat it as a CMDP) into an unconstrained one (being safe in $E\\'$).\\n\\nObject Detection Details {#app:symbolicMapping}\\n========================\\n\\n#### CenterNet [@zhou2019objects_centernet]\\n\\nCenterNet-style object detectors take an image of size $H \\\\times W \\\\times C$ (height, width, and channels, respectively) as input and output an image $Z$ of size $\\\\lfloor H / S \\\\rfloor \\\\times \\\\lfloor W / S \\\\rfloor \\\\times (N + 2)$ where $S$ is a downscaling amount to make the detection more efficient and $N$ is the number of classes to detect. For the first $N$ channels, $z_{ijk}$ is the probability that the pixel $i, j$ is the (downscaled) center of an object of the $k$th class. The final two channels of $Z$ contain x and y offsets. The offsets account for the error in detecting locations in the original image because the predictions are downscaled: a downscaled detection at $i, j$ can be converted to a detection in the original image coordinates at $i\\', j\\'$ by setting $i\\' = i * S + o_1, j\\' = j * S + o_2$ where $o_1 = z_{ijN}, o_2 = z_{ij(N+1)}$. As the objects in our environments have constant sizes, we don\\'t predict the object sizes as is done in CenterNet.\\n\\nAs in [@zhou2019objects_centernet], we set $S = 4$ and use max-pooling and thresholding to convert from the probability maps to a list of detections. In particular, there is a detection for object class $k$ at location $i,j$ if $z_{ijk} \\\\ge \\\\tau$ and $z_{ijk} == maxpool(z_{ijk})$ where $maxpool$ is a 3x3 max-pooling operation centered at $i,j$ (with zero-padding). We set $\\\\tau = 0.5$. The detector then returns a list of tuples $(k, i\\', j\\')$ containing the class id ($k$) and center point ($i\\', j\\'$) of each detection. These are used in evaluating the constraints wherever the formulas reference the location of an object of type $k$ (i.e. if a robot must avoid hazards, the constraint will be checked using the location of each hazard in the detections list).\\n\\nWe use ResNet-18 [@he2016deep_resnet] truncated to the end of the first residual block. The first layer is also modified to have only a single input channel because we use grayscale images, as is common for inputs to RL agents. This already outputs an image which is downscaled 4x relative to the input, so we do the centerpoint classification and offset prediction directly from this image, removing the need for upscaling. We use one 1x1 convolutional layer for the offset prediction (two output channels) and one for the center point classification ($N$ output channels and sigmoid activation).\\n\\n#### Training\\n\\nTo avoid introducing a dependency on heavy annotations, we restricted ourselves to a single image for each safety-relevant object in an environment and a background image. We produce images for training by pasting the objects into random locations in the background image. We also use other standard augmentations such as left-right flips and rotations. New images are generated for every batch.\\n\\nWe use the label-generation and loss function from [@zhou2019objects_centernet]. Labels for each object class are generated by evaluating, at each pixel position, a Gaussian density on the distance from that position to the center of the nearest object of the given class (see for details).\\n\\n**Input** $xs, ys$: center $x, y$ positions for each object of a type; $h, w$: label image height, width $Y \\\\leftarrow 0_{h \\\\times w}$ $\\\\Sigma \\\\leftarrow \\\\left[\\\\begin{array}{cc}\\n    h / 2 & 0  \\\\\\\\\\n    0 & w / 2 \\n\\\\end{array}\\\\right]$ $\\\\mu \\\\leftarrow [x, y]$ $Y_{ij} \\\\leftarrow \\\\max(Y_{ij}, \\\\phi_{\\\\mu, \\\\Sigma}([i, j]))$ **return** Y\\n\\nThe loss function is a focal loss: a variant of cross-entropy that focuses more on difficult examples (where the predicted probabilities are farther from the true probabilities) [@lin2017focal]. We use a modified focal loss as in [@law2018cornernet; @zhou2019objects_centernet]:\\n\\n$$\\\\begin{aligned}\\n    \\\\frac{-1}{N} \\\\sum_{i, j, k} \\\\begin{cases} \\n      (1 - \\\\hat Y_{ijk})^\\\\alpha \\\\log(\\\\hat Y_{ijk}) & \\\\text{if } Y_{ijk} = 1 \\\\\\\\\\n      (1 - Y_{ijk})^\\\\beta \\\\hat Y_{ijk}^\\\\alpha \\\\log(1 - \\\\hat Y_{ijk}) & \\\\text{otherwise} \\n  \\\\end{cases}\\\\end{aligned}$$\\n\\nwhere $N$ is the number of objects in the image (of any type); $i \\\\in [1, h]$; $j \\\\in [1, w]$; $t \\\\in [1, T]$; $w, h$ are the width and height of the image; and $T$ is the number of object classes. $\\\\hat Y_{ijk}$ is the predicted probability of an object of type $t$ being centered at position $(x, y)$ in the image and $Y_{ijk}$ is the \"true\" probability. $\\\\alpha, \\\\beta$ are hyperparameters that we set to 2 and 4, respectively, as done by [@law2018cornernet; @zhou2019objects_centernet]. We remove the division by $N$ if an image has no objects present. The loss for the offsets is mean-squared error, and we weight the focal loss and offset loss equally. We use the Adam optimizer [@kingma2014adam] with learning rate 0.000125, $beta_1 = 0.9$, $\\\\beta_2 = 0.999$, as in [@zhou2019objects_centernet]. We decrease the learning rate by a factor of 10 if the loss on a validation set of 5,000 new images doesn\\'t improve within 10 training epochs of 20,000 images. The batch size is 32 as in [@zhou2019objects_centernet]. We keep the model which had the best validation loss.\\n\\nReinforcement Learning Details {#appendix:fullcode}\\n==============================\\n\\nIn all of our experiments, we use PPO as the reinforcement learning algorithm. Our hyperparameter settings are listed in Table [2](#tab:ppo_params){reference-type=\"ref\" reference=\"tab:ppo_params\"}. We run several environments in parallel to increase training efficiency using the method and implementation from [@stooke2019rlpyt].\\n\\nWe use grayscale images as inputs to the RL agent, and the CNN architecture from [@espeholt2018impala].\\n\\n::: {#tab:ppo_params}\\n  Hyperparameter              Value\\n  --------------------------- -------------------------\\n  Adam learning rate          $0.001 \\\\times \\\\alpha$\\n  Num. epochs                 4\\n  Number of actors            32\\n  Horizon (T)                 64\\n  Minibatch size              $2048 ~(=32 \\\\times 64)$\\n  Discount ($\\\\gamma$)         0.99\\n  GAE parameter ($\\\\lambda$)   0.98\\n  Clipping parameter          $0.1 \\\\times \\\\alpha$\\n  Value function coeff.       1\\n  Entropy coeff.              0.01\\n  Gradient norm clip          1\\n\\n  : Hyperparameters used for PPO in all experiments. $\\\\alpha$ is linearly annealed from 1 at the start of each experiment to 0 at the end.\\n:::\\n\\n[^1]: The authors acknowledges support from the MIT-IBM Watson AI Lab. The email addresses of the authors are: nhunt\\\\@mit.edu, {nathan, sara.magliacane, nghiaht, subhro.das}\\\\@ibm.com, asolar\\\\@csail.mit.edu.\\n',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{51}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and\\n  Abbeel]{DBLP:conf/icml/AchiamHTA17}\\nAchiam, J., Held, D., Tamar, A., and Abbeel, P.\\n\\\\newblock Constrained policy optimization.\\n\\\\newblock In Precup, D. and Teh, Y.~W. (eds.), \\\\emph{International Conference\\n  on Machine Learning ({ICML} 2017)}, volume~70 of \\\\emph{Proceedings of Machine\\n  Learning Research}, pp.\\\\  22--31. {PMLR}, 2017.\\n\\n\\\\bibitem[Alshiekh et~al.(2018)Alshiekh, Bloem, Ehlers, K{\\\\\"{o}}nighofer,\\n  Niekum, and Topcu]{DBLP:conf/aaai/AlshiekhBEKNT18}\\nAlshiekh, M., Bloem, R., Ehlers, R., K{\\\\\"{o}}nighofer, B., Niekum, S., and\\n  Topcu, U.\\n\\\\newblock Safe reinforcement learning via shielding.\\n\\\\newblock In \\\\emph{{AAAI} Conference on Artificial Intelligence}, 2018.\\n\\n\\\\bibitem[Berkenkamp et~al.(2017)Berkenkamp, Turchetta, Schoellig, and\\n  Krause]{berkenkamp2017safe}\\nBerkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A.\\n\\\\newblock Safe model-based reinforcement learning with stability guarantees.\\n\\\\newblock In \\\\emph{Advances in neural information processing systems}, pp.\\\\\\n  908--918, 2017.\\n\\n\\\\bibitem[Cheng et~al.(2019)Cheng, Orosz, Murray, and Burdick]{cheng2019end}\\nCheng, R., Orosz, G., Murray, R.~M., and Burdick, J.~W.\\n\\\\newblock End-to-end safe reinforcement learning through barrier functions for\\n  safety-critical continuous control tasks.\\n\\\\newblock In \\\\emph{Proceedings of the AAAI Conference on Artificial\\n  Intelligence}, volume~33, pp.\\\\  3387--3395, 2019.\\n\\n\\\\bibitem[Clarke et~al.(2018)Clarke, Henzinger, Veith, and\\n  Bloem]{clarke2018handbook}\\nClarke, E.~M., Henzinger, T.~A., Veith, H., and Bloem, R. (eds.).\\n\\\\newblock \\\\emph{Handbook of Model Checking}.\\n\\\\newblock Springer, 2018.\\n\\n\\\\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecerik, Hester, Paduraru, and\\n  Tassa]{dalal2018safe}\\nDalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y.\\n\\\\newblock Safe exploration in continuous action spaces.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1801.08757}, 2018.\\n\\n\\\\bibitem[De~Giacomo et~al.(2019)De~Giacomo, Iocchi, Favorito, and\\n  Patrizi]{de2019foundations}\\nDe~Giacomo, G., Iocchi, L., Favorito, M., and Patrizi, F.\\n\\\\newblock Foundations for restraining bolts: Reinforcement learning with\\n  ltlf/ldlf restraining specifications.\\n\\\\newblock In \\\\emph{International Conference on Automated Planning and\\n  Scheduling ({ICAPS} 2019)}, 2019.\\n\\n\\\\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,\\n  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}\\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,\\n  Y., Firoiu, V., Harley, T., Dunning, I., et~al.\\n\\\\newblock Impala: Scalable distributed deep-rl with importance weighted\\n  actor-learner architectures.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1802.01561}, 2018.\\n\\n\\\\bibitem[Fulton \\\\& Platzer(2018)Fulton and Platzer]{aaai18}\\nFulton, N. and Platzer, A.\\n\\\\newblock Safe reinforcement learning via formal methods: Toward safe control\\n  through proof and learning.\\n\\\\newblock In \\\\emph{{AAAI} Conference on Artificial Intelligence}, 2018.\\n\\n\\\\bibitem[Fulton \\\\& Platzer(2019)Fulton and Platzer]{DBLP:conf/tacas/FultonP19}\\nFulton, N. and Platzer, A.\\n\\\\newblock Verifiably safe off-model reinforcement learning.\\n\\\\newblock In Vojnar, T. and Zhang, L. (eds.), \\\\emph{{TACAS} 2019}, volume 11427\\n  of \\\\emph{Lecture Notes in Computer Science}, pp.\\\\  413--430. Springer, 2019.\\n\\\\newblock ISBN 978-3-030-17461-3.\\n\\\\newblock \\\\doi{10.1007/978-3-030-17462-0\\\\_28}.\\n\\n\\\\bibitem[Fulton et~al.(2015)Fulton, Mitsch, Quesel, V{\\\\\"o}lp, and\\n  Platzer]{DBLP:conf/cade/FultonMQVP15}\\nFulton, N., Mitsch, S., Quesel, J.-D., V{\\\\\"o}lp, M., and Platzer, A.\\n\\\\newblock {KeYmaera X}: An axiomatic tactical theorem prover for hybrid\\n  systems.\\n\\\\newblock In \\\\emph{CADE}, 2015.\\n\\n\\\\bibitem[Fulton et~al.(2017)Fulton, Mitsch, Bohrer, and Platzer]{bellerophon}\\nFulton, N., Mitsch, S., Bohrer, B., and Platzer, A.\\n\\\\newblock Bellerophon: Tactical theorem proving for hybrid systems.\\n\\\\newblock In \\\\emph{International Conference on Interactive Theorem Proving},\\n  2017.\\n\\n\\\\bibitem[Garc{\\\\i}a \\\\& Fern{\\\\\\'a}ndez(2015)Garc{\\\\i}a and\\n  Fern{\\\\\\'a}ndez]{garcia2015comprehensive}\\nGarc{\\\\i}a, J. and Fern{\\\\\\'a}ndez, F.\\n\\\\newblock A comprehensive survey on safe reinforcement learning.\\n\\\\newblock \\\\emph{Journal of Machine Learning Research}, 2015.\\n\\n\\\\bibitem[Garnelo et~al.(2016)Garnelo, Arulkumaran, and\\n  Shanahan]{garnelo2016towards}\\nGarnelo, M., Arulkumaran, K., and Shanahan, M.\\n\\\\newblock Towards deep symbolic reinforcement learning.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1609.05518}, 2016.\\n\\n\\\\bibitem[Goel et~al.(2018)Goel, Weng, and Poupart]{goel2018unsupervised_morel}\\nGoel, V., Weng, J., and Poupart, P.\\n\\\\newblock Unsupervised video object segmentation for deep reinforcement\\n  learning.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems}, 2018.\\n\\n\\\\bibitem[Hahn et~al.(2019)Hahn, Perez, Schewe, Somenzi, Trivedi, and\\n  Wojtczak]{DBLP:conf/tacas/HahnPSSTW19}\\nHahn, E.~M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., and Wojtczak, D.\\n\\\\newblock Omega-regular objectives in model-free reinforcement learning.\\n\\\\newblock In \\\\emph{{TACAS} 2019}, 2019.\\n\\n\\\\bibitem[Hasanbeig et~al.(2018{\\\\natexlab{a}})Hasanbeig, Abate, and\\n  Kroening]{HasanbeigKroening}\\nHasanbeig, M., Abate, A., and Kroening, D.\\n\\\\newblock Logically-correct reinforcement learning.\\n\\\\newblock \\\\emph{CoRR}, abs/1801.08099, 2018{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Hasanbeig et~al.(2018{\\\\natexlab{b}})Hasanbeig, Abate, and\\n  Kroening]{hasanbeig2018}\\nHasanbeig, M., Abate, A., and Kroening, D.\\n\\\\newblock Logically-constrained reinforcement learning.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1801.08099}, 2018{\\\\natexlab{b}}.\\n\\n\\\\bibitem[{Hasanbeig} et~al.(2019){Hasanbeig}, {Kantaros}, {Abate}, {Kroening},\\n  {Pappas}, and {Lee}]{Hasanbeig2019}\\n{Hasanbeig}, M., {Kantaros}, Y., {Abate}, A.~r., {Kroening}, D., {Pappas},\\n  G.~J., and {Lee}, I.\\n\\\\newblock {Reinforcement Learning for Temporal Logic Control Synthesis with\\n  Probabilistic Satisfaction Guarantees}.\\n\\\\newblock \\\\emph{arXiv e-prints}, art. arXiv:1909.05304, September 2019.\\n\\n\\\\bibitem[Hasanbeig et~al.(2020)Hasanbeig, Abate, and\\n  Kroening]{hasanbeig2020cautious}\\nHasanbeig, M., Abate, A., and Kroening, D.\\n\\\\newblock Cautious reinforcement learning with logical constraints.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:2002.12156}, 2020.\\n\\n\\\\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep_resnet}\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\n\\\\newblock Deep residual learning for image recognition.\\n\\\\newblock In \\\\emph{Proceedings of the IEEE conference on computer vision and\\n  pattern recognition}, pp.\\\\  770--778, 2016.\\n\\n\\\\bibitem[{ISO-26262}(2011)]{iso26262}\\n{ISO-26262}.\\n\\\\newblock {International Organization for Standardization} 26262 road vehicles\\n   functional safety.\\n\\\\newblock 2011.\\n\\n\\\\bibitem[Kalra \\\\& Paddock(2016)Kalra and Paddock]{RANDDriveToSafety}\\nKalra, N. and Paddock, S.~M.\\n\\\\newblock \\\\emph{Driving to Safety: How Many Miles of Driving Would It Take to\\n  Demonstrate Autonomous Vehicle Reliability?}\\n\\\\newblock RAND Corporation, 2016.\\n\\n\\\\bibitem[Kingma \\\\& Ba(2014)Kingma and Ba]{kingma2014adam}\\nKingma, D.~P. and Ba, J.\\n\\\\newblock Adam: A method for stochastic optimization.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1412.6980}, 2014.\\n\\n\\\\bibitem[Koller et~al.(2018)Koller, Berkenkamp, Turchetta, and\\n  Krause]{koller2018learning}\\nKoller, T., Berkenkamp, F., Turchetta, M., and Krause, A.\\n\\\\newblock Learning-based model predictive control for safe exploration.\\n\\\\newblock In \\\\emph{2018 IEEE Conference on Decision and Control (CDC)}, pp.\\\\\\n  6059--6066. IEEE, 2018.\\n\\n\\\\bibitem[Law \\\\& Deng(2018)Law and Deng]{law2018cornernet}\\nLaw, H. and Deng, J.\\n\\\\newblock Cornernet: Detecting objects as paired keypoints.\\n\\\\newblock In \\\\emph{European Conference on Computer Vision}, 2018.\\n\\n\\\\bibitem[Li et~al.(2018)Li, Sycara, and Iyer]{li2018object_odrl}\\nLi, Y., Sycara, K., and Iyer, R.\\n\\\\newblock Object-sensitive deep reinforcement learning.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1809.06064}, 2018.\\n\\n\\\\bibitem[Liang \\\\& Boularias(2018)Liang and Boularias]{liang2018task}\\nLiang, J. and Boularias, A.\\n\\\\newblock Task-relevant object discovery and categorization for playing\\n  first-person shooter games.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1806.06392}, 2018.\\n\\n\\\\bibitem[Lin et~al.(2017)Lin, Goyal, Girshick, He, and\\n  Doll{\\\\\\'a}r]{lin2017focal}\\nLin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll{\\\\\\'a}r, P.\\n\\\\newblock Focal loss for dense object detection.\\n\\\\newblock In \\\\emph{IEEE international conference on computer vision}, 2017.\\n\\n\\\\bibitem[Lu et~al.(2018)Lu, Zhang, Stone, and Chen]{lu2018robot}\\nLu, K., Zhang, S., Stone, P., and Chen, X.\\n\\\\newblock Robot representing and reasoning with knowledge from reinforcement\\n  learning.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1809.11074}, 2018.\\n\\n\\\\bibitem[Lyu et~al.(2019)Lyu, Yang, Liu, and Gustafson]{lyu2019sdrl}\\nLyu, D., Yang, F., Liu, B., and Gustafson, S.\\n\\\\newblock {SDRL}: interpretable and data-efficient deep reinforcement learning\\n  leveraging symbolic planning.\\n\\\\newblock In \\\\emph{AAAI\\'19}, 2019.\\n\\n\\\\bibitem[Mitsch \\\\& Platzer(2016)Mitsch and\\n  Platzer]{DBLP:journals/fmsd/MitschP16}\\nMitsch, S. and Platzer, A.\\n\\\\newblock {ModelPlex}: Verified runtime validation of verified cyber-physical\\n  system models.\\n\\\\newblock \\\\emph{Form. Methods Syst. Des.}, 49\\\\penalty0 (1):\\\\penalty0 33--74,\\n  2016.\\n\\\\newblock Special issue of selected papers from RV\\'14.\\n\\n\\\\bibitem[Mitsch et~al.(2013)Mitsch, Ghorbal, and\\n  Platzer]{DBLP:conf/rss/MitschGP13}\\nMitsch, S., Ghorbal, K., and Platzer, A.\\n\\\\newblock On provably safe obstacle avoidance for autonomous robotic ground\\n  vehicles.\\n\\\\newblock In Newman, P., Fox, D., and Hsu, D. (eds.), \\\\emph{Robotics: Science\\n  and Systems}, 2013.\\n\\n\\\\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,\\n  Wierstra, and Riedmiller]{mnih-atari-2013}\\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,\\n  D., and Riedmiller, M.\\n\\\\newblock Playing atari with deep reinforcement learning.\\n\\\\newblock In \\\\emph{NIPS Deep Learning Workshop}. 2013.\\n\\n\\\\bibitem[Phan et~al.(2019)Phan, Paoletti, Grosu, Jansen, Smolka, and\\n  Stoller]{DBLP:journals/corr/neuralsimplex}\\nPhan, D., Paoletti, N., Grosu, R., Jansen, N., Smolka, S.~A., and Stoller,\\n  S.~D.\\n\\\\newblock Neural simplex architecture.\\n\\\\newblock 2019.\\n\\n\\\\bibitem[Platzer(2008)]{DBLP:journals/jar/Platzer08}\\nPlatzer, A.\\n\\\\newblock Differential dynamic logic for hybrid systems.\\n\\\\newblock \\\\emph{J. Autom. Reas.}, 41\\\\penalty0 (2):\\\\penalty0 143--189, 2008.\\n\\n\\\\bibitem[Platzer(2010)]{Platzer10}\\nPlatzer, A.\\n\\\\newblock \\\\emph{Logical Analysis of Hybrid Systems: Proving Theorems for\\n  Complex Dynamics}.\\n\\\\newblock Springer, Heidelberg, 2010.\\n\\n\\\\bibitem[Platzer(2012)]{DBLP:conf/lics/Platzer12a}\\nPlatzer, A.\\n\\\\newblock Logics of dynamical systems.\\n\\\\newblock In \\\\emph{LICS}, pp.\\\\  13--24. IEEE, 2012.\\n\\n\\\\bibitem[Platzer(2015)]{DBLP:conf/cade/Platzer15}\\nPlatzer, A.\\n\\\\newblock A uniform substitution calculus for differential dynamic logic.\\n\\\\newblock In \\\\emph{CADE}, 2015.\\n\\n\\\\bibitem[Platzer(2017)]{DBLP:journals/jar/Platzer17}\\nPlatzer, A.\\n\\\\newblock A complete uniform substitution calculus for differential dynamic\\n  logic.\\n\\\\newblock \\\\emph{J. Autom. Reas.}, 59\\\\penalty0 (2):\\\\penalty0 219--266, 2017.\\n\\n\\\\bibitem[Platzer \\\\& Clarke(2007)Platzer and\\n  Clarke]{DBLP:conf/hybrid/PlatzerC07}\\nPlatzer, A. and Clarke, E.~M.\\n\\\\newblock The image computation problem in hybrid systems model checking.\\n\\\\newblock In Bemporad, A., Bicchi, A., and Buttazzo, G. (eds.), \\\\emph{HSCC},\\n  volume 4416 of \\\\emph{LNCS}, pp.\\\\  473--486. Springer, 2007.\\n\\\\newblock ISBN 978-3-540-71492-7.\\n\\\\newblock \\\\doi{10.1007/978-3-540-71493-4_37}.\\n\\n\\\\bibitem[Quesel et~al.(2016)Quesel, Mitsch, Loos, Arechiga, and\\n  Platzer]{DBLP:journals/sttt/QueselMLAP16}\\nQuesel, J., Mitsch, S., Loos, S.~M., Arechiga, N., and Platzer, A.\\n\\\\newblock How to model and prove hybrid systems with {KeYmaera}: a tutorial on\\n  safety.\\n\\\\newblock \\\\emph{{STTT}}, 18\\\\penalty0 (1):\\\\penalty0 67--91, 2016.\\n\\n\\\\bibitem[Ray et~al.(2019)Ray, Achiam, and Amodei]{Ray2019}\\nRay, A., Achiam, J., and Amodei, D.\\n\\\\newblock {Benchmarking Safe Exploration in Deep Reinforcement Learning}.\\n\\\\newblock 2019.\\n\\n\\\\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and\\n  Moritz]{DBLP:conf/icml/SchulmanLAJM15}\\nSchulman, J., Levine, S., Abbeel, P., Jordan, M.~I., and Moritz, P.\\n\\\\newblock Trust region policy optimization.\\n\\\\newblock In Bach, F.~R. and Blei, D.~M. (eds.), \\\\emph{Proceedings of the 32nd\\n  International Conference on Machine Learning ({ICML} 2015)}, volume~37 of\\n  \\\\emph{{JMLR} Workshop and Conference Proceedings}, pp.\\\\  1889--1897, 2015.\\n\\n\\\\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and\\n  Klimov]{DBLP:journals/corr/SchulmanWDRK17}\\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.\\n\\\\newblock Proximal policy optimization algorithms.\\n\\\\newblock 2017.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1707.06347}.\\n\\n\\\\bibitem[Stooke \\\\& Abbeel(2019)Stooke and Abbeel]{stooke2019rlpyt}\\nStooke, A. and Abbeel, P.\\n\\\\newblock rlpyt: A research code base for deep reinforcement learning in\\n  pytorch, 2019.\\n\\n\\\\bibitem[Sutton \\\\& Barto(1998)Sutton and Barto]{sutton.barto:reinforcement}\\nSutton, R.~S. and Barto, A.~G.\\n\\\\newblock \\\\emph{Reinforcement Learning: An Introduction}.\\n\\\\newblock MIT Press, Cambridge, MA, 1998.\\n\\n\\\\bibitem[Xiang et~al.(2018)Xiang, Musau, Wild, Lopez, Hamilton, Yang,\\n  Rosenfeld, and Johnson]{xiang2018verification}\\nXiang, W., Musau, P., Wild, A.~A., Lopez, D.~M., Hamilton, N., Yang, X.,\\n  Rosenfeld, J., and Johnson, T.~T.\\n\\\\newblock Verification for machine learning, autonomy, and neural networks\\n  survey.\\n\\\\newblock \\\\emph{arXiv}, 2018.\\n\\n\\\\bibitem[Yang et~al.(2018)Yang, Lyu, Liu, and Gustafson]{yang2018peorl}\\nYang, F., Lyu, D., Liu, B., and Gustafson, S.\\n\\\\newblock Peorl: Integrating symbolic planning and hierarchical reinforcement\\n  learning for robust decision-making.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1804.07779}, 2018.\\n\\n\\\\bibitem[Yang et~al.(2019)Yang, Gustafson, Elkholy, Lyu, and\\n  Liu]{yang2019program}\\nYang, F., Gustafson, S., Elkholy, A., Lyu, D., and Liu, B.\\n\\\\newblock Program search for machine learning pipelines leveraging symbolic\\n  planning and reinforcement learning.\\n\\\\newblock In \\\\emph{Genetic Programming Theory and Practice XVI}. 2019.\\n\\n\\\\bibitem[Zhou et~al.(2019)Zhou, Wang, and\\n  Kr{\\\\\"a}henb{\\\\\"u}hl]{zhou2019objects_centernet}\\nZhou, X., Wang, D., and Kr{\\\\\"a}henb{\\\\\"u}hl, P.\\n\\\\newblock Objects as points.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1904.07850}, 2019.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '@article{Ray2019,\\n    author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},\\n    title = {{Benchmarking Safe Exploration in Deep Reinforcement Learning}},\\n    year = {2019}\\n}\\n\\n\\n@misc{stooke2019rlpyt,\\n    title={rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch},\\n    author={Adam Stooke and Pieter Abbeel},\\n    year={2019},\\n    eprint={1909.01500},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.LG}\\n}\\n\\n#############  ICAPS Papers  ###########\\n\\n@article{DBLP:journals/corr/abs-1904-07850,\\n  author    = {Xingyi Zhou and\\n               Dequan Wang and\\n               Philipp Kr{\\\\\"{a}}henb{\\\\\"{u}}hl},\\n  title     = {Objects as Points},\\n  journal   = {CoRR},\\n  volume    = {abs/1904.07850},\\n  year      = {2019},\\n  url       = {http://arxiv.org/abs/1904.07850},\\n  archivePrefix = {arXiv},\\n  eprint    = {1904.07850},\\n  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-07850},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/hybrid/PlatzerC07,\\n  author    = {Andr{\\\\\\'e} Platzer and\\n               Edmund M. Clarke},\\n  title     = {The Image Computation Problem in Hybrid\\n               Systems Model Checking},\\n  booktitle = {HSCC},\\n  longbooktitle = {Hybrid Systems: Computation and Control,\\n               10th International Conference, HSCC 2007,\\n               Pisa, Italy, Proceedings},\\n  year      = {2007},\\n  pages     = {473-486},\\n  doi       = {10.1007/978-3-540-71493-4_37},\\n  editor    = {Alberto Bemporad and\\n               Antonio Bicchi and\\n               Giorgio Buttazzo},\\n  publisher = {Springer},\\n  series    = {LNCS},\\n  volume    = {4416},\\n  isbn      = {978-3-540-71492-7},\\n  keywords  = {model checking, hybrid systems, image\\n               computation},\\n  abstract  = {\\n    In this paper, we analyze limits of approximation\\n    techniques for (non-linear) continuous image\\n    computation in model checking hybrid systems. In\\n    particular, we show that even a single step of\\n    continuous image computation is not semidecidable\\n    numerically even for a very restricted class of\\n    functions. Moreover, we show that symbolic insight\\n    about derivative bounds provides sufficient additional\\n    information for approximation refinement model\\n    checking. Finally, we prove that purely numerical\\n    algorithms can perform continuous image computation\\n    with arbitrarily high probability. Using these results,\\n    we analyze the prerequisites for a safe operation of\\n    the roundabout maneuver in air traffic collision\\n    avoidance.},\\n}\\n\\n\\n@article{DBLP:journals/jair/FoxL06,\\n  author    = {Maria Fox and\\n               Derek Long},\\n  title     = {Modelling Mixed Discrete-Continuous Domains for Planning},\\n  journal   = {{}JAIR}},\\n  volume    = {27},\\n  pages     = {235--297},\\n  year      = {2006},\\n  url       = {https://doi.org/10.1613/jair.2044},\\n  doi       = {10.1613/jair.2044},\\n  timestamp = {Sat, 19 Oct 2019 19:56:37 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/jair/FoxL06},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@article{DBLP:journals/corr/SchulmanLMJA15,\\n  author    = {John Schulman and\\n               Sergey Levine and\\n               Philipp Moritz and\\n               Michael I. Jordan and\\n               Pieter Abbeel},\\n  title     = {Trust Region Policy Optimization},\\n  journal   = {CoRR},\\n  volume    = {abs/1502.05477},\\n  year      = {2015},\\n  url       = {http://arxiv.org/abs/1502.05477},\\n  archivePrefix = {arXiv},\\n  eprint    = {1502.05477},\\n  timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanLMJA15},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n\\n@inproceedings{DBLP:conf/aips/Asai19,\\n  author    = {Masataro Asai},\\n  title     = {Unsupervised Grounding of Plannable First-Order Logic Representation\\n               from Images},\\n  booktitle = {{ICAPS} 2019},\\n  pages     = {583--591},\\n  year      = {2019},\\n  url       = {https://aaai.org/ojs/index.php/ICAPS/article/view/3525},\\n  timestamp = {Tue, 19 Nov 2019 08:03:24 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/aips/Asai19},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@article{DBLP:journals/amai/KvarnstromD00,\\n  author    = {Jonas Kvarnstr{\\\\\"{o}}m and\\n               Patrick Doherty},\\n  title     = {{TAL}planner: {A} temporal logic based forward chaining planner},\\n  journal   = {AMAI},\\n  volume    = {30},\\n  number    = {1-4},\\n  pages     = {119--169},\\n  year      = {2000},\\n  url       = {https://doi.org/10.1023/A:1016619613658},\\n  doi       = {10.1023/A:1016619613658},\\n  timestamp = {Mon, 18 Nov 2019 15:00:05 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/amai/KvarnstromD00},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@proceedings{DBLP:conf/aips/2019,\\n  editor    = {J. Benton and\\n               Nir Lipovetzky and\\n               Eva Onaindia and\\n               David E. Smith and\\n               Siddharth Srivastava},\\n  title     = {Proceedings of the Twenty-Ninth International Conference on Automated\\n               Planning and Scheduling, {ICAPS} 2018, Berkeley, CA, USA, July 11-15,\\n               2019},\\n  publisher = {{AAAI} Press},\\n  year      = {2019},\\n  url       = {https://aaai.org/ojs/index.php/ICAPS/issue/view/239},\\n  timestamp = {Tue, 19 Nov 2019 08:03:24 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/aips/2019},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{rewardFunctionSpecification,\\n  author    = {Alberto Camacho and\\n               Rodrigo Toro-Icarte and\\n               Toryn Q. Klassen and\\n               Richard Valenzano and\\n               Sheila A. McIlraith},\\n  title     = {{LTL} and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning},\\n  booktitle = {International Joint Conference on Artificial Intelligence},\\n  year      = {2019}\\n}\\n\\n@inproceedings{cam-mci-icaps19,\\n  author    = {Alberto Camacho and\\n               Sheila A. McIlraith},\\n  title     = {Learning Interpretable Models in Linear Temporal Logic},\\n  booktitle = {International Conference on Automated Planning and Scheduling ({ICAPS} 2019)},\\n  year      = {2019}\\n}\\n\\n\\n@inproceedings{DBLP:conf/ijcai/PatriziLGG11,\\n  author    = {Fabio Patrizi and\\n               Nir Lipovetzky and\\n               Giuseppe {De Giacomo} and\\n               Hector Geffner},\\n  title     = {Computing Infinite Plans for {LTL} Goals Using a Classical Planner},\\n  booktitle = {{IJCAI} 2011},\\n  year      = {2011}\\n}\\n\\n@proceedings{DBLP:conf/ijcai/2011,\\n  editor    = {Toby Walsh},\\n  title     = {International Joint Conference on Artificial Intelligence},\\n  year      = {2011}\\n}\\n\\n\\n\\n@inproceedings{de2019foundations,\\n  title={Foundations for Restraining Bolts: Reinforcement Learning with LTLf/LDLf Restraining Specifications},\\n  author={De Giacomo, Giuseppe and Iocchi, Luca and Favorito, Marco and Patrizi, Fabio},\\n  booktitle={International Conference on Automated Planning and Scheduling ({ICAPS} 2019)},\\n  year={2019}\\n}\\n\\n@inproceedings{sharma2019reinforcement,\\n  title={Reinforcement Learning Based Querying in Camera Networks for Efficient Target Tracking},\\n  author={Sharma, Anil and Anand, Saket and Kaul, Sanjit K},\\n  booktitle={International Conference on Automated Planning and Scheduling ({ICAPS} 2019)},\\n  year={2019}\\n}\\n\\n@inproceedings{speck2019symbolic,\\n  title={Symbolic Planning with Axioms},\\n  author={Speck, David and Gei{\\\\ss}er, Florian and Mattm{\\\\\"u}ller, Robert and Torralba, {\\\\\\'A}lvaro},\\n  booktitle={International Conference on Automated Planning and Scheduling ({ICAPS} 2019)},\\n  year={2019}\\n}\\n\\n###########################\\n\\n\\n@inproceedings{DBLP:conf/tacas/HahnPSSTW19,\\n  author    = {Ernst Moritz Hahn and\\n               Mateo Perez and\\n               Sven Schewe and\\n               Fabio Somenzi and\\n               Ashutosh Trivedi and\\n               Dominik Wojtczak},\\n  title     = {Omega-Regular Objectives in Model-Free Reinforcement Learning},\\n  booktitle = {{TACAS} 2019},\\n  year      = {2019}\\n}\\n\\n@article{DBLP:journals/corr/neuralsimplex,\\n  author    = {Dung Phan and\\n               Nicola Paoletti and\\n               Radu Grosu and\\n               Nils Jansen and\\n               Scott A. Smolka and\\n               Scott D. Stoller},\\n  title     = {Neural Simplex Architecture},\\n  year      = {2019}\\n}\\n\\n@inproceedings{cheng2019qatm,\\n  title={QATM: Quality-Aware Template Matching For Deep Learning},\\n  author={Cheng, Jiaxin and Wu, Yue and Abd-Almageed, Wael and Natarajan, Premkumar},\\n  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},\\n  year={2019}\\n}\\n\\n@inproceedings{lin2017focal,\\n  title={Focal loss for dense object detection},\\n  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\\\\\\'a}r, Piotr},\\n  booktitle={IEEE international conference on computer vision},\\n  year={2017}\\n}\\n\\n@inproceedings{law2018cornernet,\\n  title={Cornernet: Detecting objects as paired keypoints},\\n  author={Law, Hei and Deng, Jia},\\n  booktitle={European Conference on Computer Vision},\\n  year={2018}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/aaai/CamachoTMBM17,\\n  author    = {Alberto Camacho and\\n               Eleni Triantafillou and\\n               Christian J. Muise and\\n               Jorge A. Baier and\\n               Sheila A. McIlraith},\\n  title     = {Non-Deterministic Planning with Temporally Extended Goals: {LTL} over Finite and Infinite Traces},\\n  booktitle = {{AAAI} 2017},\\n  year      = {2017}\\n}\\n\\n@proceedings{DBLP:conf/aaai/2017,\\n  editor    = {Satinder P. Singh and\\n               Shaul Markovitch},\\n  title     = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,\\n               February 4-9, 2017, San Francisco, California, {USA}},\\n  publisher = {{AAAI} Press},\\n  year      = {2017},\\n  url       = {http://www.aaai.org/Library/AAAI/aaai17contents.php},\\n  timestamp = {Mon, 06 Mar 2017 08:17:31 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/aaai/2017},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@article{zhou2019objects_centernet,\\n  title={Objects as Points},\\n  author={Zhou, Xingyi and Wang, Dequan and Kr{\\\\\"a}henb{\\\\\"u}hl, Philipp},\\n  journal={arXiv preprint arXiv:1904.07850},\\n  year={2019}\\n}\\n\\n@article{garnelo2016towards,\\n  title={Towards deep symbolic reinforcement learning},\\n  author={Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},\\n  journal={arXiv preprint arXiv:1609.05518},\\n  year={2016}\\n}\\n\\n@inproceedings{lyu2019sdrl,\\n  title={{SDRL}: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning},\\n  author={Lyu, Daoming and Yang, Fangkai and Liu, Bo and Gustafson, Steven},\\n  booktitle={AAAI\\'19},\\n  year={2019}\\n}\\n\\n@inproceedings{saunders2018trial_hirl,\\n  title={Trial without error: Towards safe reinforcement learning via human intervention},\\n  author={Saunders, William and Sastry, Girish and Stuhlmueller, Andreas and Evans, Owain},\\n  booktitle={International Conference on Autonomous Agents and MultiAgent Systems},\\n  year={2018}\\n}\\n\\n@article{li2018object_odrl,\\n  title={Object-sensitive deep reinforcement learning},\\n  author={Li, Yuezhang and Sycara, Katia and Iyer, Rahul},\\n  journal={arXiv preprint arXiv:1809.06064},\\n  year={2018}\\n}\\n\\n@article{liang2018task,\\n  title={Task-Relevant Object Discovery and Categorization for Playing First-person Shooter Games},\\n  author={Liang, Junchi and Boularias, Abdeslam},\\n  journal={arXiv preprint arXiv:1806.06392},\\n  year={2018}\\n}\\n\\n@inproceedings{goel2018unsupervised_morel,\\n  title={Unsupervised video object segmentation for deep reinforcement learning},\\n  author={Goel, Vikash and Weng, Jameson and Poupart, Pascal},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  year={2018}\\n}\\n\\n@article{yang2018peorl,\\n  title={Peorl: Integrating symbolic planning and hierarchical reinforcement learning for robust decision-making},\\n  author={Yang, Fangkai and Lyu, Daoming and Liu, Bo and Gustafson, Steven},\\n  journal={arXiv preprint arXiv:1804.07779},\\n  year={2018}\\n}\\n\\n@incollection{yang2019program,\\n  title={Program Search for Machine Learning Pipelines Leveraging Symbolic Planning and Reinforcement Learning},\\n  author={Yang, Fangkai and Gustafson, Steven and Elkholy, Alexander and Lyu, Daoming and Liu, Bo},\\n  booktitle={Genetic Programming Theory and Practice XVI},\\n  year={2019}\\n}\\n\\n@article{lu2018robot,\\n  title={Robot representing and reasoning with knowledge from reinforcement learning},\\n  author={Lu, Keting and Zhang, Shiqi and Stone, Peter and Chen, Xiaoping},\\n  journal={arXiv preprint arXiv:1809.11074},\\n  year={2018}\\n}\\n\\n@inproceedings{cheng2019end,\\n  title={End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks},\\n  author={Cheng, Richard and Orosz, G{\\\\\\'a}bor and Murray, Richard M and Burdick, Joel W},\\n  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},\\n  volume={33},\\n  pages={3387--3395},\\n  year={2019}\\n}\\n\\n\\n@inproceedings{onsymbolic,\\n  title={Symbolic Planning and Model-Free Reinforcement Learning: Training Taskable Agents},\\n  author={Le{\\\\\\'o}n Illanes, Le and Yan, Xi and Icarte, Rodrigo Toro and McIlraith, Sheila A},\\n  booktitle=\"Multi-disciplinary Conference on Reinforcement Learning and Decision Making\",\\n  year={2019}\\n}\\n\\n@article{nourbakhsh,\\n ISSN = {00157120},\\n URL = {http://www.jstor.org/stable/24483813},\\n author = {Illah Reza Nourbakhsh},\\n journal = {Foreign Affairs},\\n number = {4},\\n pages = {23--28},\\n publisher = {Council on Foreign Relations},\\n title = {The Coming Robot Dystopia: All Too Inhuman},\\n volume = {94},\\n year = {2015}\\n}\\n\\n\\n@incollection{mnih-atari-2013,\\n  title = {Playing Atari With Deep Reinforcement Learning},\\n  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},\\n  booktitle = {NIPS Deep Learning Workshop},\\n  year = {2013}\\n}\\n\\n\\n@misc{dean2019robust,\\n    title={Robust Guarantees for Perception-Based Control},\\n    author={Sarah Dean and Nikolai Matni and Benjamin Recht and Vickie Ye},\\n    year={2019},\\n    eprint={1907.03680},\\n    archivePrefix={arXiv},\\n    primaryClass={math.OC}\\n}\\n\\n@article{jiang2018integrating,\\n  title={Integrating Task-Motion Planning with Reinforcement Learning for Robust Decision Making in Mobile Robots},\\n  author={Jiang, Yuqian and Yang, Fangkai and Zhang, Shiqi and Stone, Peter},\\n  journal={arXiv preprint arXiv:1811.08955},\\n  year={2018}\\n}\\n\\n@article{hasanbeig2018,\\n  title={Logically-constrained reinforcement learning},\\n  author={Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},\\n  journal={arXiv preprint arXiv:1801.08099},\\n  year={2018}\\n}\\n\\n@inproceedings{mnih16,\\n  title={Asynchronous methods for deep reinforcement learning},\\n  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},\\n  booktitle={International conference on machine learning},\\n  year={2016}\\n}\\n\\n@inproceedings{wicker2018feature,\\n  title={Feature-guided black-box safety testing of deep neural networks},\\n  author={Wicker, Matthew and Huang, Xiaowei and Kwiatkowska, Marta},\\n  booktitle={International Conference on Tools and Algorithms for the Construction and Analysis of Systems},\\n  year={2018}\\n}\\n\\n@article{xiang2018verification,\\n  title={Verification for machine learning, autonomy, and neural networks survey},\\n  author={Xiang, Weiming and Musau, Patrick and Wild, Ayana A and Lopez, Diego Manzanas and Hamilton, Nathaniel and Yang, Xiaodong and Rosenfeld, Joel and Johnson, Taylor T},\\n  journal={arXiv},\\n  year={2018}\\n}\\n\\n@inproceedings{abbeel2005exploration,\\n  title={Exploration and apprenticeship learning in reinforcement learning},\\n  author={Abbeel, Pieter and Ng, Andrew Y},\\n  booktitle={International Conference on Machine learning},\\n  year={2005}\\n}\\n\\n@inproceedings{wu17,\\ntitle={Training agent for first-person shooter game with actor-critic curriculum\\nlearning},\\nauthor={Y. Wu and Y. Tian},\\nbooktitle={International Conference on Learning Representations},\\nyear={2017}\\n}\\n\\n@article{garcia2015comprehensive,\\n  title={A comprehensive survey on safe reinforcement learning},\\n  author={Garc{\\\\i}a, Javier and Fern{\\\\\\'a}ndez, Fernando},\\n  journal={Journal of Machine Learning Research},\\n  year={2015}\\n}\\n\\n@incollection{bouchaud2009markets,\\n  title={How markets slowly digest changes in supply and demand},\\n  author={Bouchaud, Jean-Philippe and Farmer, J Doyne and Lillo, Fabrizio},\\n  booktitle={Handbook of financial markets: dynamics and evolution},\\n  year={2009}\\n}\\n\\n\\n@inproceedings{DBLP:journals/corr/PascanuGCB13,\\n  author    = {Razvan Pascanu and\\n               {\\\\c{C}}aglar G{\\\\\"{u}}l{\\\\c{c}}ehre and\\n               Kyunghyun Cho and\\n               Yoshua Bengio},\\n  title     = {How to Construct Deep Recurrent Neural Networks},\\n  booktitle = {International Conference on Learning Representations},\\n  year      = {2014}\\n}\\n\\n@proceedings{DBLP:conf/iclr/2014,\\n  editor    = {Yoshua Bengio and\\n               Yann LeCun},\\n  title     = {International Conference on Learning Representations},\\n  year      = {2014}\\n}\\n\\n\\n@book{Goodfellow-et-al-2016,\\n    title={Deep Learning},\\n    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\\n    publisher={MIT Press},\\n    year={2016}\\n}\\n\\n@incollection{DBLP:series/faia/Biere09,\\n  author    = {Armin Biere},\\n  title     = {Bounded Model Checking},\\n  booktitle = {Handbook of Satisfiability},\\n  year      = {2009}\\n}\\n\\n@proceedings{DBLP:series/faia/2009-185,\\n  editor    = {Armin Biere and\\n               Marijn Heule and\\n               Hans van Maaren and\\n               Toby Walsh},\\n  title     = {Handbook of Satisfiability},\\n  series    = {Frontiers in Artificial Intelligence and Applications},\\n  year      = {2009}\\n}\\n\\n\\n@inproceedings{chang2018antisymmetricrnn,\\ntitle={Antisymmetric{RNN}: A Dynamical System View on Recurrent Neural Networks},\\nauthor={Bo Chang and Minmin Chen and Eldad Haber and Ed H. Chi},\\nbooktitle={International Conference on Learning Representations},\\nyear={2019}\\n}\\n\\n@inproceedings{DBLP:conf/acl/TaiSM15,\\n  author    = {Kai Sheng Tai and\\n               Richard Socher and\\n               Christopher D. Manning},\\n  title     = {Improved Semantic Representations From Tree-Structured Long Short-Term\\n               Memory Networks},\\n  booktitle = {Annual Meeting of the Association for Computational  Linguistics},\\n  year      = {2015}\\n}\\n\\n@proceedings{DBLP:conf/acl/2015-1,\\n  title     = {Annual Meeting of the Association for Computational Linguistics},\\n  publisher = {The Association for Computer Linguistics},\\n  year      = {2015},\\n  url       = {http://aclweb.org/anthology/P/P15/},\\n  isbn      = {978-1-941643-72-3},\\n  timestamp = {Sun, 02 Aug 2015 19:10:39 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/acl/2015-1},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@article{floydlogic,\\nauthor=\"Floyd, Robert W.\",\\neditor=\"Colburn, Timothy R.\\nand Fetzer, James H.\\nand Rankin, Terry L.\",\\ntitle=\"Assigning Meanings to Programs\",\\nbookTitle=\"Program Verification: Fundamental Issues in Computer Science\",\\nyear=\"1993\",\\npublisher=\"Springer\"\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/nips/ZhangWCHD18,\\n  author    = {Huan Zhang and\\n               Tsui{-}Wei Weng and\\n               Pin{-}Yu Chen and\\n               Cho{-}Jui Hsieh and\\n               Luca Daniel},\\n  title     = {Efficient Neural Network Robustness Certification with General Activation\\n               Functions},\\n  booktitle = {Advances in Neural Information Processing Systems},\\n  year      = {2018}\\n}\\n\\n@proceedings{DBLP:conf/nips/2018,\\n  editor    = {Samy Bengio and\\n               Hanna M. Wallach and\\n               Hugo Larochelle and\\n               Kristen Grauman and\\n               Nicol{\\\\`{o}} Cesa{-}Bianchi and\\n               Roman Garnett},\\n  title     = {Advances in Neural Information Processing Systems},\\n  year      = {2018}\\n}\\n\\n\\n\\n@article{DBLP:journals/corr/abs-1811-12395,\\n  author    = {Akhilan Boopathy and\\n               Tsui{-}Wei Weng and\\n               Pin{-}Yu Chen and\\n               Sijia Liu and\\n               Luca Daniel},\\n  title     = {CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional\\n               Neural Networks},\\n  journal   = {CoRR},\\n  volume    = {abs/1811.12395},\\n  year      = {2018}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/aisc/AkbarpourP08,\\n  author    = {Behzad Akbarpour and\\n               Lawrence C. Paulson},\\n  title     = {MetiTarski: An Automatic Prover for the Elementary Functions},\\n  booktitle = {International Conference on Intelligent Computer Mathematics},\\n  year      = {2008}\\n}\\n\\n@proceedings{DBLP:conf/aisc/2008,\\n  editor    = {Serge Autexier and\\n               John A. Campbell and\\n               Julio Rubio and\\n               Volker Sorge and\\n               Masakazu Suzuki and\\n               Freek Wiedijk},\\n  title     = {Intelligent Computer Mathematics, 9th International Conference, {AISC}\\n               2008, 15th Symposium, Calculemus 2008, 7th International Conference,\\n               {MKM} 2008, Birmingham, UK, July 28 - August 1, 2008. Proceedings},\\n  series    = {Lecture Notes in Computer Science},\\n  volume    = {5144},\\n  publisher = {Springer},\\n  year      = {2008}\\n}\\n\\n\\n\\n@article{DBLP:journals/neco/HochreiterS97,\\n  author    = {Sepp Hochreiter and\\n               J{\\\\\"{u}}rgen Schmidhuber},\\n  title     = {Long Short-Term Memory},\\n  journal   = {Neural Computation},\\n  year      = {1997}\\n}\\n\\n@article{BeerRNN,\\n author = {Beer, Randall D.},\\n title = {On the Dynamics of Small Continuous-time Recurrent Neural Networks},\\n journal = {Adapt Behav},\\n issue_date = {Spring 1995},\\n volume = {3},\\n number = {4},\\n month = jan,\\n year = {1995},\\n issn = {1059-7123},\\n pages = {469--509},\\n numpages = {41},\\n doi = {10.1177/105971239500300405},\\n publisher = {Sage Publications, Inc.},\\n address = {Thousand Oaks, CA, USA},\\n} \\n\\n@article {Hopfield3088,\\n\\tauthor = {Hopfield, J J},\\n\\ttitle = {Neurons with graded response have collective computational properties like those of two-state neurons},\\n\\tvolume = {81},\\n\\tnumber = {10},\\n\\tpages = {3088--3092},\\n\\tyear = {1984},\\n\\tdoi = {10.1073/pnas.81.10.3088},\\n\\tpublisher = {National Academy of Sciences},\\n\\tissn = {0027-8424},\\n\\tURL = {https://www.pnas.org/content/81/10/3088},\\n\\teprint = {https://www.pnas.org/content/81/10/3088.full.pdf},\\n\\tjournal = {Proceedings of the National Academy of Sciences}\\n}\\n\\n@article{DBLP:journals/corr/rnndfa,\\n  author    = {Qinglong Wang and\\n               Kaixuan Zhang and\\n               Xue Liu and\\n               C. Lee Giles},\\n  title     = {Verification of Recurrent Neural Networks Through Rule Extraction},\\n  journal   = {CoRR},\\n  volume    = {abs/1811.06029},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1811.06029},\\n  archivePrefix = {arXiv},\\n  eprint    = {1811.06029},\\n  timestamp = {Sat, 24 Nov 2018 17:52:00 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-06029},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/kr/AkitundeLMP18,\\n  author    = {Michael Akintunde and\\n               Alessio Lomuscio and\\n               Lalit Maganti and\\n               Edoardo Pirovano},\\n  title     = {Reachability Analysis for Neural Agent-Environment Systems},\\n  booktitle = {Principles of Knowledge Representation and Reasoning: Proceedings\\n               of the Sixteenth International Conference, {KR} 2018, Tempe, Arizona,\\n               30 October - 2 November 2018.},\\n  pages     = {184--193},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/kr/2018},\\n  url       = {https://aaai.org/ocs/index.php/KR/KR18/paper/view/17991},\\n  timestamp = {Wed, 21 Nov 2018 06:34:38 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/kr/AkitundeLMP18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/kr/2018,\\n  editor    = {Michael Thielscher and\\n               Francesca Toni and\\n               Frank Wolter},\\n  title     = {Principles of Knowledge Representation and Reasoning: Proceedings\\n               of the Sixteenth International Conference, {KR} 2018, Tempe, Arizona,\\n               30 October - 2 November 2018},\\n  publisher = {{AAAI} Press},\\n  year      = {2018},\\n  url       = {http://www.aaai.org/Library/KR/kr18contents.php},\\n  isbn      = {978-1-57735-803-9},\\n  timestamp = {Thu, 18 Oct 2018 09:44:31 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/kr/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@article{DBLP:journals/ijon/RubioY07,\\n  author    = {Jos{\\\\\\'{e}} de Jes{\\\\\\'{u}}s Rubio and\\n               Wen Yu},\\n  title     = {Nonlinear system identification with recurrent neural networks and\\n               dead-zone Kalman filter algorithm},\\n  journal   = {Neurocomputing},\\n  volume    = {70},\\n  number    = {13-15},\\n  pages     = {2460--2466},\\n  year      = {2007},\\n  url       = {https://doi.org/10.1016/j.neucom.2006.09.004},\\n  doi       = {10.1016/j.neucom.2006.09.004},\\n  timestamp = {Tue, 06 Jun 2017 22:23:28 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/ijon/RubioY07},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/adprl/HuhT09,\\n  author    = {Dongsung Huh and\\n               Emanuel Todorov},\\n  title     = {Real-time motor control using recurrent neural networks},\\n  booktitle = {{IEEE} Symposium on Adaptive Dynamic Programming and Reinforcement\\n               Learning, {ADPRL} 2009, Nashville, TN, USA, March 31 - April 1, 2009},\\n  pages     = {42--49},\\n  year      = {2009},\\n  crossref  = {DBLP:conf/adprl/2009},\\n  url       = {https://doi.org/10.1109/ADPRL.2009.4927524},\\n  doi       = {10.1109/ADPRL.2009.4927524},\\n  timestamp = {Fri, 26 May 2017 00:49:09 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/adprl/HuhT09},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/adprl/2009,\\n  title     = {{IEEE} Symposium on Adaptive Dynamic Programming and Reinforcement\\n               Learning, {ADPRL} 2009, Nashville, TN, USA, March 31 - April 1, 2009},\\n  publisher = {{IEEE}},\\n  year      = {2009},\\n  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=4910084},\\n  isbn      = {978-1-4244-2761-1},\\n  timestamp = {Wed, 02 Jul 2014 20:01:07 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/adprl/2009},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@incollection{nips/2009/graves,\\ntitle = {Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks},\\nauthor = {Graves, Alex and Schmidhuber, J\\\\\"{u}rgen},\\nbooktitle = {Advances in Neural Information Processing Systems 21},\\neditor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},\\npages = {545--552},\\nyear = {2009},\\npublisher = {Curran Associates, Inc.},\\nurl = {http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf}\\n}\\n\\n@inproceedings{koller2018learning,\\n  title={Learning-based model predictive control for safe exploration},\\n  author={Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},\\n  booktitle={2018 IEEE Conference on Decision and Control (CDC)},\\n  pages={6059--6066},\\n  year={2018},\\n  organization={IEEE}\\n}\\n\\n\\n\\n@article{Nature/1986/Rumelhart,\\n\\tAbstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden\\'units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},\\n\\tAuthor = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},\\n\\tDa = {1986/10/01},\\n\\tDate-Added = {2019-04-22 15:39:12 +0000},\\n\\tDate-Modified = {2019-04-22 15:39:12 +0000},\\n\\tDoi = {10.1038/323533a0},\\n\\tIsbn = {1476-4687},\\n\\tJournal = {Nature},\\n\\tNumber = {6088},\\n\\tPages = {533--536},\\n\\tTitle = {Learning representations by back-propagating errors},\\n\\tTy = {JOUR},\\n\\tUrl = {https://doi.org/10.1038/323533a0},\\n\\tVolume = {323},\\n\\tYear = {1986},\\n\\tBdsk-Url-1 = {https://doi.org/10.1038/323533a0}}\\n\\n\\n@inproceedings{DBLP:conf/adhs/MusauJ18,\\n  author    = {Patrick Musau and\\n               Taylor T. Johnson},\\n  title     = {Verification of Continuous Time Recurrent Neural Networks (Benchmark\\n               Proposal)},\\n  booktitle = {{ARCH18.} 5th International Workshop on Applied Verification of Continuous\\n               and Hybrid Systems, ARCH@ADHS 2018, Oxford, UK, July 13, 2018},\\n  pages     = {196--207},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/adhs/2018arch},\\n  url       = {http://www.easychair.org/publications/paper/K6SZ},\\n  timestamp = {Fri, 15 Feb 2019 14:30:10 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/adhs/MusauJ18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/adhs/2018arch,\\n  editor    = {Goran Frehse and\\n               Matthias Althoff and\\n               Sergiy Bogomolov and\\n               Taylor T. Johnson},\\n  title     = {{ARCH18.} 5th International Workshop on Applied Verification of Continuous\\n               and Hybrid Systems, ARCH@ADHS 2018, Oxford, UK, July 13, 2018},\\n  series    = {EPiC Series in Computing},\\n  volume    = {54},\\n  publisher = {EasyChair},\\n  year      = {2018},\\n  url       = {http://www.easychair.org/publications/volume/ARCH18},\\n  timestamp = {Fri, 15 Feb 2019 14:30:10 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/adhs/2018arch},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@book{modelBasedDesignEmbeddedSystems,\\n  title = {Model-Based Design for Embedded Systems},\\n  edition={First},\\n  year={2017},\\n  publisher={{CRC} {P}ress},\\n  series = {Computational Analysis, Synthesis, and Design of Dynamic Systems},\\n  author = {Gabriela Nicolescu and Pieter J. Mosterman}\\n}\\n\\n@book{IntroToTensorCalculusBook,\\n  title = {Introduction to Tensor Calculus},\\n  author = {Kees Dullemond and Kasper Peeters},\\n  year = {1992}\\n}\\n\\n@article{paszke2017automatic,\\n  title={Automatic differentiation in PyTorch},\\n  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},\\n  year={2017}\\n}\\n\\n@inproceedings{DBLP:conf/osdi/AbadiBCCDDDGIIK16,\\n  author    = {Mart{\\\\\\'{\\\\i}}n Abadi and\\n               Paul Barham and\\n               Jianmin Chen and\\n               Zhifeng Chen and\\n               Andy Davis and\\n               Jeffrey Dean and\\n               Matthieu Devin and\\n               Sanjay Ghemawat and\\n               Geoffrey Irving and\\n               Michael Isard and\\n               Manjunath Kudlur and\\n               Josh Levenberg and\\n               Rajat Monga and\\n               Sherry Moore and\\n               Derek Gordon Murray and\\n               Benoit Steiner and\\n               Paul A. Tucker and\\n               Vijay Vasudevan and\\n               Pete Warden and\\n               Martin Wicke and\\n               Yuan Yu and\\n               Xiaoqiang Zheng},\\n  title     = {TensorFlow: {A} System for Large-Scale Machine Learning},\\n  booktitle = {12th {USENIX} Symposium on Operating Systems Design and Implementation\\n               ({OSDI} 2016)},\\n  pages     = {265--283},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/osdi/2016},\\n  url       = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},\\n  timestamp = {Tue, 29 Jan 2019 17:35:36 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/osdi/AbadiBCCDDDGIIK16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/osdi/2016,\\n  editor    = {Kimberly Keeton and\\n               Timothy Roscoe},\\n  title     = {12th {USENIX} Symposium on Operating Systems Design and Implementation,\\n               {OSDI} 2016, Savannah, GA, USA, November 2-4, 2016},\\n  publisher = {{USENIX} Association},\\n  year      = {2016},\\n  url       = {https://www.usenix.org/conference/osdi16},\\n  timestamp = {Tue, 29 Jan 2019 17:35:36 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/osdi/2016},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@article{DBLP:journals/corr/abs-1801-10578,\\n  author    = {Tsui{-}Wei Weng and\\n               Huan Zhang and\\n               Pin{-}Yu Chen and\\n               Jinfeng Yi and\\n               Dong Su and\\n               Yupeng Gao and\\n               Cho{-}Jui Hsieh and\\n               Luca Daniel},\\n  title     = {Evaluating the Robustness of Neural Networks: An Extreme Value Theory\\n               Approach},\\n  journal   = {CoRR},\\n  volume    = {abs/1801.10578},\\n  year      = {2018},\\n  url       = {http://arxiv.org/abs/1801.10578},\\n  archivePrefix = {arXiv},\\n  eprint    = {1801.10578},\\n  timestamp = {Mon, 13 Aug 2018 16:46:14 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-10578},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/abs-1901-01761,\\n  author    = {Th{\\\\\\'{e}}ophane Weber and\\n               Nicolas Heess and\\n               Lars Buesing and\\n               David Silver},\\n  title     = {Credit Assignment Techniques in Stochastic Computation Graphs},\\n  journal   = {CoRR},\\n  volume    = {abs/1901.01761},\\n  year      = {2019},\\n  url       = {http://arxiv.org/abs/1901.01761},\\n  archivePrefix = {arXiv},\\n  eprint    = {1901.01761},\\n  timestamp = {Thu, 31 Jan 2019 13:52:49 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-01761},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/nips/KrizhevskySH12,\\n  author    = {Alex Krizhevsky and\\n               Ilya Sutskever and\\n               Geoffrey E. Hinton},\\n  title     = {ImageNet Classification with Deep Convolutional Neural Networks},\\n  booktitle = {26th Annual Conference on Neural Information Processing Systems ({NIPS} 2012)},\\n  pages     = {1106--1114},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/nips/2012},\\n  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},\\n  timestamp = {Thu, 11 Dec 2014 17:34:07 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/nips/KrizhevskySH12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/nips/2012,\\n  editor    = {Peter L. Bartlett and\\n               Fernando C. N. Pereira and\\n               Christopher J. C. Burges and\\n               L{\\\\\\'{e}}on Bottou and\\n               Kilian Q. Weinberger},\\n  title     = {26th Annual Conference on Neural Information Processing Systems ({NIPS} 2012)},\\n  year      = {2012},\\n  url       = {http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012},\\n  timestamp = {Thu, 11 Dec 2014 17:34:07 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/nips/2012},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@article{DBLP:journals/nature/MnihKSRVBGRFOPB15,\\n  author    = {Volodymyr Mnih and\\n               Koray Kavukcuoglu and\\n               David Silver and\\n               Andrei A. Rusu and\\n               Joel Veness and\\n               Marc G. Bellemare and\\n               Alex Graves and\\n               Martin A. Riedmiller and\\n               Andreas Fidjeland and\\n               Georg Ostrovski and\\n               Stig Petersen and\\n               Charles Beattie and\\n               Amir Sadik and\\n               Ioannis Antonoglou and\\n               Helen King and\\n               Dharshan Kumaran and\\n               Daan Wierstra and\\n               Shane Legg and\\n               Demis Hassabis},\\n  title     = {Human-level control through deep reinforcement learning},\\n  journal   = {Nature},\\n  volume    = {518},\\n  number    = {7540},\\n  pages     = {529--533},\\n  year      = {2015},\\n  doi       = {10.1038/nature14236},\\n  timestamp = {Wed, 14 Nov 2018 10:30:43 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/nature/MnihKSRVBGRFOPB15},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/nature/SilverHMGSDSAPL16,\\n  author    = {David Silver and\\n               Aja Huang and\\n               Chris J. Maddison and\\n               Arthur Guez and\\n               Laurent Sifre and\\n               George van den Driessche and\\n               Julian Schrittwieser and\\n               Ioannis Antonoglou and\\n               Vedavyas Panneershelvam and\\n               Marc Lanctot and\\n               Sander Dieleman and\\n               Dominik Grewe and\\n               John Nham and\\n               Nal Kalchbrenner and\\n               Ilya Sutskever and\\n               Timothy P. Lillicrap and\\n               Madeleine Leach and\\n               Koray Kavukcuoglu and\\n               Thore Graepel and\\n               Demis Hassabis},\\n  title     = {Mastering the game of Go with deep neural networks and tree search},\\n  journal   = {Nature},\\n  volume    = {529},\\n  number    = {7587},\\n  pages     = {484--489},\\n  year      = {2016},\\n  url       = {https://doi.org/10.1038/nature16961},\\n  doi       = {10.1038/nature16961},\\n  timestamp = {Wed, 14 Nov 2018 10:30:42 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/nature/SilverHMGSDSAPL16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/lpar/LoosISK17,\\n  author    = {Sarah M. Loos and\\n               Geoffrey Irving and\\n               Christian Szegedy and\\n               Cezary Kaliszyk},\\n  title     = {Deep Network Guided Proof Search},\\n  booktitle = {21st International Conference on Logic for Programming, Artificial\\n               Intelligence and Reasoning ({LPAR}-21)},\\n  pages     = {85--105},\\n  year      = {2017},\\n  crossref  = {DBLP:conf/lpar/2017},\\n  timestamp = {Thu, 23 Nov 2017 16:56:11 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/lpar/LoosISK17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/lpar/2017,\\n  editor    = {Thomas Eiter and\\n               David Sands},\\n  title     = {21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning ({LPAR}-21)},\\n  series    = {EPiC Series in Computing},\\n  volume    = {46},\\n  publisher = {EasyChair},\\n  year      = {2017},\\n  url       = {http://www.easychair.org/publications/volume/LPAR-21},\\n  timestamp = {Thu, 23 Nov 2017 16:56:11 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/lpar/2017},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/coling/GoldbergHLZ18,\\n  author    = {Yoav Goldberg and\\n               Graeme Hirst and\\n               Yang Liu and\\n               Meng Zhang},\\n  title     = {Neural Network Methods for Natural Language Processing},\\n  ISBN      = {9781627052986},\\n  journal   = {Computational Linguistics},\\n  volume    = {44},\\n  number    = {1},\\n  year      = {2018},\\n  timestamp = {Mon, 26 Mar 2018 12:15:36 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/coling/GoldbergHLZ18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/aaip/Kitzelmann09,\\n  author    = {Emanuel Kitzelmann},\\n  title     = {Inductive Programming: {A} Survey of Program Synthesis Techniques},\\n  booktitle = {Third International\\n               Workshop on Approaches and Applications of Inductive Programming ({AAIP} 2009)},\\n  pages     = {50--73},\\n  year      = {2009},\\n  crossref  = {DBLP:conf/aaip/2009},\\n  timestamp = {Mon, 22 May 2017 17:11:12 +0200},\\n}\\n@proceedings{DBLP:conf/aaip/2009,\\n  editor    = {Ute Schmid and\\n               Emanuel Kitzelmann and\\n               Rinus Plasmeijer},\\n  title     = {Third International\\n               Workshop on Approaches and Applications of Inductive Programming ({AAIP} 2009)},\\n  series    = {Lecture Notes in Computer Science},\\n  volume    = {5812},\\n  publisher = {Springer},\\n  year      = {2010},\\n  doi       = {10.1007/978-3-642-11931-6},\\n  isbn      = {978-3-642-11930-9},\\n  timestamp = {Mon, 22 May 2017 17:11:12 +0200}\\n}\\n\\n@PHDTTHESIS{Kevorchian,\\nauthor = {Andreea Kevorchian},\\ntitle = {Verification of Recurrent Neural Networks},\\nschool = {Imperial College London Department of Computing},\\nyear = {2018}\\n}\\n\\n@PHDTHESIS{FultonThesis,                                                                               \\n  author = {Nathan Fulton},                                                 \\n  title  = {Verifiably Safe Autonomy for Cyber-Physical Systems},  \\n      school = {Computer Science Department, School of Computer Science, Carnegie Mellon University}      ,                            \\n  year      = {2018}                                                                                \\n}\\n\\n@inproceedings{itc18,                                                                               \\n  author = {Nathan Fulton and Andr{\\\\\\'{e}} Platzer},                                                 \\n  title  = {{S}afe {AI} for {CPS} (invited paper)},                                                 \\n  booktitle = {{IEEE} International Test Conference ({ITC} 2018)},                                       \\n  year      = {2018}                                                                                \\n}\\n\\n@inproceedings{DBLP:conf/atva/BrazdilCCFKKPU14,\\n  author    = {Tom{\\\\\\'{a}}s Br{\\\\\\'{a}}zdil and\\n               Krishnendu Chatterjee and\\n               Martin Chmelik and\\n               Vojtech Forejt and\\n               Jan Kret{\\\\\\'{\\\\i}}nsk{\\\\\\'{y}} and\\n               Marta Z. Kwiatkowska and\\n               David Parker and\\n               Mateusz Ujma},\\n  title     = {Verification of Markov Decision Processes Using Learning Algorithms},\\n  booktitle = {Automated Technology for Verification and Analysis - 12th International\\n               Symposium ({ATVA} 2014)},\\n  pages     = {98--114},\\n  year      = {2014},\\n  timestamp = {Fri, 02 Nov 2018 09:35:17 +0100},\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/qest/HenriquesMZPC12,\\n  author    = {David Henriques and\\n               Jo{\\\\~a}o G. Martins and\\n               Paolo Zuliani and\\n               Andr{\\\\\\'e} Platzer and\\n               Edmund M. Clarke},\\n  title     = {Statistical Model Checking for\\n               {Markov} Decision Processes},\\n  booktitle = {QEST},\\n  year      = {2012},\\n  pages     = {84-93},\\n  doi       = {10.1109/QEST.2012.19},\\n  longbooktitle = {Ninth International Conference on\\n               Quantitative Evaluation of Systems ({QEST} 2012)},\\n  publisher = {IEEE Computer Society},\\n  keywords  = {statistical model checking,\\n               Markov decision processes,\\n               reinforcement learning}\\n               }\\n\\n@inproceedings{Gaskett2003,\\n  title = {{Reinforcement learning under circumstances beyond its control}},\\n  booktitle = {International Conference on Computational Intelligence for Modelling Control and Automation},\\n  year = {2003},\\n  author = {Chris Gaskett}\\n}\\n  \\n@inproceedings{DBLP:conf/tacas/Junges0DTK16,\\n  author    = {Sebastian Junges and\\n               Nils Jansen and\\n               Christian Dehnert and\\n               Ufuk Topcu and\\n               Joost{-}Pieter Katoen},\\n  title     = {Safety-Constrained Reinforcement Learning for MDPs},\\n  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 22nd International Conference ({TACAS}/{ETAPS} 2016)},\\n  pages     = {130--146},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/tacas/2016},\\n\\n  timestamp = {Wed, 14 Nov 2018 10:56:57 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/Junges0DTK16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/tacas/2016,\\n  editor    = {Marsha Chechik and\\n               Jean{-}Fran{\\\\c{c}}ois Raskin},\\n  title     = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 22nd International Conference, {TACAS} 2016, Held as Part of the\\n               European Joint Conferences on Theory and Practice of Software, {ETAPS}\\n               2016, Eindhoven, The Netherlands, April 2-8, 2016, Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {9636},\\n  publisher = {Springer},\\n  year      = {2016},\\n\\n\\n  timestamp = {Fri, 02 Nov 2018 09:42:33 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/2016},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{itc18,\\n  author = {Nathan Fulton and Andr{\\\\\\'{e}} Platzer},\\n  title  = {{S}afe {AI} for {CPS} (invited paper)},\\n  booktitle = {{IEEE} International Test Conference ({ITC})},\\n  year      = {2018}\\n}\\n\\n@inproceedings{DBLP:conf/icra/FridovichKeilH18,\\n  author    = {David Fridovich{-}Keil and\\n               Sylvia L. Herbert and\\n               Jaime F. Fisac and\\n               Sampada Deglurkar and\\n               Claire J. Tomlin},\\n  title     = {Planning, Fast and Slow: {A} Framework for Adaptive Real-Time Safe\\n               Trajectory Planning},\\n  booktitle = {{IEEE} International Conference on Robotics and Automation ({ICRA})},\\n  pages     = {387--394},\\n  year      = {2018},\\n\\n  timestamp = {Wed, 19 Sep 2018 09:21:27 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icra/Fridovich-KeilH18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{HasanbeigKroening,\\n  author    = {Mohammadhosein Hasanbeig and\\n               Alessandro Abate and\\n               Daniel Kroening},\\n  title     = {Logically-Correct Reinforcement Learning},\\n  journal   = {CoRR},\\n  volume    = {abs/1801.08099},\\n  year      = {2018},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1801.08099},\\n  timestamp = {Fri, 02 Feb 2018 14:20:25 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-08099},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@article{hasanbeig2020cautious,\\n  title={Cautious reinforcement learning with logical constraints},\\n  author={Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},\\n  journal={arXiv preprint arXiv:2002.12156},\\n  year={2020}\\n}\\n\\n\\n@ARTICLE{Hasanbeig2019,\\n       author = {{Hasanbeig}, Mohammadhosein and {Kantaros}, Yiannis and {Abate}, Alessand\\n        ro and {Kroening}, Daniel and {Pappas}, George J. and {Lee}, Insup},\\n        title = \"{Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees}\",\\n      journal = {arXiv e-prints},\\n     keywords = {Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},\\n         year = 2019,\\n        month = sep,\\n          eid = {arXiv:1909.05304},\\n        pages = {arXiv:1909.05304},\\narchivePrefix = {arXiv},\\n       eprint = {1909.05304},\\n primaryClass = {cs.LO},\\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190905304H},\\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\\n}\\n\\n\\n@article{DBLP:journals/tac/CoraluppiM00,\\n  author    = {Stefano P. Coraluppi and\\n               Steven I. Marcus},\\n  title     = {Mixed risk-neutral/minimax control of discrete-time, finite-state\\n               {M}arkov decision processes},\\n  journal   = {{IEEE} Trans. Automat. Contr.},\\n  volume    = {45},\\n  number    = {3},\\n  pages     = {528--532},\\n  year      = {2000}\\n}\\n@article{DBLP:journals/automatica/CoraluppiM99,\\n  author    = {Stefano P. Coraluppi and\\n               Steven I. Marcus},\\n  title     = {Risk-sensitive and minimax control of discrete-time, finite-state\\n               {M}arkov decision processes},\\n  journal   = {Automatica},\\n  volume    = {35},\\n  number    = {2},\\n  pages     = {301--309},\\n  year      = {1999}\\n}\\n\\n\\n@book{clarke2018handbook,\\n  editor    = {Edmund M. Clarke and\\n               Thomas A. Henzinger and\\n               Helmut Veith and\\n               Roderick Bloem},\\n  title     = {Handbook of Model Checking},\\n  publisher = {Springer},\\n  year      = {2018},\\n\\n\\n\\n  timestamp = {Tue, 29 May 2018 12:30:27 +0200},\\n  biburl    = {https://dblp.org/rec/bib/reference/mc/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@book{hilbertOriginal,\\n  title={Grundz{\\\\\"u}ge der theoretischen Logik},\\n  author={Hilbert, D. and Ackermann, W.},\\n\\n  series={Grundlehren der mathematischen Wissenschaften},\\n\\n  year={1938},\\n  publisher={Springer }\\n}\\n\\n@book{hilbertTranslation,\\n  title={Principles of Mathematical Logic},\\n  author={Hilbert, D. and Ackermann, W. and Luce, R.E. and Hammond, L.M.},\\n\\n  lccn={99015531},\\n  series={AMS Chelsea Publishing Series},\\n\\n  year={1999},\\n  publisher={American Mathematical Society}\\n}\\n\\n@article{poincare1885,\\nauthor = \"Poincar\\\\\\'{e}, H.\",\\n\\nfjournal = \"Acta Mathematica\",\\njournal = \"Acta Math.\",\\npages = \"259--380\",\\npublisher = \"Institut Mittag-Leffler\",\\ntitle = \"Sur l\\'quilibre d\\'une masse fluide anime d\\'un mouvement de rotation\",\\nvolume = \"7\",\\nyear = \"1885\"\\n}\\n\\n\\n@book{blanchard2011differential,\\n  title={Differential Equations},\\n  author={Blanchard, P. and Devaney, R.L. and Hall, G.R.},\\n\\n  year={2011},\\n  publisher={Brooks/ Cole, Cengage Learning}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/cdc/HerbertCHBFT17,\\n  author    = {Sylvia L. Herbert and\\n               Mo Chen and\\n               SooJean Han and\\n               Somil Bansal and\\n               Jaime F. Fisac and\\n               Claire J. Tomlin},\\n  title     = {{FaSTrack}: {A} modular framework for fast and guaranteed safe motion\\n               planning},\\n  booktitle = {{IEEE} Annual Conference on Decision and Control ({CDC})},\\n  pages     = {1517--1522},\\n  year      = {2017},\\n  timestamp = {Wed, 24 Jan 2018 18:24:16 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/cdc/HerbertCHBFT17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/icml/AchiamHTA17,\\n  author    = {Joshua Achiam and\\n               David Held and\\n               Aviv Tamar and\\n               Pieter Abbeel},\\n  title     = {Constrained Policy Optimization},\\n  booktitle = {International Conference on Machine Learning\\n               ({ICML} 2017)},\\n  pages     = {22--31},\\n  year      = {2017},\\n  crossref  = {DBLP:conf/icml/2017},\\n\\n  timestamp = {Wed, 16 Aug 2017 11:08:55 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/AchiamHTA17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icml/2017,\\n  editor    = {Doina Precup and\\n               Yee Whye Teh},\\n  title     = {International Conference on Machine Learning,\\n               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},\\n  series    = {Proceedings of Machine Learning Research},\\n  volume    = {70},\\n  publisher = {{PMLR}},\\n  year      = {2017},\\n\\n  timestamp = {Wed, 16 Aug 2017 11:08:55 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/2017},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@ARTICLE{AsarinSynthesis, \\nauthor={E. Asarin and O. Bournez and T. Dang and O. Maler and A. Pnueli}, \\njournal={Proceedings of the IEEE}, \\ntitle={Effective synthesis of switching controllers for linear systems}, \\nyear={2000}, \\nvolume={88}, \\nnumber={7}, \\npages={1011-1025}, \\nkeywords={control system synthesis;linear systems;linear differential equations;iterative methods;variable structure systems;reachability analysis;controllability;effective switching controller synthesis;linear systems;continuous systems;hybrid systems;linear differential equations;iterative computation;reachable states;reachability analysis;Control system synthesis;Linear systems;Control systems;Mathematical model;Differential equations;Signal sampling;Switches;Iterative algorithms;Mathematics;Concrete}, \\n\\nmonth={July},}\\n\\n\\n@inproceedings{DBLP:conf/hybrid/SadraddiniB18,\\n  author    = {Sadra Sadraddini and\\n               Calin Belta},\\n  title     = {Formal Guarantees in Data-Driven Model Identification and Control\\n               Synthesis},\\n  booktitle = {International Conference on Hybrid Systems:\\n               Computation and Control ({HSCC} 2018)},\\n  pages     = {147--156},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/hybrid/2018},\\n\\n\\n  timestamp = {Fri, 13 Apr 2018 08:48:21 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/SadraddiniB18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@article{DBLP:journals/corr/SchulmanWDRK17,\\n  author    = {John Schulman and\\n               Filip Wolski and\\n               Prafulla Dhariwal and\\n               Alec Radford and\\n               Oleg Klimov},\\n  title     = {Proximal Policy Optimization Algorithms},\\n  year      = {2017},\\n  url       = {http://arxiv.org/abs/1707.06347},\\n  archivePrefix = {arXiv},\\n  eprint    = {1707.06347},\\n  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icml/SchulmanLAJM15,\\n  author    = {John Schulman and\\n               Sergey Levine and\\n               Pieter Abbeel and\\n               Michael I. Jordan and\\n               Philipp Moritz},\\n  title     = {Trust Region Policy Optimization},\\n  booktitle = {Proceedings of the 32nd International Conference on Machine Learning\\n               ({ICML} 2015)},\\n  pages     = {1889--1897},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/icml/2015},\\n\\n  timestamp = {Wed, 29 Mar 2017 16:45:25 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/SchulmanLAJM15},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icml/2015,\\n  editor    = {Francis R. Bach and\\n               David M. Blei},\\n  title     = {Proceedings of the 32nd International Conference on Machine Learning,\\n               {ICML} 2015, Lille, France, 6-11 July 2015},\\n  series    = {{JMLR} Workshop and Conference Proceedings},\\n  volume    = {37},\\n  year      = {2015},\\n\\n  timestamp = {Wed, 29 Mar 2017 16:45:25 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/2015},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{akametalu2014reachability,\\n  title={Reachability-based safe learning with Gaussian processes},\\n  author={Akametalu, Anayo K and Fisac, Jaime F and Gillula, Jeremy H and Kaynama, Shahab and Zeilinger, Melanie N and Tomlin, Claire J},\\n  booktitle={53rd IEEE Conference on Decision and Control},\\n  pages={1424--1431},\\n  year={2014},\\n  organization={IEEE}\\n}\\n\\n\\n@InProceedings{AkazakiCPSfalsification,\\nauthor=\"Akazaki, Takumi\\nand Liu, Shuang\\nand Yamagata, Yoriyuki\\nand Duan, Yihai\\nand Hao, Jianye\",\\neditor=\"Havelund, Klaus\\nand Peleska, Jan\\nand Roscoe, Bill\\nand de Vink, Erik\",\\ntitle=\"Falsification of Cyber-Physical Systems Using Deep Reinforcement Learning\",\\nbooktitle=\"Formal Methods\",\\nyear=\"2018\",\\npublisher=\"Springer International Publishing\",\\naddress=\"Cham\",\\npages=\"456--465\",\\nabstract=\"With the rapid development of software and distributed computing, Cyber-Physical Systems (CPS) are widely adopted in many application areas, e.g., smart grid, autonomous automobile. It is difficult to detect defects in CPS models due to the complexities involved in the software and physical systems. To find defects in CPS models efficiently, robustness guided falsification of CPS is introduced. Existing methods use several optimization techniques to generate counterexamples, which falsify the given properties of a CPS. However those methods may require a large number of simulation runs to find the counterexample and are far from practical. In this work, we explore state-of-the-art Deep Reinforcement Learning (DRL) techniques to reduce the number of simulation runs required to find such counterexamples. We report our method and the preliminary evaluation results.\",\\n\\n}\\n\\n\\n@misc{Mathematica,\\n  author = {Wolfram Research{,} Inc.},\\n  title = {Mathematica, {V}ersion 11.3},\\n  note = {Champaign, IL, 2018}\\n}\\n\\n@misc{MathematicaReduce,\\n author = {Eric W. Weisstein}, \\n title = {Quantifier Elimination},\\n howpublished = {{MathWorld}--A Wolfram Web Resource},\\n url = {http://mathworld.wolfram.com/QuantifierElimination.html}\\n\\n} \\n\\n\\n@InProceedings{z3,\\nauthor=\"de Moura, Leonardo\\nand Bj{\\\\o}rner, Nikolaj\",\\neditor=\"Ramakrishnan, C. R.\\nand Rehof, Jakob\",\\ntitle=\"Z3: An Efficient {SMT} Solver\",\\nbooktitle=\"Tools and Algorithms for the Construction and Analysis of Systems\",\\nyear=\"2008\",\\npublisher=\"Springer\",\\npages=\"337--340\",\\nabstract=\"Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.\",\\n\\n}\\n\\n@InProceedings{CADComplexity,\\n  author    = {Matthew England and\\n               James H. Davenport},\\n  title     = {The Complexity of Cylindrical Algebraic Decomposition with Respect\\n               to Polynomial Degree},\\n  booktitle = {Computer Algebra in Scientific Computing - 18th International Workshop ({CASC} 2016)},\\n  pages     = {172--192},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/casc/2016},\\n\\n  timestamp = {Thu, 15 Jun 2017 21:32:34 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/casc/EnglandD16}\\n}\\n\\n@proceedings{DBLP:conf/casc/2016,\\n  editor    = {Vladimir P. Gerdt and\\n               Wolfram Koepf and\\n               Werner M. Seiler and\\n               Evgenii V. Vorozhtsov},\\n  title     = {Computer Algebra in Scientific Computing - 18th International Workshop,\\n               ({CASC} 2016)},\\n  series    = {LNCS},\\n  volume    = {9890},\\n  publisher = {Springer},\\n  year      = {2016},\\n\\n\\n  timestamp = {Mon, 22 May 2017 17:10:56 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/casc/2016},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@book{church1996introduction,\\n  title={Introduction to Mathematical Logic},\\n  author={Church, A.},\\n  volume={13},\\n\\n  series={Annals of Mathematics Studies},\\n\\n  year={1996},\\n  publisher={Princeton University Press}\\n}\\n\\n@article{DBLP:journals/corr/GaoKCC14,\\n  author    = {Sicun Gao and\\n               Soonho Kong and\\n               Wei Chen and\\n               Edmund M. Clarke},\\n  title     = {Delta-Complete Analysis for Bounded Reachability of Hybrid Systems},\\n  journal   = {CoRR},\\n  volume    = {abs/1404.7171},\\n  year      = {2014},\\n  archivePrefix = {arXiv},\\n  eprint    = {1404.7171},\\n  timestamp = {Mon, 13 Aug 2018 16:48:24 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/GaoKCC14},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@InProceedings{opensmt,\\nauthor=\"Bruttomesso, Roberto\\nand Pek, Edgar\\nand Sharygina, Natasha\\nand Tsitovich, Aliaksei\",\\neditor=\"Esparza, Javier\\nand Majumdar, Rupak\",\\ntitle={The {O}pen{SMT} Solver},\\nbooktitle=\"Tools and Algorithms for the Construction and Analysis of Systems\",\\nyear=\"2010\",\\npublisher=\"Springer\",\\npages=\"150--153\",\\nabstract=\"This paper describes OpenSMT, an incremental, efficient, and open-source SMT-solver. OpenSMT has been specifically designed to be easily extended with new theory-solvers, in order to be accessible for non-experts for the development of customized algorithms. We sketch the solver\\'s architecture and interface. We discuss its distinguishing features w.r.t. other state-of-the-art solvers.\",\\n\\n}\\n\\n\\n\\n@article{stooke2019,\\n  author    = {Adam Stooke and\\n               Pieter Abbeel},\\n  title     = {rlpyt: {A} Research Code Base for Deep Reinforcement Learning in PyTorch},\\n  journal   = {CoRR},\\n  volume    = {abs/1909.01500},\\n  year      = {2019},\\n  url       = {http://arxiv.org/abs/1909.01500},\\n  archivePrefix = {arXiv},\\n  eprint    = {1909.01500},\\n  timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},\\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-01500.bib},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n\\n@incollection{intervalConstraintProp,\\ntitle = {Continuous and Interval Constraints},\\neditor = \"Francesca Rossi and Peter van Beek and Toby Walsh\",\\nseries = \"Foundations of Artificial Intelligence\",\\npublisher = \"Elsevier\",\\nvolume = \"2\",\\npages = \"571 - 603\",\\nyear = \"2006\",\\nbooktitle = \"Handbook of Constraint Programming\",\\n\\n\\nauthor = \"Frdric Benhamou and Laurent Granvilliers\",\\nabstract = \"Publisher Summary\\nThis chapter reviews that continuous constraint solving has been widely studied in several fields of applied mathematics and computer science. In computer algebra, continuous constraints are viewed as formulas from first-order logic interpreted over the real numbers. The symbolic algorithms transform the constraint systems within the same equivalence class in the interpretation domain according to some simplification ordering. The chapter also discusses the interval analysis, which is a set extension of numerical analysis such that the floating-point numbers are replaced with the intervals. The interval approximations are defined so as to enclose the computed real quantities and the algorithms are said to be complete. In constraint programming, continuous constraints are viewed as relations. The complete solving of nonlinear systems is implemented by exhaustive search techniques that compute solution space coverings by means of multi-dimensional boxes. The search is commonly accelerated through propagation-based algorithms. It reviews that continuous and interval constraints are generally contrasted with non negative integer or more generally discrete constraints. These last constraints, sometimes also called finite domain constraints, are studied in the constraint satisfaction problems (CSP) framework and are basic components of most current constraint-based languages.\"\\n}\\n\\n@article{HENZINGER199894,\\ntitle = \"What\\'s Decidable about Hybrid Automata?\",\\njournal = \"Journal of Computer and System Sciences\",\\nvolume = \"57\",\\nnumber = \"1\",\\npages = \"94 - 124\",\\nyear = \"1998\",\\n\\nauthor = \"Thomas A. Henzinger and Peter W. Kopke and Anuj Puri and Pravin Varaiya\"\\n}\\n\\n@article{DBLP:journals/corr/bettina20182,\\n  author    = {Nils Jansen and\\n               Bettina K{\\\\\"{o}}nighofer and\\n               Sebastian Junges and\\n               Roderick Bloem},\\n  title     = {Shielded Decision-Making in {MDP}s},\\n  journal   = {CoRR},\\n  volume    = {abs/1807.06096},\\n  year      = {2018},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1807.06096},\\n  timestamp = {Mon, 13 Aug 2018 01:00:00 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-06096},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{relyguaranteecanonical,                                                                    \\n author = {Jones, C. B.},                                                                           \\n title = {Tentative Steps Toward a Development Method for Interfering Programs},                    \\n journal = {ACM Trans. Program. Lang. Syst.},                                                       \\n issue_date = {Oct. 1983},                                                                          \\n volume = {5},                                                                                      \\n number = {4},                                                                                      \\n month = oct,                                                                                       \\n year = {1983},                                                                                     \\n pages = {596--619},                                                                                \\n numpages = {24},                                                                                   \\n\\n\\n acmid = {69577},                                                                                   \\n publisher = {ACM},                                                                                 \\n address = {New York, NY, USA},                                                                     \\n}  \\n\\n@article{DBLP:journals/deds/BartoM03,                                                               \\n  author    = {Andrew G. Barto and                                                                  \\n               Sridhar Mahadevan},                                                                  \\n  title     = {Recent Advances in Hierarchical Reinforcement Learning},                             \\n  journal   = {Discrete Event Dynamic Systems},                                                     \\n  volume    = {13},                                                                                 \\n  number    = {1-2},                                                                                \\n  pages     = {41--77},                                                                             \\n  year      = {2003},                                                                               \\n\\n\\n  timestamp = {Sat, 27 May 2017 14:25:36 +0200},                                                    \\n  biburl    = {https://dblp.org/rec/bib/journals/deds/BartoM03},                                    \\n  bibsource = {dblp computer science bibliography, https://dblp.org}                                \\n}  \\n\\n@article{fischer1979propositional,\\n  title={Propositional dynamic logic of regular programs},\\n  author={Fischer, Michael J and Ladner, Richard E},\\n  journal={Journal of computer and system sciences},\\n  volume={18},\\n  number={2},\\n  pages={194--211},\\n  year={1979},\\n  publisher={Academic Press}\\n}\\n\\n@article{lindelof1894application,\\n  title={Sur lapplication de la m{\\\\\\'e}thode des approximations successives aux {\\\\\\'e}quations diff{\\\\\\'e}rentielles ordinaires du premier ordre},\\n  author={Lindel{\\\\\"o}f, Ernest},\\n  journal={Comptes rendus hebdomadaires des s{\\\\\\'e}ances de lAcad{\\\\\\'e}mie des sciences},\\n  volume={116},\\n  number={3},\\n  pages={454--457},\\n  year={1894}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/tableaux/Platzer07,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Differential Dynamic Logic for Verifying Parametric Hybrid Systems.},\\n  booktitle = {TABLEAUX},\\n  longbooktitle = {Automated Reasoning with Analytic Tableaux and Related Methods, 16th International\\n               Conference, TABLEAUX 2007, Aix en Provence, France, July 3-6, 2007, Proceedings},\\n  year      = {2007},\\n  pages     = {216-232},\\n  crossref  = {DBLP:conf/tableaux/2007},\\n\\n  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.8368},\\n  keywords  = {dynamic logic, sequent calculus, verification of parametric hybrid systems, quantifier elimination},\\n  abstract  = {\\n      We introduce a first-order dynamic logic for reasoning about\\n      systems with discrete and continuous state transitions, and we\\n      present a sequent calculus for this logic.  As a uniform model,\\n      our logic supports hybrid programs with discrete and\\n      differential actions.  For handling real arithmetic during\\n      proofs, we lift quantifier elimination to dynamic logic.  To\\n      obtain a modular combination, we use side deductions for\\n      verifying interacting dynamics.  With this, our logic supports\\n      deductive verification of hybrid systems with symbolic\\n      parameters and first-order definable flows.  Using our calculus,\\n      we prove a parametric inductive safety constraint for speed\\n      supervision in a train control system.}\\n}\\n@PROCEEDINGS{DBLP:conf/tableaux/2007,\\n  title     = {Automated Reasoning with Analytic Tableaux and Related Methods, 16th International\\n               Conference, TABLEAUX 2007, Aix en Provence, France, July 3-6, 2007, Proceedings},\\n  year      = {2007},\\n  editor    = {Nicola Olivetti},\\n  volume    = {4548},\\n  series    = {LNCS},\\n  publisher = {Springer},\\n  booktitle = {TABLEAUX},\\n\\n  OPTmonth     = {Jul}\\n}\\n\\n\\n@incollection{DBLP:conf/tphol/HarrisonSA06,\\n  author    = {John Harrison and\\n               Konrad Slind and\\n               Rob Arthan},\\n  title     = {{HOL}},\\n  booktitle = {The Seventeen Provers of the World, Foreword by Dana S. Scott},\\n  pages     = {11--19},\\n  year      = {2006},\\n  crossref  = {DBLP:conf/tphol/2006provers},\\n\\n\\n  timestamp = {Thu, 23 Nov 2017 14:40:29 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/tphol/HarrisonSA06},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@book{DBLP:conf/tphol/2006provers,\\n  editor    = {Freek Wiedijk},\\n  title     = {The Seventeen Provers of the World, Foreword by Dana S. Scott},\\n  series    = {{LNCS}},\\n  volume    = {3600},\\n  publisher = {Springer},\\n  year      = {2006},\\n\\n\\n\\n  timestamp = {Thu, 23 Nov 2017 14:40:29 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/tphol/2006provers},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/tphol/SlindN08,\\n  author    = {Konrad Slind and\\n               Michael Norrish},\\n  title     = {A Brief Overview of {HOL4}},\\n  booktitle = {Theorem Proving in Higher Order Logics, 21st International Conference ({TPHOLS} 2000)},\\n  pages     = {28--32},\\n  year      = {2008},\\n  crossref  = {DBLP:conf/tphol/2008},\\n\\n\\n  timestamp = {Tue, 13 Jun 2017 10:37:56 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tphol/SlindN08},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/tphol/2008,\\n  editor    = {Otmane A{\\\\\"{\\\\i}}t Mohamed and\\n               C{\\\\\\'{e}}sar A. Mu{\\\\~{n}}oz and\\n               Sofi{\\\\`{e}}ne Tahar},\\n  title     = {Theorem Proving in Higher Order Logics, 21st International Conference,\\n               TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {5170},\\n  publisher = {Springer},\\n  year      = {2008},\\n\\n\\n\\n  timestamp = {Tue, 13 Jun 2017 10:37:56 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tphol/2008},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/aplas/LiuLQZZZZ10,\\n  author    = {Jiang Liu and\\n               Jidong Lv and\\n               Zhao Quan and\\n               Naijun Zhan and\\n               Hengjun Zhao and\\n               Chaochen Zhou and\\n               Liang Zou},\\n  title     = {A Calculus for Hybrid {CSP}},\\n  booktitle = {Programming Languages and Systems - 8th Asian Symposium ({APLAS} 2010)},\\n  pages     = {1--15},\\n  year      = {2010},\\n  crossref  = {DBLP:conf/aplas/2010},\\n\\n\\n  timestamp = {Fri, 19 May 2017 01:25:54 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/aplas/LiuLQZZZZ10},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/aplas/2010,\\n  editor    = {Kazunori Ueda},\\n  title     = {Programming Languages and Systems - 8th Asian Symposium, {APLAS} 2010,\\n               Shanghai, China, November 28 - December 1, 2010. Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {6461},\\n  publisher = {Springer},\\n  year      = {2010},\\n\\n\\n\\n  timestamp = {Fri, 19 May 2017 01:25:54 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/aplas/2010},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@ARTICLE{DBLP:journals/jar/Platzer17,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {A Complete Uniform Substitution Calculus \\n               for Differential Dynamic Logic},\\n  journal   = {J. Autom. Reas.},\\n  longjournal = {Journal of Automated Reasoning},\\n  medjournal= {J. Autom. Reasoning},\\n  year      = {2017},\\n  volume    = {59},\\n  number    = {2},\\n  pages     = {219-266},\\n\\n  arXiv     = {1507.04943}\\n}\\n\\n@proceedings{DBLP:conf/itp/2013,\\n  editor    = {Sandrine Blazy and\\n               Christine Paulin-Mohring and\\n               David Pichardie},\\n  title     = ITP2013,\\n  booktitle = ITP2013,\\n  publisher = {Springer},\\n  series    = LNCS,\\n  volume    = {7998},\\n  year      = {2013},\\n\\n  bibsource = {DBLP, http://dblp.uni-trier.de}\\n}\\n\\n  @inproceedings{berkenkamp2017safe,\\n  title={Safe model-based reinforcement learning with stability guarantees},\\n  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},\\n  booktitle={Advances in neural information processing systems},\\n  pages={908--918},\\n  year={2017}\\n}\\n\\n@ARTICLE{DBLP:journals/fmsd/PlatzerC09,\\n  author    = {Andr{\\\\\\'e} Platzer and Edmund M. Clarke},\\n  title     = {Computing Differential Invariants of Hybrid Systems as Fixedpoints},\\n  journal   = {Form. Methods Syst. Des.},\\n  longjournal = {Formal Methods in System Design},\\n  year      = {2009},\\n  volume    = {35},\\n  number    = {1},\\n  pages     = {98-120},\\n  \\n\\n\\n  keywords  = {verification of hybrid systems, differential invariants, verification logic, fixedpoint engine},\\n  abstract  = {\\n      We introduce a fixedpoint algorithm for verifying safety\\n      properties of hybrid systems with differential equations\\n      whose right-hand sides are polynomials in the state\\n      variables. In order to verify nontrivial systems without\\n      solving their differential equations and without numerical\\n      errors, we use a continuous generalization of induction, for\\n      which our algorithm computes the required differential\\n      invariants. As a means for combining local differential\\n      invariants into global system invariants in a sound way, our\\n      fixedpoint algorithm works with a compositional verification\\n      logic for hybrid systems. With this compositional approach\\n      we exploit locality in system designs. To improve the\\n      verification power, we further introduce a saturation\\n      procedure that refines the system dynamics successively with\\n      differential invariants until safety becomes provable. By\\n      complementing our symbolic verification algorithm with a\\n      robust version of numerical falsification, we obtain a fast\\n      and sound verification procedure. We verify roundabout\\n      maneuvers in air traffic management and collision avoidance\\n      in train control and car control.}\\n}\\n\\n@proceedings{DBLP:conf/itp/2016,\\n  editor    = {Jasmin Christian Blanchette and\\n               Stephan Merz},\\n  title     = ITP2016,\\n  booktitle = ITP2016,\\n  series    = LNCS,\\n  volume    = {9807},\\n  publisher = {Springer},\\n  year      = {2016},\\n\\n\\n\\n  timestamp = {Mon, 08 Aug 2016 15:10:56 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/itp/2016},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@STRING{lncs = {LNCS}}\\n\\n@ARTICLE{DBLP:journals/fmsd/MitschP16,\\n  author    = {Stefan Mitsch and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {{ModelPlex}: Verified Runtime Validation of\\n               Verified Cyber-Physical System Models},\\n  journal   = {Form. Methods Syst. Des.},\\n  longjournal = {Formal Methods in System Design},\\n  year      = {2016},\\n  volume    = {49},\\n  number    = {1},\\n  pages     = {33-74},\\n\\n  note      = {Special issue of selected papers from RV\\'14},\\n}\\n\\n@inproceedings{DBLP:conf/cpp/FultonP16,\\n  author    = {Nathan Fulton and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {A logic of proofs for differential dynamic logic: toward independently\\n               checkable proof certificates for dynamic logics},\\n  booktitle = {Proceedings of the 5th {ACM} {SIGPLAN} Conference on Certified Programs\\n               and Proofs, 2016},\\n  pages     = {110--121},\\n  year      = {2016},\\n\\n\\n  timestamp = {Mon, 18 Jan 2016 19:35:36 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cpp/FultonP16},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/lics/PlatzerT18,\\n  author    = {Andr{\\\\\\'{e}} Platzer and\\n               Yong Kiam Tan},\\n  title     = {Differential Equation Axiomatization:\\n               The Impressive Power of Differential Ghosts},\\n  booktitle = {LICS},\\n  year      = {2018},\\n  pages     = {819-828},\\n\\n  editor    = {Anuj Dawar and\\n               Erich Gr{\\\\\"{a}}del},\\n  longbooktitle = {Proceedings of the 33rd Annual ACM/IEEE Symposium on Logic in\\n               Computer Science},\\n  publisher = {ACM},\\n  year      = {2018},\\n  key       = {LICS},\\n\\n  address   = {New York},\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/pldi/BohrerTMMP18,\\n  author    = {Brandon Bohrer and\\n               Yong Kiam Tan and\\n               Stefan Mitsch and\\n               Magnus O. Myreen and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {{VeriPhy}: Verified Controller Executables\\n               from Verified Cyber-Physical System Models},\\n  pages     = {617-630},\\n  year      = {2018},\\n\\n  publisher = {{ACM}},\\n  editor    = {Dan Grossman},\\n  booktitle = {Proceedings of the 39th {ACM} {SIGPLAN}\\n               Conference on Programming Language\\n               Design and Implementation ({PLDI} 2018)}\\n}\\n\\n\\n\\n@INPROCEEDINGS{DBLP:conf/cpp/BohrerRVVP17,\\n  author    = {Brandon Bohrer and\\n               Vincent Rahli and\\n               Ivana Vukotic and\\n               Marcus V{\\\\\"o}lp and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {Formally Verified Differential Dynamic Logic},\\n  pages     = {208-221},\\n  year      = {2017},\\n\\n  booktitle = {Proceedings of the 6th {ACM SIGPLAN} Conference on Certified Programs and Proofs (CPP 2017)},\\n  publisher = {{ACM}},\\n\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/tacas/FultonP19,\\n  author    = {Nathan Fulton and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {Verifiably Safe Off-Model Reinforcement Learning},\\n  booktitle = {{TACAS} 2019},\\n  pages     = {413--430},\\n  year      = {2019},\\n  crossref  = {DBLP:conf/tacas/2019-1},\\n  doi       = {10.1007/978-3-030-17462-0\\\\_28},\\n  timestamp = {Thu, 04 Apr 2019 12:18:13 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/FultonP19},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/tacas/2019-1,\\n  editor    = {Tom{\\\\\\'{a}}s Vojnar and\\n               Lijun Zhang},\\n  title     = {Tools and Algorithms for the Construction and Analysis of Systems ({TACAS} 2019)},\\n  series    = {Lecture Notes in Computer Science},\\n  volume    = {11427},\\n  publisher = {Springer},\\n  year      = {2019},\\n  doi       = {10.1007/978-3-030-17462-0},\\n  isbn      = {978-3-030-17461-3},\\n  timestamp = {Thu, 04 Apr 2019 12:18:13 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/2019-1},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/cav/FrehseGDCRLRGDM11,\\n  author    = {Goran Frehse and\\n               Colas Le Guernic and\\n               Alexandre Donz{\\\\\\'{e}} and\\n               Scott Cotton and\\n               Rajarshi Ray and\\n               Olivier Lebeltel and\\n               Rodolfo Ripado and\\n               Antoine Girard and\\n               Thao Dang and\\n               Oded Maler},\\n  title     = {{SpaceEx}: Scalable Verification of Hybrid Systems},\\n  booktitle = {23rd CAV, 2011},\\n  pages     = {379--395},\\n  year      = {2011},\\n  crossref  = {DBLP:conf/cav/2011},\\n\\n  timestamp = {Mon, 11 Jul 2011 13:24:02 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cav/FrehseGDCRLRGDM11},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cav/2011,\\n  editor    = {Ganesh Gopalakrishnan and\\n               Shaz Qadeer},\\n  title     = {Computer Aided Verification - 23rd International Conference, {CAV}\\n               2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings},\\n  series    = lncs,\\n  volume    = {6806},\\n  publisher = {Springer},\\n  year      = {2011},\\n\\n\\n  timestamp = {Mon, 11 Jul 2011 13:20:54 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cav/2011},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@book{DBLP:books/daglib/0068834,\\n  author    = {Robert L. Constable and\\n               Stuart F. Allen and\\n               Mark Bromley and\\n               et. al.},\\n  title     = {Implementing mathematics with the Nuprl proof development system},\\n  publisher = {Prentice Hall},\\n  year      = {1986},\\n\\n  timestamp = {Fri, 26 Aug 2011 14:36:50 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/books/daglib/0068834},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/tphol/2003,\\n  editor    = {David A. Basin and\\n               Burkhart Wolff},\\n  title     = {Theorem Proving in Higher Order Logics, 16th International Conference,\\n               TPHOLs 2003, Rom, Italy, September 8-12, 2003, Proceedings},\\n  series    = lncs,\\n  volume    = {2758},\\n  publisher = {Springer},\\n  year      = {2003},\\n\\n  timestamp = {Fri, 06 Feb 2004 08:38:42 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tphol/2003},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/tacas/KongGCC15,\\n  author    = {Soonho Kong and\\n               Sicun Gao and\\n               Wei Chen and\\n               Edmund M. Clarke},\\n  title     = {{dReach}: {\\\\(\\\\delta\\\\)}-Reachability Analysis for Hybrid Systems},\\n  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 21st International Conference, {TACAS} 2015},\\n  pages     = {200--205},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/tacas/2015},\\n\\n  timestamp = {Wed, 08 Apr 2015 11:31:07 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tacas/KongGCC15},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/tacas/2015,\\n  editor    = {Christel Baier and\\n               Cesare Tinelli},\\n  title     = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 21st International Conference, {TACAS} 2015, Held as Part of the\\n               European Joint Conferences on Theory and Practice of Software, {ETAPS}\\n               2015, London, UK, April 11-18, 2015. Proceedings},\\n  series    = lncs,\\n  volume    = {9035},\\n  publisher = {Springer},\\n  year      = {2015},\\n\\n\\n  timestamp = {Wed, 08 Apr 2015 11:30:56 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tacas/2015},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/GaoKC13,\\n  author    = {Sicun Gao and\\n               Soonho Kong and\\n               Edmund M. Clarke},\\n  title     = {{dReal}: An {SMT} Solver for Nonlinear Theories over the Reals},\\n  booktitle = {24th International Conference on Automated Deduction ({CADE-24})},\\n  pages     = {208--214},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/cade/2013},\\n\\n\\n  timestamp = {Sat, 15 Jun 2013 19:33:15 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/GaoKC13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2013,\\n  editor    = {Maria Paola Bonacina},\\n  booktitle = {24th International Conference on Automated Deduction ({CADE-24})},\\n  series    = lncs,\\n  volume    = {7898},\\n  publisher = {Springer},\\n  year      = {2013},\\n\\n\\n\\n  timestamp = {Sat, 15 Jun 2013 19:30:45 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2013},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/sttt/Frehse08,\\n  author    = {Goran Frehse},\\n  title     = {{PHAVer}: algorithmic verification of hybrid systems past {HyTech}},\\n  journal   = {{STTT}},\\n  volume    = {10},\\n  number    = {3},\\n  pages     = {263--279},\\n  year      = {2008},\\n\\n\\n  timestamp = {Tue, 25 Nov 2008 14:46:58 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/sttt/Frehse08},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/Paulson10,\\n  author    = {Lawrence C. Paulson},\\n  title     = {Three Years of Experience with Sledgehammer, a Practical Link between\\n               Automatic and Interactive Theorem Provers},\\n  booktitle = {Proceedings of the 2nd Workshop on Practical Aspects of Automated\\n               Reasoning, PAAR-2010, Edinburgh, Scotland, UK, July 14, 2010},\\n  pages     = {1--10},\\n  year      = {2010},\\n  crossref  = {DBLP:conf/cade/2010paar},\\n  opturl       = {http://www.easychair.org/publications/?page=560965337},\\n  timestamp = {Fri, 15 Jan 2016 13:43:38 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/Paulson10},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2010paar,\\n  editor    = {Renate A. Schmidt and\\n               Stephan Schulz and\\n               Boris Konev},\\n  title     = {Proceedings of the 2nd Workshop on Practical Aspects of Automated\\n               Reasoning, PAAR-2010, Edinburgh, Scotland, UK, July 14, 2010},\\n  series    = {EPiC Series},\\n  volume    = {9},\\n  publisher = {EasyChair},\\n  year      = {2012},\\n  opturl       = {http://www.easychair.org/publications/?page=453348161},\\n  timestamp = {Fri, 15 Jan 2016 13:43:38 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2010paar},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/mkm/BarrasGHRTWW13,\\n  author    = {Bruno Barras and\\n               Lourdes Del Carmen Gonz{\\\\\\'{a}}lez{-}Huesca and\\n               Hugo Herbelin and\\n               Yann R{\\\\\\'{e}}gis{-}Gianas and\\n               Enrico Tassi and\\n               Makarius Wenzel and\\n               Burkhart Wolff},\\n  title     = {Pervasive Parallelism in Highly-Trustable Interactive Theorem Proving\\n               Systems},\\n  booktitle = {Intelligent Computer Mathematics - MKM, Calculemus, DML, and Systems\\n               and Projects 2013},\\n  pages     = {359--363},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/mkm/2013},\\n\\n  timestamp = {Sun, 04 Jun 2017 10:10:26 +0200},\\n  biburl    = {http://dblp2.uni-trier.de/rec/bib/conf/mkm/BarrasGHRTWW13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/mkm/2013,\\n  editor    = {Jacques Carette and\\n               David Aspinall and\\n               Christoph Lange and\\n               Petr Sojka and\\n               Wolfgang Windsteiger},\\n  title     = {Intelligent Computer Mathematics - MKM, Calculemus, DML, and Systems and Projects 2013, Held as Part of {CICM} 2013, Bath, UK, July 8-12,\\n               2013. Proceedings},\\n  series    = lncs,\\n  volume    = {7961},\\n  publisher = {Springer},\\n  year      = {2013},\\n\\n\\n\\n  timestamp = {Wed, 09 Sep 2015 16:45:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/mkm/2013},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@book{DBLP:books/daglib/0035083,\\n  author    = {Adam Chlipala},\\n  title     = {Certified Programming with Dependent Types - {A} Pragmatic Introduction\\n               to the Coq Proof Assistant},\\n  publisher = {{MIT} Press},\\n  year      = {2013},\\n  opturl       = {http://mitpress.mit.edu/books/certified-programming-dependent-types},\\n\\n  timestamp = {Mon, 11 May 2015 16:37:23 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/books/daglib/0035083},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n\\n\\n@inproceedings{DBLP:journals/corr/LeinoW14,\\n  author    = {K. Rustan M. Leino and\\n               Valentin W{\\\\\"{u}}stholz},\\n  title     = {The Dafny Integrated Development Environment},\\n  booktitle = {Proceedings 1st Workshop on Formal Integrated Development Environment\\n               ({F-IDE} 2014)},\\n  pages     = {3--15},\\n  year      = {2014},\\n  crossref  = {DBLP:journals/corr/DuboisGM14},\\n\\n  timestamp = {Wed, 12 Sep 2018 01:05:15 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/LeinoW14},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:journals/corr/DuboisGM14,\\n  editor    = {Catherine Dubois and\\n               Dimitra Giannakopoulou and\\n               Dominique M{\\\\\\'{e}}ry},\\n  title     = {Proceedings 1st Workshop on Formal Integrated Development Environment,\\n               {F-IDE} 2014, Grenoble, France, April 6, 2014},\\n  series    = {{EPTCS}},\\n  volume    = {149},\\n  year      = {2014},\\n\\n  timestamp = {Wed, 12 Sep 2018 01:05:12 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/DuboisGM14},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/vstte/AhrendtBBBGGHHHKMSSU14,\\n  author    = {Wolfgang Ahrendt and\\n               Bernhard Beckert and\\n               Daniel Bruns and\\n               Richard Bubel and\\n               Christoph Gladisch and\\n               Sarah Grebing and\\n               Reiner H{\\\\\"{a}}hnle and\\n               Martin Hentschel and\\n               Mihai Herda and\\n               Vladimir Klebanov and\\n               Wojciech Mostowski and\\n               Christoph Scheben and\\n               Peter H. Schmitt and\\n               Mattias Ulbrich},\\n  title     = {The {KeY} Platform for Verification and Analysis of {Java} Programs},\\n  booktitle = {Verified Software: Theories, Tools and Experiments - 6th International\\n               Conference ({VSTTE} 2014)},\\n  pages     = {55--71},\\n  year      = {2014},\\n  crossref  = {DBLP:conf/vstte/2014},\\n\\n\\n  timestamp = {Sun, 26 Oct 2014 19:11:02 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/vstte/AhrendtBBBGGHHHKMSSU14},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/icons/HerdeEFT08,\\n  author    = {Christian Herde and\\n               Andreas Eggers and\\n               Martin Fr{\\\\\"{a}}nzle and\\n               Tino Teige},\\n  title     = {Analysis of Hybrid Systems Using {HySAT}},\\n  booktitle = {The Third International Conference on Systems ({ICONS} 2008)},\\n  pages     = {196--201},\\n  year      = {2008},\\n  crossref  = {DBLP:conf/icons/2008},\\n\\n  timestamp = {Tue, 23 May 2017 01:11:30 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icons/HerdeEFT08},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/icons/2008,\\n  title     = {The Third International Conference on Systems, {ICONS} 2008, April\\n               13-18, 2008, Cancun, Mexico},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {2008},\\n\\n  timestamp = {Mon, 06 Jul 2015 16:45:33 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icons/2008},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@article{DBLP:journals/fmsd/FranzleH07,\\n  author    = {Martin Fr{\\\\\"{a}}nzle and\\n               Christian Herde},\\n  title     = {HySAT: An efficient proof engine for bounded model checking of hybrid\\n               systems},\\n  journal   = {Formal Methods in System Design},\\n  volume    = {30},\\n  number    = {3},\\n  pages     = {179--198},\\n  year      = {2007},\\n\\n  timestamp = {Sat, 27 May 2017 14:24:04 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/fmsd/FranzleH07},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@article{DBLP:journals/oms/KurzhanskiV02,\\n  author    = {Alexander B. Kurzhanski and\\n               Pravin Varaiya},\\n  title     = {On Ellipsoidal Techniques for Reachability Analysis. Part {I:} External\\n               Approximations},\\n  journal   = {Optimization Methods and Software},\\n  volume    = {17},\\n  number    = {2},\\n  pages     = {177--206},\\n  year      = {2002},\\n\\n\\n  timestamp = {Fri, 10 Nov 2017 14:11:10 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/oms/KurzhanskiV02},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/oms/KurzhanskiV02a,\\n  author    = {Alexander B. Kurzhanski and\\n               Pravin Varaiya},\\n  title     = {On Ellipsoidal Techniques for Reachability Analysis. Part {II:} Internal\\n               Approximations Box-valued Constraints},\\n  journal   = {Optimization Methods and Software},\\n  volume    = {17},\\n  number    = {2},\\n  pages     = {207--237},\\n  year      = {2002},\\n\\n\\n  timestamp = {Fri, 10 Nov 2017 14:11:10 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/oms/KurzhanskiV02a},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/hybrid/Girard05,\\n  author    = {Antoine Girard},\\n  title     = {Reachability of Uncertain Linear Systems Using Zonotopes},\\n  booktitle = {Hybrid Systems: Computation and Control, 8th International Workshop,\\n               {HSCC} 2005, Zurich, Switzerland, March 9-11, 2005, Proceedings},\\n  pages     = {291--305},\\n  year      = {2005},\\n  crossref  = {DBLP:conf/hybrid/2005},\\n  timestamp = {Tue, 30 May 2017 16:36:53 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/Girard05},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/hybrid/2005,\\n  editor    = {Manfred Morari and\\n               Lothar Thiele},\\n  title     = {Hybrid Systems: Computation and Control, 8th International Workshop,\\n               {HSCC} 2005, Zurich, Switzerland, March 9-11, 2005, Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {3414},\\n  publisher = {Springer},\\n  year      = {2005},\\n\\n\\n  timestamp = {Tue, 30 May 2017 16:36:53 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/2005},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n\\n@proceedings{DBLP:conf/vstte/2014,\\n  editor    = {Dimitra Giannakopoulou and\\n               Daniel Kroening},\\n  title     = {Verified Software: Theories, Tools and Experiments - 6th International\\n               Conference, {VSTTE} 2014, Vienna, Austria, July 17-18, 2014, Revised\\n               Selected Papers},\\n  series    = lncs,\\n  volume    = {8471},\\n  publisher = {Springer},\\n  year      = {2014},\\n\\n\\n\\n  timestamp = {Wed, 15 Oct 2014 16:26:44 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/vstte/2014},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/MouraKADR15,\\n  author    = {Leonardo Mendon{\\\\c{c}}a de Moura et. al.},\\n  title     = {The {Lean} Theorem Prover (System Description)},\\n  booktitle = {{CADE-25}},\\n  pages     = {378--388},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/cade/2015},\\n\\n\\n  timestamp = {Fri, 31 Jul 2015 13:18:27 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/MouraKADR15},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2015,\\n  title     = {Conference on Automated Deduction - {CADE-25}},\\n  series    = lncs,\\n  volume    = {9195},\\n  publisher = {Springer},\\n  year      = {2015},\\n\\n\\n\\n  timestamp = {Fri, 31 Jul 2015 13:15:07 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2015},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/PlatzerQR09,\\n  author    = {Andr{\\\\\\'{e}} Platzer and\\n               Jan{-}David Quesel and\\n               Philipp R{\\\\\"{u}}mmer},\\n  title     = {Real World Verification},\\n  booktitle = {CADE\\'22},\\n  pages     = {485--501},\\n  year      = {2009},\\n  crossref  = {DBLP:conf/cade/2009},\\n\\n\\n  timestamp = {Wed, 29 Jul 2009 15:18:30 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/PlatzerQR09},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2009,\\n  editor    = {Renate A. Schmidt},\\n  title     = {Automated Deduction - CADE-22, 22nd International Conference on Automated\\n               Deduction, Montreal, Canada, August 2-7, 2009. Proceedings},\\n  series    = lncs,\\n  volume    = {5663},\\n  publisher = {Springer},\\n  year      = {2009},\\n\\n\\n\\n  timestamp = {Wed, 29 Jul 2009 15:18:20 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2009},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/tacas/JeanninGKGSZP15,\\n  author    = {Jean{-}Baptiste Jeannin and\\n               Khalil Ghorbal and\\n\\t\\t\\t   Yanni Kouskoulas and\\n\\t\\t\\t   Ryan Gardner and\\n\\t\\t\\t   Aurora Schmidt and\\n\\t\\t\\t   Erik Zawadzki and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {A Formally Verified Hybrid System for the\\n               Next-generation Airborne Collision Avoidance System},\\n  booktitle = {TACAS},\\n  year      = {2015},\\n  pages     = {21-36},\\n\\n  editor    = {Christel Baier and\\n               Cesare Tinelli},\\n  longbooktitle = {Tools and Algorithms for the Construction\\n               and Analysis of Systems - 21st International\\n\\t\\t\\t   Conference, TACAS 2015, London, UK, April\\n\\t\\t\\t   11-18, 2015, Proceedings},\\n  series    = lncs,\\n  volume    = {9035},\\n  publisher = {Springer},\\n}\\n\\n@inproceedings{DBLP:conf/cade/HeiselRS90,\\n  author    = {Maritta Heisel and\\n               Wolfgang Reif and\\n               Werner Stephan},\\n  title     = {Tactical Theorem Proving in Program Verification},\\n  booktitle = {CADE},\\n  optbooktitle = {10th International Conference on Automated Deduction, Kaiserslautern,\\n               FRG, July 24-27, 1990, Proceedings},\\n  pages     = {117--131},\\n  year      = {1990},\\n  crossref  = {DBLP:conf/cade/1990},\\n\\n\\n  timestamp = {Tue, 07 Oct 2014 08:56:44 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/HeiselRS90},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/cade/FultonMQVP15,\\n  author    = {Nathan Fulton and\\n               Stefan Mitsch and\\n               Jan-David Quesel and\\n               Marcus V{\\\\\"o}lp and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {{KeYmaera X}: An Axiomatic Tactical Theorem Prover\\n               for Hybrid Systems},\\n  booktitle = {CADE},\\n  longbooktitle = {International Conference on Automated\\n               Deduction},\\n  year      = {2015}\\n}\\n@proceedings{DBLP:conf/cade/1990,\\n  editor    = {Mark E. Stickel},\\n  title     = {10th International Conference on Automated Deduction, Kaiserslautern,\\n               FRG, July 24-27, 1990, Proceedings},\\n  series    = lncs,\\n  volume    = {449},\\n  publisher = {Springer},\\n  year      = {1990},\\n\\n  timestamp = {Wed, 03 Jul 2002 09:55:41 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/1990},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/FeltyH94,\\n  author    = {Amy P. Felty and\\n               Douglas J. Howe},\\n  title     = {Tactic Theorem Proving with Refinement-Tree Proofs and Metavariables},\\n  booktitle = {CADE},\\n  longbooktitle = {Automated Deduction - CADE-12, 12th International Conference on Automated\\n               Deduction, Nancy, France, June 26 - July 1, 1994, Proceedings},\\n  pages     = {605--619},\\n  year      = {1994},\\n  crossref  = {DBLP:conf/cade/1994},\\n\\n\\n  timestamp = {Mon, 12 Oct 2009 13:28:26 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/FeltyH94},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/1994,\\n  editor    = {Alan Bundy},\\n  title     = {Automated Deduction - CADE-12, 12th International Conference on Automated\\n               Deduction, Nancy, France, June 26 - July 1, 1994, Proceedings},\\n  series    = lncs,\\n  volume    = {814},\\n  publisher = {Springer},\\n  year      = {1994},\\n\\n  timestamp = {Wed, 03 Jul 2002 09:55:41 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/1994},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@book{KeYBook,\\n author = {Beckert, Bernhard and H\\\\\"{a}hnle, Reiner and Schmitt, Peter H.},\\n title = {Verification of Object-oriented Software: The KeY Approach},\\n year = {2007},\\n\\n publisher = {Springer-Verlag},\\n address = {Berlin, Heidelberg}\\n}\\n\\n@ARTICLE{DBLP:journals/jar/Platzer08,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Differential Dynamic Logic for Hybrid\\n               Systems.},\\n  journal   = {J. Autom. Reas.},\\n  longjournal = {Journal of Automated Reasoning},\\n  year      = {2008},\\n  volume    = {41},\\n  number    = {2},\\n  pages     = {143-189},\\n\\n  keywords  = {dynamic logic, differential equations,\\n               sequent calculus, axiomatisation, automated\\n               theorem proving, verification of hybrid\\n               systems},\\n  abstract  = {\\n    Hybrid systems are models for complex physical systems\\n    and are defined as dynamical systems with interacting\\n    discrete transitions and continuous evolutions along\\n    differential equations. With the goal of developing a\\n    theoretical and practical foundation for deductive\\n    verification of hybrid systems, we introduce a dynamic\\n    logic for hybrid programs, which is a program notation\\n    for hybrid systems. As a verification technique that is\\n    suitable for automation, we introduce a free variable\\n    proof calculus with a novel combination of real-valued\\n    free variables and Skolemisation for lifting quantifier\\n    elimination for real arithmetic to dynamic logic. The\\n    calculus is compositional, i.e., it reduces properties\\n    of hybrid programs to properties of their parts. Our\\n    main result proves that this calculus axiomatises the\\n    transition behaviour of hybrid systems completely\\n    relative to differential equations. In a case study\\n    with cooperating traffic agents of the European Train\\n    Control System, we further show that our calculus is\\n    well-suited for verifying realistic hybrid systems with\\n    parametric system dynamics.\\n  }\\n}\\n\\n@ARTICLE{DBLP:journals/corr/Platzer14:dGL,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Differential Game Logic},\\n  journal   = {CoRR},\\n  volume    = {abs/1408.1980},\\n  year      = {2014},\\n  eprint    = {1408.1980}\\n}\\n\\n\\n@TECHREPORT{Platzer13:dGL,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {A Complete Axiomatization for Differential\\n               Game Logic for Hybrid Games},\\n  number    = {CMU-CS-13-100R},\\n  year      = {2013},\\n  month     = {January},\\n  institution = {School of Computer Science,\\n               Carnegie Mellon University},\\n  note      = {Extended in revised version from July 2013},\\n  pdf = {http://reports-archive.adm.cs.cmu.edu/anon/2013/CMU-CS-13-100R.pdf}\\n}\\n\\n@ARTICLE{DBLP:journals/corr/abs-1205-4788,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Dynamic Logics of Dynamical Systems},\\n  journal   = {CoRR},\\n  volume    = {\\\\href{http://arxiv.org/abs/1205.4788}{abs/1205.4788}},\\n  year      = {2012},\\n  OPTurl       = {http://arxiv.org/abs/1205.4788},\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/lics/Platzer12a,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Logics of Dynamical Systems},\\n  booktitle = {LICS},\\n  year      = {2012},\\n  pages     = {13-24},\\n\\n  longbooktitle = {Proceedings of the 27th Annual ACM/IEEE\\n               Symposium on Logic in Computer Science, LICS\\n               2012, Dubrovnik, Croatia, June 25???28, 2012},\\n  publisher = {IEEE},\\n\\n  keywords  = {logic of dynamical systems, dynamic logic,\\n               differential dynamic logic, hybrid systems,\\n               axiomatization, deduction},\\n  abstract  = {\\n    We study the logic of dynamical systems, that is,\\n    logics and proof principles for properties of dynamical\\n    systems. Dynamical systems are mathematical models\\n    describing how the state of a system evolves over time.\\n    They are important in modeling and understanding many\\n    applications, including embedded systems and\\n    cyber-physical systems. In discrete dynamical systems,\\n    the state evolves in discrete steps, one step at a\\n    time, as described by a difference equation or discrete\\n    state transition relation. In continuous dynamical\\n    systems, the state evolves continuously along a\\n    function, typically described by a differential\\n    equation. Hybrid dynamical systems or hybrid systems\\n    combine both discrete and continuous dynamics.\\n\\n     This is a brief survey of differential dynamic logic\\n    for specifying and verifying properties of hybrid\\n    systems. We explain hybrid system models, differential\\n    dynamic logic, its semantics, and its axiomatization\\n    for proving logical formulas about hybrid systems. We\\n    study differential invariants, i.e., induction\\n    principles for differential equations. We briefly\\n    survey theoretical results, including soundness and\\n    completeness and deductive power. Differential dynamic\\n    logic has been implemented in automatic and interactive\\n    theorem provers and has been used successfully to\\n    verify safety-critical applications in automotive,\\n    aviation, railway, robotics, and analogue electrical\\n    circuits.}\\n}\\n\\n@inproceedings{DBLP:conf/fm/MitschQP14,\\n  author    = {Stefan Mitsch and\\n               Jan-David Quesel and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {Refactoring, Refinement, and Reasoning - A Logical Characterization\\n               for Hybrid Systems},\\n  booktitle = {FM},\\n  year      = {2014},\\n  pages     = {481-496},\\n\\n  bibsource = {DBLP, http://dblp.uni-trier.de}\\n}\\n@proceedings{DBLP:conf/fm/2014,\\n  editor    = {Cliff B. Jones and\\n               Pekka Pihlajasaari and\\n               Jun Sun},\\n  title     = {FM 2014: Formal Methods - 19th International Symposium,\\n               Singapore, May 12-16, 2014. Proceedings},\\n  booktitle = {FM},\\n  publisher = {Springer},\\n  series    = lncs,\\n  volume    = {8442},\\n  year      = {2014},\\n\\n\\n  bibsource = {DBLP, http://dblp.uni-trier.de}\\n}\\n\\n\\n@inproceedings{DBLP:conf/hybrid/AlurCHH92,\\n  author    = {Rajeev Alur and\\n               Costas Courcoubetis and\\n               Thomas A. Henzinger and\\n               Pei{-}Hsin Ho},\\n  title     = {Hybrid Automata: An Algorithmic Approach to the Specification and\\n               Verification of Hybrid Systems},\\n  booktitle = {Hybrid Systems},\\n  pages     = {209--229},\\n  year      = {1992},\\n  crossref  = {DBLP:conf/hybrid/1992},\\n\\n\\n  timestamp = {Tue, 14 Jun 2011 20:33:33 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/hybrid/AlurCHH92},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\t\\n\\t@proceedings{DBLP:conf/hybrid/1992,\\n  editor    = {Robert L. Grossman and\\n               Anil Nerode and\\n               Anders P. Ravn and\\n               Hans Rischel},\\n  title     = {Hybrid Systems},\\n  series    = lncs,\\n  volume    = {736},\\n  publisher = {Springer},\\n  year      = {1993},\\n\\n  timestamp = {Thu, 09 Jan 2003 11:08:03 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/hybrid/1992},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@book{Nipkow:2002:IPA:1791547,\\n author = {Nipkow, Tobias and Wenzel, Markus and Paulson, Lawrence C.},\\n title = {{Isabelle}/{HOL}: A Proof Assistant for Higher-order Logic},\\n year = {2002},\\n\\n publisher = {Springer-Verlag},\\n address = {Berlin, Heidelberg},\\n}\\n\\n@inproceedings{DBLP:conf/birthday/FosterW17,\\n  author    = {Simon Foster and\\n               Jim Woodcock},\\n  title     = {Towards Verification of Cyber-Physical Systems with {UTP} and {Isabelle/HOL}},\\n  booktitle = {Concurrency, Security, and Puzzles - Essays Dedicated to Andrew William\\n               Roscoe on the Occasion of His 60th Birthday},\\n  pages     = {39--64},\\n  year      = {2017},\\n  crossref  = {DBLP:conf/birthday/2017roscoe},\\n\\n\\n  timestamp = {Mon, 06 Nov 2017 12:14:22 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/birthday/FosterW17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/birthday/2017roscoe,\\n  editor    = {Thomas Gibson{-}Robinson and\\n               Philippa J. Hopcroft and\\n               Ranko Lazic},\\n  title     = {Concurrency, Security, and Puzzles - Essays Dedicated to Andrew William\\n               Roscoe on the Occasion of His 60th Birthday},\\n  series    = {{LNCS}},\\n  volume    = {10160},\\n  publisher = {Springer},\\n  year      = {2017},\\n\\n\\n\\n  timestamp = {Mon, 06 Nov 2017 12:14:22 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/birthday/2017roscoe},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/cade/PlatzerQ08,\\n  author    = {Andr{\\\\\\'{e}} Platzer and\\n               Jan{-}David Quesel},\\n  title     = {{KeYmaera}: {A} Hybrid Theorem Prover for Hybrid Systems (System Description)},\\n  booktitle = {IJCAR 2008},\\n  pages     = {171--178},\\n  year      = {2008},\\n  crossref  = {DBLP:conf/cade/2008},\\n\\n\\n  timestamp = {Fri, 05 Sep 2008 08:28:33 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/PlatzerQ08},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2008,\\n  editor    = {Alessandro Armando and\\n               Peter Baumgartner and\\n               Gilles Dowek},\\n  title     = {Automated Reasoning, 4th International Joint Conference, {IJCAR} 2008,\\n               Sydney, Australia, August 12-15, 2008, Proceedings},\\n  series    = lncs,\\n  volume    = {5195},\\n  publisher = {Springer},\\n  year      = {2008},\\n\\n  timestamp = {Fri, 05 Sep 2008 08:22:54 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2008},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@ARTICLE{BowenStavridou,\\nauthor={Bowen, J. and Stavridou, V.},\\njournal={Software Engineering Journal},\\ntitle={Safety-critical systems, formal methods and standards},\\nyear={1993},\\nmonth={Jul},\\nvolume={8},\\nnumber={4},\\npages={189-209},\\nkeywords={formal specification;real-time systems;safety;software reliability;standards;embedded computer-based systems;formal methods;real-time systems;safety-critical systems;software crisis;standards},\\n}\\n\\n@article{DBLP:journals/sosym/AhrendtBBBGHMMRSS05,\\n  author    = {Wolfgang Ahrendt and\\n               Thomas Baar and\\n               Bernhard Beckert and\\n               Richard Bubel and\\n               Martin Giese and\\n               Reiner H{\\\\\"{a}}hnle and\\n               Wolfram Menzel and\\n               Wojciech Mostowski and\\n               Andreas Roth and\\n               Steffen Schlager and\\n               Peter H. Schmitt},\\n  title     = {The {K}e{Y} tool},\\n  journal   = {Software and System Modeling},\\n  volume    = {4},\\n  number    = {1},\\n  pages     = {32--54},\\n  year      = {2005},\\n  OPTurl       = {http://www.springerlink.com/index/10.1007/s10270-004-0058-x},\\n  timestamp = {Fri, 22 Apr 2005 10:31:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/sosym/AhrendtBBBGHMMRSS05},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/fm/LoosPN11,\\n  author    = {Sarah M. Loos and\\n               Andr{\\\\\\'{e}} Platzer and\\n               Ligia Nistor},\\n  title     = {Adaptive Cruise Control: Hybrid, Distributed, and Now Formally Verified},\\n  booktitle = {International Symposium on Formal\\n               Methods},\\n  year      = {2011}\\n}\\n\\n@inproceedings{DBLP:conf/iccps/MitschLP12,\\n  author    = {Stefan Mitsch and\\n               Sarah M. Loos and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {Towards Formal Verification of Freeway Traffic Control},\\n  booktitle = {2012 {IEEE/ACM} Third International Conference on Cyber-Physical Systems,\\n               Beijing, China, April 17-19, 2012},\\n  pages     = {171--180},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/iccps/2012},\\n  OPTurl       = {http://doi.ieeecomputersociety.org/10.1109/ICCPS.2012.25},\\n\\n  timestamp = {Thu, 12 Jul 2012 09:29:10 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/iccps/MitschLP12},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/hybrid/LoosRP13,\\n  author    = {Sarah M. Loos and\\n               David W. Renshaw and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {Formal verification of distributed aircraft controllers},\\n  booktitle = {Proceedings of the 16th international conference on Hybrid systems:\\n               computation and control, {HSCC} 2013, April 8-11, 2013, Philadelphia,\\n               PA, {USA}},\\n  pages     = {125--130},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/hybrid/2013},\\n  opturl       = {http://doi.acm.org/10.1145/2461328.2461350},\\n\\n  timestamp = {Fri, 03 May 2013 21:31:34 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/hybrid/LoosRP13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/jacic/GhorbalJZPGC14,\\n  author    = {Khalil Ghorbal and\\n               Jean{-}Baptiste Jeannin and\\n               Erik Zawadzki and\\n               Andr{\\\\\\'{e}} Platzer and\\n               Geoffrey J. Gordon and\\n               Peter Capell},\\n  title     = {Hybrid Theorem Proving of Aerospace Systems: Applications and Challenges},\\n  journal   = {J. Aerospace Inf. Sys.},\\n  volume    = {11},\\n  number    = {10},\\n  pages     = {702--713},\\n  year      = {2014},\\n\\n\\n  timestamp = {Mon, 24 Nov 2014 14:34:19 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jacic/GhorbalJZPGC14},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@TECHREPORT{JeanninGKGSZP14:TR,\\n  author    = {Jean-Baptiste Jeannin and\\n               Khalil Ghorbal and\\n               Yanni Kouskoulas and\\n               Ryan Garnder and\\n               Aurora Schmidt and\\n               Erik Zawadzki and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {A Formally Verified Hybrid System for the\\n               Next-Generation Airborne Collision Avoidance System},\\n  number    = {CMU-CS-14-138},\\n  year      = {2014},\\n  month     = {},\\n  institution = {School of Computer Science,\\n               Carnegie Mellon University},\\n  address   = {Pittsburgh, PA},\\n  pdf = {http://reports-archive.adm.cs.cmu.edu/anon/2014/CMU-CS-14-138.pdf}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/rss/MitschGP13,\\n  author    = {Stefan Mitsch and\\n               Khalil Ghorbal and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {On Provably Safe Obstacle Avoidance for\\n               Autonomous Robotic Ground Vehicles},\\n  booktitle = {Robotics: Science and Systems},\\n  year      = {2013},\\n  editor    = {Paul Newman and\\n               Dieter Fox and\\n               David Hsu},\\n  longbooktitle = {Robotics: Science and Systems IX,\\n               Technische Universit{\\\\\"a}t Berlin,\\n               Berlin, Germany, June 24 - June 28,\\n               2013},\\n\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/hybrid/KouskoulasRPK13,\\n  author    = {Yanni Kouskoulas and\\n               David W. Renshaw and\\n               Andr{\\\\\\'e} Platzer and\\n               Peter Kazanzides},\\n  title     = {Certifying the Safe Design of a Virtual\\n               Fixture Control Algorithm for a Surgical\\n               Robot},\\n  year      = {2013},\\n  pages     = {263-272},\\n\\n  publisher = {ACM},\\n  editor    = {Calin Belta and\\n               Franjo Ivancic},\\n  booktitle = {Hybrid Systems: Computation and Control\\n               ({HSCC}\\'13)},\\n}\\n\\n@inproceedings{DBLP:conf/lics/Henzinger96,\\n  author    = {Thomas A. Henzinger},\\n  title     = {The Theory of Hybrid Automata},\\n  booktitle = {Proceedings, 11th Annual {IEEE} Symposium on Logic in Computer Science},\\n  pages     = {278--292},\\n  year      = {1996},\\n  crossref  = {DBLP:conf/lics/1996},\\n\\n\\n  timestamp = {Sat, 22 Nov 2014 13:44:45 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/lics/Henzinger96},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@BOOK{Platzer10,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Logical Analysis of Hybrid Systems:\\n               Proving Theorems for Complex Dynamics},\\n  publisher = {Springer},\\n  address   = {Heidelberg},\\n  year      = {2010},\\n\\n\\n}\\n\\n@article{DBLP:journals/sttt/QueselMLAP16,\\n  author    = {Jan{-}David Quesel and\\n               Stefan Mitsch and\\n               Sarah M. Loos and\\n               Nikos Arechiga and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {How to model and prove hybrid systems with {KeYmaera}: a tutorial on\\n               safety},\\n  journal   = {{STTT}},\\n  volume    = {18},\\n  number    = {1},\\n  pages     = {67--91},\\n  year      = {2016},\\n\\n\\n  timestamp = {Thu, 08 Jun 2017 09:05:41 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/sttt/QueselMLAP16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@INPROCEEDINGS{DBLP:conf/vmcai/GhorbalSP15,\\n  author    = {Khalil Ghorbal and\\n               Andrew Sogokon and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {A Hierarchy of Proof Rules for Checking\\n               Differential Invariance of Algebraic Sets},\\n  booktitle = {VMCAI},\\n  year      = {2015},\\n  pages     = {431-448},\\n\\n  editor    = {Deepak D\\'Souza and\\n               Akash Lal and\\n               Kim Guldstrand Larsen},\\n  longbooktitle = {Verification, Model Checking, and Abstract\\n               Interpretation - 16th International Conference,\\n               {VMCAI} 2015, Mumbai, India, January 12-14, 2015,\\n               Proceedings},\\n  series    = lncs,\\n  volume    = {8931},\\n  publisher = {Springer},\\n}\\n\\n\\n\\n@INPROCEEDINGS{DBLP:conf/icfem/PlatzerQ09,\\n  author    = {Andr{\\\\\\'e} Platzer and\\n               Jan-David Quesel},\\n  title     = {{European Train Control System}:\\n               A Case Study in Formal Verification},\\n  booktitle = {ICFEM},\\n  year      = {2009},\\n  pages     = {246-265},\\n\\n  editor    = {Karin Breitman and\\n               Ana Cavalcanti},\\n  longbooktitle = {Formal Methods and Software Engineering,\\n               11th International Conference on Formal\\n               Engineering Methods, ICFEM 2009,\\n               Rio de Janeiro, Brasil, December 9-12, 2009.\\n               Proceedings},\\n  booktitle = {ICFEM},\\n  publisher = {Springer},\\n  series    = lncs,\\n  volume    = {5885},\\n\\n  keywords  = {formal verification of hybrid systems,\\n               train control, theorem proving,\\n               parameter constraint identification,\\n               disturbances},\\n  abstract  = {\\n    Complex physical systems have several degrees of\\n    freedom. They only work correctly when their control\\n    parameters obey corresponding constraints. Based on the\\n    informal specification of the European Train Control\\n    System (ETCS), we design a controller for its\\n    cooperation protocol. For its free parameters, we\\n    successively identify constraints that are required to\\n    ensure collision freedom. We formally prove the\\n    parameter constraints to be sharp by characterizing\\n    them equivalently in terms of reachability properties\\n    of the hybrid system dynamics. Using our deductive\\n    verification tool KeYmaera, we formally verify\\n    controllability, safety, liveness, and reactivity\\n    properties of the ETCS protocol that entail collision\\n    freedom. We prove that the ETCS protocol remains\\n    correct even in the presence of perturbation by\\n    disturbances in the dynamics. We verify that safety is\\n    preserved when a PI controlled speed supervision is\\n    used.}\\n}\\n\\n@Misc{symbolarisCaseStudies,\\n  author = \\t {A. Platzer},\\n  title = \\t {Tools for Logic, Computer Science {\\\\&} Mathematics},\\n  month = \\t {Feb.},\\n  year = \\t {2015},\\n  opturl = \\t {http://symbolaris.com/info/index.html}\\n}\\n\\n@proceedings{DBLP:conf/lics/1996,\\n  title     = {Proceedings, 11th Annual {IEEE} Symposium on Logic in Computer Science,\\n               New Brunswick, New Jersey, USA, July 27-30, 1996},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {1996},\\n\\n  timestamp = {Tue, 15 Sep 4453316 17:28:00 +},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/lics/1996},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@proceedings{DBLP:conf/hybrid/2002,\\n\\tEditor = {Tomlin, Claire and Greenstreet, Mark R.},\\n\\tPublisher = {Springer},\\n\\tTitle = {Hybrid Systems: Computation and Control, 5th International Workshop, HSCC 2002, Stanford, CA, USA, March 25-27, 2002, Proceedings},\\n\\tVolume = {2289},\\n\\tYear = {2002}\\n}\\n\\n\\n\\n% eh? -> this is what they ask us to cite.\\n@Manual{Coq:manual,\\n  title =        {The Coq proof assistant reference manual},\\n  author =       {\\\\mbox{The Coq development team}},\\n  organization = {LogiCal Project},\\n  note =         {Version 8.0},\\n  year =         {2004},\\n\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/rv/MitschP14,\\n  author    = {Stefan Mitsch and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {{ModelPlex}: Verified Runtime Validation\\n               of Verified Cyber-Physical System Models},\\n  booktitle = {RV},\\n  year      = {2014},\\n  pages     = {199-214},\\n\\n  editor    = {Borzoo Bonakdarpour and\\n               Scott A. Smolka},\\n  longbooktitle = {Runtime Verification - 5th\\n               International Conference, RV\\n               2014, Toronto, ON, Canada, September 22--25,\\n               2014. Proceedings},\\n  publisher = {Springer},\\n  series    = lncs,\\n  volume    = {8734},\\n}\\n\\n@article{Boldo+Lelay+Melquiond:mcs:coquelicot:2015,\\n  author    = {Sylvie Boldo and\\n               Catherine Lelay and\\n               Guillaume Melquiond},\\n  title     = {Coquelicot: {A} User-Friendly Library of Real Analysis for {Coq}},\\n  journal   = {Mathematics in Computer Science},\\n  volume    = {9},\\n  number    = {1},\\n  pages     = {41--62},\\n  year      = {2015},\\n\\n\\n  timestamp = {Mon, 09 Feb 2015 13:20:22 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/mics/BoldoLM15},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/abs-1106-3448,\\n  author    = {Robbert Krebbers and\\n               Bas Spitters},\\n  title     = {Type classes for efficient exact real arithmetic in {Coq}},\\n  journal   = {Logical Methods in Computer Science},\\n  volume    = {9},\\n  number    = {1},\\n  year      = {2011},\\n\\n\\n  timestamp = {Wed, 14 Jun 2017 20:37:24 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1106-3448},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/tphol/Harrison05,\\n  author    = {John Harrison},\\n  title     = {A {HOL} {Theory of Euclidean Space}},\\n  booktitle = {TPHOLs},\\n  pages     = {114--129},\\n  year      = {2005},\\n  crossref  = {DBLP:conf/tphol/2005},\\n\\n\\n  timestamp = {Tue, 30 May 2017 16:36:53 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tphol/Harrison05},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/tphol/2005,\\n  editor    = {Joe Hurd and\\n               Thomas F. Melham},\\n  title     = {TPHOLs},\\n  series    = {LNCS},\\n  volume    = {3603},\\n  publisher = {Springer},\\n  year      = {2005},\\n\\n\\n\\n  timestamp = {Tue, 30 May 2017 16:36:53 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tphol/2005},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/itp/HolzlIH13,\\n  author    = {Johannes H{\\\\\"{o}}lzl and\\n               Fabian Immler and\\n               Brian Huffman},\\n  title     = {Type Classes and Filters for Mathematical Analysis in {Isabelle}/{HOL}},\\n  booktitle = {Fourth Conference on Interactive Theorem Proving ({ITP} 2013)},\\n  pages     = {279--294},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/itp/2013},\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:18:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/itp/HolzlIH13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/itp/2013,\\n  editor    = {Sandrine Blazy and\\n               Christine Paulin{-}Mohring and\\n               David Pichardie},\\n  title     = {Interactive Theorem Proving - 4th International Conference, {ITP}\\n               2013, Rennes, France, July 22-26, 2013. Proceedings},\\n  series    = {LNCS},\\n  volume    = {7998},\\n  publisher = {Springer},\\n  year      = {2013},\\n\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:18:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/itp/2013},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/itp/2016,\\n  editor    = {Jasmin Christian Blanchette and\\n               Stephan Merz},\\n  title     = {Interactive Theorem Proving - 7th International Conference, {ITP}\\n               2016, Nancy, France, August 22-25, 2016, Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {9807},\\n  publisher = {Springer},\\n  year      = {2016},\\n\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:18:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/itp/2016},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/itp/ImmlerT16,\\n  author    = {Fabian Immler and\\n               Christoph Traut},\\n  title     = {The Flow of {ODEs}},\\n  booktitle = {{ITP} 2016},\\n  editor    = {Jasmin Christian Blanchette and\\n               Stephan Merz},\\n  pages     = {184--199},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/itp/2016},\\n\\n\\n  timestamp = {Mon, 08 Aug 2016 15:11:37 +0200},\\n\\n  timestamp = {Wed, 24 Jul 2013 20:48:35 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/itp/HolzlIH13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{COLLINS1991299,\\n  author    = {George E. Collins and\\n               H. Hong},\\n  title     = {Partial Cylindrical Algebraic Decomposition for Quantifier Elimination},\\n  journal   = {J. Symb. Comput.},\\n  volume    = {12},\\n  number    = {3},\\n  pages     = {299--328},\\n  year      = {1991},\\n\\n\\n  timestamp = {Wed, 17 May 2017 14:25:48 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jsc/CollinsH91},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@article{DAVENPORT198829,\\n  author    = {James H. Davenport and\\n               Joos Heintz},\\n  title     = {Real Quantifier Elimination is Doubly Exponential},\\n  journal   = {J. Symb. Comput.},\\n  volume    = {5},\\n  number    = {1/2},\\n  pages     = {29--35},\\n  year      = {1988},\\n\\n\\n  timestamp = {Wed, 17 May 2017 14:25:49 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jsc/DavenportH88},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n%sic?\\n@inproceedings{DBLP:journals/corr/abs-1301-1702,\\n  added-at = {2013-05-24T00:00:00.000+0200},\\n  author = {Solovyev, Alexey and Hales, Thomas C.},\\n  biburl = {http://www.bibsonomy.org/bibtex/2717dc19a420cd5b70cdf0c37e8b551f9/dblp},\\n  booktitle = {NASA Formal Methods},\\n  crossref = {conf/nfm/2013},\\n  editor = {Brat, Guillaume and Rungta, Neha and Venet, Arnaud},\\n\\n  interhash = {e2e15494b42b815bce04f61f16b3a7eb},\\n  intrahash = {717dc19a420cd5b70cdf0c37e8b551f9},\\n\\n  keywords = {dblp},\\n  pages = {383-397},\\n  publisher = {Springer},\\n  series = {LNCS},\\n  timestamp = {2017-03-30T11:58:19.000+0200},\\n  title = {{Formal Verification of Nonlinear Inequalities with Taylor Interval Approximations}},\\n  volume = 7871,\\n  year = 2013\\n}\\n\\n@proceedings{DBLP:conf/cade/2015,\\n  editor    = {Amy P. Felty and\\n               Aart Middeldorp},\\n  title     = {Automated Deduction - {CADE-25} - 25th International Conference on\\n               Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {9195},\\n  publisher = {Springer},\\n  year      = {2015},\\n\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:17:17 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cade/2015},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/ecml/2006,\\n  editor    = {Johannes F{\\\\\"{u}}rnkranz and\\n               Tobias Scheffer and\\n               Myra Spiliopoulou},\\n  title     = {Machine Learning: {ECML} 2006, 17th European Conference on Machine\\n               Learning, Berlin, Germany, September 18-22, 2006, Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {4212},\\n  publisher = {Springer},\\n  year      = {2006},\\n\\n\\n\\n  timestamp = {Sat, 03 Jun 2017 13:16:18 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/ecml/2006},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/icml/2012,\\n  title     = {Proceedings of the 29th International Conference on Machine Learning,\\n               {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},\\n  publisher = {icml.cc / Omnipress},\\n  year      = {2012},\\n  timestamp = {Wed, 29 Mar 2017 16:45:25 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/2012},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/cav/2017-1,\\n  editor    = {Rupak Majumdar and\\n               Viktor Kuncak},\\n  title     = {Computer Aided Verification - 29th International Conference ({CAV} 2017)}, \\n  series    = {{LNCS}},\\n  volume    = {10426},\\n  publisher = {Springer},\\n  year      = {2017},\\n\\n\\n\\n  timestamp = {Fri, 14 Jul 2017 12:52:51 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/2017-1},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@proceedings{DBLP:conf/icml/2017,\\n  editor    = {Doina Precup and\\n               Yee Whye Teh},\\n  title     = {Proceedings of the 34th International Conference on Machine Learning,\\n               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},\\n  series    = {Proceedings of Machine Learning Research},\\n  volume    = {70},\\n  publisher = {{PMLR}},\\n  year      = {2017},\\n\\n  timestamp = {Wed, 16 Aug 2017 11:08:55 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/2017},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@proceedings{DBLP:conf/tphol/2003,\\n  editor    = {David A. Basin and\\n               Burkhart Wolff},\\n  title     = {Theorem Proving in Higher Order Logics, 16th International Conference},\\n  series    = {{LNCS}},\\n  volume    = {2758},\\n  publisher = {Springer},\\n  year      = {2003},\\n\\n\\n\\n  timestamp = {Mon, 29 May 2017 16:53:44 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tphol/2003},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/tphol/HickeyNCABBEGKKKLSWY03,\\n  author    = {Jason Hickey and\\n               Aleksey Nogin and\\n               Robert L. Constable and\\n               Brian E. Aydemir and\\n               Eli Barzilay and\\n               Yegor Bryukhov and\\n               Richard Eaton and\\n               Adam Granicz and\\n               Alexei Kopylov and\\n               Christoph Kreitz and\\n               Vladimir Krupski and\\n               Lori Lorigo and\\n               Stephan Schmitt and\\n               Carl Witty and\\n               Xin Yu},\\n  title     = {{MetaPRL} - {A} Modular Logical Environment},\\n  booktitle = {TPHOLs},\\n  pages     = {287--303},\\n  year      = {2003},\\n  crossref  = {DBLP:conf/tphol/2003},\\n\\n  timestamp = {Mon, 29 May 2017 16:53:44 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tphol/HickeyNCABBEGKKKLSWY03},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n\\n\\n\\n\\n@inproceedings{katz2017reluplex,\\n  title={Reluplex: An efficient SMT solver for verifying deep neural networks},\\n  author={Katz, Guy and Barrett, Clark and Dill, David L and Julian, Kyle and Kochenderfer, Mykel J},\\n  booktitle={International Conference on Computer Aided Verification},\\n  pages={97--117},\\n  year={2017},\\n  organization={Springer}\\n}\\n\\n\\n\\n\\n@INPROCEEDINGS{aaai18,\\n  author    = {Nathan Fulton and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {Safe Reinforcement Learning via Formal Methods:\\n               Toward Safe Control Through Proof and Learning},\\n  booktitle = {{AAAI} Conference\\n               on Artificial Intelligence},\\n  year      = {2018}\\n}\\n\\n@ARTICLE{2018arXiv180706096J,\\n   author = {{Jansen}, N. and {K{\\\\\"o}nighofer}, B. and {Junges}, S. and {Bloem}, R.\\n\\t},\\n    title = \"{Shielded Decision-Making in {MDP}s}\",\\n  journal = {ArXiv e-prints},\\narchivePrefix = \"arXiv\",\\n   eprint = {1807.06096},\\n primaryClass = \"cs.AI\",\\n keywords = {Computer Science - Artificial Intelligence},\\n     year = 2018,\\n    month = jul,\\n   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180706096J},\\n  adsnote = {Provided by the SAO/NASA Astrophysics Data System}\\n}\\n\\n@inproceedings{DBLP:conf/aaai/AlshiekhBEKNT18,\\n  author    = {Mohammed Alshiekh and\\n               Roderick Bloem and\\n               R{\\\\\"{u}}diger Ehlers and\\n               Bettina K{\\\\\"{o}}nighofer and\\n               Scott Niekum and\\n               Ufuk Topcu},\\n  title     = {Safe Reinforcement Learning via Shielding},\\n  booktitle = {{AAAI} Conference on Artificial Intelligence},\\n  year      = {2018}\\n}\\n\\n@proceedings{DBLP:conf/aaai/2018,\\n  editor    = {Sheila A. McIlraith and\\n               Kilian Q. Weinberger},\\n  title     = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence ({AAAI} 2018)},\\n  publisher = {{AAAI} Press},\\n  year      = {2018},\\n\\n  timestamp = {Thu, 03 May 2018 17:02:03 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/aaai/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5\\n\\n\\n\\n\\n\\n@STRING{lncs = {LNCS}}\\n\\n\\n@INPROCEEDINGS{DBLP:conf/lics/Platzer12b,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {The Complete Proof Theory of\\n               Hybrid Systems},\\n  booktitle = {LICS},\\n  year      = {2012},\\n  pages     = {541-550},\\n\\n  longbooktitle = {Proceedings of the 27th Annual ACM/IEEE\\n               Symposium on Logic in Computer Science, LICS\\n               2012, Dubrovnik, Croatia, June 25???28, 2012},\\n  publisher = {IEEE},\\n\\n  keywords  = {proof theory, hybrid dynamical systems,\\n               differential dynamic logic, axiomatization,\\n               completeness},\\n  abstract  = {\\n    Hybrid systems are a fusion of continuous dynamical\\n    systems and discrete dynamical systems. They freely\\n    combine dynamical features from both worlds. For that\\n    reason, it has often been claimed that hybrid systems\\n    are more challenging than continuous dynamical systems\\n    and than discrete systems. We now show that,\\n    proof-theoretically, this is not the case. We present a\\n    complete proof-theoretical alignment that interreduces\\n    the discrete dynamics and the continuous dynamics of\\n    hybrid systems. We give a sound and complete\\n    axiomatization of hybrid systems relative to continuous\\n    dynamical systems and a sound and complete\\n    axiomatization of hybrid systems relative to discrete\\n    dynamical systems. Thanks to our axiomatization,\\n    proving properties of hybrid systems is exactly the\\n    same as proving properties of continuous dynamical\\n    systems and again, exactly the same as proving\\n    properties of discrete dynamical systems. This\\n    fundamental cornerstone sheds light on the nature of\\n    hybridness and enables flexible and provably perfect\\n    combinations of discrete reasoning with continuous\\n    reasoning that lift to all aspects of hybrid systems\\n    and their fragments.}\\n}\\n\\n\\n@ARTICLE{DBLP:journals/jar/Platzer17,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {A Complete Uniform Substitution Calculus \\n               for Differential Dynamic Logic},\\n  journal   = {J. Autom. Reas.},\\n  longjournal = {Journal of Automated Reasoning},\\n  medjournal= {J. Autom. Reasoning},\\n  year      = {2017},\\n  volume    = {59},\\n  number    = {2},\\n  pages     = {219-265},\\n\\n  arXiv     = {1507.04943}\\n}\\n\\n@Book{sutton.barto:reinforcement,\\n  title   = {Reinforcement Learning: An Introduction},\\n  author  = {Richard S. Sutton and Andrew G. Barto},\\n  publisher = {MIT Press},\\n  address = {Cambridge, MA},\\n  year    = 1998,\\n  keywords  = {RL},\\n  readdate  = {2002/06/04}\\n}\\n\\n@inproceedings{DBLP:conf/cav/KatzBDJK17,\\n  author    = {Guy Katz and\\n               Clark W. Barrett and\\n               David L. Dill and\\n               Kyle Julian and\\n               Mykel J. Kochenderfer},\\n  title     = {Reluplex: An Efficient {SMT} Solver for Verifying Deep Neural Networks},\\n  booktitle = {Computer Aided Verification - 29th International Conference ({CAV}\\n               2017)},\\n  pages     = {97--117},\\n  year      = {2017},\\n  crossref  = {DBLP:conf/cav/2017-1},\\n\\n  timestamp = {Fri, 14 Jul 2017 12:55:54 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cav/KatzBDJK17},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{bellerophon,\\n  author    = {Nathan Fulton and\\n               Stefan Mitsch and\\n               Brandon Bohrer and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {Bellerophon: Tactical Theorem Proving for Hybrid Systems},\\n  booktitle = {International Conference on Interactive Theorem Proving},\\n  year      = {2017}\\n\\n  timestamp = {Fri, 02 Nov 2018 09:45:32 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/itp/FultonMBP17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/itp/2017,\\n  editor    = {Mauricio Ayala{-}Rinc{\\\\\\'{o}}n and\\n               C{\\\\\\'{e}}sar A. Mu{\\\\~{n}}oz},\\n  title     = {Interactive Theorem Proving - 8th International Conference, {ITP}\\n               2017, Bras{\\\\\\'{\\\\i}}lia, Brazil, September 26-29, 2017, Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {10499},\\n  publisher = {Springer},\\n  year      = {2017},\\n\\n\\n  timestamp = {Wed, 06 Sep 2017 14:53:52 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/itp/2017},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@article{a3c,\\n  author    = {Volodymyr Mnih and\\n               Adri{\\\\`{a}} Puigdom{\\\\`{e}}nech Badia and\\n               Mehdi Mirza and\\n               Alex Graves and\\n               Timothy P. Lillicrap and\\n               Tim Harley and\\n               David Silver and\\n               Koray Kavukcuoglu},\\n  title     = {Asynchronous Methods for Deep Reinforcement Learning},\\n  journal   = {CoRR},\\n  volume    = {abs/1602.01783},\\n  year      = {2016},\\n  timestamp = {Wed, 07 Jun 2017 14:43:09 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MnihBMGLHSK16},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{openaigym,\\n  author    = {Greg Brockman and\\n               Vicki Cheung and\\n               Ludwig Pettersson and\\n               Jonas Schneider and\\n               John Schulman and\\n               Jie Tang and\\n               Wojciech Zaremba},\\n  title     = {{OpenAI} Gym},\\n  journal   = {CoRR},\\n  volume    = {abs/1606.01540},\\n  year      = {2016},\\n  timestamp = {Wed, 07 Jun 2017 14:41:24 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BrockmanCPSSTZ16},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cav/FrehseGDCRLRGDM11,\\n  author    = {Goran Frehse and\\n               Colas Le Guernic and\\n               Alexandre Donz{\\\\\\'{e}} and\\n               Scott Cotton and\\n               Rajarshi Ray and\\n               Olivier Lebeltel and\\n               Rodolfo Ripado and\\n               Antoine Girard and\\n               Thao Dang and\\n               Oded Maler},\\n  title     = {{SpaceEx}: Scalable Verification of Hybrid Systems},\\n  booktitle = {Computer Aided Verification - 23rd International Conference ({CAV}\\n               2011)},\\n  pages     = {379--395},\\n  year      = {2011},\\n  crossref  = {DBLP:conf/cav/2011},\\n\\n\\n  timestamp = {Mon, 11 Jul 2011 13:24:02 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cav/FrehseGDCRLRGDM11},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/rtss/ChenAS12,\\n  author    = {Xin Chen and\\n               Erika {\\\\\\'{A}}brah{\\\\\\'{a}}m and\\n               Sriram Sankaranarayanan},\\n  title     = {Taylor Model Flowpipe Construction for Non-linear Hybrid Systems},\\n  booktitle = {Proceedings of the 33rd {IEEE} Real-Time Systems Symposium\\n              ({RTSS} 2012)},\\n  pages     = {183--192},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/rtss/2012},\\n\\n\\n  timestamp = {Mon, 05 Jun 2017 12:39:58 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/rtss/ChenAS12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/rtss/2012,\\n  title     = {Proceedings of the 33rd {IEEE} Real-Time Systems Symposium, {RTSS}\\n               2012, San Juan, PR, USA, December 4-7, 2012},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {2012},\\n\\n\\n  timestamp = {Tue, 20 Jan 2015 18:18:59 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/rtss/2012},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n\\n@inproceedings{DBLP:conf/cav/GuernicG09,\\n  author    = {Colas Le Guernic and\\n               Antoine Girard},\\n  title     = {Reachability Analysis of Hybrid Systems Using Support Functions},\\n  booktitle = {Computer Aided Verification, 21st International Conference ({CAV}\\n               2009)},\\n  pages     = {540--554},\\n  year      = {2009},\\n  crossref  = {DBLP:conf/cav/2009},\\n  timestamp = {Wed, 03 Oct 2018 12:55:00 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/GuernicG09},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/cav/2009,\\n  editor    = {Ahmed Bouajjani and\\n               Oded Maler},\\n  title     = {Computer Aided Verification, 21st International Conference ({CAV}\\n               2009)},\\n  series    = {{LNCS}},\\n  volume    = {5643},\\n  publisher = {Springer},\\n  year      = {2009},\\n\\n\\n  timestamp = {Thu, 25 May 2017 00:39:07 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/2009},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@book{heyting1934mathematische,\\n  title={Mathematische Grundlagenforschung: Intuitionismus, Beweistheorie},\\n  author={Heyting, A.},\\n  number={v. 3, no. 4},\\n  lccn={35000418},\\n  series={Ergebnisse der mathematik und ihrer grenzgebiete 3. bd.,4},\\n  year={1934},\\n  publisher={Julius Springer}\\n}\\n\\n@Article{Kolmogoroff1932,\\nauthor=\"Kolmogoroff, A.\",\\ntitle=\"Zur Deutung der intuitionistischen Logik\",\\njournal=\"Mathematische Zeitschrift\",\\nyear=\"1932\",\\nmonth=\"Dec\",\\nday=\"01\",\\nvolume=\"35\",\\nnumber=\"1\",\\npages=\"58--65\",\\n\\n}\\n\\n@book{Troelstra1991,\\n\\tyear = {1991},\\n\\tauthor = {A. S. Troelstra},\\n\\tpublisher = {University of Amsterdam},\\n\\ttitle = {History of Constructivism in the 20th Century Vol. Ml-91-05}\\n}\\n\\n\\n\\n@article{DBLP:journals/tcs/Scott93,\\n  author    = {Dana S. Scott},\\n  title     = {A Type-Theoretical Alternative to ISWIM, CUCH, {OWHY}},\\n  journal   = {Theor. Comput. Sci.},\\n  volume    = {121},\\n  number    = {1{\\\\&}2},\\n  pages     = {411--440},\\n  year      = {1993},\\n\\n  timestamp = {Wed, 14 Nov 2018 10:33:30 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/tcs/Scott93},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@article{DBLP:journals/jmlr/GarciaF15,\\n  author    = {Javier Garc{\\\\\\'{\\\\i}}a and\\n               Fernando Fern{\\\\\\'{a}}ndez},\\n  title     = {A comprehensive survey on safe reinforcement learning},\\n  journal   = {Journal of Machine Learning Research},\\n  volume    = {16},\\n  pages     = {1437--1480},\\n  year      = {2015},\\n  timestamp = {Wed, 14 Jun 2017 16:06:09 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jmlr/GarciaF15},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@article{DBLP:journals/fmsd/MitschP16,\\n  author    = {Stefan Mitsch and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {{ModelPlex}: Verified Runtime Validation of\\n               Verified Cyber-Physical System Models},\\n  journal   = {Form. Methods Syst. Des.},\\n  longjournal = {Formal Methods in System Design},\\n  year      = {2016},\\n  volume    = {49},\\n  number    = {1},\\n  pages     = {33-74},\\n\\n  note      = {Special issue of selected papers from RV\\'14},\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/sosp/KleinEHACDEEKNSTW09,\\n  author    = {Gerwin Klein and\\n               Kevin Elphinstone and\\n               Gernot Heiser and\\n               June Andronick and\\n               David Cock and\\n               Philip Derrin and\\n               Dhammika Elkaduwe and\\n               Kai Engelhardt and\\n               Rafal Kolanski and\\n               Michael Norrish and\\n               Thomas Sewell and\\n               Harvey Tuch and\\n               Simon Winwood},\\n  title     = {{SeL4}: formal verification of an {OS} kernel},\\n  booktitle = {Proceedings of the 22nd {ACM} Symposium on Operating Systems Principles\\n               2009 ({SOSP} 2009)},\\n  pages     = {207--220},\\n  year      = {2009},\\n  crossref  = {DBLP:conf/sosp/2009},\\n\\n\\n  timestamp = {Tue, 06 Nov 2018 16:59:32 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/sosp/KleinEHACDEEKNSTW09},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/sosp/2009,\\n  editor    = {Jeanna Neefe Matthews and\\n               Thomas E. Anderson},\\n  title     = {Proceedings of the 22nd {ACM} Symposium on Operating Systems Principles\\n               2009, {SOSP} 2009, Big Sky, Montana, USA, October 11-14, 2009},\\n  publisher = {{ACM}},\\n  year      = {2009},\\n\\n\\n\\n  timestamp = {Tue, 06 Nov 2018 16:59:32 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/sosp/2009},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@ARTICLE{ZhuInductiveSynthesis2019,\\n       author = {{Zhu}, He and {Xiong}, Zikang and {Magill}, Stephen and\\n         {Jagannathan}, Suresh},\\n        title = \"{An Inductive Synthesis Framework for Verifiable Reinforcement Learning}\",\\n      journal = {arXiv e-prints},\\n     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, 00-02},\\n         year = 2019,\\n        month = jul,\\n          eid = {arXiv:1907.07273},\\n        pages = {arXiv:1907.07273},\\narchivePrefix = {arXiv},\\n       eprint = {1907.07273},\\n primaryClass = {cs.LG},\\n       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190707273Z},\\n      adsnote = {Provided by the SAO/NASA Astrophysics Data System}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/popl/KumarMNO14,\\n  author    = {Ramana Kumar and\\n               Magnus O. Myreen and\\n               Michael Norrish and\\n               Scott Owens},\\n  title     = {{CakeML}: a verified implementation of {ML}},\\n  booktitle = {The 41st Annual {ACM} {SIGPLAN-SIGACT} Symposium on Principles of\\n               Programming Languages ({POPL} 2014)},\\n  pages     = {179--192},\\n  year      = {2014},\\n  crossref  = {DBLP:conf/popl/2014},\\n  timestamp = {Tue, 06 Nov 2018 11:07:43 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/popl/KumarMNO14},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/popl/2014,\\n  editor    = {Suresh Jagannathan and\\n               Peter Sewell},\\n  title     = {The 41st Annual {ACM} {SIGPLAN-SIGACT} Symposium on Principles of\\n               Programming Languages, {POPL} \\'14, San Diego, CA, USA, January 20-21,\\n               2014},\\n  publisher = {{ACM}},\\n  year      = {2014},\\n\\n\\n  timestamp = {Thu, 09 Jan 2014 08:21:22 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/popl/2014},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/itp/KrebbersLW14,\\n  author    = {Robbert Krebbers and\\n               Xavier Leroy and\\n               Freek Wiedijk},\\n  title     = {Formal {C} Semantics: CompCert and the {C} Standard},\\n  booktitle = {Interactive Theorem Proving - 5th International Conference, {ITP}\\n               2014, Held as Part of the Vienna Summer of Logic, {VSL} 2014, Vienna,\\n               Austria, July 14-17, 2014. Proceedings},\\n  pages     = {543--548},\\n  year      = {2014},\\n  crossref  = {DBLP:conf/itp/2014},\\n\\n  timestamp = {Sun, 21 May 2017 00:18:59 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/itp/KrebbersLW14},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/itp/2014,\\n  editor    = {Gerwin Klein and\\n               Ruben Gamboa},\\n  title     = {Interactive Theorem Proving - 5th International Conference, {ITP}\\n               2014, Held as Part of the Vienna Summer of Logic, {VSL} 2014, Vienna,\\n               Austria, July 14-17, 2014. Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {8558},\\n  publisher = {Springer},\\n  year      = {2014},\\n\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:18:59 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/itp/2014},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n\\n\\n@article{KADOTA2006279,\\ntitle = \"Discounted {Markov} decision processes with utility constraints\",\\njournal = \"Computers and Mathematics with Applications\",\\nvolume = \"51\",\\nnumber = \"2\",\\npages = \"279 - 284\",\\nyear = \"2006\",\\nnote = \"\",\\n\\nauthor = \"Yoshinobu Kadota and Masami Kurano and Masami Yasuda\",\\nkeywords = \"{Markov} decision processes\",\\nkeywords = \"Utility constraints\",\\nkeywords = \"Discount criterion\",\\nkeywords = \"Lagrange technique\",\\nkeywords = \"Saddle-point\",\\nkeywords = \"Constrained optimal policy\"\\n}\\n\\n@inproceedings{DBLP:conf/ecml/Geibel06,\\n  author    = {Peter Geibel},\\n  title     = {Reinforcement Learning for {MDP}s with Constraints},\\n  booktitle = {Machine Learning: {ECML} 2006, 17th European Conference on Machine\\n               Learning, Berlin, Germany, September 18-22, 2006, Proceedings},\\n  pages     = {646--653},\\n  year      = {2006},\\n  crossref  = {DBLP:conf/ecml/2006},\\n\\n  timestamp = {Mon, 23 Oct 2006 13:40:58 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ecml/Geibel06},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n\\n@proceedings{DBLP:conf/cav/2011,\\n  editor    = {Ganesh Gopalakrishnan and\\n               Shaz Qadeer},\\n  title     = {Computer Aided Verification - 23rd International Conference ({CAV}\\n               2011)},\\n  series    = lncs,\\n  volume    = {6806},\\n  publisher = {Springer},\\n  year      = {2011},\\n\\n\\n\\n  timestamp = {Mon, 11 Jul 2011 13:20:54 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cav/2011},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@book{DBLP:books/daglib/0068834,\\n  author    = {Robert L. Constable and\\n               Stuart F. Allen and\\n               Mark Bromley and\\n               Rance Cleaveland and\\n               J. F. Cremer and\\n               R. W. Harper and\\n               Douglas J. Howe and\\n               Todd B. Knoblock and\\n               N. P. Mendler and\\n               Prakash Panangaden and\\n               James T. Sasaki and\\n               Scott F. Smith},\\n  title     = {Implementing mathematics with the Nuprl proof development system},\\n  publisher = {Prentice Hall},\\n  year      = {1986},\\n  opturl       = {http://dl.acm.org/citation.cfm?id=10510},\\n\\n  timestamp = {Fri, 26 Aug 2011 14:36:50 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/books/daglib/0068834},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/tphol/HickeyNCABBEGKKKLSWY03,\\n  author    = {Jason Hickey and\\n               Aleksey Nogin and\\n               Robert L. Constable and\\n               Brian E. Aydemir and\\n               Eli Barzilay and\\n               Yegor Bryukhov and\\n               Richard Eaton and\\n               Adam Granicz and\\n               Alexei Kopylov and\\n               Christoph Kreitz and\\n               Vladimir Krupski and\\n               Lori Lorigo and\\n               Stephan Schmitt and\\n               Carl Witty and\\n               Xin Yu},\\n  title     = {{MetaPRL} - {A} Modular Logical Environment},\\n  booktitle = {Theorem Proving in Higher Order Logics, 16th International Conference,\\n               TPHOLs 2003, Rome, Italy, September 8-12, 2003, Proceedings},\\n  pages     = {287--303},\\n  year      = {2003},\\n  crossref  = {DBLP:conf/tphol/2003},\\n\\n\\n  timestamp = {Tue, 05 Jul 2011 11:09:26 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tphol/HickeyNCABBEGKKKLSWY03},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/tphol/2003,\\n  editor    = {David A. Basin and\\n               Burkhart Wolff},\\n  title     = {Theorem Proving in Higher Order Logics, 16th International Conference,\\n               TPHOLs 2003, Rom, Italy, September 8-12, 2003, Proceedings},\\n  series    = lncs,\\n  volume    = {2758},\\n  publisher = {Springer},\\n  year      = {2003},\\n\\n  timestamp = {Fri, 06 Feb 2004 08:38:42 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tphol/2003},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/tacas/KongGCC15,\\n  author    = {Soonho Kong and\\n               Sicun Gao and\\n               Wei Chen and\\n               Edmund M. Clarke},\\n  title     = {{dReach}: {\\\\(\\\\delta\\\\)}-Reachability Analysis for Hybrid Systems},\\n  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 21st International Conference, {TACAS} 2015, Held as Part of the\\n               European Joint Conferences on Theory and Practice of Software, {ETAPS}\\n               2015, London, UK, April 11-18, 2015. Proceedings},\\n  pages     = {200--205},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/tacas/2015},\\n\\n\\n  timestamp = {Wed, 08 Apr 2015 11:31:07 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tacas/KongGCC15},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/tacas/2015,\\n  editor    = {Christel Baier and\\n               Cesare Tinelli},\\n  title     = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 21st International Conference, {TACAS} 2015, Held as Part of the\\n               European Joint Conferences on Theory and Practice of Software, {ETAPS}\\n               2015, London, UK, April 11-18, 2015. Proceedings},\\n  series    = lncs,\\n  volume    = {9035},\\n  publisher = {Springer},\\n  year      = {2015},\\n\\n\\n\\n  timestamp = {Wed, 08 Apr 2015 11:30:56 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/tacas/2015},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/GaoKC13,\\n  author    = {Sicun Gao and\\n               Soonho Kong and\\n               Edmund M. Clarke},\\n  title     = {{dReal}: An {SMT} Solver for Nonlinear Theories over the Reals},\\n  booktitle = {24th International Conference on Automated Deduction ({CADE-24})},\\n  pages     = {208--214},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/cade/2013},\\n\\n\\n  timestamp = {Sat, 15 Jun 2013 19:33:15 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/GaoKC13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2013,\\n  editor    = {Maria Paola Bonacina},\\n  title = {24th International Conference on Automated Deduction ({CADE-24})},\\n  series    = lncs,\\n  volume    = {7898},\\n  publisher = {Springer},\\n  year      = {2013},\\n\\n\\n\\n  timestamp = {Sat, 15 Jun 2013 19:30:45 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2013},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/sttt/Frehse08,\\n  author    = {Goran Frehse},\\n  title     = {{PHAVer}: algorithmic verification of hybrid systems past {HyTech}},\\n  journal   = {{STTT}},\\n  volume    = {10},\\n  number    = {3},\\n  pages     = {263--279},\\n  year      = {2008},\\n\\n\\n  timestamp = {Tue, 25 Nov 2008 14:46:58 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/sttt/Frehse08},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/Paulson10,\\n  author    = {Lawrence C. Paulson},\\n  title     = {Three Years of Experience with Sledgehammer, a Practical Link between\\n               Automatic and Interactive Theorem Provers},\\n  booktitle = {Proceedings of the 2nd Workshop on Practical Aspects of Automated\\n               Reasoning, PAAR-2010, Edinburgh, Scotland, UK, July 14, 2010},\\n  pages     = {1--10},\\n  year      = {2010},\\n  crossref  = {DBLP:conf/cade/2010paar},\\n  opturl       = {http://www.easychair.org/publications/?page=560965337},\\n  timestamp = {Fri, 15 Jan 2016 13:43:38 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/Paulson10},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2010paar,\\n  editor    = {Renate A. Schmidt and\\n               Stephan Schulz and\\n               Boris Konev},\\n  title     = {Proceedings of the 2nd Workshop on Practical Aspects of Automated\\n               Reasoning, PAAR-2010, Edinburgh, Scotland, UK, July 14, 2010},\\n  series    = {EPiC Series},\\n  volume    = {9},\\n  publisher = {EasyChair},\\n  year      = {2012},\\n  opturl       = {http://www.easychair.org/publications/?page=453348161},\\n  timestamp = {Fri, 15 Jan 2016 13:43:38 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2010paar},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/mkm/BarrasGHRTWW13,\\n  author    = {Bruno Barras and\\n               Lourdes Del Carmen Gonz{\\\\\\'{a}}lez{-}Huesca and\\n               Hugo Herbelin and\\n               Yann R{\\\\\\'{e}}gis{-}Gianas and\\n               Enrico Tassi and\\n               Makarius Wenzel and\\n               Burkhart Wolff},\\n  title     = {Pervasive Parallelism in Highly-Trustable Interactive Theorem Proving Systems},\\n  booktitle = {Intelligent Computer Mathematics - MKM, Calculemus, DML, and Systems and Projects 2013, Held as Part of {CICM} 2013, Bath, UK, July 8-12,\\n               2013. Proceedings},\\n  pages     = {359--363},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/mkm/2013},\\n\\n\\n  timestamp = {Wed, 09 Sep 2015 16:45:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/mkm/BarrasGHRTWW13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/mkm/2013,\\n  editor    = {Jacques Carette and\\n               David Aspinall and\\n               Christoph Lange and\\n               Petr Sojka and\\n               Wolfgang Windsteiger},\\n  title     = {Intelligent Computer Mathematics - MKM, Calculemus, DML, and Systems and Projects 2013, Held as Part of {CICM} 2013, Bath, UK, July 8-12,\\n               2013. Proceedings},\\n  series    = lncs,\\n  volume    = {7961},\\n  publisher = {Springer},\\n  year      = {2013},\\n\\n\\n\\n  timestamp = {Wed, 09 Sep 2015 16:45:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/mkm/2013},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@book{DBLP:books/daglib/0035083,\\n  author    = {Adam Chlipala},\\n  title     = {Certified Programming with Dependent Types - {A} Pragmatic Introduction\\n               to the Coq Proof Assistant},\\n  publisher = {{MIT} Press},\\n  year      = {2013},\\n  opturl       = {http://mitpress.mit.edu/books/certified-programming-dependent-types},\\n\\n  timestamp = {Mon, 11 May 2015 16:37:23 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/books/daglib/0035083},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/vstte/AhrendtBBBGGHHHKMSSU14,\\n  author    = {Wolfgang Ahrendt and\\n               Bernhard Beckert and\\n               Daniel Bruns and\\n               Richard Bubel and\\n               Christoph Gladisch and\\n               Sarah Grebing and\\n               Reiner H{\\\\\"{a}}hnle and\\n               Martin Hentschel and\\n               Mihai Herda and\\n               Vladimir Klebanov and\\n               Wojciech Mostowski and\\n               Christoph Scheben and\\n               Peter H. Schmitt and\\n               Mattias Ulbrich},\\n  title     = {The {KeY} Platform for Verification and Analysis of {Java} Programs},\\n  booktitle = {Verified Software: Theories, Tools and Experiments - 6th International\\n               Conference ({VSTTE} 2014)},\\n  pages     = {55--71},\\n  year      = {2014},\\n  crossref  = {DBLP:conf/vstte/2014},\\n\\n\\n  timestamp = {Sun, 26 Oct 2014 19:11:02 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/vstte/AhrendtBBBGGHHHKMSSU14},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cav/FrehseGDCRLRGDM11,\\n  author    = {Goran Frehse and\\n               Colas Le Guernic and\\n               Alexandre Donz{\\\\\\'{e}} and\\n               Scott Cotton and\\n               Rajarshi Ray and\\n               Olivier Lebeltel and\\n               Rodolfo Ripado and\\n               Antoine Girard and\\n               Thao Dang and\\n               Oded Maler},\\n  title     = {SpaceEx: Scalable Verification of Hybrid Systems},\\n  booktitle = {Computer Aided Verification - 23rd International Conference ({CAV}\\n               2011)},\\n  pages     = {379--395},\\n  year      = {2011},\\n  crossref  = {DBLP:conf/cav/2011},\\n\\n\\n  timestamp = {Thu, 08 Mar 2018 12:34:37 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/FrehseGDCRLRGDM11},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/cav/2011,\\n  editor    = {Ganesh Gopalakrishnan and\\n               Shaz Qadeer},\\n  title     = {Computer Aided Verification - 23rd International Conference ({CAV}\\n               2011)},\\n  series    = {{LNCS}},\\n  volume    = {6806},\\n  publisher = {Springer},\\n  year      = {2011},\\n\\n\\n\\n  timestamp = {Thu, 25 May 2017 00:39:08 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/2011},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:journals/corr/abs-1204-6671,\\n  author    = {Sicun Gao and\\n               Jeremy Avigad and\\n               Edmund M. Clarke},\\n  title     = {Delta-Decidability over the Reals},\\n  booktitle = {Proceedings of the 27th Annual {IEEE} Symposium on Logic in Computer\\n               Science, {LICS} 2012, Dubrovnik, Croatia, June 25-28, 2012},\\n  pages     = {305--314},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/lics/2012},\\n\\n\\n  timestamp = {Thu, 25 May 2017 00:42:41 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/lics/GaoAC12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/lics/2012,\\n  title     = {Proceedings of the 27th Annual {IEEE} Symposium on Logic in Computer\\n               Science, {LICS} 2012, Dubrovnik, Croatia, June 25-28, 2012},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {2012},\\n\\n  timestamp = {Fri, 21 Nov 2014 14:08:56 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/lics/2012},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/lics/Henzinger96,\\n  author    = {Thomas A. Henzinger},\\n  title     = {The Theory of Hybrid Automata},\\n  booktitle = {Proceedings, 11th Annual {IEEE} Symposium on Logic in Computer Science,\\n               New Brunswick, New Jersey, USA, July 27-30, 1996},\\n  pages     = {278--292},\\n  year      = {1996},\\n  crossref  = {DBLP:conf/lics/1996},\\n\\n\\n  timestamp = {Thu, 25 May 2017 00:42:40 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/lics/Henzinger96},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/lics/1996,\\n  title     = {Proceedings, 11th Annual {IEEE} Symposium on Logic in Computer Science,\\n               New Brunswick, New Jersey, USA, July 27-30, 1996},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {1996},\\n\\n\\n  timestamp = {Fri, 21 Nov 2014 14:08:55 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/lics/1996},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@proceedings{DBLP:conf/vstte/2014,\\n  editor    = {Dimitra Giannakopoulou and\\n               Daniel Kroening},\\n  title     = {Verified Software: Theories, Tools and Experiments - 6th International\\n               Conference, {VSTTE} 2014, Vienna, Austria, July 17-18, 2014, Revised\\n               Selected Papers},\\n  series    = lncs,\\n  volume    = {8471},\\n  publisher = {Springer},\\n  year      = {2014},\\n\\n\\n\\n  timestamp = {Wed, 15 Oct 2014 16:26:44 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/vstte/2014},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/MouraKADR15,\\n  author    = {Leonardo Mendon{\\\\c{c}}a de Moura and\\n               Soonho Kong and\\n               Jeremy Avigad and\\n               Floris van Doorn and\\n               Jakob von Raumer},\\n  title     = {The {Lean} Theorem Prover (System Description)},\\n  booktitle = {Automated Deduction - {CADE-25} - 25th International Conference on\\n               Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings},\\n  pages     = {378--388},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/cade/2015},\\n\\n\\n  timestamp = {Fri, 31 Jul 2015 13:18:27 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/MouraKADR15},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2015,\\n  editor    = {Amy P. Felty and\\n               Aart Middeldorp},\\n  title     = {Automated Deduction - {CADE-25} - 25th International Conference on\\n               Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings},\\n  series    = lncs,\\n  volume    = {9195},\\n  publisher = {Springer},\\n  year      = {2015},\\n\\n\\n\\n  timestamp = {Fri, 31 Jul 2015 13:15:07 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2015},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/PlatzerQR09,\\n  author    = {Andr{\\\\\\'{e}} Platzer and\\n               Jan{-}David Quesel and\\n               Philipp R{\\\\\"{u}}mmer},\\n  title     = {Real World Verification},\\n  booktitle = {Automated Deduction - CADE-22, 22nd International Conference on Automated\\n               Deduction, Montreal, Canada, August 2-7, 2009. Proceedings},\\n  pages     = {485--501},\\n  year      = {2009},\\n  crossref  = {DBLP:conf/cade/2009},\\n\\n\\n  timestamp = {Wed, 29 Jul 2009 15:18:30 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/PlatzerQR09},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2009,\\n  editor    = {Renate A. Schmidt},\\n  title     = {Automated Deduction - CADE-22, 22nd International Conference on Automated\\n               Deduction, Montreal, Canada, August 2-7, 2009. Proceedings},\\n  series    = lncs,\\n  volume    = {5663},\\n  publisher = {Springer},\\n  year      = {2009},\\n\\n\\n\\n  timestamp = {Wed, 29 Jul 2009 15:18:20 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2009},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/tacas/JeanninGKGSZP15,\\n  author    = {Jean{-}Baptiste Jeannin and\\n               Khalil Ghorbal and\\n\\t\\t\\t   Yanni Kouskoulas and\\n\\t\\t\\t   Ryan Gardner and\\n\\t\\t\\t   Aurora Schmidt and\\n\\t\\t\\t   Erik Zawadzki and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {A Formally Verified Hybrid System for the\\n               Next-generation Airborne Collision Avoidance System},\\n  booktitle = {TACAS},\\n  year      = {2015},\\n  pages     = {21-36},\\n\\n  editor    = {Christel Baier and\\n               Cesare Tinelli},\\n  longbooktitle = {Tools and Algorithms for the Construction\\n               and Analysis of Systems - 21st International\\n\\t\\t\\t   Conference, TACAS 2015, London, UK, April\\n\\t\\t\\t   11-18, 2015, Proceedings},\\n  series    = lncs,\\n  volume    = {9035},\\n  publisher = {Springer},\\n}\\n\\n@inproceedings{DBLP:conf/cade/HeiselRS90,\\n  author    = {Maritta Heisel and\\n               Wolfgang Reif and\\n               Werner Stephan},\\n  title     = {Tactical Theorem Proving in Program Verification},\\n  booktitle = {CADE},\\n  optbooktitle = {10th International Conference on Automated Deduction, Kaiserslautern,\\n               FRG, July 24-27, 1990, Proceedings},\\n  pages     = {117--131},\\n  year      = {1990},\\n  crossref  = {DBLP:conf/cade/1990},\\n\\n\\n  timestamp = {Tue, 07 Oct 2014 08:56:44 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/HeiselRS90},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/cade/FultonMQVP15,\\n  author    = {Nathan Fulton and\\n               Stefan Mitsch and\\n               Jan-David Quesel and\\n               Marcus V{\\\\\"o}lp and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {{KeYmaera X}: An Axiomatic Tactical Theorem Prover\\n               for Hybrid Systems},\\n  booktitle = {Proceedings of the 25th International Conference on Automated Deduction ({CADE}-25)},\\n  longbooktitle = {International Conference on Automated\\n               Deduction, CADE\\'15, Berlin, Germany, Proceedings},\\n  year      = {2015},\\n  pages     = {527--538},\\n\\n  editor    = {Amy P. Felty and\\n               Aart Middeldorp},\\n  publisher = {Springer},\\n  series    = lncs,\\n  volume    = {9195},\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/cpp/FultonP16,\\n  author    = {Nathan Fulton and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {A Logic of Proofs for Differential Dynamic Logic:\\n               Toward Independently Checkable Proof Certificates for Dynamic Logics},\\n  booktitle = {Proceedings of the 5th {ACM SIGPLAN} Conference on Certified Programs and Proofs,\\n               ({CPP} 2016),\\n  pages     = {110-121},\\n  year      = {2016},\\n\\n  editor    = {Jeremy Avigad and\\n               Adam Chlipala},\\n  publisher = {{ACM}},\\n}\\n\\n@proceedings{DBLP:conf/cade/1990,\\n  editor    = {Mark E. Stickel},\\n  title     = {Proceedings of the 10th International Conference on Automated Deduction ({CADE}-10)},\\n  series    = {LNCS},\\n  volume    = {449},\\n  publisher = {Springer},\\n  year      = {1990},\\n  timestamp = {Wed, 03 Jul 2002 09:55:41 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/1990},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/FeltyH94,\\n  author    = {Amy P. Felty and\\n               Douglas J. Howe},\\n  title     = {Tactic Theorem Proving with Refinement-Tree Proofs and Metavariables},\\n  booktitle = {CADE},\\n  longbooktitle = {Automated Deduction - CADE-12, 12th International Conference on Automated\\n               Deduction, Nancy, France, June 26 - July 1, 1994, Proceedings},\\n  pages     = {605--619},\\n  year      = {1994},\\n  crossref  = {DBLP:conf/cade/1994},\\n\\n\\n  timestamp = {Mon, 12 Oct 2009 13:28:26 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/FeltyH94},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/1994,\\n  editor    = {Alan Bundy},\\n  title     = {Automated Deduction - CADE-12, 12th International Conference on Automated\\n               Deduction, Nancy, France, June 26 - July 1, 1994, Proceedings},\\n  series    = lncs,\\n  volume    = {814},\\n  publisher = {Springer},\\n  year      = {1994},\\n\\n  timestamp = {Wed, 03 Jul 2002 09:55:41 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/1994},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@book{KeYBook,\\n author = {Beckert, Bernhard and H\\\\\"{a}hnle, Reiner and Schmitt, Peter H.},\\n title = {Verification of Object-oriented Software: The KeY Approach},\\n year = {2007},\\n\\n publisher = {Springer-Verlag},\\n address = {Berlin, Heidelberg}\\n}\\n\\n@ARTICLE{DBLP:journals/jar/Platzer08,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Differential Dynamic Logic for Hybrid\\n               Systems.},\\n  journal   = {J. Autom. Reas.},\\n  longjournal = {Journal of Automated Reasoning},\\n  year      = {2008},\\n  volume    = {41},\\n  number    = {2},\\n  pages     = {143-189},\\n\\n  keywords  = {dynamic logic, differential equations,\\n               sequent calculus, axiomatisation, automated\\n               theorem proving, verification of hybrid\\n               systems},\\n  abstract  = {\\n    Hybrid systems are models for complex physical systems\\n    and are defined as dynamical systems with interacting\\n    discrete transitions and continuous evolutions along\\n    differential equations. With the goal of developing a\\n    theoretical and practical foundation for deductive\\n    verification of hybrid systems, we introduce a dynamic\\n    logic for hybrid programs, which is a program notation\\n    for hybrid systems. As a verification technique that is\\n    suitable for automation, we introduce a free variable\\n    proof calculus with a novel combination of real-valued\\n    free variables and Skolemisation for lifting quantifier\\n    elimination for real arithmetic to dynamic logic. The\\n    calculus is compositional, i.e., it reduces properties\\n    of hybrid programs to properties of their parts. Our\\n    main result proves that this calculus axiomatises the\\n    transition behaviour of hybrid systems completely\\n    relative to differential equations. In a case study\\n    with cooperating traffic agents of the European Train\\n    Control System, we further show that our calculus is\\n    well-suited for verifying realistic hybrid systems with\\n    parametric system dynamics.\\n  }\\n}\\n\\n@ARTICLE{DBLP:journals/corr/Platzer14:dGL,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Differential Game Logic},\\n  journal   = {CoRR},\\n  volume    = {abs/1408.1980},\\n  year      = {2014},\\n  eprint    = {1408.1980}\\n}\\n\\n\\n@TECHREPORT{Platzer13:dGL,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {A Complete Axiomatization for Differential\\n               Game Logic for Hybrid Games},\\n  number    = {CMU-CS-13-100R},\\n  year      = {2013},\\n  month     = {January},\\n  institution = {School of Computer Science,\\n               Carnegie Mellon University},\\n  address   = {Pittsburgh, PA},\\n  note      = {Extended in revised version from July 2013},\\n  pdf = {http://reports-archive.adm.cs.cmu.edu/anon/2013/CMU-CS-13-100R.pdf}\\n}\\n\\n@ARTICLE{DBLP:journals/corr/abs-1205-4788,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Dynamic Logics of Dynamical Systems},\\n  journal   = {CoRR},\\n  volume    = {\\\\href{http://arxiv.org/abs/1205.4788}{abs/1205.4788}},\\n  year      = {2012},\\n  OPTurl       = {http://arxiv.org/abs/1205.4788},\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/lics/Platzer12a,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Logics of Dynamical Systems},\\n  booktitle = {LICS},\\n  year      = {2012},\\n  pages     = {13-24},\\n\\n  longbooktitle = {Proceedings of the 27th Annual ACM/IEEE\\n               Symposium on Logic in Computer Science, LICS\\n               2012, Dubrovnik, Croatia, June 25???28, 2012},\\n  publisher = {IEEE},\\n\\n  keywords  = {logic of dynamical systems, dynamic logic,\\n               differential dynamic logic, hybrid systems,\\n               axiomatization, deduction},\\n  abstract  = {\\n    We study the logic of dynamical systems, that is,\\n    logics and proof principles for properties of dynamical\\n    systems. Dynamical systems are mathematical models\\n    describing how the state of a system evolves over time.\\n    They are important in modeling and understanding many\\n    applications, including embedded systems and\\n    cyber-physical systems. In discrete dynamical systems,\\n    the state evolves in discrete steps, one step at a\\n    time, as described by a difference equation or discrete\\n    state transition relation. In continuous dynamical\\n    systems, the state evolves continuously along a\\n    function, typically described by a differential\\n    equation. Hybrid dynamical systems or hybrid systems\\n    combine both discrete and continuous dynamics.\\n\\n     This is a brief survey of differential dynamic logic\\n    for specifying and verifying properties of hybrid\\n    systems. We explain hybrid system models, differential\\n    dynamic logic, its semantics, and its axiomatization\\n    for proving logical formulas about hybrid systems. We\\n    study differential invariants, i.e., induction\\n    principles for differential equations. We briefly\\n    survey theoretical results, including soundness and\\n    completeness and deductive power. Differential dynamic\\n    logic has been implemented in automatic and interactive\\n    theorem provers and has been used successfully to\\n    verify safety-critical applications in automotive,\\n    aviation, railway, robotics, and analogue electrical\\n    circuits.}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/cade/Platzer15,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {A Uniform Substitution Calculus for Differential Dynamic Logic},\\n  booktitle = {CADE},\\n  longbooktitle = {International Conference on Automated Deduction, CADE\\'15, Berlin, Germany, Proceedings},\\n  year      = {2015},\\n  pages     = {},\\n  crossref  = {DBLP:conf/cade/2015},\\n\\n}\\n\\n@inproceedings{DBLP:conf/fm/MitschQP14,\\n  author    = {Stefan Mitsch and\\n               Jan-David Quesel and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {Refactoring, Refinement, and Reasoning - A Logical Characterization\\n               for Hybrid Systems},\\n  booktitle = {FM},\\n  year      = {2014},\\n  pages     = {481-496},\\n\\n  crossref  = {DBLP:conf/fm/2014},\\n  bibsource = {DBLP, http://dblp.uni-trier.de}\\n}\\n@proceedings{DBLP:conf/fm/2014,\\n  editor    = {Cliff B. Jones and\\n               Pekka Pihlajasaari and\\n               Jun Sun},\\n  title     = {FM 2014: Formal Methods - 19th International Symposium,\\n               Singapore, May 12-16, 2014. Proceedings},\\n  booktitle = {FM},\\n  publisher = {Springer},\\n  series    = lncs,\\n  volume    = {8442},\\n  year      = {2014},\\n\\n\\n  bibsource = {DBLP, http://dblp.uni-trier.de}\\n}\\n\\n\\n@inproceedings{DBLP:conf/hybrid/AlurCHH92,\\n  author    = {Rajeev Alur and\\n               Costas Courcoubetis and\\n               Thomas A. Henzinger and\\n               Pei{-}Hsin Ho},\\n  title     = {Hybrid Automata: An Algorithmic Approach to the Specification and\\n               Verification of Hybrid Systems},\\n  booktitle = {Hybrid Systems},\\n  pages     = {209--229},\\n  year      = {1992},\\n  crossref  = {DBLP:conf/hybrid/1992},\\n\\n\\n  timestamp = {Tue, 14 Jun 2011 20:33:33 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/hybrid/AlurCHH92},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\t\\n\\t@proceedings{DBLP:conf/hybrid/1992,\\n  editor    = {Robert L. Grossman and\\n               Anil Nerode and\\n               Anders P. Ravn and\\n               Hans Rischel},\\n  title     = {Hybrid Systems},\\n  series    = lncs,\\n  volume    = {736},\\n  publisher = {Springer},\\n  year      = {1993},\\n\\n  timestamp = {Thu, 09 Jan 2003 11:08:03 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/hybrid/1992},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@book{Nipkow:2002:IPA:1791547,\\n author = {Nipkow, Tobias and Wenzel, Markus and Paulson, Lawrence C.},\\n title = {{Isabelle/HOL}: A Proof Assistant for Higher-order Logic},\\n year = {2002},\\n\\n publisher = {Springer-Verlag},\\n address = {Berlin, Heidelberg},\\n}\\n\\n\\n@inproceedings{DBLP:conf/cade/PlatzerQ08,\\n  author    = {Andr{\\\\\\'{e}} Platzer and\\n               Jan{-}David Quesel},\\n  title     = {{KeYmaera}: {A} Hybrid Theorem Prover for Hybrid Systems (System Description)},\\n  booktitle = {Automated Reasoning, 4th International Joint Conference, {IJCAR} 2008,\\n               Sydney, Australia, August 12-15, 2008, Proceedings},\\n  pages     = {171--178},\\n  year      = {2008},\\n  crossref  = {DBLP:conf/cade/2008},\\n\\n\\n  timestamp = {Fri, 05 Sep 2008 08:28:33 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/PlatzerQ08},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2008,\\n  editor    = {Alessandro Armando and\\n               Peter Baumgartner and\\n               Gilles Dowek},\\n  title     = {Automated Reasoning, 4th International Joint Conference, {IJCAR} 2008,\\n               Sydney, Australia, August 12-15, 2008, Proceedings},\\n  series    = lncs,\\n  volume    = {5195},\\n  publisher = {Springer},\\n  year      = {2008},\\n\\n  timestamp = {Fri, 05 Sep 2008 08:22:54 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cade/2008},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@ARTICLE{BowenStavridou,\\nauthor={Bowen, J. and Stavridou, V.},\\njournal={Software Engineering Journal},\\ntitle={Safety-critical systems, formal methods and standards},\\nyear={1993},\\nmonth={Jul},\\nvolume={8},\\nnumber={4},\\npages={189-209},\\nkeywords={formal specification;real-time systems;safety;software reliability;standards;embedded computer-based systems;formal methods;real-time systems;safety-critical systems;software crisis;standards},\\n}\\n\\n@article{DBLP:journals/sosym/AhrendtBBBGHMMRSS05,\\n  author    = {Wolfgang Ahrendt and\\n               Thomas Baar and\\n               Bernhard Beckert and\\n               Richard Bubel and\\n               Martin Giese and\\n               Reiner H{\\\\\"{a}}hnle and\\n               Wolfram Menzel and\\n               Wojciech Mostowski and\\n               Andreas Roth and\\n               Steffen Schlager and\\n               Peter H. Schmitt},\\n  title     = {The {K}e{Y} tool},\\n  journal   = {Software and System Modeling},\\n  volume    = {4},\\n  number    = {1},\\n  pages     = {32--54},\\n  year      = {2005},\\n  OPTurl       = {http://www.springerlink.com/index/10.1007/s10270-004-0058-x},\\n  timestamp = {Fri, 22 Apr 2005 10:31:59 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/sosym/AhrendtBBBGHMMRSS05},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/fm/LoosPN11,\\n  author    = {Sarah M. Loos and\\n               Andr{\\\\\\'{e}} Platzer and\\n               Ligia Nistor},\\n  title     = {Adaptive Cruise Control: Hybrid, Distributed, and Now Formally Verified},\\n  booktitle = {International Symposium on Formal\\n               Methods},\\n  year      = {2011}\\n}\\n\\n@inproceedings{DBLP:conf/iccps/MitschLP12,\\n  author    = {Stefan Mitsch and\\n               Sarah M. Loos and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {Towards Formal Verification of Freeway Traffic Control},\\n  booktitle = {2012 {IEEE/ACM} Third International Conference on Cyber-Physical Systems,\\n               Beijing, China, April 17-19, 2012},\\n  pages     = {171--180},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/iccps/2012},\\n  OPTurl       = {http://doi.ieeecomputersociety.org/10.1109/ICCPS.2012.25},\\n\\n  timestamp = {Thu, 12 Jul 2012 09:29:10 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/iccps/MitschLP12},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/hybrid/LoosRP13,\\n  author    = {Sarah M. Loos and\\n               David W. Renshaw and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {Formal verification of distributed aircraft controllers},\\n  booktitle = {Proceedings of the 16th international conference on Hybrid systems:\\n               computation and control, {HSCC} 2013, April 8-11, 2013, Philadelphia,\\n               PA, {USA}},\\n  pages     = {125--130},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/hybrid/2013},\\n  opturl       = {http://doi.acm.org/10.1145/2461328.2461350},\\n\\n  timestamp = {Fri, 03 May 2013 21:31:34 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/hybrid/LoosRP13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/jacic/GhorbalJZPGC14,\\n  author    = {Khalil Ghorbal and\\n               Jean{-}Baptiste Jeannin and\\n               Erik Zawadzki and\\n               Andr{\\\\\\'{e}} Platzer and\\n               Geoffrey J. Gordon and\\n               Peter Capell},\\n  title     = {Hybrid Theorem Proving of Aerospace Systems: Applications and Challenges},\\n  journal   = {J. Aerospace Inf. Sys.},\\n  volume    = {11},\\n  number    = {10},\\n  pages     = {702--713},\\n  year      = {2014},\\n\\n\\n  timestamp = {Mon, 24 Nov 2014 14:34:19 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/jacic/GhorbalJZPGC14},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@TECHREPORT{JeanninGKGSZP14:TR,\\n  author    = {Jean-Baptiste Jeannin and\\n               Khalil Ghorbal and\\n               Yanni Kouskoulas and\\n               Ryan Garnder and\\n               Aurora Schmidt and\\n               Erik Zawadzki and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {A Formally Verified Hybrid System for the\\n               Next-Generation Airborne Collision Avoidance System},\\n  number    = {CMU-CS-14-138},\\n  year      = {2014},\\n  month     = {},\\n  institution = {School of Computer Science,\\n               Carnegie Mellon University},\\n  address   = {Pittsburgh, PA},\\n  pdf = {http://reports-archive.adm.cs.cmu.edu/anon/2014/CMU-CS-14-138.pdf}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/rss/MitschGP13,\\n  author    = {Stefan Mitsch and\\n               Khalil Ghorbal and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {On Provably Safe Obstacle Avoidance for\\n               Autonomous Robotic Ground Vehicles},\\n  booktitle = {Robotics: Science and Systems},\\n  year      = {2013},\\n  editor    = {Paul Newman and\\n               Dieter Fox and\\n               David Hsu},\\n  longbooktitle = {Robotics: Science and Systems IX,\\n               Technische Universit{\\\\\"a}t Berlin,\\n               Berlin, Germany, June 24 - June 28,\\n               2013},\\n\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/hybrid/KouskoulasRPK13,\\n  author    = {Yanni Kouskoulas and\\n               David W. Renshaw and\\n               Andr{\\\\\\'e} Platzer and\\n               Peter Kazanzides},\\n  title     = {Certifying the Safe Design of a Virtual\\n               Fixture Control Algorithm for a Surgical\\n               Robot},\\n  year      = {2013},\\n  pages     = {263-272},\\n\\n  publisher = {ACM},\\n  editor    = {Calin Belta and\\n               Franjo Ivancic},\\n  booktitle = {Hybrid Systems: Computation and Control\\n               (part of CPS Week 2013), HSCC\\'13,\\n               Philadelphia, PA, USA, April 8-13, 2013},\\n}\\n\\n@inproceedings{DBLP:conf/lics/Henzinger96,\\n  author    = {Thomas A. Henzinger},\\n  title     = {The Theory of Hybrid Automata},\\n  booktitle = {Proceedings, 11th Annual {IEEE} Symposium on Logic in Computer Science,\\n               New Brunswick, New Jersey, USA, July 27-30, 1996},\\n  pages     = {278--292},\\n  year      = {1996},\\n  crossref  = {DBLP:conf/lics/1996},\\n\\n\\n  timestamp = {Sat, 22 Nov 2014 13:44:45 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/lics/Henzinger96},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@BOOK{Platzer10,\\n  author    = {Andr{\\\\\\'e} Platzer},\\n  title     = {Logical Analysis of Hybrid Systems:\\n               Proving Theorems for Complex Dynamics},\\n  publisher = {Springer},\\n  address   = {Heidelberg},\\n  year      = {2010},\\n\\n\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/vmcai/GhorbalSP15,\\n  author    = {Khalil Ghorbal and\\n               Andrew Sogokon and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {A Hierarchy of Proof Rules for Checking\\n               Differential Invariance of Algebraic Sets},\\n  booktitle = {VMCAI},\\n  year      = {2015},\\n  pages     = {431-448},\\n\\n  editor    = {Deepak D\\'Souza and\\n               Akash Lal and\\n               Kim Guldstrand Larsen},\\n  longbooktitle = {Verification, Model Checking, and Abstract\\n               Interpretation - 16th International Conference,\\n               {VMCAI} 2015, Mumbai, India, January 12-14, 2015,\\n               Proceedings},\\n  series    = lncs,\\n  volume    = {8931},\\n  publisher = {Springer},\\n}\\n\\n\\n\\n@INPROCEEDINGS{DBLP:conf/icfem/PlatzerQ09,\\n  author    = {Andr{\\\\\\'e} Platzer and\\n               Jan-David Quesel},\\n  title     = {{European Train Control System}:\\n               A Case Study in Formal Verification},\\n  booktitle = {ICFEM},\\n  year      = {2009},\\n  pages     = {246-265},\\n\\n  editor    = {Karin Breitman and\\n               Ana Cavalcanti},\\n  longbooktitle = {Formal Methods and Software Engineering,\\n               11th International Conference on Formal\\n               Engineering Methods, ICFEM 2009,\\n               Rio de Janeiro, Brasil, December 9-12, 2009.\\n               Proceedings},\\n  booktitle = {ICFEM},\\n  publisher = {Springer},\\n  series    = lncs,\\n  volume    = {5885},\\n\\n  keywords  = {formal verification of hybrid systems,\\n               train control, theorem proving,\\n               parameter constraint identification,\\n               disturbances},\\n  abstract  = {\\n    Complex physical systems have several degrees of\\n    freedom. They only work correctly when their control\\n    parameters obey corresponding constraints. Based on the\\n    informal specification of the European Train Control\\n    System (ETCS), we design a controller for its\\n    cooperation protocol. For its free parameters, we\\n    successively identify constraints that are required to\\n    ensure collision freedom. We formally prove the\\n    parameter constraints to be sharp by characterizing\\n    them equivalently in terms of reachability properties\\n    of the hybrid system dynamics. Using our deductive\\n    verification tool KeYmaera, we formally verify\\n    controllability, safety, liveness, and reactivity\\n    properties of the ETCS protocol that entail collision\\n    freedom. We prove that the ETCS protocol remains\\n    correct even in the presence of perturbation by\\n    disturbances in the dynamics. We verify that safety is\\n    preserved when a PI controlled speed supervision is\\n    used.}\\n}\\n\\n@Misc{symbolarisCaseStudies,\\n  author = \\t {A. Platzer},\\n  title = \\t {Tools for Logic, Computer Science {\\\\&} Mathematics},\\n  month = \\t {Feb.},\\n  year = \\t {2015},\\n  opturl = \\t {http://symbolaris.com/info/index.html}\\n}\\n\\n@proceedings{DBLP:conf/lics/1996,\\n  title     = {Proceedings, 11th Annual {IEEE} Symposium on Logic in Computer Science,\\n               New Brunswick, New Jersey, USA, July 27-30, 1996},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {1996},\\n\\n  timestamp = {Tue, 15 Sep 4453316 17:28:00 +},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/lics/1996},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n\\n@proceedings{DBLP:conf/hybrid/2002,\\n\\tEditor = {Tomlin, Claire and Greenstreet, Mark R.},\\n\\tPublisher = {Springer},\\n\\tTitle = {Hybrid Systems: Computation and Control, 5th International Workshop, HSCC 2002, Stanford, CA, USA, March 25-27, 2002, Proceedings},\\n\\tVolume = {2289},\\n\\tYear = {2002}\\n}\\n\\n\\n\\n% eh?\\n@Manual{Coq:manual,\\n  title =        {The Coq proof assistant reference manual},\\n  author =       {\\\\mbox{The Coq development team}},\\n  organization = {LogiCal Project},\\n  note =         {Version 8.0},\\n  year =         {2004},\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/rv/MitschP14,\\n  author    = {Stefan Mitsch and\\n               Andr{\\\\\\'e} Platzer},\\n  title     = {{ModelPlex}: Verified Runtime Validation\\n               of Verified Cyber-Physical System Models},\\n  booktitle = {RV},\\n  year      = {2014},\\n  pages     = {199-214},\\n\\n  editor    = {Borzoo Bonakdarpour and\\n               Scott A. Smolka},\\n  longbooktitle = {Runtime Verification - 5th\\n               International Conference, RV\\n               2014, Toronto, ON, Canada, September 22--25,\\n               2014. Proceedings},\\n  publisher = {Springer},\\n  series    = lncs,\\n  volume    = {8734},\\n}\\n\\n@inproceedings{DBLP:conf/icml/MoldovanA12,\\n  author    = {Teodor Mihai Moldovan and\\n               Pieter Abbeel},\\n  title     = {Safe Exploration in {Markov} Decision Processes},\\n  booktitle = {Proceedings of the 29th International Conference on Machine Learning,\\n               {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/icml/2012},\\n  timestamp = {Wed, 29 Mar 2017 16:45:25 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icml/MoldovanA12},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/ml/DriessensD04,\\n  author    = {Kurt Driessens and\\n               Saso Dzeroski},\\n  title     = {Integrating Guidance into Relational Reinforcement Learning},\\n  journal   = {Machine Learning},\\n  volume    = {57},\\n  number    = {3},\\n  pages     = {271--304},\\n  year      = {2004},\\n\\n  timestamp = {Thu, 08 Dec 2005 10:51:59 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/ml/DriessensD04},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{do178c,\\nauthor = {RTCA},\\ntitle = {{DO-178C} Software Considerations in Airborne Systems and Equipment Certification.},\\nyear = {2012}\\n}\\n\\n@article{iso26262,\\nauthor = {{ISO-26262}},\\ntitle = {{International Organization for Standardization} 26262 Road vehicles  Functional safety},\\nyear = 2011\\n}\\n\\n@article{FANourbakhsh,\\nauthor = {Illah Reza Nourbakhsh},\\ntitle = {The Coming Robot Dystopia: All To Inhuman},\\nyear = {2015},\\nvolume = {94},\\nnumber = {4},\\njournal = {Foreign Affairs},\\nmonth = {July/August}\\n}\\n\\n@inproceedings{DBLP:conf/nips/NilimG03,\\n  author    = {Arnab Nilim and\\n               Laurent El Ghaoui},\\n  title     = {Robustness in {Markov} Decision Problems with Uncertain Transition Matrices},\\n  booktitle = {{Neural Information Processing Systems}},\\n  pages     = {839--846},\\n  publisher = {{MIT} Press},\\n  year      = {2003}\\n}\\n\\n@techreport{Bagnell2001,\\nauthor = {J. Andrew (Drew) Bagnell and Andrew Y. Ng and Jeff Schneider},\\ntitle = {Solving Uncertain {Markov} Decision Problems},\\nyear = {2001},\\nmonth = {August},\\ninstitution = {Carnegie Mellon University},\\naddress = {Pittsburgh, PA},\\nnumber = {CMU-RI-TR-01-25},\\nkeywords = {Uncertainty, MDPs, robust control, stochastic optimal control, dynamic programming, reinforcement learning, risk sensitive control},\\n}\\n\\n@article{DBLP:journals/mor/LimXM16,\\n  author    = {Shiau Hong Lim and\\n               Huan Xu and\\n               Shie Mannor},\\n  title     = {Reinforcement Learning in Robust {Markov} Decision Processes},\\n  journal   = {Mathematical Operations Research},\\n  volume    = {41},\\n  number    = {4},\\n  pages     = {1325--1353},\\n  year      = {2016}\\n}\\n\\n@inproceedings{DBLP:conf/nips/LimXM13,\\n  author    = {Shiau Hong Lim and\\n               Huan Xu and\\n               Shie Mannor},\\n  title     = {Reinforcement Learning in Robust {Markov} Decision Processes},\\n  booktitle = {{Neural Information Processing Systems}},\\n  pages     = {701--709},\\n  year      = {2013}\\n}\\n\\n\\n\\n\\n\\n@inproceedings{DBLP:conf/icml/Heger94,\\n  author    = {Matthias Heger},\\n  title     = {Consideration of Risk in Reinformance Learning},\\n  booktitle = {Machine Learning, Proceedings of the Eleventh International Conference,\\n               Rutgers University, New Brunswick, NJ, USA, July 10-13, 1994},\\n  pages     = {105--111},\\n  year      = {1994},\\n  crossref  = {DBLP:conf/icml/1994},\\n  timestamp = {Fri, 23 Dec 2011 14:54:21 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icml/Heger94},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{Nilim05,\\n\\tAuthor = {Arnab Nilim and Laurent {El Ghaoui}},\\n        Journal = {Operations Research},\\n\\tNumber = {5},\\n\\tPages = {780--798},\\n\\tTitle = {Robust Control of {M}arkov Decision Processes with Uncertain Transition Matrices},\\n\\tVolume = {53},\\n\\tYear = {2005},\\n\\tMonth = {September-October}}\\n\\n\\n@proceedings{DBLP:conf/icml/1994,\\n  editor    = {William W. Cohen and\\n               Haym Hirsh},\\n  title     = {Machine Learning, Proceedings of the Eleventh International Conference,\\n               Rutgers University, New Brunswick, NJ, USA, July 10-13, 1994},\\n  publisher = {Morgan Kaufmann},\\n  year      = {1994},\\n\\n  timestamp = {Fri, 16 Dec 2011 12:43:25 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icml/1994},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/TamarXM13,\\n  author    = {Aviv Tamar and\\n               Huan Xu and\\n               Shie Mannor},\\n  title     = {Scaling Up Robust {MDP}s by Reinforcement Learning},\\n  journal   = {CoRR},\\n  volume    = {abs/1306.6189},\\n  year      = {2013},\\n\\n  timestamp = {Wed, 07 Jun 2017 14:42:39 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/TamarXM13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{DBLP:journals/ior/NilimG05,\\n  author    = {Arnab Nilim and\\n               Laurent El Ghaoui},\\n  title     = {Robust Control of {Markov} Decision Processes with Uncertain Transition\\n               Matrices},\\n  journal   = {Operations Research},\\n  volume    = {53},\\n  number    = {5},\\n  pages     = {780--798},\\n  year      = {2005},\\n\\n\\n  timestamp = {Sat, 27 May 2017 14:23:35 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/ior/NilimG05},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cpp/FultonP16,\\n  author    = {Nathan Fulton and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {A logic of proofs for differential dynamic logic: toward independently\\n               checkable proof certificates for dynamic logics},\\n  booktitle = {Proceedings of the 5th {ACM} {SIGPLAN} Conference on Certified Programs\\n               and Proofs ({CPP} 2016)},\\n  pages     = {110--121},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/cpp/2016},\\n\\n\\n  timestamp = {Mon, 18 Jan 2016 19:35:36 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cpp/FultonP16},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n@proceedings{DBLP:conf/cpp/2016,\\n  editor    = {Jeremy Avigad and\\n               Adam Chlipala},\\n  title     = {Proceedings of the 5th {ACM} {SIGPLAN} Conference on Certified Programs\\n               and Proofs ({CPP} 2016)},\\n  publisher = {{ACM}},\\n  year      = {2016},\\n\\n\\n  timestamp = {Mon, 18 Jan 2016 19:35:36 +0100},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cpp/2016},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@book{RANDDriveToSafety,\\n\\n abstract = {<p>How safe are autonomous vehicles? The answer is crucial for developing sound policies to govern their deployment. This report evaluates whether it is practical to test-drive autonomous vehicles to assess their safety. We find that it would take hundreds of millions of miles and sometimes hundreds of billions of miles of test-driving to provide clear statistical evidence of autonomous vehicle safety, suggesting it is not a practical approach.</p>},\\n author = {Nidhi Kalra and Susan M. Paddock},\\n publisher = {RAND Corporation},\\n title = {Driving to Safety: How Many Miles of Driving Would It Take to Demonstrate Autonomous Vehicle Reliability?},\\n year = {2016}\\n}\\n\\n@article{DBLP:journals/corr/RussellDT16,\\n  author    = {Stuart J. Russell and\\n               Daniel Dewey and\\n               Max Tegmark},\\n  title     = {Research Priorities for Robust and Beneficial Artificial Intelligence},\\n  journal   = {CoRR},\\n  volume    = {abs/1602.03506},\\n  year      = {2016},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1602.03506},\\n  timestamp = {Wed, 07 Jun 2017 14:42:44 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/RussellDT16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icml/WuSHDR18,\\n  author    = {Yi Wu and\\n               Siddharth Srivastava and\\n               Nicholas Hay and\\n               Simon Du and\\n               Stuart J. Russell},\\n  title     = {Discrete-Continuous Mixtures in Probabilistic Programming: Generalized\\n               Semantics and Inference Algorithms},\\n  booktitle = {Proceedings of the 35th International Conference on Machine Learning,\\n               {ICML} 2018, Stockholmsm{\\\\\"{a}}ssan, Stockholm, Sweden, July\\n               10-15, 2018},\\n  pages     = {5339--5348},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/icml/2018},\\n\\n  timestamp = {Fri, 13 Jul 2018 14:58:25 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/WuSHDR18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icml/2018,\\n  editor    = {Jennifer G. Dy and\\n               Andreas Krause},\\n  title     = {Proceedings of the 35th International Conference on Machine Learning,\\n               {ICML} 2018, Stockholmsm{\\\\\"{a}}ssan, Stockholm, Sweden, July\\n               10-15, 2018},\\n  series    = {{JMLR} Workshop and Conference Proceedings},\\n  volume    = {80},\\n  year      = {2018},\\n\\n  timestamp = {Fri, 13 Jul 2018 14:57:00 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icml/VermaMSKC18,\\n  author    = {Abhinav Verma and\\n               Vijayaraghavan Murali and\\n               Rishabh Singh and\\n               Pushmeet Kohli and\\n               Swarat Chaudhuri},\\n  title     = {Programmatically Interpretable Reinforcement Learning},\\n  booktitle = {Proceedings of the 35th International Conference on Machine Learning\\n               ({ICML} 2018)},\\n  pages     = {5052--5061},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/icml/2018},\\n\\n  timestamp = {Fri, 13 Jul 2018 14:58:25 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/VermaMSKC18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icml/2018,\\n  editor    = {Jennifer G. Dy and\\n               Andreas Krause},\\n  title     = {Proceedings of the 35th International Conference on Machine Learning ({ICML} 2018)},\\n  series    = {{JMLR} Workshop and Conference Proceedings},\\n  volume    = {80},\\n  year      = {2018},\\n\\n  timestamp = {Fri, 13 Jul 2018 14:57:00 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icml/AchiamHTA17,\\n  author    = {Joshua Achiam and\\n               David Held and\\n               Aviv Tamar and\\n               Pieter Abbeel},\\n  title     = {{C}onstrained {P}olicy {O}ptimization},\\n  booktitle = {Proceedings of the 34th International Conference on Machine Learning,\\n               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},\\n  pages     = {22--31},\\n  year      = {2017},\\n  crossref  = {DBLP:conf/icml/2017},\\n\\n  timestamp = {Wed, 16 Aug 2017 11:08:55 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/AchiamHTA17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icml/WongK18,\\n  author    = {Eric Wong and\\n               J. Zico Kolter},\\n  title     = {Provable Defenses against Adversarial Examples via the Convex Outer\\n               Adversarial Polytope},\\n  booktitle = {Proceedings of the 35th International Conference on Machine Learning,\\n               {ICML} 2018, Stockholmsm{\\\\\"{a}}ssan, Stockholm, Sweden, July\\n               10-15, 2018},\\n  pages     = {5283--5292},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/icml/2018},\\n\\n  timestamp = {Fri, 13 Jul 2018 14:58:25 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/WongK18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icml/2018,\\n  editor    = {Jennifer G. Dy and\\n               Andreas Krause},\\n  title     = {Proceedings of the 35th International Conference on Machine Learning,\\n               {ICML} 2018, Stockholmsm{\\\\\"{a}}ssan, Stockholm, Sweden, July\\n               10-15, 2018},\\n  series    = {{JMLR} Workshop and Conference Proceedings},\\n  volume    = {80},\\n  year      = {2018},\\n\\n  timestamp = {Fri, 13 Jul 2018 14:57:00 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/abs-1805-10265,\\n  author    = {Krishnamurthy Dvijotham and\\n               Sven Gowal and\\n               Robert Stanforth and\\n               Relja Arandjelovic and\\n               Brendan O\\'Donoghue and\\n               Jonathan Uesato and\\n               Pushmeet Kohli},\\n  title     = {Training verified learners with learned verifiers},\\n  journal   = {CoRR},\\n  volume    = {abs/1805.10265},\\n  year      = {2018},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1805.10265},\\n  timestamp = {Tue, 05 Jun 2018 18:50:11 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1805-10265},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icse/TianPJR18,\\n  author    = {Yuchi Tian and\\n               Kexin Pei and\\n               Suman Jana and\\n               Baishakhi Ray},\\n  title     = {DeepTest: automated testing of deep-neural-network-driven autonomous\\n               cars},\\n  booktitle = {Proceedings of the 40th International Conference on Software Engineering,\\n               {ICSE} 2018, Gothenburg, Sweden, May 27 - June 03, 2018},\\n  pages     = {303--314},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/icse/2018},\\n\\n\\n  timestamp = {Sat, 07 Jul 2018 13:50:13 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icse/TianPJR18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icse/2018,\\n  editor    = {Michel Chaudron and\\n               Ivica Crnkovic and\\n               Marsha Chechik and\\n               Mark Harman},\\n  title     = {Proceedings of the 40th International Conference on Software Engineering,\\n               {ICSE} 2018, Gothenburg, Sweden, May 27 - June 03, 2018},\\n  publisher = {{ACM}},\\n  year      = {2018},\\n\\n\\n  timestamp = {Sat, 07 Jul 2018 13:12:38 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icse/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/abs-1804-10829,\\n  author    = {Shiqi Wang and\\n               Kexin Pei and\\n               Justin Whitehouse and\\n               Junfeng Yang and\\n               Suman Jana},\\n  title     = {Formal Security Analysis of Neural Networks using Symbolic Intervals},\\n  journal   = {CoRR},\\n  volume    = {abs/1804.10829},\\n  year      = {2018},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1804.10829},\\n  timestamp = {Wed, 02 May 2018 15:55:01 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-10829},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/abs-1712-01785,\\n  author    = {Kexin Pei and\\n               Yinzhi Cao and\\n               Junfeng Yang and\\n               Suman Jana},\\n  title     = {Towards Practical Verification of Machine Learning: The Case of Computer\\n               Vision Systems},\\n  journal   = {CoRR},\\n  volume    = {abs/1712.01785},\\n  year      = {2017},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1712.01785},\\n  timestamp = {Wed, 03 Jan 2018 12:33:17 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01785},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/jss/CalinescuCGKP18,\\n  author    = {Radu Calinescu and\\n               Milan Ceska and\\n               Simos Gerasimou and\\n               Marta Kwiatkowska and\\n               Nicola Paoletti},\\n  title     = {Efficient synthesis of robust models for stochastic systems},\\n  journal   = {Journal of Systems and Software},\\n  volume    = {143},\\n  pages     = {140--158},\\n  year      = {2018},\\n\\n\\n  timestamp = {Tue, 10 Jul 2018 09:45:26 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/jss/CalinescuCGKP18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/sttt/KwiatkowskaPW18,\\n  author    = {Marta Kwiatkowska and\\n               David Parker and\\n               Clemens Wiltsche},\\n  title     = {PRISM-games: verification and strategy synthesis for stochastic multi-player\\n               games with multiple objectives},\\n  journal   = {{STTT}},\\n  volume    = {20},\\n  number    = {2},\\n  pages     = {195--210},\\n  year      = {2018},\\n\\n\\n  timestamp = {Tue, 26 Jun 2018 14:09:50 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/sttt/KwiatkowskaPW18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@MISC{Altman99constrainedmarkov,\\n\\tauthor = {Eitan Altman},\\n\\ttitle = {{Constrained Markov Decision Processes}},\\n\\tyear = {1999}\\n}\\n\\n@inproceedings{DBLP:conf/icml/Heger94,\\n  author    = {Matthias Heger},\\n  title     = {Consideration of Risk in Reinformance Learning},\\n  booktitle = {Machine Learning, Proceedings of the Eleventh International Conference,\\n               Rutgers University, New Brunswick, NJ, USA, July 10-13, 1994},\\n  pages     = {105--111},\\n  year      = {1994},\\n  crossref  = {DBLP:conf/icml/1994},\\n  timestamp = {Fri, 23 Dec 2011 14:54:21 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/Heger94},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icml/1994,\\n  editor    = {William W. Cohen and\\n               Haym Hirsh},\\n  title     = {Machine Learning, Proceedings of the Eleventh International Conference,\\n               Rutgers University, New Brunswick, NJ, USA, July 10-13, 1994},\\n  publisher = {Morgan Kaufmann},\\n  year      = {1994},\\n\\n  timestamp = {Fri, 16 Dec 2011 12:43:25 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/icml/1994},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/icra/HeldMZSA17,\\n  author    = {David Held and\\n               Zoe McCarthy and\\n               Michael Zhang and\\n               Fred Shentu and\\n               Pieter Abbeel},\\n  title     = {Probabilistically safe policy transfer},\\n  booktitle = {2017 {IEEE} International Conference on Robotics and Automation, {ICRA}\\n               2017, Singapore, Singapore, May 29 - June 3, 2017},\\n  pages     = {5798--5805},\\n  year      = {2017},\\n  crossref  = {DBLP:conf/icra/2017},\\n\\n\\n  timestamp = {Wed, 26 Jul 2017 15:17:30 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icra/HeldMZSA17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icra/2017,\\n  title     = {2017 {IEEE} International Conference on Robotics and Automation, {ICRA}\\n               2017, Singapore, Singapore, May 29 - June 3, 2017},\\n  publisher = {{IEEE}},\\n  year      = {2017},\\n\\n\\n  timestamp = {Wed, 26 Jul 2017 15:16:56 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icra/2017},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@InProceedings{Garg,\\nauthor=\"Garg, A.\\nand Tai, K.\\nand Panda, B. N.\",\\neditor=\"Dash, Subhransu Sekhar\\nand Vijayakumar, K.\\nand Panigrahi, Bijaya Ketan\\nand Das, Swagatam\",\\ntitle=\"System Identification: Survey on Modeling Methods and Models\",\\nbooktitle=\"Artificial Intelligence and Evolutionary Computations in Engineering Systems\",\\nyear=\"2017\",\\npublisher=\"Springer\",\\npages=\"607--615\",\\nabstract=\"System identification (SI) is referred to as the procedure of building mathematical models for the dynamic systems using the measured data. Several modeling methods and types of models were studied by classifying SI in different ways, such as (1) black box, gray box, and white box; (2) parametric and non-parametric; and (3) linear SI, nonlinear SI, and evolutionary SI. A study of the literature also reveals that extensive focus has been paid to computational intelligence methods for modeling the output variables of the systems because of their ability to formulate the models based only on data obtained from the system. It was also learned that by embedding the features of several methods from different fields of SI into a given method, it is possible to improve its generalization ability. Popular variants of genetic programming such as multi-gene genetic programming is suggested as an alternative approach with its four shortcomings discussed as future aspects in paving way for evolutionary system identification.\",\\n\\n}\\n\\n@InProceedings{Deistler,\\nauthor=\"Deistler, Manfred\",\\neditor=\"Pasik-Duncan, Bozenna\",\\ntitle=\"System Identification and Time Series Analysis: Past, Present, and Future\",\\nbooktitle=\"Stochastic Theory and Control\",\\nyear=\"2002\",\\npublisher=\"Springer\",\\npages=\"97--109\",\\nabstract=\"The aim of this contribution is to describe main features in the development of system identification, in the sense of modelling from time series data. Given the restrictions in space, such an effort is necessarely fragmentary. Clearly, subjective judgements cannot be avoided. System identification has been developed in a number of different scientific communities, the most important of which are econometrics, statistics and system- and control theory.\",\\n\\n}\\n\\n@InProceedings{Juloski,\\nauthor=\"Juloski, Aleksandar Lj.\\nand Heemels, W. P. M. H.\\nand Ferrari-Trecate, Giancarlo\\nand Vidal, Ren{\\\\\\'e}\\nand Paoletti, Simone\\nand Niessen, J. H. G.\",\\neditor=\"Morari, Manfred\\nand Thiele, Lothar\",\\ntitle=\"Comparison of Four Procedures for the Identification of Hybrid Systems\",\\nbooktitle=\"Hybrid Systems: Computation and Control\",\\nyear=\"2005\",\\npublisher=\"Springer \",\\naddress=\"Berlin, Heidelberg\",\\npages=\"354--369\",\\nabstract=\"In this paper we compare four recently proposed procedures for the identification of PieceWise AutoRegressive eXogenous (PWARX) and switched ARX models. We consider the clustering-based procedure, the bounded-error procedure, and the Bayesian procedure which all identify PWARX models. We also study the algebraic procedure, which identifies switched linear models. We introduce quantitative measures for assessing the quality of the obtained models. Specific behaviors of the procedures are pointed out, using suitably constructed one dimensional examples. The methods are also applied to the experimental identification of the electronic component placement process in pick-and-place machines.\",\\n\\n}\\n\\n@inproceedings{DBLP:conf/fm/RothenbergG16,\\n  author    = {Bat{-}Chen Rothenberg and\\n               Orna Grumberg},\\n  title     = {Sound and Complete Mutation-Based Program Repair},\\n  booktitle = {Formal Methods - 21st International Symposium ({FM} 2016)},\\n  pages     = {593--611},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/fm/2016},\\n\\n\\n  timestamp = {Mon, 22 May 2017 17:11:19 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/fm/RothenbergG16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/fm/2016,\\n  editor    = {John S. Fitzgerald and\\n               Constance L. Heitmeyer and\\n               Stefania Gnesi and\\n               Anna Philippou},\\n  title     = {{FM} 2016: Formal Methods - 21st International Symposium, Limassol,\\n               Cyprus, November 9-11, 2016, Proceedings},\\n  series    = {{LNCS}},\\n  volume    = {9995},\\n  year      = {2016},\\n\\n\\n\\n  timestamp = {Mon, 22 May 2017 17:11:19 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/fm/2016},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/tse/GouesNFW12,\\n  author    = {Claire {Le Goues} and\\n               ThanhVu Nguyen and\\n               Stephanie Forrest and\\n               Westley Weimer},\\n  title     = {GenProg: {A} Generic Method for Automatic Software Repair},\\n  journal   = {{IEEE} Trans. Software Eng.},\\n  volume    = {38},\\n  number    = {1},\\n  pages     = {54--72},\\n  year      = {2012},\\n\\n\\n  timestamp = {Wed, 17 May 2017 10:56:36 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/tse/GouesNFW12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icse/GouesDFW12,\\n  author    = {Claire {Le Goues} and\\n               Michael Dewey{-}Vogt and\\n               Stephanie Forrest and\\n               Westley Weimer},\\n  title     = {A systematic study of automated program repair: Fixing 55 out of 105\\n               bugs for {\\\\textdollar}8 each},\\n  booktitle = {34th International Conference on Software Engineering, {ICSE} 2012,\\n               June 2-9, 2012, Zurich, Switzerland},\\n  pages     = {3--13},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/icse/2012},\\n\\n\\n  timestamp = {Tue, 23 May 2017 01:11:51 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icse/GouesDFW12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/icsm/LeLLG16,\\n  author    = {Xuan{-}Bach D. Le and\\n               Quang Loc Le and\\n               David Lo and\\n               Claire {Le Goues}},\\n  title     = {Enhancing Automated Program Repair with Deductive Verification},\\n  booktitle = {2016 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME} 2016)},\\n  pages     = {428--432},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/icsm/2016},\\n\\n\\n  timestamp = {Mon, 22 May 2017 17:11:01 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/icsm/LeLLG16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/icsm/2016,\\n  title     = {2016 {IEEE} International Conference on Software Maintenance and Evolution ({ICSME} 2016},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {2016},\\n\\n\\n  timestamp = {Wed, 18 Jan 2017 14:34:35 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/icsm/2016},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@Inbook{deBruijn1983,\\nauthor=\"de Bruijn, N. G.\",\\neditor=\"Siekmann, J{\\\\\"o}rg H.\\nand Wrightson, Graham\",\\ntitle=\"AUTOMATH, a Language for Mathematics\",\\nbookTitle=\"Automation of Reasoning: 2: Classical Papers on Computational Logic 1967--1970\",\\nyear=\"1983\",\\npublisher=\"Springer \",\\naddress=\"Berlin, Heidelberg\",\\npages=\"159--200\",\\nabstract=\"AUTOMATH is a language intended for expressing detailed mathematical thoughts. It is not a programming language, although it has several features in common with existing programming languages. It is defined by a grammar, and every text written according to its rules is claimed to correspond to correct mathematics. It can be used to express a large part (see 1.6) of mathematics, and admits many ways for laying the foundations. The rules are such that a computer can be instructed to check whether texts written in the language are correct. These texts are not restricted to proofs of single theorems; they can contain entire mathematical theories, including the rules of inference used in such theories.\",\\n\\n\\n\\n}\\n\\n@article{pml,\\nauthor={Per Martin-L\\\\\"{o}f},\\ntitle={{An Intuitionistic Theory of Types}},\\nbooktitle={Twenty-Five Years of Constructive Type Theory: Proceedings of a Congress held in Venice, October 1995},\\nyear={1998},\\neditor={Giovanni Sambin and Jan M. Smith.},\\npublisher={Oxford University Press}\\n}\\n\\n@TECHREPORT{Artemov95operationalmodal,\\n    author = {Sergei N. Artemov},\\n    title = {Operational Modal Logic},\\n    institution = {Cornell University},\\n    number = {MSI 9529},\\n    year = {1995}\\n}\\n\\n@incollection{ArtemovProvabilityLogic,\\nyear={2005},\\n\\nbooktitle={Handbook of Philosophical Logic, 2nd Edition},\\nvolume={13},\\nseries={Handbook of Philosophical Logic},\\neditor={Gabbay, D.M. and Guenthner, F.},\\n\\ntitle={{Provability Logic}},\\npublisher={Springer Netherlands},\\nauthor={Sergei N. Artemov and Lev D. Beklemishev},\\npages={189-360},\\nlanguage={English}\\n}\\n\\n@article{dalal2018safe,\\n  title={Safe exploration in continuous action spaces},\\n  author={Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},\\n  journal={arXiv preprint arXiv:1801.08757},\\n  year={2018}\\n}\\n\\n\\n@TECHREPORT{Artemov95operationalmodal,\\n    author = {Sergei N. Artemov},\\n    title = {Operational Modal Logic},\\n    institution = {Cornell University},\\n    number = {MSI 9529},\\n    year = {1995}\\n}\\n\\n@TECHREPORT{Artemov95operationalmodal,                                                              \\n    author = {Sergei N. Artemov},                                                                   \\n    title = {Operational Modal Logic},                                                              \\n    institution = {Cornell University},                                                             \\n    number = {MSI 9529},                                                                           \\n    year = {1995}                                                                                   \\n}                                                                                                   \\n                                                                                                    \\n@book{Isabelle,                                                                                     \\n  author    = {Tobias Nipkow and                                                                    \\n               Lawrence C. Paulson and                                                              \\n               Markus Wenzel},                                                                      \\n  title     = {{Isabelle/HOL} - {A} Proof Assistant for Higher-Order Logic},                          \\n  series    = {{LNCS}},\\n  volume    = {2283},                                                                               \\n  publisher = {Springer},                                                                           \\n  year      = {2002},                                                                               \\n\\n\\n\\n  timestamp = {Mon, 27 Jun 2011 15:25:29 +0200},                                                    \\n  biburl    = {http://dblp.uni-trier.de/rec/bib/books/sp/NipkowPW02},                               \\n  bibsource = {dblp computer science bibliography, http://dblp.org}                                 \\n}                                                                                                   \\n                                                                                                    \\n@Manual{Coq:manual,                                                                                 \\n  title =        {The Coq proof assistant reference manual},                                        \\n  author =       {\\\\mbox{The Coq development team}},                                                 \\n  note =         {Version 8.0},                                                                     \\n  year =         {2004},                                                                            \\n\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/cade/PlatzerQ08,                                                           \\n  author    = {Andr{\\\\\\'e} Platzer and                                                                \\n               Jan-David Quesel},                                                                   \\n  title     = {{KeYmaera}: A Hybrid Theorem Prover for Hybrid Systems.},                            \\n  booktitle = {IJCAR},                                                                              \\n  longbooktitle = {Automated Reasoning, Fourth International Joint Conference,                      \\n               IJCAR 2008, Sydney, Australia, Proceedings},                                         \\n  year      = {2008},                                                                               \\n  pages     = {171-178},                                                                            \\n  crossref  = {DBLP:conf/cade/2008},                                                                \\n\\n  citeseerurl = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.6523},                 \\n  keywords  = {dynamic logic, automated theorem proving, decision procedures, computer algebra, verification of hybrid systems},\\n  abstract  = {                                                                                     \\n      KeYmaera is a hybrid verification tool for hybrid systems that                                \\n      combines deductive, real algebraic, and computer algebraic                                    \\n      prover technologies.  It is an automated and interactive theorem                              \\n      prover for a natural specification and verification logic for                                 \\n      hybrid systems.  KeYmaera supports differential dynamic logic,                                \\n      which is a real-valued first-order dynamic logic for hybrid                                   \\n      programs, a program notation for hybrid automata.  For                                        \\n      automating the verification process, KeYmaera implements a                                    \\n      generalized free-variable sequent calculus and automatic proof                                \\n      strategies that decompose the hybrid system specification                                     \\n      symbolically.  To overcome the complexity of real arithmetic, we                              \\n      integrate real quantifier elimination following an iterative                                  \\n      background closure strategy.  Our tool is particularly suitable                               \\n      for verifying parametric hybrid systems and has been used                                     \\n      successfully for verifying collision avoidance in case studies                                \\n      from train control and air traffic management.}                                               \\n}  \\n\\n\\n@article{DBLP:journals/corr/MouraAKR15,\\n  author    = {Leonardo Mendon{\\\\c{c}}a de Moura and\\n               Jeremy Avigad and\\n               Soonho Kong and\\n               Cody Roux},\\n  title     = {Elaboration in Dependent Type Theory},\\n  journal   = {CoRR},\\n  volume    = {abs/1505.04324},\\n  year      = {2015},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1505.04324},\\n  timestamp = {Wed, 07 Jun 2017 14:40:40 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/MouraAKR15},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/MouraKADR15,\\n  author    = {Leonardo Mendon{\\\\c{c}}a de Moura and\\n               Soonho Kong and\\n               Jeremy Avigad and\\n               Floris van Doorn and\\n               Jakob von Raumer},\\n  title     = {The Lean Theorem Prover (System Description)},\\n  booktitle = {Automated Deduction - {CADE-25} - 25th International Conference on\\n               Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings},\\n  pages     = {378--388},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/cade/2015},\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:17:17 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cade/MouraKADR15},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/rp/BenvenutiBCFGV12,\\n  author    = {Luca Benvenuti and\\n               Davide Bresolin and\\n               Pieter Collins and\\n               Alberto Ferrari and\\n               Luca Geretti and\\n               Tiziano Villa},\\n  title     = {Ariadne: Dominance Checking of Nonlinear Hybrid Automata Using Reachability\\n               Analysis},\\n  booktitle = {Reachability Problems - 6th International Workshop ({RP} 2012)},\\n  pages     = {79--91},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/rp/2012},\\n\\n\\n  timestamp = {Thu, 15 Jun 2017 21:39:43 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/rp/BenvenutiBCFGV12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/rp/2012,\\n  editor    = {Alain Finkel and\\n               J{\\\\\\'{e}}r{\\\\^{o}}me Leroux and\\n               Igor Potapov},\\n  title     = {Reachability Problems - 6th International Workshop, {RP} 2012, Bordeaux,\\n               France, September 17-19, 2012. Proceedings},\\n  series = {{LNCS}},\\n  volume    = {7550},\\n  publisher = {Springer},\\n  year      = {2012},\\n\\n\\n\\n  timestamp = {Fri, 26 May 2017 00:49:43 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/rp/2012},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/hybrid/DuggiralaPM015,\\n  author    = {Parasara Sridhar Duggirala and\\n               Matthew Potok and\\n               Sayan Mitra and\\n               Mahesh Viswanathan},\\n  title     = {{C2E2:} a tool for verifying annotated hybrid systems},\\n  booktitle = {Proceedings of the 18th International Conference on Hybrid Systems:\\n               Computation and Control (HSCC 2015)},\\n  pages     = {307--308},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/hybrid/2015},\\n\\n\\n  timestamp = {Sun, 17 May 2015 09:37:16 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/DuggiralaPM015},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/hybrid/2015,\\n  editor    = {Antoine Girard and\\n               Sriram Sankaranarayanan},\\n  title     = {Proceedings of the 18th International Conference on Hybrid Systems:\\n               Computation and Control, HSCC\\'15, Seattle, WA, USA, April 14-16, 2015},\\n  publisher = {{ACM}},\\n  year      = {2015},\\n\\n\\n  timestamp = {Sun, 17 May 2015 09:37:16 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/2015},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/fmcad/AlurGW00,\\n  author    = {Rajeev Alur and\\n               Radu Grosu and\\n               Bow{-}Yaw Wang},\\n  title     = {Automated Refinement Checking for Asynchronous Processes},\\n  booktitle = {Formal Methods in Computer-Aided Design, Third International Conference ({FMCAD} 2000)},\\n  pages     = {55--72},\\n  year      = {2000},\\n  crossref  = {DBLP:conf/fmcad/2000},\\n\\n\\n  timestamp = {Wed, 24 May 2017 15:40:42 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/fmcad/AlurGW00},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/fmcad/2000,\\n  editor    = {Warren A. Hunt Jr. and\\n               Steven D. Johnson},\\n  title     = {Formal Methods in Computer-Aided Design, Third International Conference ({FMCAD} 2000)},\\n  series = {{LNCS}},\\n  volume    = {1954},\\n  publisher = {Springer},\\n  year      = {2000},\\n\\n\\n\\n  timestamp = {Wed, 24 May 2017 15:40:42 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/fmcad/2000},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/ki/NickSGH07,\\n  author    = {Markus Nick and\\n               S{\\\\\"{o}}ren Schneickert and\\n               J{\\\\\"{u}}rgen Grotepa{\\\\ss} and\\n               Ingo Heine},\\n  title     = {{CheckMATE}},\\n  journal   = {{KI}},\\n  volume    = {21},\\n  number    = {4},\\n  pages     = {34--37},\\n  year      = {2007},\\n\\n  timestamp = {Mon, 15 Jun 2009 13:54:18 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/ki/NickSGH07},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cade/GaoKC13,\\n  author    = {Sicun Gao and\\n               Soonho Kong and\\n               Edmund M. Clarke},\\n  title     = {dReal: An {SMT} Solver for Nonlinear Theories over the Reals},\\n  booktitle = {24th International Conference on Automated Deduction ({CADE-24})},\\n  pages     = {208--214},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/cade/2013},\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:17:17 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cade/GaoKC13},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/cade/2013,\\n  editor    = {Maria Paola Bonacina},\\n  booktitle = {24th International Conference on Automated Deduction ({CADE-24})},\\n  series = {{LNCS}},\\n  volume    = {7898},\\n  publisher = {Springer},\\n  year      = {2013},\\n\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:17:17 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cade/2013},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/WangZKGC14,\\n  author    = {Qinsi Wang and\\n               Paolo Zuliani and\\n               Soonho Kong and\\n               Sicun Gao and\\n               Edmund M. Clarke},\\n  title     = {SReach: Combining Statistical Tests and Bounded Model Checking for\\n               Nonlinear Hybrid Systems with Parametric Uncertainty},\\n  journal   = {CoRR},\\n  volume    = {abs/1404.7206},\\n  year      = {2014},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1404.7206},\\n  timestamp = {Wed, 07 Jun 2017 14:42:32 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/WangZKGC14},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/tacas/KongGCC15,\\n  author    = {Soonho Kong and\\n               Sicun Gao and\\n               Wei Chen and\\n               Edmund M. Clarke},\\n  title     = {dReach: $\\\\delta$-Reachability Analysis for Hybrid Systems},\\n  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems ({TACAS} 2015)},\\n  pages     = {200--205},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/tacas/2015},\\n\\n\\n  timestamp = {Thu, 15 Jun 2017 21:37:10 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/KongGCC15},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/tacas/2015,\\n  editor    = {Christel Baier and\\n               Cesare Tinelli},\\n  title     = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 21st International Conference, {TACAS} 2015, Held as Part of the\\n               European Joint Conferences on Theory and Practice of Software, {ETAPS}\\n               2015, London, UK, April 11-18, 2015. Proceedings},\\n  series = {{LNCS}},\\n  volume    = {9035},\\n  publisher = {Springer},\\n  year      = {2015},\\n\\n\\n\\n  timestamp = {Thu, 15 Jun 2017 21:37:10 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/2015},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/hybrid/BengtssonLLPY95,\\n  author    = {Johan Bengtsson and\\n               Kim Guldstrand Larsen and\\n               Fredrik Larsson and\\n               Paul Pettersson and\\n               Wang Yi},\\n  title     = {{\\\\sc {UPPAAL}} - a Tool Suite for Automatic Verification of Real-Time Systems},\\n  \\n  booktitle = {Hybrid Systems {III:} Verification and Control, Proceedings of the\\n               {DIMACS/SYCON} Workshop on Verification and Control of Hybrid Systems},\\n  pages     = {208--219},\\n  year      = {1995},\\n  crossref  = {DBLP:conf/hybrid/1995},\\n\\n  timestamp = {Wed, 22 Nov 2017 15:29:19 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/BengtssonLLPY95},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/hybrid/1995,\\n  editor    = {Rajeev Alur and\\n               Thomas A. Henzinger and\\n               Eduardo D. Sontag},\\n  title     = {Hybrid Systems {III:} Verification and Control, Proceedings of the\\n               {DIMACS/SYCON} Workshop on Verification and Control of Hybrid Systems,\\n               October 22-25, 1995, Ruttgers University, New Brunswick, NJ, {USA}},\\n  series = {{LNCS}},\\n  volume    = {1066},\\n  publisher = {Springer},\\n  year      = {1996},\\n\\n\\n\\n  timestamp = {Wed, 22 Nov 2017 15:29:19 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/1995},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{bllpw:dimacs95,\\n   title = {{{\\\\sc {UPPAAL}} --- a Tool Suite for Automatic Verification of Real--Time Systems}},\\n   author = {Johan Bengtsson and Kim G.\\\\ Larsen and Fredrik Larsson and Paul Pettersson and Wang Yi},\\n   booktitle = {Proc.\\\\ of Workshop on Verification and Control of Hybrid Systems III}, \\n   series = {{LNCS}},\\n   number = 1066,\\n   pages = {232--243},\\n   publisher = {Springer--Verlag},\\n   month = Oct, \\n   year = 1995\\n}\\n\\n@InProceedings{lpw:cav97,\\n  author = \\t {Kim G.\\\\ Larsen and Paul Pettersson and Wang Yi},\\n  title = \\t {{\\\\sc {UPPAAL}}: Status and Developments},\\n  booktitle = \\t {Computer Aided Verification {CAV 1997})},\\n  pages = \\t {456-459},\\n  year = \\t {1997},\\n  editor = \\t {Orna Grumberg},\\n  number = \\t 1254,\\n  series = \\t LNCS,\\n  month = \\t Jun,\\n  publisher = {Springer--Verlag}\\n}\\n\\n@Article{David2015,\\n  author=\"David, Alexandre and Larsen, Kim G. and Legay, Axel and Miku{\\\\v{c}}ionis, Marius and Poulsen, Danny B{\\\\o}gsted\",\\n  title=\"{\\\\sc {UPPAAL}} {SMC} tutorial\",\\n  journal=\"International Journal on Software Tools for Technology Transfer\",\\n  year=\"2015\",\\n  month=\"Aug\",\\n  day=\"01\",\\n  volume=\"17\",\\n  number=\"4\",\\n  pages=\"397--415\",\\n  abstract=\"This tutorial paper surveys the main features of Uppaal SMC, a model checking approach in Uppaal family that allows us to reason on networks of complex real-timed systems with a stochastic semantic. We demonstrate the modeling features of the tool, new verification algorithms and ways of applying them to potentially complex case studies.\",\\n\\n}\\n\\n@inproceedings{DBLP:conf/cav/ChenAS13,\\n  author    = {Xin Chen and\\n               Erika {\\\\\\'{A}}brah{\\\\\\'{a}}m and\\n               Sriram Sankaranarayanan},\\n  title     = {Flow*: An Analyzer for Non-linear Hybrid Systems},\\n  booktitle = {Computer Aided Verification - 25th International Conference ({CAV}\\n               2013)},\\n  pages     = {258--263},\\n  year      = {2013},\\n  crossref  = {DBLP:conf/cav/2013},\\n\\n\\n  timestamp = {Thu, 01 Jun 2017 18:57:43 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/ChenAS13},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/cav/2013,\\n  editor    = {Natasha Sharygina and\\n               Helmut Veith},\\n  title     = {Computer Aided Verification - 25th International Conference, {CAV}\\n               2013, Saint Petersburg, Russia, July 13-19, 2013. Proceedings},\\n  series = {{LNCS}},\\n  volume    = {8044},\\n  publisher = {Springer},\\n  year      = {2013},\\n\\n\\n\\n  timestamp = {Thu, 25 May 2017 00:39:06 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/2013},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/cpsweek/0002SA15,\\n  author    = {Xin Chen and\\n               Sriram Sankaranarayanan and\\n               Erika {\\\\\\'{A}}brah{\\\\\\'{a}}m},\\n  title     = {Flow* 1.2: More Effective to Play with Hybrid Systems},\\n  booktitle = {1st and 2nd International Workshop on Applied veRification for Continuous\\n               and Hybrid Systems (ARCH 2014 and ARCH 2015)},\\n  pages     = {152--159},\\n  year      = {2015},\\n  crossref  = {DBLP:conf/cpsweek/2014-15arch},\\n\\n  timestamp = {Tue, 25 Jul 2017 11:35:36 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cpsweek/0002SA15},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/cpsweek/2014-15arch,\\n  editor    = {Goran Frehse and\\n               Matthias Althoff},\\n  title     = {1st and 2nd International Workshop on Applied veRification for Continuous\\n               and Hybrid Systems (ARCH 2014)},\\n  series    = {EPiC Series in Computing},\\n  volume    = {34},\\n  publisher = {EasyChair},\\n  year      = {2015},\\n\\n  timestamp = {Thu, 16 Jun 2016 17:11:03 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cpsweek/2014-15arch},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/iccps/KushnerBMS18,\\n  author    = {Taisa Kushner and\\n               David Bortz and\\n               David M. Maahs and\\n               Sriram Sankaranarayanan},\\n  title     = {A data-driven approach to artificial pancreas verification and synthesis},\\n  booktitle = {Proceedings of the 9th {ACM/IEEE} International Conference on Cyber-Physical\\n               Systems ({ICCPS} 2018)},\\n  pages     = {242--252},\\n  year      = {2018},\\n  crossref  = {DBLP:conf/iccps/2018},\\n\\n  timestamp = {Thu, 26 Apr 2018 16:19:01 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/iccps/KushnerBMS18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/iccps/2018,\\n  editor    = {Chris Gill and\\n               Bruno Sinopoli and\\n               Xue Liu and\\n               Paulo Tabuada},\\n  title     = {Proceedings of the 9th {ACM/IEEE} International Conference on Cyber-Physical\\n               Systems ({ICCPS} 2018)},\\n  publisher = {{IEEE} / {ACM}},\\n  year      = {2018},\\n\\n  timestamp = {Thu, 26 Apr 2018 16:19:01 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/iccps/2018},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@incollection{DBLP:series/lncs/BartocciDDFMNS18,\\n  author    = {Ezio Bartocci and\\n               Jyotirmoy V. Deshmukh and\\n               Alexandre Donz{\\\\\\'{e}} and\\n               Georgios E. Fainekos and\\n               Oded Maler and\\n               Dejan Nickovic and\\n               Sriram Sankaranarayanan},\\n  title     = {Specification-Based Monitoring of Cyber-Physical Systems: {A} Survey\\n               on Theory, Tools and Applications},\\n  booktitle = {Lectures on Runtime Verification - Introductory and Advanced Topics},\\n  pages     = {135--175},\\n  year      = {2018},\\n  crossref  = {DBLP:series/lncs/10457},\\n\\n\\n  timestamp = {Wed, 14 Feb 2018 12:47:13 +0100},\\n  biburl    = {https://dblp.org/rec/bib/series/lncs/BartocciDDFMNS18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@book{DBLP:series/lncs/10457,\\n  editor    = {Ezio Bartocci and\\n               Yli{\\\\`{e}}s Falcone},\\n  title     = {Lectures on Runtime Verification - Introductory and Advanced Topics},\\n  series = {{LNCS}},\\n  volume    = {10457},\\n  publisher = {Springer},\\n  year      = {2018},\\n\\n\\n\\n  timestamp = {Wed, 14 Feb 2018 12:45:56 +0100},\\n  biburl    = {https://dblp.org/rec/bib/series/lncs/10457},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@inproceedings{DBLP:conf/lics/LoosP16,\\n  author    = {Sarah M. Loos and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {Differential Refinement Logic},\\n  booktitle = {Proceedings of the 31st Annual {ACM/IEEE} Symposium on Logic in Computer\\n               Science, {LICS} \\'16, New York, NY, USA, July 5-8, 2016},\\n  pages     = {505--514},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/lics/2016},\\n\\n\\n  timestamp = {Thu, 15 Jun 2017 21:41:12 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/lics/LoosP16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/lics/2016,\\n  editor    = {Martin Grohe and\\n               Eric Koskinen and\\n               Natarajan Shankar},\\n  title     = {Proceedings of the 31st Annual {ACM/IEEE} Symposium on Logic in Computer\\n               Science, {LICS} \\'16, New York, NY, USA, July 5-8, 2016},\\n  publisher = {{ACM}},\\n  year      = {2016},\\n\\n\\n\\n  timestamp = {Thu, 27 Oct 2016 11:23:41 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/lics/2016},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/ram/GiustiZIPCCA18,\\n  author    = {Andrea Giusti and\\n               Martijn J. A. Zeestraten and\\n               Esra Icer and\\n               Aaron Pereira and\\n               Darwin G. Caldwell and\\n               Sylvain Calinon and\\n               Matthias Althoff},\\n  title     = {Flexible Automation Driven by Demonstration: Leveraging Strategies\\n               that Simplify Robotics},\\n  journal   = {{IEEE} Robot. Automat. Mag.},\\n  volume    = {25},\\n  number    = {2},\\n  pages     = {18--27},\\n  year      = {2018},\\n\\n\\n  timestamp = {Tue, 10 Jul 2018 09:45:38 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/ram/GiustiZIPCCA18},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cpsweek/AlthoffG16,\\n  author    = {Matthias Althoff and\\n               Dmitry Grebenyuk},\\n  title     = {Implementation of Interval Arithmetic in {CORA} 2016},\\n  booktitle = {3rd International Workshop on Applied Verification\\n               for Continuous and Hybrid Systems ({ARCH} 2016)},\\n  pages     = {91--105},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/cpsweek/2016arch},\\n\\n  timestamp = {Tue, 25 Jul 2017 11:35:36 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cpsweek/AlthoffG16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/cpsweek/2016arch,\\n  editor    = {Goran Frehse and\\n               Matthias Althoff},\\n  title     = {ARCH@CPSWeek 2016, 3rd International Workshop on Applied Verification\\n               for Continuous and Hybrid Systems, Vienna, Austria},\\n  series    = {EPiC Series in Computing},\\n  volume    = {43},\\n  publisher = {EasyChair},\\n  year      = {2017},\\n\\n  timestamp = {Wed, 26 Apr 2017 13:57:14 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cpsweek/2016arch},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n\\n@MISC{HybTBX, \\n   author = {A. Bemporad}, \\n   title = {{Hybrid Toolbox - User\\'s Guide}},\\n   year = {2004}, \\n}\\n\\n\\n@inproceedings{DBLP:conf/cpsweek/MakhloufHK16,\\n  author    = {Ibtissem Ben Makhlouf and\\n               Norman Hansen and\\n               Stefan Kowalewski},\\n  title     = {{HyReach}: {A} Reachability Tool for Linear Hybrid Systems Based on\\n               Support Functions},\\n  booktitle = {3rd International Workshop on Applied Verification\\n               for Continuous and Hybrid Systems ({ARCH} 2016)},\\n  pages     = {68--79},\\n  year      = {2016},\\n  crossref  = {DBLP:conf/cpsweek/2016arch},\\n\\n  timestamp = {Tue, 25 Jul 2017 11:35:36 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cpsweek/MakhloufHK16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/cpsweek/2016arch,\\n  editor    = {Goran Frehse and\\n               Matthias Althoff},\\n  title     = {ARCH@CPSWeek 2016, 3rd International Workshop on Applied Verification\\n               for Continuous and Hybrid Systems, Vienna, Austria},\\n  series    = {EPiC Series in Computing},\\n  volume    = {43},\\n  publisher = {EasyChair},\\n  year      = {2017},\\n\\n  timestamp = {Wed, 26 Apr 2017 13:57:14 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cpsweek/2016arch},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/sttt/HenzingerHW97,\\n  author    = {Thomas A. Henzinger and\\n               Pei{-}Hsin Ho and\\n               Howard Wong{-}Toi},\\n  title     = {{HYTECH:} {A} Model Checker for Hybrid Systems},\\n  journal   = {{STTT}},\\n  volume    = {1},\\n  number    = {1-2},\\n  pages     = {110--122},\\n  year      = {1997},\\n\\n\\n  timestamp = {Thu, 18 May 2017 09:53:11 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/sttt/HenzingerHW97},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/rtss/AlurHH93,\\n  author    = {Rajeev Alur and\\n               Thomas A. Henzinger and\\n               Pei{-}Hsin Ho},\\n  title     = {Automatic Symbolic Verification of Embedded Systems},\\n  booktitle = {Proceedings of the Real-Time Systems Symposium},\\n  pages     = {2--11},\\n  year      = {1993},\\n  crossref  = {DBLP:conf/rtss/1993},\\n\\n\\n  timestamp = {Thu, 05 Jul 2018 07:45:48 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/rtss/AlurHH93},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/rtss/1993,\\n  title     = {Proceedings of the Real-Time Systems Symposium},\\n  publisher = {{IEEE} Computer Society},\\n  year      = {1993},\\n\\n\\n  timestamp = {Thu, 05 Jul 2018 07:45:48 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/rtss/1993},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/hybrid/DawsOTY95,\\n  author    = {Conrado Daws and\\n               Alfredo Olivero and\\n               Stavros Tripakis and\\n               Sergio Yovine},\\n  title     = {The Tool {KRONOS}},\\n  booktitle = {Hybrid Systems {III:} Verification and Control, Proceedings of the\\n               {DIMACS/SYCON} Workshop on Verification and Control of Hybrid Systems},\\n  pages     = {208--219},\\n  year      = {1995},\\n  crossref  = {DBLP:conf/hybrid/1995},\\n\\n\\n  timestamp = {Wed, 22 Nov 2017 15:29:19 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/DawsOTY95},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/hybrid/1995,\\n  editor    = {Rajeev Alur and\\n               Thomas A. Henzinger and\\n               Eduardo D. Sontag},\\n  title     = {Hybrid Systems {III:} Verification and Control, Proceedings of the\\n               {DIMACS/SYCON} Workshop on Verification and Control of Hybrid Systems,\\n               October 22-25, 1995, Ruttgers University, New Brunswick, NJ, {USA}},\\n  series = {{LNCS}},\\n  volume    = {1066},\\n  publisher = {Springer},\\n  year      = {1996},\\n\\n\\n\\n  timestamp = {Wed, 22 Nov 2017 15:29:19 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/1995},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@presentation{Brooks16PtolemyIIOpensourcePlatformForExperimentingWithActororiented,\\n    author = {Christopher Brooks},\\n    title = {Ptolemy {II}: An open-source platform for\\n              experimenting with actor-oriented design},\\n    day = {11},\\n    month = {February},\\n    year = {2016},\\n    howpublished = {Berkeley EECS Annual Research Symposium ({BEARS})},\\n    abstract = {Ptolemy II is an open-source software framework\\n              supporting experimentation with actor-oriented\\n              design. Actors are software components that\\n              execute concurrently and communicate through\\n              messages sent via interconnected ports. A model is\\n              a hierarchical interconnection of actors. In\\n              Ptolemy II, the semantics of a model is not\\n              determined by the framework, but rather by a\\n              software component in the model called a director,\\n              which implements a model of computation. The\\n              Ptolemy Project has developed directors supporting\\n              process networks (PN), discrete-events (DE),\\n              dataflow (SDF), synchronous/reactive(SR),\\n              rendezvous-based models, 3-D visualization, and\\n              continuous-time models. Each level of the\\n              hierarchy in a model can have its own director,\\n              and distinct directors can be composed\\n              hierarchically. A major emphasis of the project\\n              has been on understanding the heterogeneous\\n              combinations of models of computation realized by\\n              these directors. Directors can be combined\\n              hierarchically with state machines to make modal\\n              models [2]. A hierarchical combination of\\n              continuous-time models with state machines yields\\n              hybrid systems; a combination of\\n              synchronous/reactive with state machines yields\\n              StateCharts (the Ptolemy II variant is close to\\n              SyncCharts).},\\n\\n}\\n\\n@article{EkerJanneckLeeLiuLiuLudvigSachsXiongNeuendorffer03TamingHeterogeneityPtolemyApproach,\\n    author = {Johan Eker and Jorn W Janneck and Edward A. Lee\\n              and Jie Liu and Xiaojun Liu and Jozsef Ludvig and\\n              Sonia Sachs and Yuhong Xiong and Stephen\\n              Neuendorffer},\\n    title = { Taming heterogeneity - the {Ptolemy} approach},\\n    journal = {Proceedings of the IEEE},\\n    volume = {91},\\n    number = {1},\\n    pages = {127-144},\\n    year = {2003},\\n    abstract = {Modern embedded computing systems tend to be\\n              heterogeneous in the sense of being composed of\\n              subsystems with very different characteristics,\\n              which communicate and interact in a variety of\\n              ways-synchronous or asynchronous, buffered or\\n              unbuffered, etc. Obviously, when designing such\\n              systems, a modeling language needs to reflect this\\n              heterogeneity. Today\\'s modeling environments\\n              usually offer a variant of what we call amorphous\\n              heterogeneity to address this problem. This paper\\n              argues that modeling systems in this manner leads\\n              to unexpected and hard-to-analyze interactions\\n              between the communication mechanisms and proposes\\n              a more structured approach to heterogeneity,\\n              called hierarchical heterogeneity, to solve this\\n              problem. It proposes a model structure and\\n              semantic framework that support this form of\\n              heterogeneity, and discusses the issues arising\\n              from heterogeneous component interaction and the\\n              desire for component reuse. It introduces the\\n              notion of domain polymorphism as a way to address\\n              these issues.},\\n\\n}\\n\\n \\n@INPROCEEDINGS{hypro1,\\n     author = {H{\\\\\"{u}}ls, Jannik and Schupp, Stefan and Remke, Anne and Abraham, Erika},\\n      title = {Analyzing Hybrid Petri nets with multiple stochastic firings using {HyPro}},\\n  booktitle = {Proc. of the 11th EAI International Conference on Performance Evaluation Methodologies and Tools (VALUETOOLS\\'17)},\\n       year = {2017}\\n}\\n\\n \\n@INPROCEEDINGS{hypro2,\\n     author = {Schupp, Stefan and Abraham, Erika and Ben Makhlouf, Ibtissem and Kowalewski, Stefan},\\n      title = {{HyPro}: A {C++} Library for State Set Representations for Hybrid Systems Reachability Analysis},\\n  booktitle = {Proc. of the 9th NASA Formal Methods Symposium (NFM\\'17)},\\n     series = {LNCS},\\n     volume = {10227},\\n       year = {2017},\\n      pages = {288--294},\\n  publisher = {Springer International Publishing}\\n}\\n\\n\\n\\n@inproceedings{DBLP:conf/issac/ViehmannKA17,\\n  author    = {Tarik Viehmann and\\n               Gereon Kremer and\\n               Erika {\\\\\\'{A}}brah{\\\\\\'{a}}m},\\n  title     = {Comparing Different Projection Operators in the Cylindrical Algebraic\\n               Decomposition for {SMT} Solving},\\n  booktitle = {Proceedings of the 2nd International Workshop on Satisfiability Checking\\n               and Symbolic Computation co-located with the 42nd International Symposium\\n               on Symbolic and Algebraic Computation {(ISSAC} 2017), Kaiserslautern,\\n               Germany, July 29, 2017.},\\n  year      = {2017},\\n  crossref  = {DBLP:conf/issac/2017sc},\\n\\n  timestamp = {Sun, 29 Oct 2017 17:33:14 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/issac/ViehmannKA17}\\n}\\n\\n@proceedings{DBLP:conf/issac/2017sc,\\n  editor    = {Matthew England and\\n               Vijay Ganesh},\\n  title     = {Proceedings of the 2nd International Workshop on Satisfiability Checking\\n               and Symbolic Computation co-located with the 42nd International Symposium\\n               on Symbolic and Algebraic Computation {(ISSAC} 2017), Kaiserslautern,\\n               Germany, July 29, 2017},\\n  series    = {{CEUR} Workshop Proceedings},\\n  volume    = {1974},\\n  publisher = {CEUR-WS.org},\\n  year      = {2017},\\n\\n  urn       = {urn:nbn:de:0074-1974-4},\\n  timestamp = {Sun, 29 Oct 2017 17:32:47 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/issac/2017sc},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/tacas/AnnpureddyLFS11,\\n  author    = {Yashwanth Annpureddy and\\n               Che Liu and\\n               Georgios E. Fainekos and\\n               Sriram Sankaranarayanan},\\n  title     = {{S-TaLiRo}: {A} Tool for Temporal Logic Falsification for Hybrid Systems},\\n  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems ({TACAS} 2011)},\\n  pages     = {254--257},\\n  year      = {2011},\\n  crossref  = {DBLP:conf/tacas/2011},\\n\\n\\n  timestamp = {Tue, 26 Jun 2018 14:11:58 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/AnnpureddyLFS11},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/tacas/2011,\\n  editor    = {Parosh Aziz Abdulla and\\n               K. Rustan M. Leino},\\n  title     = {Tools and Algorithms for the Construction and Analysis of Systems\\n               - 17th International Conference, {TACAS} 2011, Held as Part of the\\n               Joint European Conferences on Theory and Practice of Software, {ETAPS}\\n               2011, Saarbr{\\\\\"{u}}cken, Germany, March 26-April 3, 2011. Proceedings},\\n  series = {{LNCS}},\\n  volume    = {6605},\\n  publisher = {Springer},\\n  year      = {2011},\\n\\n\\n\\n  timestamp = {Wed, 24 May 2017 08:28:32 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/tacas/2011},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/cav/AsarinDM02,\\n  author    = {Eugene Asarin and\\n               Thao Dang and\\n               Oded Maler},\\n  title     = {The d/dt Tool for Verification of Hybrid Systems},\\n  booktitle = {Computer Aided Verification, 14th International Conference, {CAV}\\n               2002,Copenhagen, Denmark, July 27-31, 2002, Proceedings},\\n  pages     = {365--370},\\n  year      = {2002},\\n  crossref  = {DBLP:conf/cav/2002},\\n\\n\\n  timestamp = {Fri, 26 May 2017 14:09:14 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/AsarinDM02},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/cav/2002,\\n  editor    = {Ed Brinksma and\\n               Kim Guldstrand Larsen},\\n  title     = {Computer Aided Verification, 14th International Conference, {CAV}\\n               2002,Copenhagen, Denmark, July 27-31, 2002, Proceedings},\\n  series = {{LNCS}},\\n  volume    = {2404},\\n  publisher = {Springer},\\n  year      = {2002},\\n\\n\\n\\n  timestamp = {Fri, 26 May 2017 14:09:14 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/cav/2002},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@book{Kuznetsov:1998:EAB:289919,\\n author = {Kuznetsov, Yuri A.},\\n title = {Elements of Applied Bifurcation Theory (2Nd Ed.)},\\n year = {1998},\\n\\n publisher = {Springer-Verlag},\\n address = {Berlin, Heidelberg},\\n}\\n\\n@article{DBLP:journals/tecs/JohnsonBCS16,\\n  author    = {Taylor T. Johnson and\\n               Stanley Bak and\\n               Marco Caccamo and\\n               Lui Sha},\\n  title     = {Real-Time Reachability for Verified Simplex Design},\\n  journal   = {{ACM} Trans. Embedded Comput. Syst.},\\n  volume    = {15},\\n  number    = {2},\\n  pages     = {26:1--26:27},\\n  year      = {2016},\\n\\n\\n  timestamp = {Tue, 03 Jan 2017 10:49:11 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/tecs/JohnsonBCS16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/tiv/PadenCYYF16,\\n  author    = {Brian Paden and\\n               Michal C{\\\\\\'{a}}p and\\n               Sze Zheng Yong and\\n               Dmitry S. Yershov and\\n               Emilio Frazzoli},\\n  title     = {A Survey of Motion Planning and Control Techniques for Self-Driving\\n               Urban Vehicles},\\n  journal   = {{IEEE} Trans. Intelligent Vehicles},\\n  volume    = {1},\\n  number    = {1},\\n  pages     = {33--55},\\n  year      = {2016},\\n\\n\\n  timestamp = {Fri, 26 May 2017 22:54:06 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/tiv/PadenCYYF16},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/sttt/Frehse08,\\n  author    = {Goran Frehse},\\n  title     = {PHAVer: algorithmic verification of hybrid systems past HyTech},\\n  journal   = {{STTT}},\\n  volume    = {10},\\n  number    = {3},\\n  pages     = {263--279},\\n  year      = {2008},\\n\\n\\n  timestamp = {Thu, 18 May 2017 09:53:11 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/sttt/Frehse08},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/KumarKZ17,\\n  author    = {Peeyush Kumar and\\n               Wolf Kohn and\\n               Zelda B. Zabinsky},\\n  title     = {Near Optimal Hamiltonian-Control and Learning via Chattering},\\n  journal   = {CoRR},\\n  volume    = {abs/1703.06485},\\n  year      = {2017},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1703.06485},\\n  timestamp = {Wed, 07 Jun 2017 14:40:28 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/KumarKZ17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/hybrid/KohnNR94,\\n  author    = {Wolf Kohn and\\n               Anil Nerode and\\n               Jeffrey B. Remmel},\\n  title     = {Hybrid Systems as Finsler Manifolds: Finite State Control as Approximation\\n               to Connections},\\n  booktitle = {Hybrid Systems II, Proceedings of the Third International Workshop\\n               on Hybrid Systems, Ithaca, NY, USA, October 1994},\\n  pages     = {294--321},\\n  year      = {1994},\\n  crossref  = {DBLP:conf/hybrid/1994},\\n\\n\\n  timestamp = {Wed, 22 Nov 2017 15:17:48 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/KohnNR94},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/hybrid/1994,\\n  editor    = {Panos J. Antsaklis and\\n               Wolf Kohn and\\n               Anil Nerode and\\n               Shankar Sastry},\\n  title     = {Hybrid Systems II, Proceedings of the Third International Workshop\\n               on Hybrid Systems, Ithaca, NY, USA, October 1994},\\n  series = {{LNCS}},\\n  volume    = {999},\\n  publisher = {Springer},\\n  year      = {1995},\\n\\n\\n\\n  timestamp = {Wed, 22 Nov 2017 15:17:48 +0100},\\n  biburl    = {https://dblp.org/rec/bib/conf/hybrid/1994},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:journals/corr/MitschP17,\\n  author    = {Stefan Mitsch and\\n               Andr{\\\\\\'{e}} Platzer},\\n  title     = {The {KeYmaera} {X} Proof {IDE} - Concepts on Usability in Hybrid Systems\\n               Theorem Proving},\\n  booktitle = {Proceedings of the Third Workshop on Formal Integrated Development\\n               Environment, F-IDE@FM 2016, Limassol, Cyprus, November 8, 2016.},\\n  pages     = {67--81},\\n  year      = {2016},\\n  crossref  = {DBLP:journals/corr/DuboisMM17},\\n\\n\\n  timestamp = {Mon, 06 Nov 2017 12:14:01 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/MitschP17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:journals/corr/DuboisMM17,\\n  editor    = {Catherine Dubois and\\n               Paolo Masci and\\n               Dominique M{\\\\\\'{e}}ry},\\n  title     = {Proceedings of the Third Workshop on Formal Integrated Development\\n               Environment, F-IDE@FM 2016, Limassol, Cyprus, November 8, 2016},\\n  series    = {{EPTCS}},\\n  volume    = {240},\\n  year      = {2017},\\n\\n\\n  timestamp = {Mon, 06 Nov 2017 12:14:01 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/DuboisMM17},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@INPROCEEDINGS{DBLP:conf/fm/SogokonGTP18,\\n  author    = {Andrew Sogokon and\\n               Khalil Ghorbal and\\n               Yong Kiam Tan and\\n               Andr\\\\\\'{e} Platzer},\\n  title     = {Vector Barrier Certificates and Comparison Systems},\\n  booktitle = {FM},\\n  crossref  = {DBLP:conf/fm/2018},\\n  year      = {2018},\\n  pages     = {418-437},\\n  editor    = {Klaus Havelund and\\n               Bill Roscoe and\\n               Jan Peleska},\\n  longbooktitle = {22nd International Symposium on Formal Methods ({FM} 2018)},\\n  publisher = {Springer},\\n  series    = {LNCS},\\n  volume    = {10951},\\n  address   = {},\\n}\\n\\n@article{DBLP:journals/afp/ImmlerH12,\\n  author    = {Fabian Immler and\\n               Johannes H{\\\\\"{o}}lzl},\\n  title     = {Ordinary Differential Equations},\\n  journal   = {Archive of Formal Proofs},\\n  volume    = {2012},\\n  year      = {2012},\\n\\n  timestamp = {Mon, 19 Sep 2016 15:58:56 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/afp/ImmlerH12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/itp/ImmlerH12,\\n  author    = {Fabian Immler and\\n               Johannes H{\\\\\"{o}}lzl},\\n  title     = {Numerical Analysis of Ordinary Differential Equations in {Isabelle/HOL}},\\n  booktitle = {Interactive Theorem Proving - Third International Conference ({ITP}\\n               2012)},\\n  pages     = {377--392},\\n  year      = {2012},\\n  crossref  = {DBLP:conf/itp/2012},\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:18:59 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/itp/ImmlerH12},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/itp/2012,\\n  editor    = {Lennart Beringer and\\n               Amy P. Felty},\\n  title     = {Interactive Theorem Proving - Third International Conference, {ITP}\\n               2012, Princeton, NJ, USA, August 13-15, 2012. Proceedings},\\n  series = {{LNCS}},\\n  volume    = {7406},\\n  publisher = {Springer},\\n  year      = {2012},\\n\\n\\n\\n  timestamp = {Sun, 21 May 2017 00:18:59 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/itp/2012},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/corr/ghoshetal,\\n  author    = {Shromona Ghosh and\\n               Felix Berkenkamp and\\n               Gireeja Ranade and\\n               Shaz Qadeer and\\n               Ashish Kapoor},\\n  title     = {{Verifying Controllers Against Adversarial Examples with Bayesian Optimization}},\\n  journal   = {CoRR},\\n  volume    = {abs/1802.08678},\\n  year      = {2018},\\n\\n  archivePrefix = {arXiv},\\n  eprint    = {1802.08678},\\n  timestamp = {Fri, 02 Mar 2018 13:46:22 +0100},\\n  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-08678},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@InProceedings{KitzelmannInductiveProgrammingSurvey,\\nauthor=\"Kitzelmann, Emanuel\",\\neditor=\"Schmid, Ute\\nand Kitzelmann, Emanuel\\nand Plasmeijer, Rinus\",\\ntitle=\"Inductive Programming: A Survey of Program Synthesis Techniques\",\\nbooktitle=\"Approaches and Applications of Inductive Programming\",\\nyear=\"2010\",\\npublisher=\"Springer\",\\npages=\"50--73\",\\nabstract=\"Inductive programming (IP)---the use of inductive reasoning methods for programming, algorithm design, and software development---is a currently emerging research field. A major subfield is inductive program synthesis, the (semi-)automatic construction of programs from exemplary behavior. Inductive program synthesis is not a unified research field until today but scattered over several different established research fields such as machine learning, inductive logic programming, genetic programming, and functional programming. This impedes an exchange of theory and techniques and, as a consequence, a progress of inductive programming. In this paper we survey theoretical results and methods of inductive program synthesis that have been developed in different research fields until today.\",\\n\\n}\\n\\n\\n\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n% LPdL Stuff\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n@InProceedings{Pientka10:Beluga,\\n    author    = {Brigitte Pientka and Joshua Dunfield},\\n    title     = {{Beluga}: A Framework for Programming and Reasoning with Deductive Systems (System Description)},\\n    booktitle = {Int\\'l Joint Conference on Automated Reasoning (IJCAR 2010)},\\n    pages     = {15--21},\\n    month     = jul,\\n    year      = {2010}\\n}\\n\\n@incollection{Twelf,\\nyear={1999},\\n\\nbooktitle={Automated Deduction  CADE-16},\\nvolume={1632},\\nseries = {{LNCS}},\\n\\ntitle={System Description: Twelf  A Meta-Logical Framework for Deductive Systems},\\n\\npublisher={Springer},\\nauthor={Pfenning, Frank and Sch\\\\\"{u}rmann, Carsten},\\npages={202-206},\\nlanguage={English}\\n}\\n\\n\\n@inproceedings{mechanizedSML,\\n author = {Lee, Daniel K. and Crary, Karl and Harper, Robert},\\n title = {{Towards a Mechanized Metatheory of Standard ML}},\\n booktitle = {Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},\\n series = {POPL 2007},\\n year = {2007},\\n\\n location = {Nice, France},\\n pages = {173--184},\\n numpages = {12},\\n\\n acmid = {1190245},\\n publisher = {ACM},\\n address = {New York, NY, USA},\\n keywords = {language definitions, logical frameworks, mechanized metatheory, standard ML, twelf, type safety},\\n} \\n\\n@article{ELF,\\n author = {Harper, Robert and Honsell, Furio and Plotkin, Gordon},\\n title = {{A Framework for Defining Logics}},\\n journal = {J. ACM},\\n issue_date = {January 1993},\\n volume = {40},\\n number = {1},\\n month = jan,\\n year = {1993},\\n pages = {143--184},\\n numpages = {42},\\n\\n acmid = {138060},\\n publisher = {ACM},\\n address = {New York, NY, USA},\\n keywords = {formal systems, interactive theorem proving, proof checking, typed lambda calculus},\\n} \\n\\n@INPROCEEDINGS{Mahboubi05programmingand,\\n    author = {Assia Mahboubi},\\n    title = {Programming and certifying the {CAD} algorithm inside the {Coq} system},\\n    booktitle = {Mathematics, Algorithms, Proofs, volume 05021 of Dagstuhl Seminar Proceedings, Schloss Dagstuhl},\\n    year = {2005}\\n}\\n\\n%TODO is this the appropriate canonical citation for HOL Light?\\n@inproceedings{DBLP:conf/fmcad/Harrison96,\\n  author    = {John Harrison},\\n  title     = {{HOL} Light: {A} Tutorial Introduction},\\n  booktitle = {Formal Methods in Computer-Aided Design, First International Conference ({FMCAD} 1996)},\\n  pages     = {265--269},\\n  year      = {1996},\\n\\n\\n  timestamp = {Mon, 27 Jun 2011 15:26:08 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/fmcad/Harrison96},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n\\n@article{Fitting2005,\\ntitle = {{The logic of proofs, semantically}},\\njournal = \"Annals of Pure and Applied Logic \",\\nvolume = \"132\",\\nnumber = \"1\",\\npages = \"1 - 25\",\\nyear = \"2005\",\\nnote = \"\",\\n\\nauthor = \"Melvin Fitting\",\\n}\\n\\n@incollection{Howard80,\\n    author = {Howard, William A.},\\n    booktitle = {To H. B. Curry: Essays on Combinatory Logic, Lambda Calculus, and Formalism},\\n    citeulike-article-id = {5917448},\\n    comment = {Reprint of 1969 article},\\n    editor = {Seldin, J. P. and Hindley, J. R.},\\n    keywords = {dissertation},\\n    pages = {479--490},\\n    posted-at = {2009-10-09 20:21:48},\\n    priority = {0},\\n    publisher = {Academic Press},\\n    title = {{The formulas-as-types notion of construction}},\\n    year = {1980}\\n}\\n\\n@TECHREPORT{Artemov95operationalmodal,\\n    author = {Sergei N. Artemov},\\n    title = {Operational Modal Logic},\\n    institution = {Cornell University},\\n    number = {MSI 9529},\\n    year = {1995}\\n}\\n\\n@inproceedings{alenda2012nested,\\n\\tAuthor = {Alenda, R{\\\\\\'e}gis and Olivetti, Nicola and Pozzato, Gian Luca},\\n\\tBooktitle = {Logics in Artificial Intelligence},\\n\\tDate-Added = {2013-03-05 20:13:34 +0000},\\n\\tDate-Modified = {2013-03-05 21:16:23 +0000},\\n\\tEditor = {Fari{\\\\~n}as del Cerro, Luis and Herzig, Andreas and Mengin, J{\\\\\\'e}r{\\\\^o}me},\\n\\tKeywords = {deep inference},\\n\\tPages = {14--27},\\n\\tPublisher = {Springer-Verlag},\\n\\tseries = {{LNCS}},\\n\\tTitle = {Nested Sequent Calculi for Conditional Logics},\\n\\tVolume = {7519},\\n\\tYear = {2012}}\\n\\n@incollection{conf/lfcs/Woltzenlogel13,\\nyear={2013},\\n\\nbooktitle={Logical Foundations of Computer Science},\\nvolume={7734},\\nseries = {{LNCS}},\\neditor={Artemov, Sergei and Nerode, Anil},\\n\\ntitle={Contextual Natural Deduction},\\n\\npublisher={Springer },\\nauthor={Woltzenlogel Paleo, Bruno},\\npages={372-386},\\nlanguage={English}\\n}\\n\\n@inproceedings{DBLP:conf/lmo/Odersky09,\\n  author    = {Martin Odersky},\\n  title     = {Essentials of {S}cala},\\n  booktitle = {Langages et Mod{\\\\`{e}}les {\\\\`{a}} Objets ({LMO} 2009)},\\n  pages     = {2},\\n  year      = {2009},\\n  crossref  = {DBLP:conf/lmo/2009},\\n  timestamp = {Fri, 25 Apr 2014 17:36:40 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/lmo/Odersky09},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/lmo/2009,\\n  editor    = {Bernard Carr{\\\\\\'{e}} and\\n               Olivier Zendra},\\n  title     = {Langages et Mod{\\\\`{e}}les {\\\\`{a}} Objets, {LMO} 2009, Nancy, France,\\n               25-27 mars 2009},\\n  series    = {{RNTI}},\\n  volume    = {{L-3}},\\n  publisher = {C{\\\\\\'{e}}padu{\\\\`{e}}s-{\\\\\\'{E}}ditions},\\n  year      = {2009},\\n\\n\\n  timestamp = {Fri, 25 Apr 2014 17:33:28 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/lmo/2009},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{DBLP:conf/birthday/Gordon00,\\n  author    = {Mike Gordon},\\n  title     = {From {LCF} to {HOL:} a short history},\\n  booktitle = {Proof, Language, and Interaction, Essays in Honour of Robin Milner},\\n  pages     = {169--186},\\n  year      = {2000},\\n  crossref  = {DBLP:conf/birthday/1999milner},\\n  timestamp = {Mon, 23 Jun 2008 08:52:23 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/birthday/Gordon00},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n@proceedings{DBLP:conf/birthday/1999milner,\\n  editor    = {Gordon D. Plotkin and\\n               Colin Stirling and\\n               Mads Tofte},\\n  title     = {Proof, Language, and Interaction, Essays in Honour of Robin Milner},\\n  publisher = {The {MIT} Press},\\n  year      = {2000},\\n\\n  timestamp = {Mon, 23 Jun 2008 08:50:50 +0200},\\n  biburl    = {https://dblp.org/rec/bib/conf/birthday/1999milner},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@article{DBLP:journals/tit/NewellS56,\\n  author    = {Allen Newell and\\n               Herbert A. Simon},\\n  title     = {The logic theory machine-A complex information processing system},\\n  journal   = {{IRE} Trans. Information Theory},\\n  volume    = {2},\\n  number    = {3},\\n  pages     = {61--79},\\n  year      = {1956},\\n\\n\\n  timestamp = {Sun, 28 May 2017 13:18:53 +0200},\\n  biburl    = {https://dblp.org/rec/bib/journals/tit/NewellS56},\\n  bibsource = {dblp computer science bibliography, https://dblp.org}\\n}\\n\\n@inproceedings{he2016deep_resnet,\\n  title={Deep residual learning for image recognition},\\n  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},\\n  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},\\n  pages={770--778},\\n  year={2016}\\n}\\n\\n@article{espeholt2018impala,\\n  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},\\n  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},\\n  journal={arXiv preprint arXiv:1802.01561},\\n  year={2018}\\n}\\n\\n@article{kingma2014adam,\\n  title={Adam: A method for stochastic optimization},\\n  author={Kingma, Diederik P and Ba, Jimmy},\\n  journal={arXiv preprint arXiv:1412.6980},\\n  year={2014}\\n}\\n\\n@incollection{paszke2019pytorch,\\ntitle = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},\\nauthor = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},\\nbooktitle = {Advances in Neural Information Processing Systems 32},\\neditor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\\\textquotesingle Alch\\\\\\'{e}-Buc and E. Fox and R. Garnett},\\npages = {8024--8035},\\nyear = {2019},\\npublisher = {Curran Associates, Inc.},\\nurl = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}\\n}',\n",
       "  'arxiv_citations': {'1801.08757': True,\n",
       "   '1802.01561': True,\n",
       "   '1609.05518': True,\n",
       "   '1801.08099': True,\n",
       "   '1909.05304': True,\n",
       "   '2002.12156': True,\n",
       "   '1412.6980': True,\n",
       "   '1809.06064': True,\n",
       "   '1806.06392': True,\n",
       "   '1809.11074': True,\n",
       "   '1707.06347': True,\n",
       "   '1804.07779': True,\n",
       "   '1904.07850': True,\n",
       "   '1502.05477': True,\n",
       "   '1811.08955': True,\n",
       "   '1811.12395': True,\n",
       "   '1811.06029': True,\n",
       "   '1801.10578': True,\n",
       "   '1901.01761': True,\n",
       "   '1404.7171': True,\n",
       "   '1909.01500': True,\n",
       "   '1807.06096': True,\n",
       "   '1408.1980': True,\n",
       "   '1205.4788': True,\n",
       "   '1602.01783': True,\n",
       "   '1606.01540': True,\n",
       "   '1907.07273': True,\n",
       "   '1306.6189': True,\n",
       "   '1602.03506': True,\n",
       "   '1805.10265': True,\n",
       "   '1804.10829': True,\n",
       "   '1712.01785': True,\n",
       "   '1505.04324': True,\n",
       "   '1404.7206': True,\n",
       "   '1703.06485': True,\n",
       "   '1802.08678': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Verification',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #124',\n",
       "   'newsletter_url': 'https://mailchi.mp/d1da78ed4aac/an-124provably-safe-exploration-through-shielding',\n",
       "   'summarizer': 'Rohin',\n",
       "   'summary': 'As we saw in the highlight, applications of formal verification to reinforcement learning and safe exploration often rely on _shielding_, in which any proposed unsafe actions are replaced by randomly chosen safe actions. Typically, this requires having an MDP model in a high-level, symbolic state space, such as by defining the MDP over the Atari simulator state, rather than learning from pixels.\\n\\nThis paper demonstrates that we can relax this requirement and learn policies on low-level observations, while still getting the safety guarantees of the shielding approach. The approach is simple: we define (manually) an abstract model of the environment, with a symbolic state space and dynamics model, and use this to create a shield as usual. Then, to learn the policy (which gets pixels as input), we use an object detector to transform the pixels into a symbolic state, and then use the shield if necessary to select which action to take. The authors show that as long as the error of the object detection step is low, the overall policy learning will remain safe.',\n",
       "   'opinion': 'nan',\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '2007.01223v1',\n",
       "   'arxiv_id': '2007.01223',\n",
       "   'title': 'Verifiably Safe Exploration for End-to-End Reinforcement Learning',\n",
       "   'authors': ['Nathan Hunt',\n",
       "    'Nathan Fulton',\n",
       "    'Sara Magliacane',\n",
       "    'Nghia Hoang',\n",
       "    'Subhro Das',\n",
       "    'Armando Solar-Lezama'],\n",
       "   'date_published': '2020-07-02 16:12:20+00:00',\n",
       "   'data_last_modified': '2020-07-02 16:12:20+00:00',\n",
       "   'url': 'http://arxiv.org/abs/2007.01223v1',\n",
       "   'abstract': 'Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.',\n",
       "   'author_comment': 'None',\n",
       "   'journal_ref': 'None',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'cs.AI',\n",
       "   'categories': \"['cs.AI', 'cs.LG', 'cs.LO', 'F.3.1; I.2.8']\",\n",
       "   'individual_summary': 'Title: Verifiably Safe Exploration for End-to-End Reinforcement Learning\\nAuthors: Nathan Hunt, Nathan Fulton, Sara Magliacane, Nghia Hoang, Subhro Das, Armando Solar-Lezama\\nPaper abstract: Deploying deep reinforcement learning in safety-critical settings requires developing algorithms that obey hard constraints during exploration. This paper contributes a first approach toward enforcing formal safety constraints on end-to-end policies with visual inputs. Our approach draws on recent advances in object detection and automated reasoning for hybrid dynamical systems. The approach is evaluated on a novel benchmark that emphasizes the challenge of safely exploring in the presence of hard constraints. Our benchmark draws from several proposed problem sets for safe learning and includes problems that emphasize challenges such as reward signals that are not aligned with safety constraints. On each of these benchmark problems, our algorithm completely avoids unsafe behavior while remaining competitive at optimizing for as much reward as is safe. We also prove that our method of enforcing the safety constraints preserves all safe policies from the original environment.\\nSummary: As we saw in the highlight, applications of formal verification to reinforcement learning and safe exploration often rely on _shielding_, in which any proposed unsafe actions are replaced by randomly chosen safe actions. Typically, this requires having an MDP model in a high-level, symbolic state space, such as by defining the MDP over the Atari simulator state, rather than learning from pixels.\\n\\nThis paper demonstrates that we can relax this requirement and learn policies on low-level observations, while still getting the safety guarantees of the shielding approach. The approach is simple: we define (manually) an abstract model of the environment, with a symbolic state space and dynamics model, and use this to create a shield as usual. Then, to learn the policy (which gets pixels as input), we use an object detector to transform the pixels into a symbolic state, and then use the shield if necessary to select which action to take. The authors show that as long as the error of the object detection step is low, the overall policy learning will remain safe.\\nMy opinion: nan',\n",
       "   'paper_text': '',\n",
       "   'text': \"HIGHLIGHTS\\n[Neurosymbolic Reinforcement Learning with Formally Verified Exploration](http://arxiv.org/abs/2009.12612) *(Greg Anderson et al)* (summarized by Rohin): A typical approach to formally verified safe exploration in RL is to compute a *shield*, which identifies a safe set of states and actions. After this shield is computed, it is wrapped around the environment to ensure that if a potentially unsafe action is about to be taken, it is replaced with a safe one. Then, a policy learning algorithm is applied as normal to learn a good policy.The key insight of this paper is to compute shields for specific *policies*, rather than creating a one-time shield that must apply to the entire state space. Since any given policy will only visit a small fraction of the state space, the shields are easier to compute and can be more permissive.They assume access to a *worst-case dynamics model*, which given a state and action outputs a *set* of states that could be visited. Given a policy , an *inductive safety invariant* is a set of safe states that includes all possible initial states and is closed under worst-case transitions: if you start at a state in the set, for any action that  suggests and for any state from the worst-case transition dynamics, that new state will still be in the set. Our algorithm will ensure that any policy we execute will have a corresponding inductive safety invariant.Formal verification techniques allow us to find inductive safety invariants for restricted classes of policies. This paper uses the space of deterministic, piecewise linear policies as its set of symbolic policies. But how do we apply this to neural nets? The key idea is to start with a safe symbolic policy, convert it to a neurosymbolic policy, take a neural net gradient step, convert back to a safe symbolic policy, and repeat until done. Lets go over each of these steps.First, lets suppose we have a symbolic policy g with inductive safety invariant . Then for any neural net f, we construct the policy h = f(s) if no matter what we stay within , otherwise g(s). It is easy to see that  is also an inductive safety invariant for h. Which f should we use to create h? The authors train a neural net to imitate g, and use that as their f. (Note that imitating g only requires executing g in the environment, and we know that g is safe.)Now that we have our neurosymbolic policy h, we need to take gradient steps on it. We collect data in the environment using h, but then for the gradient we ignore the symbolic part, and take a gradient step as though the data were collected using f. (It seems they used an on-policy algorithm for this, introducing bias; I am not sure why they didnt simply use an off-policy algorithm.) This produces a new neurosymbolic policy h that is still safe (since g and  are unchanged, and thats what guarantees safety).Finally, we need to convert h back into a symbolic policy g. This is done by a version of imitation learning that works in the symbolic policy space, where a new inductive safety invariant for g is found using formal verification techniques.To start off the whole process, we need an initial symbolic policy, which must be constructed by hand. The authors show using experiments in simple continuous control environments that this method can learn high-reward policies without ever having a safety violation. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** I really like this as an example of combining the performance of neural networks with the robustness of symbolic approaches. I especially like the fact that the shield is specialized to the current policy and updated over time: I think ML scales so well partly because it only deals with a tiny portion of the input space and can completely ignore the vast majority of possible inputs, and so if you want to add anything on top of ML you need to ensure you preserve this property to ensure scalability. Previous approaches required a shield that is correct across all possible states, failing to preserve this property; in contrast, this approach only requires a shield that is correct for the sequence of learned policies (on whichever states they visit).I should note that a large portion of why I like this paper is that it feels like it elegantly fits in *both* the formal verification *and* the ML fields. (I used to work in programming languages, of which formal verification is a subfield.) On the formal verification side, the guarantees are clean and simple, and the techniques used are canonical. On the ML side, I mentioned above why I like the fact that the shield is policy-specific and updated over time.As Ive said before, I think the real challenge in formal verification for AI alignment is how to handle fuzzy specifications. I think this paper shows a path forward: since the safety is established by an inductive invariant that can change over time, we could potentially use human feedback to establish these inductive invariants and update them over time, without requiring a human to fully specify at the outset exactly what is safe and what isnt. You could think of it as an expanding whitelist of states which the policy is allowed to visit. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n TECHNICAL AI ALIGNMENT\\n\\ufeff\\n\\n LEARNING HUMAN INTENT\\n[Imitation Learning in the Low-Data Regime](https://ai.googleblog.com/2020/09/imitation-learning-in-low-data-regime.html) *(Robert Dadashi et al)* (summarized by Zach): [Non-Adversarial Imitation Learning](http://arxiv.org/abs/2008.03525) ([AN #119](https://mailchi.mp/30b144930924/an-119ai-safety-when-agents-are-shaped-by-environments-not-rewards)) has become more popular recently due to the fact that GAN style architectures can be notoriously unstable during training. This paper makes a contribution by introducing an imitation learning strategy that relies on minimizing an upper bound on the Wasserstein distance between the imitator and expert state visitation distributions. The Wasserstein distance can be understood using the 'Earth Mover's Analogy'. In this interpretation, we view the distance as the cost of the most efficient transport strategy to move probability mass from the imitator distribution to the expert distribution. The advantage of such an approach is that the metric can be calculated in an offline way. If we calculate the distance for partial rollouts then we can create a dense, albeit non-stationary, reward for the imitator. In experiments, agents trained using the Wasserstein distance are able to learn control tasks using only a single trajectory. **Read more:** [Paper: Primal Wasserstein Imitation Learning](https://arxiv.org/abs/2006.04678) |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Zach's opinion:** With this paper, I conclude that IRL works for Mujoco-style control tasks. The performance of this method is similar to offline GAIL but is better justified and more stable. However, ultimately, I'm a bit skeptical of their claim that the method will generalize to other tasks. Results for GAIL/DAC are quite poor in Atari-like environments whereas pair-wise reward modeling seems to perform quite well. This would suggest a reward modeling approach would scale much better in more complicated settings. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n VERIFICATION\\n[An Inductive Synthesis Framework for Verifiable Reinforcement Learning](http://arxiv.org/abs/1907.07273) *(He Zhu et al)* (summarized by Rohin): This older paper has a pretty similar idea to the one in the highlighted paper. In order to compute a safety shield for a neural network RL agent, we first transform the neural network into a simpler more symbolic policy, prove safety of the symbolic policy, and then use the generated inductive safety invariant as a shield. This paper also uses deterministic piecewise linear policies as its space of symbolic policies. It only proves safety of the final learned RL policy, and so only guarantees safety at deployment, not at training time. (In other words, it does not guarantee safe exploration, and instead assumes that you are training in simulation so that safety is not a concern.) |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** Since this paper was published at PLDI, it is both longer and goes into a lot more of the details of how to actually perform each of these steps, as well as showing it with a running example on the inverted pendulum (where safety is defined as not going beyond a certain angle). Im not going to summarize them here but anyone interested in these technical details should check out this paper before the highlighted one (which is constrained by ML page limits and cant explain the techniques very well).Just as a reminder that learning programs does not automatically confer interpretability, I present to you the symbolic policy learned by their method for the inverted pendulum:Source code for symbolic policy here |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Verifiably Safe Exploration for End-to-End Reinforcement Learning](https://arxiv.org/abs/2007.01223) *(Nathan Hunt et al)* (summarized by Rohin): As we saw in the highlight, applications of formal verification to reinforcement learning and safe exploration often rely on *shielding*, in which any proposed unsafe actions are replaced by randomly chosen safe actions. Typically, this requires having an MDP model in a high-level, symbolic state space, such as by defining the MDP over the Atari simulator state, rather than learning from pixels.This paper demonstrates that we can relax this requirement and learn policies on low-level observations, while still getting the safety guarantees of the shielding approach. The approach is simple: we define (manually) an abstract model of the environment, with a symbolic state space and dynamics model, and use this to create a shield as usual. Then, to learn the policy (which gets pixels as input), we use an object detector to transform the pixels into a symbolic state, and then use the shield if necessary to select which action to take. The authors show that as long as the error of the object detection step is low, the overall policy learning will remain safe.[Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming](https://arxiv.org/abs/2010.11645) *(Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato et al)* (summarized by Rohin): In parallel with extending verification to sequential settings, as well as learning what specifications to verify, we also need to make verification significantly cheaper in order for it to be feasible to apply it to large neural networks. So far, we have only been able to achieve one of two very desirable properties at a time:1. The method can scale up to large, independently trained networks. (This has been achieved by methods using linear (LP) relaxations like [this one](https://arxiv.org/abs/1803.06567) ([AN #19](https://mailchi.mp/4b19d2caa5a9/alignment-newsletter-19)).)2. The method produces tight bounds and thus avoids producing vacuous results. (Achieved by using relaxations based on semidefinite programming (SDP) instead of linear ones.)This paper shows how you can massage the SDP version such that the resulting algorithm becomes scalable, changing the runtime and memory requirements from O(n^6) and O(n^4) to O(n) per iteration. The resulting algorithm can be applied to larger neural nets than previous SDP approaches and gives much tighter bounds than LP approaches. For example, on an adversarially trained CNN for MNIST (which SDP algorithms havent previously been applied to), they can verify 87.8% adversarial accuracy, while LP methods can only verify 0.4%. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n OTHER PROGRESS IN AI\\n\\ufeff\\n\\n REINFORCEMENT LEARNING\\n[Does On-Policy Data Collection Fix Errors in Off-Policy Reinforcement Learning?](https://bair.berkeley.edu/blog/2020/03/16/discor/) *(Aviral Kumar et al)* (summarized by Flo): Q-learning finds the optimal **Q**-function **Q\\\\*** by updating our estimate **Q(s,a)** for a state-action pair **(s,a)** to get closer to the immediate reward plus the discounted **Q**-value for the best action **a'** in the next state **s'**. To generate samples, we usually pick actions corresponding to high **Q**-values. In bandit problems where **s'** is always terminal and thus has all **Q**-values at zero, this leads to **corrective feedback**: If we overestimated an actions value, we will pick this action again soon and are quickly able to correct our misconception. In general MDPs, corrective feedback can be a lot weaker as our update of **Q(s,a)** also depends on the **Q**-values for the next state: To get corrective feedback, we need somewhat correct **Q**-values for the next state, but to get these we likely needed good values for the second to next state, etc. This is particularly problematic with function approximation as updating the current state's **Q**-value might lead to a worse estimate for values down the chain. Consequently, we might see convergence to suboptimal **Q**-functions, instable learning, or problems with sparse or noisy rewards.To deal with this, we would like to first prioritize correct estimates for states near the end of the chain. But in many branching problems, we actually observe these states with the least frequency such that their values are influenced disproportionally by other states' values when function approximation is used. The authors' approach, dubbed DisCor, reweighs the data distribution to account for this: We would like to preferentially sample states for which we expect **Q** to be close to **Q\\\\*** after the update and thus give more weight to state-action pairs when we expect the error **|Q\\\\*-Q|** to already be small. As we don't know **Q\\\\***, we rely on a bound for the error at a state-action pair **(s,a)** equal to the sum of the magnitudes of previous updates down the chain plus the initial error, discounted by the usual discount rate **** as we move back in time. Thus, the error in the next state one step ago is discounted by ****, the error in the second to next state two steps ago is discounted by **** squared and the initial error is discounted by **** to the **k**. This bound can be approximated by a neural network using a SARSA-like update rule, for which the influence of the unknown initial error fades for large **k** due to the discounting.DisCor is evaluated on MetaWorld tasks in both the single and multi-task setting and SAC augmented with DisCor clearly outperforms SAC in many settings. Similar improvements can be observed for DQN on Atari.**Read more:** [Paper: DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction](https://arxiv.org/abs/2003.07305) |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Flo's opinion:** Putting less weight on updating values with fluctuating targets seems like a good idea. As the approach does not require much additional compute if weights are shared for the **Q**-network and the network estimating the bound, and as it seems quite orthogonal to previous improvements to methods based on **Q**-functions, I would not be surprised if it became somewhat widely used. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n DEEP LEARNING\\n[Gradient Descent: The Ultimate Optimizer](https://arxiv.org/abs/1909.13371) *(Kartik Chandra et al)* (summarized by Rohin): Hyperparameter tuning is an important and tedious step for most applications of machine learning. Often this can cause a project to take significantly longer, as you need to have multiple training runs with different hyperparameters in order to identify which ones work best. How can we do better?This paper shows that in some cases, you can make the computation involving your hyperparameters differentiable, such that they too can be optimized using gradient descent *during the actual training run*. They show this for SGD and Adam (where for Adam they optimize all four hyperparameters, not just the learning rate). Since these hyperparameters are then optimized using another instantiation of gradient descent, that new instantiation also has its own hyperparameters that can once again be optimized. They show how to build an arbitrarily high stack of hyperparameter optimizers.In practice, building a stack of just 3 or 4 such optimizers makes it very robust to the initial choice of parameters by a human, while only increasing the cost of training by less than 2x. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin's opinion:** Fast hyperparameter tuning is a pretty important aspect of models. I particularly like [population-based training](https://deepmind.com/blog/article/population-based-training-neural-networks) for this purpose, because it doesnt require your computation to be differentiable. However, when you can make your computation differentiable, this method is probably significantly more efficient (and perhaps also more performant). |\\n\\n |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI'm always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2020 Alignment Newsletter, All rights reserved.*\\n\\n**\"}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1905.01034v1',\n",
       "  'title': 'Transfer of Adversarial Robustness Between Perturbation Types',\n",
       "  'authors': ['Daniel Kang',\n",
       "   'Yi Sun',\n",
       "   'Tom Brown',\n",
       "   'Dan Hendrycks',\n",
       "   'Jacob Steinhardt'],\n",
       "  'date_published': '2019-05-03 04:51:07+00:00',\n",
       "  'data_last_modified': '2019-05-03 04:51:07+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1905.01034v1',\n",
       "  'abstract': 'We study the transfer of adversarial robustness of deep neural networks between different perturbation types. While most work on adversarial examples has focused on $L_\\\\infty$ and $L_2$-bounded perturbations, these do not capture all types of perturbations available to an adversary. The present work evaluates 32 attacks of 5 different types against models adversarially trained on a 100-class subset of ImageNet. Our empirical results suggest that evaluating on a wide range of perturbation sizes is necessary to understand whether adversarial robustness transfers between perturbation types. We further demonstrate that robustness against one perturbation type may not always imply and may sometimes hurt robustness against other perturbation types. In light of these results, we recommend evaluation of adversarial defenses take place on a diverse range of perturbation types and sizes.',\n",
       "  'author_comment': '11 pages, 6 figures',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CR', 'stat.ML'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': './arxiv-20190502.tex',\n",
       "  'text': '---\\nabstract: |\\n  We study the transfer of adversarial robustness of deep neural networks between different perturbation types. While most work on adversarial examples has focused on $L_\\\\infty$ and $L_2$-bounded perturbations, these do not capture all types of perturbations available to an adversary. The present work evaluates 32 attacks of 5 different types against models adversarially trained on a 100-class subset of ImageNet. Our empirical results suggest that evaluating on a wide range of perturbation sizes is necessary to understand whether adversarial robustness transfers between perturbation types. We further demonstrate that robustness against one perturbation type *may not always* imply and may sometimes *hurt* robustness against other perturbation types. In light of these results, we recommend evaluation of adversarial defenses take place on a diverse range of perturbation types and sizes.\\nbibliography:\\n- adv-icml2019.bib\\n---\\n\\nIntroduction {#sec:intro}\\n============\\n\\nDeep networks have shown remarkable accuracy on benchmark tasks\\xa0[@he2016identity], but can also be fooled by imperceptible changes to inputs, known as adversarial examples\\xa0[@goodfellow2014explaining]. In response, researchers have studied the robustness of models, or how well models generalize in the presence of (potentially adversarial) bounded perturbations to inputs.\\n\\nHow can we tell if a model is robust? Evaluating model robustness is challenging because, while evaluating accuracy only requires a fixed distribution, evaluating the robustness of a model requires that the model have good performance in the presence of many, potentially hard to anticipate and model, perturbations. In the context of image classification, considerable work has focused on robustness to \"$L_{\\\\infty}$-bounded\" perturbations (perturbations with bounded per-pixel magnitude) [@goodfellow2014explaining; @madry2017towards; @xie2018feature]. However, models hardened against $L_{\\\\infty}$-bounded perturbations are still vulnerable to even small, perceptually minor departures from this family, such as small rotations and translations [@engstrom2017rotation]. Meanwhile, researchers continue to develop creative attacks that are difficult to even mathematically specify, such as fake eyeglasses, adversarial stickers, and 3D-printed objects [@sharif2018adversarial; @brown2017adversarial; @athalye2017synthesizing].\\n\\nThe perspective of this paper is that any single, simple-to-define type of perturbation is likely insufficient to capture what a deployed model will be subject to in the real world. To address this, we investigate robustness of models with respect to a *broad range* of perturbation types. We start with the following question:\\n\\n> When and how much does robustness to one type of perturbation transfer to other perturbations?\\n\\nWe study this question using adversarial training, a strong technique for adversarial defense applicable to any fixed attack [@goodfellow2014explaining; @madry2017towards]. We evaluate $32$ attacks of $5$ different types--$L_\\\\infty$\\xa0[@goodfellow2014explaining], $L_2$\\xa0[@carlini2017towards], $L_1$\\xa0[@chen2018ead], elastic deformations\\xa0[@xiao2018spatially], and JPEG\\xa0[@shin2017jpeg]--against adversarially trained ResNet-50 models on a 100-class subset of full-resolution ImageNet.\\n\\nOur results provide empirical evidence that models robust under one perturbation type *are not necessarily robust under other natural perturbation types*. We show that:\\n\\n1.  Evaluating on a carefully chosen range of perturbation sizes is important for measuring robustness transfer.\\n\\n2.  Adversarial training against the elastic deformation attack demonstrates that adversarial robustness against one perturbation type can transfer poorly to and at times hurt robustness to other perturbation types.\\n\\n3.  Adversarial training against the $L_2$ attack may be better than training against the widely used $L_\\\\infty$ attack.\\n\\nWhile any given set of perturbation types may not encompass all potential perturbations that can occur in practice, our results demonstrate that robustness can fail to transfer even across a small but diverse set of perturbation types. Prior work in this area [@sharma2017attacking; @jordan2019quantifying; @tramer2019adversarial] has studied transfer using single values of $\\\\varepsilon$ for each attack on lower resolution datasets; we believe our larger-scale study provides a more comprehensive and interpretable view on transfer between these attacks. We therefore suggest considering performance against several different perturbation types and sizes as a first step for rigorous evaluation of adversarial defenses.\\n\\nAdversarial attacks {#sec:attacks}\\n===================\\n\\nWe consider five types of adversarial attacks under the following framework. Let $f: \\\\mathbb{R}^{3 \\\\times 224 \\\\times 224} \\\\to \\\\mathbb{R}^{100}$ be a model mapping images to logits[^1], and let $\\\\ell(f(x), y)$ denote the cross-entropy loss. For an input $x$ with true label $y$ and a target class $y\\' \\\\neq y$, the attacks attempt to find $x\\'$ such that\\n\\n1.  the attacked image $x\\'$ is a perturbation of $x$, constrained in a sense which differs for each attack, and\\n\\n2.  the loss $\\\\ell(f(x\\'), y\\')$ is minimized (targeted attack).\\n\\nWe consider the targeted setting and the following attacks, described in more detail below:\\n\\n-   $L_\\\\infty$\\xa0[@goodfellow2014explaining]\\n\\n-   $L_2$\\xa0[@szegedy2013intriguing; @carlini2017towards]\\n\\n-   $L_1$\\xa0[@chen2018ead]\\n\\n-   JPEG\\n\\n-   Elastic deformation\\xa0[@xiao2018spatially]\\n\\nThe $L_\\\\infty$ and $L_2$ attacks are standard in the adversarial examples literature\\xa0[@athalye2018obfuscated; @papernot2016distillation; @madry2017towards; @carlini2017towards] and we chose the remaining attacks for diversity in perturbation type. We now describe each attack, with sample images in Figure [6](#fig:sample-images){reference-type=\"ref\" reference=\"fig:sample-images\"} and Appendix [5](#sec:attack-samples){reference-type=\"ref\" reference=\"sec:attack-samples\"}. We clamp output pixel values to $[0, 255]$.\\n\\nFor $L_p$ attacks with $p \\\\in \\\\{1, 2, \\\\infty\\\\}$, the constraint allows an image $x \\\\in \\\\mathbb{R}^{3 \\\\times 224 \\\\times 224}$, viewed as a vector of RGB pixel values, to be modified to an attacked image $x\\' = x + \\\\delta$ with $$\\\\|x\\' - x\\\\|_p \\\\leq \\\\varepsilon,$$ where $\\\\|\\\\cdot\\\\|_p$ denotes the $L_p$-norm on $\\\\mathbb{R}^{3 \\\\times 224 \\\\times 224}$. For the $L_\\\\infty$ and $L_2$ attacks, we optimize using randomly-initialized projected gradient descent (PGD), which optimizes the perturbation $\\\\delta$ by gradient descent and projection to the $L_\\\\infty$ and $L_2$ balls [@madry2017towards]. For the $L_1$ attack, we use the randomly-initialized Frank-Wolfe algorithm\\xa0[@frank1956algorithm], detailed in Appendix [7](#sec:fw-pseudo){reference-type=\"ref\" reference=\"sec:fw-pseudo\"}. We believe that our Frank-Wolfe algorithm is more principled than the optimization used in existing $L_1$ attacks such as EAD.\\n\\n  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n  ![Sample attacked images with label \"black swan\" for $\\\\varepsilon$ at the top end of our range. [\\\\[fig:sample-images\\\\]]{#fig:sample-images label=\"fig:sample-images\"}](figures/norm_rand/clean_10.jpg){#fig:sample-images width=\".85in\"}   ![Sample attacked images with label \"black swan\" for $\\\\varepsilon$ at the top end of our range. [\\\\[fig:sample-images\\\\]]{#fig:sample-images label=\"fig:sample-images\"}](figures/norm_rand/linf_32_10.jpg){#fig:sample-images width=\".85in\"}   ![Sample attacked images with label \"black swan\" for $\\\\varepsilon$ at the top end of our range. [\\\\[fig:sample-images\\\\]]{#fig:sample-images label=\"fig:sample-images\"}](figures/norm_rand/l2_4800_10.jpg){#fig:sample-images width=\".85in\"}\\n  clean                                                                                                                                                                                                                                      $L_\\\\infty$                                                                                                                                                                                                                                   $L_2$\\n  ![Sample attacked images with label \"black swan\" for $\\\\varepsilon$ at the top end of our range. [\\\\[fig:sample-images\\\\]]{#fig:sample-images label=\"fig:sample-images\"}](figures/norm_rand/l1_2_10.jpg){#fig:sample-images width=\".85in\"}    ![Sample attacked images with label \"black swan\" for $\\\\varepsilon$ at the top end of our range. [\\\\[fig:sample-images\\\\]]{#fig:sample-images label=\"fig:sample-images\"}](figures/norm_rand/jpeg_1_10.jpg){#fig:sample-images width=\".85in\"}    ![Sample attacked images with label \"black swan\" for $\\\\varepsilon$ at the top end of our range. [\\\\[fig:sample-images\\\\]]{#fig:sample-images label=\"fig:sample-images\"}](figures/norm_rand/elastic_8_10.jpg){#fig:sample-images width=\".85in\"}\\n  $L_1$                                                                                                                                                                                                                                      JPEG                                                                                                                                                                                                                                         elastic\\n  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\n\\nAs discussed in @shin2017jpeg as a defense, JPEG compression applies a lossy linear transformation based on the discrete cosine transform (denoted by $\\\\mathsf{JPEG}$) to image space, followed by quantization. The JPEG attack, which we believe is new to this work, imposes on the attacked image $x\\'$ an $L_\\\\infty$-constraint in this transformed space: $$\\\\|\\\\mathsf{JPEG}(x) - \\\\mathsf{JPEG}(x\\')\\\\|_\\\\infty \\\\leq \\\\varepsilon.$$ We optimize $z = \\\\mathsf{JPEG}(x\\')$ with randomly initialized PGD and apply a right inverse of $\\\\mathsf{JPEG}$ to obtain the attacked image.\\n\\nThe elastic deformation attack allows perturbations $$x\\' = \\\\mathsf{Flow}(x, V),$$ where $V: \\\\{1, \\\\ldots, 224\\\\}^2 \\\\to \\\\mathbb{R}^2$ is a vector field on pixel space, and $\\\\mathsf{Flow}$ sets the value of pixel $(i, j)$ to the (bilinearly interpolated) value at $(i, j) + V(i, j)$. We constrain $V$ to be the convolution of a vector field $W$ with a $25 \\\\times 25$ Gaussian kernel with standard deviation $3$, and enforce that $$\\\\|W(i, j)\\\\|_\\\\infty \\\\leq \\\\varepsilon\\\\qquad \\\\text{ for } i, j \\\\in \\\\{1, \\\\ldots, 224\\\\}.$$ We optimize the value of $W$ with randomly initialized PGD. Note that our attack differs in details from @xiao2018spatially, but is similar in spirit.\\n\\n![image](figures/grid_ordered.pdf){width=\"0.99\\\\\\\\linewidth\"}\\n\\nExperiments {#sec:experiments}\\n===========\\n\\nWe measure transfer of adversarial robustness by evaluating our attacks against adversarially trained models. For each attack, we adversarially train models against the attack for a range of perturbation sizes $\\\\varepsilon$. We then evaluate each adversarially trained model against each attack, giving the $2$-dimensional accuracy grid of attacks evaluated against adversarially trained models shown in Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} (analyzed in detail in Section\\xa0[3.2](#sec:results){reference-type=\"ref\" reference=\"sec:results\"}).\\n\\nExperimental setup\\n------------------\\n\\n**Dataset and model.** We use the $100$-class subset of ImageNet-1K\\xa0[@deng2009imagenet] containing classes whose WordNet ID is a multiple of $10$. We use the ResNet-50\\xa0[@he2016identity] architecture with standard 224$\\\\times$224 resolution as implemented in `torchvision`. We believe this full resolution is necessary for the elastic and JPEG attacks.\\n\\n**Training hyperparameters.** We trained on machines with 8 Nvidia V100 GPUs using standard data augmentation practices\\xa0[@he2016identity]. Following best practices for multi-GPU training\\xa0[@goyal2017accurate], we used synchronized SGD for $90$ epochs with a batch size of 32$\\\\times$8 and a learning rate schedule in which the learning rate is \"warmed up\" for 5 epochs and decayed at epochs 30, 60, and 80 by a factor of 10. Our initial learning rate after warm-up was 0.1, momentum was $0.9$, and weight decay was $5\\\\times10^{-6}$.\\n\\n  attack       optimization algorithm   $\\\\varepsilon$ or $\\\\varepsilon_{\\\\text{max}}$ values   step size                                   steps (adversarial training)   steps (eval)   \\n  ------------ ------------------------ ---------------------------------------------------- ------------------------------------------- ------------------------------ -------------- --\\n  $L_\\\\infty$   PGD                      $\\\\{2^i \\\\mid 0 \\\\leq i \\\\leq 5\\\\}$                       $\\\\frac{\\\\varepsilon}{\\\\sqrt{\\\\text{steps}}}$   10                             50             \\n  $L_2$        PGD                      $\\\\{150 \\\\cdot 2^i \\\\mid 0 \\\\leq i \\\\leq 5\\\\}$             $\\\\frac{\\\\varepsilon}{\\\\sqrt{\\\\text{steps}}}$   10                             50             \\n  $L_1$        Frank-Wolfe              $\\\\{9562.5 \\\\cdot 2^i \\\\mid 0 \\\\leq i \\\\leq 6\\\\}$          N/A                                         10                             50             \\n  JPEG         PGD                      $\\\\{0.03125 \\\\cdot 2^i \\\\mid 0 \\\\leq i \\\\leq 5\\\\}$         $\\\\frac{\\\\varepsilon}{\\\\sqrt{\\\\text{steps}}}$   10                             50             \\n  Elastic      PGD                      $\\\\{0.25 \\\\cdot 2^i \\\\mid 0 \\\\leq i \\\\leq 6\\\\}$            $\\\\frac{\\\\varepsilon}{\\\\sqrt{\\\\text{steps}}}$   30                             100            \\n\\n**Adversarial training.** We harden models against attacks using adversarial training\\xa0[@madry2017towards]. To train against attack $A$, for each mini-batch of training images, we select target classes for each image uniformly at random from the $99$ incorrect classes. We generate adversarial images by applying the targeted attack $A$ to the current model with $\\\\varepsilon$ chosen uniformly at random between $0$ and $\\\\varepsilon_{\\\\text{max}}$. Finally, we update the model with a step of synchronized SGD using these adversarial images alone.\\n\\nWe list attack parameters used for training in Table [\\\\[tab:adv-settings\\\\]](#tab:adv-settings){reference-type=\"ref\" reference=\"tab:adv-settings\"}. For the PGD attack, we chose step size $\\\\frac{\\\\varepsilon}{\\\\sqrt{\\\\text{steps}}}$, motivated by the fact that taking step size proportional to $1/\\\\sqrt{\\\\text{steps}}$ is optimal for non-smooth convex functions [@nemirovski1978cezari; @nemirovski1983complexity]. Note that the greater number of PGD steps for elastic deformation is due to the greater difficulty of its optimization problem, which we are not confident is fully solved even with this greater number of steps.\\n\\n**Attack hyperparameters.** We evaluate our adversarially trained models on the (subsetted) ImageNet-1K validation set against targeted attacks with target chosen uniformly at random from among the $99$ incorrect classes. We list attack parameters for evaluation in Table [\\\\[tab:adv-settings\\\\]](#tab:adv-settings){reference-type=\"ref\" reference=\"tab:adv-settings\"}. As suggested in [@carlini2019evaluating], we use more steps for evaluation than for adversarial training to ensure PGD converges.\\n\\nResults and analysis {#sec:results}\\n--------------------\\n\\nUsing the results of our adversarial training and evaluation experiments in Figure\\xa0[\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"}, we draw the following conclusions.\\n\\n**Choosing $\\\\varepsilon$ well is important.** Because attack strength increases with the allowed perturbation magnitude $\\\\varepsilon$, comparing robustness between different perturbation types requires a careful choice of $\\\\varepsilon$ for both attacks. First, we observe that a *range* of $\\\\varepsilon$ yielding comparable attack strengths should be used for all attacks to avoid drawing misleading conclusions. We suggest the following principles for choosing this range, which we followed for the parameters in Table [\\\\[tab:adv-settings\\\\]](#tab:adv-settings){reference-type=\"ref\" reference=\"tab:adv-settings\"}:\\n\\n1.  Models adversarially trained against the minimum value of $\\\\varepsilon$ should have validation accuracy comparable to that of a model trained on unattacked data.\\n\\n2.  Attacks with the maximum value of $\\\\varepsilon$ should substantially reduce validation accuracy in adversarial training or perturb the images enough to confuse humans.\\n\\nTo illustrate this point, we provide in Appendix [6](#sec:trunc-range){reference-type=\"ref\" reference=\"sec:trunc-range\"} a subset of Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} with $\\\\varepsilon$ ranges that differ in strength between attacks; the (deliberately) biased ranges of $\\\\varepsilon$ chosen in this subset cause the $L_1$ and elastic attacks to be perceived as stronger than our full results reveal.\\n\\nSecond, even if two attacks are evaluated on ranges of $\\\\varepsilon$ of comparable strength, the specific values of $\\\\varepsilon$ chosen within those ranges may be important. In our experiments, we scaled $\\\\varepsilon$ geometrically for all attacks, but when interpreting our results, attack strength may not scale in the same way with $\\\\varepsilon$ for different attacks. As a result, we only draw conclusions which are invariant to the precise scaling of attack strength with $\\\\varepsilon$. We illustrate this type of analysis with the following two examples.\\n\\n**Robustness against elastic transfers poorly to the other attacks.** In Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"}, the accuracies of models adversarially trained against elastic are higher against elastic than the other attacks, meaning that for these values of $\\\\varepsilon$, robustness against elastic does not imply robustness against other attacks. On the other hand, training against elastic with $\\\\varepsilon\\\\geq 4$ generally increases accuracy against elastic with $\\\\varepsilon\\\\geq 4$, but decreases accuracy against all other attacks.\\n\\nTogether, these imply that the lack of transfer we observe in Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} is not an artifact of the specific values of $\\\\varepsilon$ we chose, but rather a broader effect at the level of perturbation types. In addition, this example shows that increasing robustness to larger perturbation sizes of a given type can *hurt* robustness to other perturbation types. This effect is only visible by considering an appropriate range of $\\\\varepsilon$ and cannot be detected from a single value of $\\\\varepsilon$ alone.\\n\\n**$L_2$ adversarial training is weakly better than $L_\\\\infty$.** Comparing rows of Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} corresponding to training against $L_2$ with $\\\\varepsilon\\\\in \\\\{300, 600, 1200, 2400, 4800\\\\}$ with rows corresponding to training against $L_\\\\infty$ with $\\\\varepsilon\\\\in \\\\{1, 2, 4, 8, 16\\\\}$, we see that training against $L_2$ yields slightly lower accuracies against $L_\\\\infty$ attacks and higher accuracies against all other attacks. Because this effect extends to all $\\\\varepsilon$ for which training against $L_\\\\infty$ is helpful, it does not depend on the relation between $L_\\\\infty$ attack strength and $\\\\varepsilon$. In fact, against the stronger half of our attacks, training against $L_2$ with $\\\\varepsilon= 4800$ gives comparable or better accuracy to training against $L_\\\\infty$ with adaptive choice of $\\\\varepsilon$. This provides some evidence that $L_2$ is more effective to train against than $L_\\\\infty$.\\n\\nConclusion {#sec:conclusion}\\n==========\\n\\nThis work presents an empirical study of when and how much robustness transfers between different adversarial perturbation types. Our results on adversarial training and evaluation of 32 different attacks on a 100-class subset of ImageNet-1K highlight the importance of considering a diverse range of perturbation sizes and types for assessing transfer between types, and we recommend this as a guideline for evaluating adversarial robustness.\\n\\nAcknowledgements {#acknowledgements .unnumbered}\\n================\\n\\nD.\\xa0K.\\xa0was supported by NSF Grant DGE-1656518. Y.\\xa0S.\\xa0was supported by a Junior Fellow award from the Simons Foundation and NSF Grant DMS-1701654. D.\\xa0K., Y.\\xa0S., and J.\\xa0S.\\xa0were supported by a grant from the Open Philanthropy Project.\\n\\nSample attacked images {#sec:attack-samples}\\n======================\\n\\nIn this appendix, we give more comprehensive sample outputs for our adversarial attacks. Figures [\\\\[fig:strong-attack\\\\]](#fig:strong-attack){reference-type=\"ref\" reference=\"fig:strong-attack\"} and [\\\\[fig:weak-attack\\\\]](#fig:weak-attack){reference-type=\"ref\" reference=\"fig:weak-attack\"} show sample attacked images for attacks with relatively large and small $\\\\varepsilon$ in our range, respectively. Figure [\\\\[fig:attack-transfer\\\\]](#fig:attack-transfer){reference-type=\"ref\" reference=\"fig:attack-transfer\"} shows examples of how attacked images can be influenced by different types of adversarial training for defense models. In all cases, the images were generated by running the specified attack against an adversarially trained model with parameters specified in Table [\\\\[tab:adv-settings\\\\]](#tab:adv-settings){reference-type=\"ref\" reference=\"tab:adv-settings\"} for both evaluation and adversarial training.\\n\\n  ---------------- --------------------------------------------------------- ----------------------------------------------------------- ----------------------------------------------------------- -------------------------------------------------------- ---------------------------------------------------------- -------------------------------------------------------------\\n                   clean                                                     $L_\\\\infty$                                                  $L_2$                                                       $L_1$                                                    JPEG                                                       elastic\\n                                                                             $\\\\varepsilon=32$                                            $\\\\varepsilon=4800$                                          $\\\\varepsilon=306000$                                     $\\\\varepsilon=1$                                            $\\\\varepsilon=8$\\n  black swan       ![image](figures/norm_rand/clean_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_32_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_4800_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_2_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_1_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_8_10.jpg){width=\".85in\"}\\n  chain mail       ![image](figures/norm_rand/clean_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_32_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_4800_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_2_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_1_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_8_49.jpg){width=\".85in\"}\\n  espresso maker   ![image](figures/norm_rand/clean_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_32_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_4800_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_2_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_1_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_8_55.jpg){width=\".85in\"}\\n  manhole cover    ![image](figures/norm_rand/clean_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_32_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_4800_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_2_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_1_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_8_64.jpg){width=\".85in\"}\\n  water tower      ![image](figures/norm_rand/clean_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_32_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_4800_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_2_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_1_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_8_90.jpg){width=\".85in\"}\\n  orange           ![image](figures/norm_rand/clean_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_32_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_4800_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_2_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_1_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_8_95.jpg){width=\".85in\"}\\n  volcano          ![image](figures/norm_rand/clean_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_32_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_4800_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_2_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_1_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_8_98.jpg){width=\".85in\"}\\n  ---------------- --------------------------------------------------------- ----------------------------------------------------------- ----------------------------------------------------------- -------------------------------------------------------- ---------------------------------------------------------- -------------------------------------------------------------\\n\\n  ---------------- --------------------------------------------------------- ---------------------------------------------------------- ---------------------------------------------------------- ---------------------------------------------------------- ------------------------------------------------------------- -------------------------------------------------------------\\n                   clean                                                     $L_\\\\infty$                                                 $L_2$                                                      $L_1$                                                      JPEG                                                          elastic\\n                                                                             $\\\\varepsilon=4$                                            $\\\\varepsilon=600$                                          $\\\\varepsilon=38250$                                        $\\\\varepsilon=0.125$                                           $\\\\varepsilon=1$\\n  black swan       ![image](figures/norm_rand/clean_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_4_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_600_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_025_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_0125_10.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_1_10.jpg){width=\".85in\"}\\n  chain mail       ![image](figures/norm_rand/clean_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_4_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_600_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_025_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_0125_49.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_1_49.jpg){width=\".85in\"}\\n  espresso maker   ![image](figures/norm_rand/clean_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_4_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_600_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_025_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_0125_55.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_1_55.jpg){width=\".85in\"}\\n  manhole cover    ![image](figures/norm_rand/clean_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_4_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_600_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_025_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_0125_64.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_1_64.jpg){width=\".85in\"}\\n  water tower      ![image](figures/norm_rand/clean_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_4_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_600_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_025_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_0125_90.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_1_90.jpg){width=\".85in\"}\\n  orange           ![image](figures/norm_rand/clean_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_4_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_600_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_025_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_0125_95.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_1_95.jpg){width=\".85in\"}\\n  volcano          ![image](figures/norm_rand/clean_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/linf_4_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/l2_600_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/l1_025_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/jpeg_0125_98.jpg){width=\".85in\"}   ![image](figures/norm_rand/elastic_1_98.jpg){width=\".85in\"}\\n  ---------------- --------------------------------------------------------- ---------------------------------------------------------- ---------------------------------------------------------- ---------------------------------------------------------- ------------------------------------------------------------- -------------------------------------------------------------\\n\\n  ---------------------- -------------------------------------------------------- ------------------------------------------------------------ ----------------------------------------------------------------- ------------------------------------------------------------ -------------------------------------------------------------- ----------------------------------------------------------------- --------------------------------------------------------------\\n  attack                 clean                                                    $L_2$ $\\\\varepsilon=2400$                                     $L_2$ $\\\\varepsilon=2400$                                          $L_1$ $\\\\varepsilon= 153000$                                  $L_1$ $\\\\varepsilon=153000$                                     elastic $\\\\varepsilon=4$                                           elastic $\\\\varepsilon=4$\\n  adversarial training                                                            $L_1$ $\\\\varepsilon= 153000$                                  elastic $\\\\varepsilon= 4$                                          $L_2$ $\\\\varepsilon=2400$                                     elastic $\\\\varepsilon=4$                                        $L_2$ $\\\\varepsilon=2400$                                          $L_1$ $\\\\varepsilon=153000$\\n  black swan             ![image](figures/norm_rand/clean_10.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_l11_10.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_elastic4_10.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_l22400_10.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_elastic4_10.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l22400_10.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l11_10.jpg){width=\".7in\"}\\n  chain mail             ![image](figures/norm_rand/clean_49.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_l11_49.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_elastic4_49.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_l22400_49.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_elastic4_49.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l22400_49.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l11_49.jpg){width=\".7in\"}\\n  espresso maker         ![image](figures/norm_rand/clean_55.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_l11_55.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_elastic4_55.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_l22400_55.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_elastic4_55.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l22400_55.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l11_55.jpg){width=\".7in\"}\\n  manhole cover          ![image](figures/norm_rand/clean_64.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_l11_64.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_elastic4_64.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_l22400_64.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_elastic4_64.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l22400_64.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l11_64.jpg){width=\".7in\"}\\n  water tower            ![image](figures/norm_rand/clean_90.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_l11_90.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_elastic4_90.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_l22400_90.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_elastic4_90.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l22400_90.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l11_90.jpg){width=\".7in\"}\\n  orange                 ![image](figures/norm_rand/clean_95.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_l11_95.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_elastic4_95.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_l22400_95.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_elastic4_95.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l22400_95.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l11_95.jpg){width=\".7in\"}\\n  volcano                ![image](figures/norm_rand/clean_98.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_l11_98.jpg){width=\".7in\"}   ![image](figures/pairs/l22400_vs_elastic4_98.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_l22400_98.jpg){width=\".7in\"}   ![image](figures/pairs/l11_vs_elastic4_98.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l22400_98.jpg){width=\".7in\"}   ![image](figures/pairs/elastic4_vs_l11_98.jpg){width=\".7in\"}\\n  ---------------------- -------------------------------------------------------- ------------------------------------------------------------ ----------------------------------------------------------------- ------------------------------------------------------------ -------------------------------------------------------------- ----------------------------------------------------------------- --------------------------------------------------------------\\n\\nEvaluation on a truncated $\\\\varepsilon$ range {#sec:trunc-range}\\n=============================================\\n\\nIn this appendix, we show in Figure [\\\\[fig:grid-small\\\\]](#fig:grid-small){reference-type=\"ref\" reference=\"fig:grid-small\"} a subset of Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} with a truncated range of $\\\\varepsilon$. In particular, we omitted small values of $\\\\varepsilon$ for $L_1$, elastic, and JPEG and large values of $\\\\varepsilon$ for $L_\\\\infty$ and $L_2$. The resulting accuracy grid gives several misleading impressions, including:\\n\\n1.  The $L_1$ attack is stronger than $L_\\\\infty$, $L_2$, and JPEG.\\n\\n2.  Training against the other attacks gives almost no robustness against the elastic attack.\\n\\nThe full range of results in Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} shows that these two purported effects are artifacts of the incorrectly truncated range of $\\\\varepsilon$ used in Figure [\\\\[fig:grid-small\\\\]](#fig:grid-small){reference-type=\"ref\" reference=\"fig:grid-small\"}. In particular:\\n\\n1.  The additional smaller $\\\\varepsilon$ columns for the $L_1$ attack in Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} demonstrate its perceived strength in Figure [\\\\[fig:grid-small\\\\]](#fig:grid-small){reference-type=\"ref\" reference=\"fig:grid-small\"} is an artifact of incorrectly omitting these values.\\n\\n2.  The additional smaller $\\\\varepsilon$ columns for the elastic attack in Figure [\\\\[fig:grid\\\\]](#fig:grid){reference-type=\"ref\" reference=\"fig:grid\"} reveal that training against the other attacks is effective in defending against weak versions of the elastic attack, contrary to the impression given by Figure [\\\\[fig:grid-small\\\\]](#fig:grid-small){reference-type=\"ref\" reference=\"fig:grid-small\"}.\\n\\n![image](figures/grid_small.pdf){width=\"0.99\\\\\\\\linewidth\"}\\n\\n**Input:** function $f$, initial input $x \\\\in [0,1]^d$, $L_1$ radius $\\\\rho$, number of steps $T$. **Output:** approximate maximizer $\\\\bar{x}$ of $f$ over the truncated $L_1$ ball $B_1(\\\\rho; x) \\\\cap [0,1]^d$ centered at $x$. $x^{(0)} \\\\gets \\\\mathrm{RandomInit}(x)$ $g \\\\gets \\\\nabla f(x^{(t-1)})$ $s_k \\\\gets \\\\text{index of the coordinate of $g$ by with $k^\\\\text{th}$ largest norm}$ $S_k \\\\gets \\\\{s_1, \\\\ldots, s_k\\\\}$. $b_i \\\\gets 1-x_i$ $b_i \\\\gets -x_i$\\n\\n$M_k \\\\gets \\\\sum_{i \\\\in S_k} |b_i|$ $k^* \\\\gets \\\\max\\\\{k \\\\mid M_k \\\\leq \\\\rho\\\\}$ $\\\\hat{x}_i \\\\gets x_i + b_i$ $\\\\hat{x}_i \\\\gets x_i + (\\\\rho - M_{k^*}) \\\\operatorname{sign}(g_i)$ $\\\\hat{x}_i \\\\gets x_i$ $x^{(t)} \\\\gets (1-\\\\frac{1}{t})x^{(t-1)} + \\\\frac{1}{t}\\\\hat{x}$ $\\\\bar{x} \\\\gets x^{(T)}$\\n\\n$L_1$ Attack {#sec:fw-pseudo}\\n============\\n\\nWe chose to use the Frank-Wolfe algorithm for optimizing the $L_1$ attack, as Projected Gradient Descent would require projecting onto a truncated $L_1$ ball, which is a complicated operation. In contrast, Frank-Wolfe only requires optimizing linear functions $g^{\\\\top}x$ over a truncated $L_1$ ball; this can be done by sorting coordinates by the magnitude of $g$ and moving the top $k$ coordinates to the boundary of their range (with $k$ chosen by binary search). This is detailed in Algorithm [\\\\[alg:fw-alg\\\\]](#alg:fw-alg){reference-type=\"ref\" reference=\"alg:fw-alg\"}.\\n\\n[^1]: For all experiments, the input is a $224 \\\\times 224$ image, and the output is one of $100$ classes.\\n',\n",
       "  'bibliography_bbl': \"\\\\begin{thebibliography}{24}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Athalye et~al.(2017)Athalye, Engstrom, Ilyas, and\\n  Kwok]{athalye2017synthesizing}\\nAthalye, A., Engstrom, L., Ilyas, A., and Kwok, K.\\n\\\\newblock Synthesizing robust adversarial examples.\\n\\\\newblock \\\\emph{CoRR}, abs/1707.07397, 2017.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1707.07397}.\\n\\n\\\\bibitem[Athalye et~al.(2018)Athalye, Carlini, and\\n  Wagner]{athalye2018obfuscated}\\nAthalye, A., Carlini, N., and Wagner, D.\\n\\\\newblock Obfuscated gradients give a false sense of security: Circumventing\\n  defenses to adversarial examples.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1802.00420}, 2018.\\n\\n\\\\bibitem[Brown et~al.(2017)Brown, Man{\\\\'{e}}, Roy, Abadi, and\\n  Gilmer]{brown2017adversarial}\\nBrown, T.~B., Man{\\\\'{e}}, D., Roy, A., Abadi, M., and Gilmer, J.\\n\\\\newblock Adversarial patch.\\n\\\\newblock \\\\emph{CoRR}, abs/1712.09665, 2017.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1712.09665}.\\n\\n\\\\bibitem[Carlini \\\\& Wagner(2017)Carlini and Wagner]{carlini2017towards}\\nCarlini, N. and Wagner, D.\\n\\\\newblock Towards evaluating the robustness of neural networks.\\n\\\\newblock In \\\\emph{2017 IEEE Symposium on Security and Privacy (SP)}, pp.\\\\\\n  39--57. IEEE, 2017.\\n\\n\\\\bibitem[Carlini et~al.(2019)Carlini, Athalye, Papernot, Brendel, Rauber,\\n  Tsipras, Goodfellow, Madry, and Kurakin]{carlini2019evaluating}\\nCarlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D.,\\n  Goodfellow, I.~J., Madry, A., and Kurakin, A.\\n\\\\newblock On evaluating adversarial robustness.\\n\\\\newblock \\\\emph{CoRR}, abs/1902.06705, 2019.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1902.06705}.\\n\\n\\\\bibitem[Chen et~al.(2018)Chen, Sharma, Zhang, Yi, and Hsieh]{chen2018ead}\\nChen, P.-Y., Sharma, Y., Zhang, H., Yi, J., and Hsieh, C.-J.\\n\\\\newblock {EAD}: {E}lastic-net attacks to deep neural networks via adversarial\\n  examples.\\n\\\\newblock In \\\\emph{Thirty-second AAAI conference on artificial intelligence},\\n  2018.\\n\\n\\\\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and\\n  Fei-Fei]{deng2009imagenet}\\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.\\n\\\\newblock Imagenet: A large-scale hierarchical image database.\\n\\\\newblock In \\\\emph{2009 {IEEE} conference on computer vision and pattern\\n  recognition}, pp.\\\\  248--255. {IEEE}, 2009.\\n\\n\\\\bibitem[Engstrom et~al.(2017)Engstrom, Tran, Tsipras, Schmidt, and\\n  Madry]{engstrom2017rotation}\\nEngstrom, L., Tran, B., Tsipras, D., Schmidt, L., and Madry, A.\\n\\\\newblock A rotation and a translation suffice: Fooling {CNNs} with simple\\n  transformations.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1712.02779}, 2017.\\n\\n\\\\bibitem[Frank \\\\& Wolfe(1956)Frank and Wolfe]{frank1956algorithm}\\nFrank, M. and Wolfe, P.\\n\\\\newblock An algorithm for quadratic programming.\\n\\\\newblock \\\\emph{Naval research logistics quarterly}, 3\\\\penalty0 (1-2):\\\\penalty0\\n  95--110, 1956.\\n\\n\\\\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and\\n  Szegedy]{goodfellow2014explaining}\\nGoodfellow, I.~J., Shlens, J., and Szegedy, C.\\n\\\\newblock Explaining and harnessing adversarial examples.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1412.6572}, 2014.\\n\\n\\\\bibitem[Goyal et~al.(2017)Goyal, Doll{\\\\'a}r, Girshick, Noordhuis, Wesolowski,\\n  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}\\nGoyal, P., Doll{\\\\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,\\n  A., Tulloch, A., Jia, Y., and He, K.\\n\\\\newblock Accurate, large minibatch {SGD}: Training {I}magenet in 1 hour.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1706.02677}, 2017.\\n\\n\\\\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\n\\\\newblock Identity mappings in deep residual networks.\\n\\\\newblock In \\\\emph{European conference on computer vision}, pp.\\\\  630--645.\\n  Springer, 2016.\\n\\n\\\\bibitem[{Jordan} et~al.(2019){Jordan}, {Manoj}, {Goel}, and\\n  {Dimakis}]{jordan2019quantifying}\\n{Jordan}, M., {Manoj}, N., {Goel}, S., and {Dimakis}, A.~G.\\n\\\\newblock {Quantifying Perceptual Distortion of Adversarial Examples}.\\n\\\\newblock \\\\emph{arXiv e-prints}, art. arXiv:1902.08265, Feb 2019.\\n\\n\\\\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and\\n  Vladu]{madry2017towards}\\nMadry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.\\n\\\\newblock Towards deep learning models resistant to adversarial attacks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1706.06083}, 2017.\\n\\n\\\\bibitem[Nemirovski \\\\& Yudin(1978)Nemirovski and Yudin]{nemirovski1978cezari}\\nNemirovski, A. and Yudin, D.\\n\\\\newblock On {C}ezari's convergence of the steepest descent method for\\n  approximating saddle point of convex-concave functions.\\n\\\\newblock In \\\\emph{Soviet Math. Dokl}, volume~19, pp.\\\\  258--269, 1978.\\n\\n\\\\bibitem[Nemirovski \\\\& Yudin(1983)Nemirovski and\\n  Yudin]{nemirovski1983complexity}\\nNemirovski, A. and Yudin, D.\\n\\\\newblock \\\\emph{Problem Complexity and Method Efficiency in Optimization}.\\n\\\\newblock Intersci. Ser. Discrete Math. Wiley, New York, 1983.\\n\\n\\\\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and\\n  Swami]{papernot2016distillation}\\nPapernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A.\\n\\\\newblock Distillation as a defense to adversarial perturbations against deep\\n  neural networks.\\n\\\\newblock In \\\\emph{2016 IEEE Symposium on Security and Privacy (SP)}, pp.\\\\\\n  582--597. IEEE, 2016.\\n\\n\\\\bibitem[Sharif et~al.(2018)Sharif, Bhagavatula, Bauer, and\\n  Reiter]{sharif2018adversarial}\\nSharif, M., Bhagavatula, S., Bauer, L., and Reiter, M.~K.\\n\\\\newblock Adversarial generative nets: Neural network attacks on\\n  state-of-the-art face recognition.\\n\\\\newblock \\\\emph{CoRR}, abs/1801.00349, 2018.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1801.00349}.\\n\\n\\\\bibitem[{Sharma} \\\\& {Chen}(2017){Sharma} and {Chen}]{sharma2017attacking}\\n{Sharma}, Y. and {Chen}, P.-Y.\\n\\\\newblock {Attacking the Madry Defense Model with $L_1$-based Adversarial\\n  Examples}.\\n\\\\newblock \\\\emph{arXiv e-prints}, art. arXiv:1710.10733, Oct 2017.\\n\\n\\\\bibitem[Shin \\\\& Song(2017)Shin and Song]{shin2017jpeg}\\nShin, R. and Song, D.\\n\\\\newblock {JPEG}-resistant adversarial images.\\n\\\\newblock In \\\\emph{NIPS 2017 Workshop on Machine Learning and Computer\\n  Security}, 2017.\\n\\n\\\\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,\\n  Goodfellow, and Fergus]{szegedy2013intriguing}\\nSzegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,\\n  and Fergus, R.\\n\\\\newblock Intriguing properties of neural networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1312.6199}, 2013.\\n\\n\\\\bibitem[{Tram{\\\\`e}r} \\\\& {Boneh}(2019){Tram{\\\\`e}r} and\\n  {Boneh}]{tramer2019adversarial}\\n{Tram{\\\\`e}r}, F. and {Boneh}, D.\\n\\\\newblock {Adversarial Training and Robustness for Multiple Perturbations}.\\n\\\\newblock \\\\emph{arXiv e-prints}, art. arXiv:1904.13000, Apr 2019.\\n\\n\\\\bibitem[Xiao et~al.(2018)Xiao, Zhu, Li, He, Liu, and Song]{xiao2018spatially}\\nXiao, C., Zhu, J.-Y., Li, B., He, W., Liu, M., and Song, D.\\n\\\\newblock Spatially transformed adversarial examples.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1801.02612}, 2018.\\n\\n\\\\bibitem[Xie et~al.(2018)Xie, Wu, van~der Maaten, Yuille, and\\n  He]{xie2018feature}\\nXie, C., Wu, Y., van~der Maaten, L., Yuille, A., and He, K.\\n\\\\newblock Feature denoising for improving adversarial robustness.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1812.03411}, 2018.\\n\\n\\\\end{thebibliography}\\n\",\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1707.07397': True,\n",
       "   '1802.00420': True,\n",
       "   '1712.09665': True,\n",
       "   '1902.06705': True,\n",
       "   '1712.02779': True,\n",
       "   '1412.6572': True,\n",
       "   '1706.02677': True,\n",
       "   '1902.08265': True,\n",
       "   '1706.06083': True,\n",
       "   '1801.00349': True,\n",
       "   '1710.10733': True,\n",
       "   '1312.6199': True,\n",
       "   '1904.13000': True,\n",
       "   '1801.02612': True,\n",
       "   '1812.03411': True}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1806.00610v1',\n",
       "  'title': 'Accounting for the Neglected Dimensions of AI Progress',\n",
       "  'authors': ['Fernando Martnez-Plumed',\n",
       "   'Shahar Avin',\n",
       "   'Miles Brundage',\n",
       "   'Allan Dafoe',\n",
       "   'Sean  higeartaigh',\n",
       "   'Jos Hernndez-Orallo'],\n",
       "  'date_published': '2018-06-02 09:21:12+00:00',\n",
       "  'data_last_modified': '2018-06-02 09:21:12+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1806.00610v1',\n",
       "  'abstract': 'We analyze and reframe AI progress. In addition to the prevailing metrics of performance, we highlight the usually neglected costs paid in the development and deployment of a system, including: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, development time, etc. These costs are paid throughout the life cycle of an AI system, fall differentially on different individuals, and vary in magnitude depending on the replicability and generality of the AI solution. The multidimensional performance and cost space can be collapsed to a single utility metric for a user with transitive and complete preferences. Even absent a single utility function, AI advances can be generically assessed by whether they expand the Pareto (optimal) surface. We explore a subset of these neglected dimensions using the two case studies of Alpha* and ALE. This broadened conception of progress in AI should lead to novel ways of measuring success in AI, and can help set milestones for future progress.',\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 0.9901957247,\n",
       "  'main_tex_filename': './AIProgress.tex',\n",
       "  'text': '---\\nabstract: |\\n  We analyze and reframe AI progress. In addition to the prevailing metrics of performance, we highlight the usually neglected costs paid in the development and deployment of a system, including: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, development time, etc. These costs are paid throughout the life cycle of an AI system, fall differentially on different individuals, and vary in magnitude depending on the replicability and generality of the AI solution. The multidimensional performance and cost space can be collapsed to a single utility metric for a user with transitive and complete preferences. Even absent a single utility function, AI advances can be generically assessed by whether they expand the Pareto (optimal) surface. We explore a subset of these neglected dimensions using the two case studies of Alpha\\\\* and ALE. This broadened conception of progress in AI should lead to novel ways of measuring success in AI, and can help set milestones for future progress.\\nauthor:\\n- |\\n  Fernando Martnez-Plumed\\\\\\n  *DSIC, Universitat Politcnica de Valncia, Spain*\\\\\\n  `fmartinez@dsic.upv.es`\\\\\\n  \\\\\\n  Shahar Avin\\\\\\n  *CSER, University of Cambridge, UK*\\\\\\n  `sa478@cam.ac.uk`\\\\\\n  \\\\\\n  Miles Brundage\\\\\\n  *FHI, University of Oxford, UK*\\\\\\n  `miles.brundage@philosophy.ox.ac.uk`\\\\\\n  \\\\\\n  Allan Dafoe\\\\\\n  *FHI, University of Oxford, UK*\\\\\\n  `allan.dafoe@governance.ai`\\\\\\n  \\\\\\n  Sen  higeartaigh\\\\\\n  *CSER, University of Cambridge, UK*\\\\\\n  `so348@cam.ac.uk`\\\\\\n  \\\\\\n  Jos Hernndez-Orallo\\\\\\n  *DSIC, Universitat Politcnica de Valncia, Spain*\\\\\\n  `jorallo@dsic.upv.es`\\nbibliography:\\n- biblio.bib\\ntitle: Accounting for the Neglected Dimensions of AI Progress\\n---\\n\\nIntroduction\\n============\\n\\nMetrics of scientific progress can play an outsized role in the perception of a field and in the allocation of its resources. By contrast, that which goes unmeasured is often neglected. We argue for a more general accounting of progress in AI, so as to better map attention and metrics to scientific achievement.\\n\\nThe prevailing approach to assessing AI progress consists of measuring *performance*, such as the raw or normalized score in a game, ELO rating, error rate, accuracy, and so forth, often plotted over time to evaluate temporal progress [@eff2017; @eckersley2017measuring]. Performance, however, does not exactly correspond with social value or scientific progress in AI. Misalignment between what is measured and what is desired can lead to misallocation of energy and resources. Specifically, excessive effort is likely to go towards achieving novel performance benchmarks, and insufficient effort towards progress on other dimensions relevant to social value, economic value, and scientific progress, such as compute efficiency, data efficiency, novelty, replicability, autonomy, and generality.\\n\\nThis does not mean that quantitative assessment and benchmarks should be abandoned. On the contrary, we need more and better measurement [@moam2017]: measurement which is more comprehensive, general, and focused on the cost function of the ultimate beneficiaries. Ultimately, in assessing progress we would like to weight all the resources that users (or receivers) of a technology require to achieve their goals. For instance, to what extent does progress on a particular metric of performance in machine translation map on to user\\'s satisfaction? Does the progress also correspond to a reduction in cost per translation, or in time for execution? If a paper develops a new technique, how easily can new algorithms and applications integrate and benefit from it?\\n\\nIn general, users seek the benefits of high performance (at a set of tasks), while they seek to minimize the costs of developing and deploying their system. Sensitivity to costs is true for individual consumers, firms and developers, as well as other scientists. Some kinds of hidden costs can appear during development, when an application is produced, when reproduced at a large scale, or when adapted to other domains. Some future costs will be born by future developers or scientists, sometimes referred to as \"technical debt\" or \"research debt\". Other costs may be spread more broadly, and are thus harder to account for. As in other sectors, there are externalities from AI development and deployment which are important to be aware of; among the negative externalities are environmental footprints, user privacy, skill atrophy (e.g., the Google effect), opacity in decision making, etc. Attention to, and ideally measurement of, these externalities is beneficial, as it is a first step towards internalizing them.\\n\\nIn this paper we consider this wide range of costs, though we focus on the costs born by future developers, such as the costs in computation, data, knowledge, software, human attention, and calendar time. We will identify how costs are distributed depending on the stage in which they are incurred, the number of times they are replicated, and the actor covering each cost. These dimensions should be integral to the measurement of AI progress.\\n\\nThe *estimation* of these dimensions is fraught with difficulties. To what extent are performance benchmarks actually representative of the target problem domain? To what extent are solutions overly specialized for the performance benchmark, as opposed to being more general, thus shaping the costs of adapting the solution to an adjacent problem domain? To what extent are solutions more reproducible by other teams, due to the availability of software and datasets? As an illustration of these difficulties and how they can be overcome, we will analyze several case studies where we evaluate performance alongside these other dimensions. As a result, we overhaul the notion of progress in these domains.\\n\\nOur paper makes several contributions. First, we offer the most detailed and formal analysis to date of the dimensions of AI progress. While previous work has attempted to quantify progress in the performance of a specific system, we more fully account for the resources required and the generality of solutions. Second, in so doing we surface neglected dimensions of AI progress that may be optimized more directly. Third, we offer a novel framing under Pareto optimality for assessing performance and costs of an AI system, which suggests a more principled approach to forecasting future developments in AI, with myriad applications for policy, ethical, and economic analysis, and better research portfolio optimization within the field of AI itself.\\n\\nBackground {#sec:back}\\n==========\\n\\nThere was a time that benchmarks were unusual in AI, but today almost every area of AI has its own benchmarks and competitions [@aievaluation2017]. Most researchers accept these challenges and invest great effort in improving on these metrics of performance. Indeed, many reports about AI progress include summaries of these benchmarks [@eff2017; @eckersley2017measuring]. We discuss here four issue areas arising from excessive focus on performance: *representativeness*, *specialization*, *reproducibility* and *resources*.\\n\\nRegarding *representativeness*, many benchmarks and competitions are used in AI, but they vary in how representative they are of the fundamental problems in their respective subfields [@aievaluation2017; @aicosmos2017]. For instance, it has recently been recognized that the Winograd Schema challenge only partially represents commonsense reasoning. As a reaction, challenges in AI are realigned to achieve more and better automation [@frey2017future; @brynjolfsson2017can]), or the aspiration of more human-like AI [@lake2017building; @marcus2018deep]. A deeper concern is that most benchmarks are not really fostering the basic scientific advances needed to move the field forward, be they theoretical advances, explanatory insights, or tools to facilitate other work. This issue of non-representativeness is partly addressed through the review process, and requirements such as controlling the percentage of papers in different areas [@shah2017design].\\n\\nThe second issue, *specialization*, is related to representativeness. When a benchmark or competition becomes the target, researchers will have incentives to overly specialize their systems to performance according to that benchmark, at the cost of other features of their system, such as generalizability. If we had a satisfactory metric of generality then we could use that as a benchmark, but it remains an open question how best to operationalize generality [@moam2017], balancing between putting all the distribution mass on a few tasks [@legg2007universal]---and not really being general---or distributing it in a block-uniform way---facing the no free lunch theorems [@wolpert2012no].\\n\\nA third issue is *reproducibility*, and the wider notion of replicability. In AI this was usually understood as requiring the sharing of data and code, but the concept is becoming richer [@drummond2009replicability; @bonsignorio2015toward; @henderson2017deep]. Indeed, we must distinguish between specifically reproducing the results, and replicating the findings with some variations [@zwaan2017making]. Several initiatives have been proposed to facilitate (or even require) a wider replicability. For instance, with the \"open leaderboards\" [@spohrer2017], participants have to upload their code so that other participants can make modifications and submit another proposal.\\n\\nFinally, users are generally sensitive to the resource cost of developing and deploying an AI system, which performance benchmarks rarely explicitly take into account. Much AI progress is said to be attributable to advances in computational power [@reagen2017deep]. However, it is not straightforward to quantify what exactly can be attributed to software progress, hardware progress or several other resources [@brundage2016modeling; @grace2017]. Accordingly, perhaps it is more effective to just measure the so-called \"end-to-end performance\", including computational time and quality of the models, such as the recent DAWNBench for deep learning [@coleman2017]. Other resources, such as data, are at least as important, especially in machine learning[^1]. But it seems subjective to determine what input is seen positively or negatively, or even considered as cheating: too much data (supervised or unsupervised), too much knowledge ( constraints, rules or bias), enriched input [@bougie2017deep], etc. The question depends mostly on the cost of the resource. Human resources (\"human computation\") are also common in AI to increase performance or generality (but at the cost of autonomy).\\n\\nOverall, there are many resources involved but, at the moment, there is no integrated framework taking into account all of them. Related approaches involve the ideas of utility functions, Pareto-optimal analysis and, most especially, cost-sensitive learning [@elkan2001foundations]. [@turney2002types] identifies costs related to inputs and outputs in classification (errors, instability, attributes, labeling, actioning) data (cases), computation and human preprocessing. In this paper, we offer a general statement of this idea, applied to AI progress.\\n\\nIn the end, when assessing AI progress in a comprehensive way, one should consider the whole life cycle of research, innovation, production, and reproduction. Notions such as technical or research debt are becoming more recognized, as they incorporate some costs that are not perceived at early stages of the process but appear later on, when the technology or product is put into practice [@sculley2015hidden; @henderson2017deep; @olah2017].\\n\\nComponents and integration {#sec:comp}\\n==========================\\n\\nIn this section, we flesh out a comprehensive list of dimensions that are required for an \"AI system\" to work. We use the term \"system\" in a flexible way, including an agent, an algorithm, a product, etc., proposed in a research paper or by a company. Given the fuzzy contours of AI, human automation is usually recognized as a goal for AI. However, it is actually difficult to distinguish when reports and forecasts about \"automation\" [@frey2017future; @brynjolfsson2017can] are assuming conditions such as \"at a reasonable cost\", \"to a high degree of automation\", etc., versus \"full automation at whatever cost\". The estimated probability of automation for a given task might change completely depending on these conditions. In the end, automation is important, but it is the efficiency of the whole system what matters, including any \"human computation\" involved. This view of efficiency links us directly to the resources involved in an AI system and their associated costs.\\n\\nTable\\xa0[\\\\[tab:resources\\\\]](#tab:resources){reference-type=\"ref\" reference=\"tab:resources\"} shows the resources we identified as frequently involved in developing and deploying AI systems. These resources have fuzzy boundaries and are often fungible with each other. For instance, the distinction between data and knowledge is not always clear, and hardware and software may be highly intertwined. Human resources are typically considered under \"manipulation\", but can appear in relation to the other resources (e.g., labeled data and teaching a robot might be assigned to $r_d$ and $r_m$ respectively). This is not a problem, as long as all the resources are identified.\\n\\nIt is appealing to collapse the benefits and costs of an AI system to a single metric. For any given user with rational (transitive and complete) preferences, their preferences can be represented using a utility function. A firm\\'s utility function, for example, might correspond to risk-adjusted expected profit. A user\\'s utility function might be harder to quantify, but is generically increasing in the performance of the system and decreasing in the costs of the system. Denote a performance vector, $\\\\psi$, for a given problem, which is often a unidimensional quantitative score (such as the error), but could also have several components. A utility function maps performance and all associated resources to a single dimension:\\n\\n$$\\\\label{eq:variance}\\n \\\\ensuremath{U}\\\\xspace(\\\\psi, \\\\ensuremath{\\\\bar{r}}\\\\xspace) = \\\\ensuremath{U}\\\\xspace(\\\\psi, r_d, r_k, r_s, r_h, r_m, r_c, r_n, r_t) \\\\rightarrow \\\\ensuremath{u}\\\\xspace$$\\n\\nIn some cases this is an additively separable function, such that $\\\\ensuremath{U}\\\\xspace(\\\\psi, \\\\ensuremath{\\\\bar{r}}\\\\xspace)=\\\\ensuremath{B}\\\\xspace(\\\\psi) - \\\\sum_x \\\\ensuremath{C}\\\\xspace_x(r_x)$, with the first term accounting for the benefit according to the performance of the system minus the costs produced by the use of resources (note that the cost functions $C_x$ are different for each resource). For economic applications, we might be able to separate the utility function into performance generating revenue (in dollars), and resources imposing costs (in dollars).\\n\\nIn many cases, we are not able to collapse performance and costs into a single metric, perhaps because the utility function is not known or varies across a population of users. Still, we can productively examine the relative performance and costs of different systems. For any number of dimensions, we can assess the Pareto-optimal surface, as we do in Fig.\\xa0[1](#fig:pareto1){reference-type=\"ref\" reference=\"fig:pareto1\"} for two indicators (we explore this further in section\\xa0[5](#sec:pareto){reference-type=\"ref\" reference=\"sec:pareto\"}). We may want to focus on one dimension of costs, such as economic costs or energy costs (as per the \"carbon footprint\"). For example, Fig.\\xa0[1](#fig:pareto1){reference-type=\"ref\" reference=\"fig:pareto1\"} shows algorithms and architectures according to their MNIST prediction error and power consumption, revealing that most solutions are not on the Pareto surface on these dimensions, with notable exceptions, such as some ASIC architectures, which focus on efficiency in terms of chip space, speed and \"energy footprint\" [@chen2014diannao].\\n\\n![Performance for MNIST [@lecun1998gradient], for 22 papers, compared to power consumption (data from [@reagen2017deep]). The Pareto front is also shown (we will discuss whether the points can actually be joined by straight segments in section\\xa0[5](#sec:pareto){reference-type=\"ref\" reference=\"sec:pareto\"}). ](Pareto1-rotated-cropped-clean.png){#fig:pareto1 width=\"0.43\\\\\\\\columnwidth\"}\\n\\nThe full range of accounting {#sec:ext}\\n============================\\n\\nThe benefits and costs of developing and deploying an AI system are not incurred only once, but throughout the many uses, reuses, and follow-on contributions. Some costs are born exclusively during the initial conception and development, while others recur with each adaptation to a new application, or even each application to a particular user. In general, the total resource burden should be accounted for according to the whole cycle of the AI system.\\n\\nFig.\\xa0[2](#fig:IntExtResources){reference-type=\"ref\" reference=\"fig:IntExtResources\"} shows how the dimensions we identified can become relevant at different stages of the life cycle of an AI system. Consider a new algorithm for voice recognition. Apart from all the human thinking, there will be a great effort in terms of failed experiments, different libraries used, users testing the early systems, etc. If a company takes these ideas and builds a prototype, the tests, software, hardware, and compute will concentrate on production. When the system is reproduced (installed or shipped) to users, additional resource costs will be incurred. Further, if the idea can be adapted for other applications (e.g., adapting a voice recognition system to other languages), depending on its generality and reproducibility, the initial contribution can provide further value, at some further adaptation cost including the need for new corpora, training, semantic knowledge, etc.\\n\\n![Illustrative representation of stages of the AI system life cycle where resources might be required.](IntExtResources.pdf){#fig:IntExtResources width=\"1\\\\\\\\columnwidth\"}\\n\\nAt each stage of the life cycle, the contribution may be deployed a multiplicity of times (represented above the boxes in Fig.\\xa0[2](#fig:IntExtResources){reference-type=\"ref\" reference=\"fig:IntExtResources\"}). The total value of the contribution thus needs to take into account the scale of its deployment. For instance, some early speech recognition systems were pre-trained once (the *system cost*, denoted by $\\\\ensuremath{\\\\mbox{\\\\textsf{\\\\emph{C}}}}\\\\xspace$, covering the \"conceive\" and \"produce\" stages in Fig.\\xa0[2](#fig:IntExtResources){reference-type=\"ref\" reference=\"fig:IntExtResources\"}) and then adapted to thousands of users, with extra hours of customization per user (the *application cost*, denoted by $\\\\ensuremath{{\\\\cal{C}}\\\\xspace}^j$ with $j$ indexing each of the $n$ applications, or users, covering the \"reproduce\" and \"replicate\" stages). More recent general speech recognition systems do not need such customization. Consequently, the application cost $\\\\ensuremath{{\\\\cal{C}}\\\\xspace}^j$ is lower per user. In both cases, the total cost $\\\\ensuremath{C}\\\\xspace$ is $\\\\ensuremath{\\\\mbox{\\\\textsf{\\\\emph{C}}}}\\\\xspace+ \\\\sum_{j=1}^n \\\\ensuremath{{\\\\cal{C}}\\\\xspace}^j$. As the number of applications increases, the average cost will converge to the average application cost as the system cost is amortized. For this reason, for contributions that have many possible applications, it is worth paying additional system costs so as to make the contribution more general, adaptable, and reusable, and thereby bring down the application costs. Since AI contributions often have broad potential applicability, contributions that are general, adaptable, and reusable are likely to have especially high utility.\\n\\nFig.\\xa0[2](#fig:IntExtResources){reference-type=\"ref\" reference=\"fig:IntExtResources\"} not only covers direct \"internal\" costs ($r_x$, $r_y$, ...) but also some external \"debts\" or \"socialization\" costs ($r_x\\'$, $r_y\\'$, ...). For instance, automated customer service systems (call centers) clearly were not a Pareto improvement relative to previous systems, even though they may be a profit maximizing improvement. Companies reduce their labor costs for customer service by substituting in phone-trees and voice recognition, but in the process impose time, frustration, and other costs onto the customer. Some navigators and personal assistants can make users more dependent on them, atrophying some capabilities or leading to a simplification of language. In other words, the user adapts to the AI system, and assumes part of the effort or cost. In general, technological innovation both involves developing technology to fit a given conception of the task, and adapting conceptions of the task to fit the capabilities of technology. In the process of adapting work processes, customer expectations, relationship norms, and even urban design to what is technologically convenient, there can be consequences for society that are not internalized by the designers and deployers of these systems. This footprint of AI is not usually acknowledged in benchmarking.\\n\\nFrom the previous sections, we conclude that the contribution of an AI development should, in principle, be given a full accounting of the costs and benefits, across the contribution\\'s full life cycle. The current emphasis on targeting and reporting performance benchmarks, however, poses an obstacle to a full accounting. Reproducibility and replicability are two traditional tools for addressing this. More precisely:\\n\\n-   *Specific reproducibility* refers to whether the *same result* can be obtained from the same conditions and procedures. In AI, this requires that all the necessary code and data are given. This also assumes the same cost functions as well: $\\\\sum_{j=1}^{n} \\\\sum_x \\\\ensuremath{{\\\\cal{C}}\\\\xspace}_x^j(r_x^j) = n \\\\sum_x \\\\ensuremath{{\\\\cal{C}}\\\\xspace}_x(r_x)$.\\n\\n-   *General replicability* will check whether the AI technique can be *applied to other problems*, a set of $n$ tasks, applications, or users indexed by $j$, with an overall cost $\\\\sum_{j=1}^n \\\\sum_x \\\\ensuremath{{\\\\cal{C}}\\\\xspace}_x^j(r_x^j)$ that must consider the adaptation effort, with different resources $r_x^j$ and cost functions $\\\\ensuremath{{\\\\cal{C}}\\\\xspace}_x^j$ per user.\\n\\nEspecially for replicability, we can experiment with different hardware architectures, change some of the software and get different computational costs, apart from different performance. That means that the partial results for each $\\\\ensuremath{{\\\\cal{B}}\\\\xspace}^j$ and $\\\\ensuremath{{\\\\cal{C}}\\\\xspace}^j_x(r_x^j)$ might be different, but we still have something replicable with similar utility. A clear example of this notion of replicability is \"approximate computation\" in deep learning, where one can get much smaller computational costs without a significant change in accuracy [@reagen2017deep].\\n\\nExploring the Pareto-front of AI research {#sec:pareto}\\n=========================================\\n\\nCorporations, governments, startups, NGOs, personal users, and contemporary and future AI researchers are the intended recipients, or *receivers*, of the AI technologies being developed, and they each have different preferences, resources and constraints, or in other words different operating characteristics. The familiar concept of the ROC curve plots true positive rates (TPR) and false positive rates (FPR) for binary classifiers, and emphasizes the importance of comparing multi-dimensional surfaces, rather than single metrics.\\n\\nFor instance, Fig.\\xa0[4](#fig:pareto2){reference-type=\"ref\" reference=\"fig:pareto2\"} (left) just shows a single metric, performance, as a function of time. This plot does not explain what the cluster of attempts after 2014 really contribute, when they have more error than the already obtained human level. Other dimensions are neglected in this plot, limiting insight about progress. In the next section we will see other domains where some of the resources are actually put as dimensions.\\n\\nBefore analyzing the case studies, we have to understand how to build and work with the Pareto front. When resources are included, the analysis of optimal Pareto surfaces might be slightly different than the traditional triangulization approach. When showing performance metrics such as TPR and FPR for two models, any point in between can be obtained by interpolation, connecting any two points by a straight segment. However, we should note that these points require the implementation of both models. While some of the resources can be interpolated, others (e.g., software) will simply sum up, and the points between two other points will not be achievable with a straight line, but by an axis-parallel route.\\n\\nFor instance, Fig.\\xa0[4](#fig:pareto2){reference-type=\"ref\" reference=\"fig:pareto2\"} (right) shows performance against one particular resource. For each method, A, B, C, D, and E, the numbers represent the extremes when varying their parameters. E1 represents a random (or baseline) model. Assuming no interpolation is possible, the Pareto front here is shown in blue. Methods C and B can be discarded, as they do not reach anywhere that cannot already be achieved with A, E and D.\\n\\n![Left: Performance for the MNIST benchmark (data from EFF). Right: A schematic representation of techniques A, B, C, D, E, with variants, the areas they cover, and the resulting Pareto front.](eff-mnist.pdf \"fig:\"){#fig:pareto2 width=\"40%\"} ![Left: Performance for the MNIST benchmark (data from EFF). Right: A schematic representation of techniques A, B, C, D, E, with variants, the areas they cover, and the resulting Pareto front.](ParetoEx.pdf \"fig:\"){#fig:pareto2 width=\"40%\"}\\n\\nThe diversity of receivers and the number of dimensions suggest that a single utility metric is simplistic ---different receivers would have different subjective utilities for different dimensions. This operating condition translates into a vector, or gradient, in the multidimensional space. For example, large technology corporations may gain significant utility from a discovery that allows modest speed-ups in exchange for significantly increased compute demands, whereas individual researchers, personal users and startups may find little value in such a discovery. Conversely, the existence of real recipients whose preferences can be known in advance allows us to prioritize exploration of those configurations. From the above, we derive a few criteria to identify progress events:\\n\\n-   *Improving the Pareto front for a known group of recipients* (A1, A3 or D3 in Fig.\\xa0[4](#fig:pareto2){reference-type=\"ref\" reference=\"fig:pareto2\"}, right). This would include all-else-being-equal improvements in performance, but also reductions in computation, data, manipulation or other resources in Table\\xa0[\\\\[tab:resources\\\\]](#tab:resources){reference-type=\"ref\" reference=\"tab:resources\"}. This would not, however, consider extreme regions no recipient assigns value to.\\n\\n-   *Covering a location slightly under the Pareto front with more flexibility* (B3 in Fig.\\xa0[4](#fig:pareto2){reference-type=\"ref\" reference=\"fig:pareto2\"}, right). Instead of reaching some areas by combining existing approaches, a new technique can reach there easily with a trade-off between its own parameters, allowing more receivers to easily find their subjectively optimal trade-offs.\\n\\n-   *Covering a location slightly under the Pareto front with more diversity* (C in Fig.\\xa0[4](#fig:pareto2){reference-type=\"ref\" reference=\"fig:pareto2\"}, right, if it is very different from A). The current dominant technique or paradigm can push the Pareto-optimal front for some time, but slightly suboptimal approaches, especially if they are radically different (i.e., alternative \"research programs\"), should not be discarded because they may lead to potential improvement in the Pareto-optimal front if the current paradigm stalls.\\n\\nReceivers can be incentivized to generate and communicate their gradients (though in some cases, countervailing considerations may exist such as commercial secrecy). It is also in the interests of discoverers to show the recipients benefited by their discovery. Brokers of such information (peer-review, surveys, competitions, etc.) are in a position to meet the incentives (and gradients) of both researchers and recipients by ensuring such discoveries are properly rewarded.\\n\\nCase studies {#sec:case}\\n============\\n\\nIn this section we will examine two representative case studies of progress in AI: Alpha\\\\* and ALE.\\n\\nAlpha\\\\* refers to a series of papers and associated techniques by DeepMind to play board games. We analyzed the whole series, from [@silver2016mastering] (including the and versions, used against Fan Hui and Lee Sedol, respectively, and its latest version, , which won 60 straight online games against professional Go players), [@silver2017mastering] (a version created without using data from human games) and [@silver2017masteringB] (which uses an approach similar to to master not just Go, but also chess and shogi).\\n\\n![image](TableAlpha_labels.pdf){#tab:Alpha width=\"0.7\\\\\\\\linewidth\"} \\\\[\\\\] Dimensions (resources and performance) reported in the Alpha\\\\* papers. Systems from [@silver2016mastering; @silver2017mastering; @silver2017masteringB]. [\\\\[tab:Alpha\\\\]]{#tab:Alpha label=\"tab:Alpha\"}\\n\\nTable\\xa0[5](#tab:Alpha){reference-type=\"ref\" reference=\"tab:Alpha\"} shows whether the dimensions were reported in the papers ($\\\\checkmark$), only partially accounted for ($\\\\circ$), not mentioned but relevant ($\\\\times$) and not applicable ($-$). Many dimensions are relevant for the analysis: the data, the knowledge, the software, the hardware, manipulation, computation and, of course, performance, etc. However, only some of them are provided, which makes a comprehensive comparison of the whole space difficult. Still, we will represent three dimensions: performance (in ELO ranking, which can only be partially estimated for ), computational resources (using the equivalence: $1\\\\;TPU_{v2} \\\\simeq 3\\\\;TPU_{v1} \\\\simeq 36\\\\;GPU \\\\simeq 180\\\\;CPU$ [@jouppi2017datacenter]) and human manipulation resources (as represented quantitatively by the ELO ranking of the player or players the system learns from). Other dimensions (like knowledge[^2] about Go, software, etc.) are not included because of insufficient information from some papers.\\n\\n![Multidimensional utility space for Alpha\\\\* (left) and ALE (right). Research gradient evolution from 2013 to 2018 represented with a segmented gray arrow. The Pareto front (dashed black) does not include other resources (software, and humans used for training) that duplicate for connecting segments.](Alpha_footprint_log.pdf \"fig:\"){#fig:benchmarks width=\"0.45\\\\\\\\columnwidth\"} ![Multidimensional utility space for Alpha\\\\* (left) and ALE (right). Research gradient evolution from 2013 to 2018 represented with a segmented gray arrow. The Pareto front (dashed black) does not include other resources (software, and humans used for training) that duplicate for connecting segments.](EFF_footprint.pdf \"fig:\"){#fig:benchmarks width=\"0.45\\\\\\\\columnwidth\"}\\n\\nWhat we see in Fig.\\xa0[7](#fig:benchmarks){reference-type=\"ref\" reference=\"fig:benchmarks\"} (left) is that the Pareto front at the moment is represented by and . is discarded because needs less compute, no manipulation and gets better performance.\\n\\nWhy is a breakthrough if it is not Pareto optimal? The answer is generality. only solved one task (Go) and can solve several tasks. Finally, if we look chronologically at the plot, we see that the main gradient that has been followed has been performance.\\n\\nThe second case study is ALE [@bellemare13arcade], a collection of Atari games that has become popular for the evaluation of general-purpose RL algorithms learning from screen shots. We selected all the papers (systems) from EFF\\'s AI Progress Measurement Project [@eff2017] and the papers introducing the [@Rainbow17] and agents [@Reactor17].\\n\\n![image](TableEFF.pdf){#tab:EFF width=\"1\\\\\\\\linewidth\"} \\\\[\\\\]Same as Table\\xa0[5](#tab:Alpha){reference-type=\"ref\" reference=\"tab:Alpha\"} for the ALE papers (from EFF [@eff2017] and [@Reactor17; @Rainbow17]). [\\\\[tab:EFF\\\\]]{#tab:EFF label=\"tab:EFF\"}\\n\\nTable\\xa0[8](#tab:EFF){reference-type=\"ref\" reference=\"tab:EFF\"} shows what information we found about the resources and performance. Again, many dimensions are relevant, but only a few are systematically reported: data, computation and performance. Fig.\\xa0[7](#fig:benchmarks){reference-type=\"ref\" reference=\"fig:benchmarks\"} (right) represents computation and performance. Computation time (whenever the authors do not provide this information explicitly) is roughly estimated from the kind of approach used, whether it is follow-up work, the training setting used, etc., or from figures in more recent papers, which make explicit comparisons between them and the state of the art [@Rainbow17; @Reactor17]. What we see in Fig.\\xa0[7](#fig:benchmarks){reference-type=\"ref\" reference=\"fig:benchmarks\"} (right) is a current Pareto front dominated by variants, and . The research gradient (in gray) has changed over the years, with some disregard of compute initially and more concern in efficiency recently.[^3]\\n\\nFor this benchmark, it is common to find \"learning curves\" in the papers (e.g., [@machado17]), which show performance varying on the number of episodes. This is clearly the $r_d$ (data) but it also influences directly on computation. These learning curves give information of full regions of the multidimensional space, as we saw in Fig.\\xa0[2](#fig:IntExtResources){reference-type=\"ref\" reference=\"fig:IntExtResources\"}.\\n\\nFinally, for some papers, the comparison was not possible (e.g., due to different subsets of games). It is important to note, however, that some approaches based on genetic programming [@kelly] and on planning [@bandres2018planning] are valuable in terms of diversity.\\n\\nConclusions {#sec:conc}\\n===========\\n\\nThe interest in more comprehensive evaluation protocols, going beyond performance alone, is represented by some of the references we included in section [2](#sec:back){reference-type=\"ref\" reference=\"sec:back\"} on cost-sensitive learning, reproducibility, generality, data-efficiency and computational costs. However, in order to rigorously evaluate a novel contribution to AI progress more broadly, we need a more formal analysis. This is done by an explicit enumeration of all the dimensions (as represented by Table\\xa0[\\\\[tab:resources\\\\]](#tab:resources){reference-type=\"ref\" reference=\"tab:resources\"}) and their integration into utility functions or their representation in a multidimensional space, with a clear delimitation of the extent of accounting. This is what happened in cost-sensitive learning more than 15 years ago [@elkan2001foundations; @turney2002types], leading to a wide range of techniques that covered different operating conditions. While all these costs are nowadays integrated into the measures of performance, many other resources are not, as we have surfaced here. We hope this paper can launch the study of \"cost-sensitive AI\". Within this framework, we make a series of recommendations:\\n\\n-   Benchmarks and competitions should be defined in terms of a more comprehensive utility function, considering as many dimensions as possible, or recognize the value of all contributions that have any of the positive effects on the Pareto front identified in Section\\xa0[5](#sec:pareto){reference-type=\"ref\" reference=\"sec:pareto\"}, in short or long terms.\\n\\n-   Papers presenting or evaluating algorithms should generally try to report the whole region they cover, and how to navigate the region by modifying parameters or resources. There are many partial examples nowadays: learning curves, plots comparing the number of models vs. performance, planning performance vs. lookahead, etc.\\n\\n-   These utility functions and multidimensional spaces must also be seen in terms of replicability, for variants of the problems and at different stages of the AI life cycle. The multiplicities are more difficult to plot graphically, but we can still define operating conditions depending on the adaptation (or transfer) effort for $m$ problems, or $n$ users.\\n\\nFrequently, we will not be able to say that one technique is \\'better\\' than another: they just cover different regions of the multidimensional space. It is the receiver who will choose the system that best fits their needs. Having a representation of the Pareto front may hugely facilitate this choice for other researchers and industry, as simply as moving the gradient until touching the Pareto surface. Also, small players in AI could focus on those areas that require less resources and still contribute to the Pareto front or to diversity. Finally, the Pareto surface can help detect some societal risks, especially if we see that a powerful capability in AI can be achieved with very few resources, becoming available to malicious actors.\\n\\nThis view of the operating condition as a gradient may suggest clever approaches to push the front for some resources, as gradient descent is increasingly being used at a meta-level [@Andrychowicz16]. In general, we hope this paper will help change perceptions, promote more general and versatile techniques, highlight the trade-offs, and raise awareness of the overall \"AI footprint\", well beyond performance.\\n\\n[^1]: See <https://sites.google.com/site/dataefficientml/bibliography> for a bibliography on data-efficient ML.\\n\\n[^2]: We have the constructed features: stones to be captured or escaped, legal moves, \\'liberties\\', etc. While this knowledge is crucial, there is no cost for a new match (reproduction), but the adaptation of to other games (replication), may be important.\\n\\n[^3]: The computation times shown in Fig.\\xa0[7](#fig:benchmarks){reference-type=\"ref\" reference=\"fig:benchmarks\"} (left) include both training and deployment (system and application costs). Hence, a model that is half way between models $A$ and $B$ (choosing between them with equal probability), denoted by $\\\\overline{AB}$, has performance $\\\\psi(\\\\overline{AB}) = 0.5 \\\\psi(A) + 0.5 \\\\psi(B)$, but has a computational cost of $r_c(\\\\overline{AB}) = r_c(A) + r_c(B)$. This is why the Pareto front in Fig.\\xa0[7](#fig:benchmarks){reference-type=\"ref\" reference=\"fig:benchmarks\"} (left) has parallel segments, as in Fig.\\xa0[4](#fig:pareto2){reference-type=\"ref\" reference=\"fig:pareto2\"} (right). However, in Fig.\\xa0[7](#fig:benchmarks){reference-type=\"ref\" reference=\"fig:benchmarks\"} (right), we can have $A$ train and play for half of the ALE games and $B$ train and play for the rest. As we average for the whole set of games, we can actually have $r_c(\\\\overline{AB}) = 0.5 r_c(A) + 0.5 r_c(B)$, at least if there is no transfer effort between games. This is why the Pareto front on the right is shown with direct straight segments.\\n',\n",
       "  'bibliography_bbl': \"\\\\begin{thebibliography}{10}\\n\\n\\\\bibitem{Andrychowicz16}\\nMarcin Andrychowicz, Misha Denil, Sergio~Gomez Colmenarejo, Matthew~W. Hoffman,\\n  David Pfau, Tom Schaul, and Nando de~Freitas.\\n\\\\newblock Learning to learn by gradient descent by gradient descent.\\n\\\\newblock {\\\\em CoRR}, abs/1606.04474, 2016.\\n\\n\\\\bibitem{bandres2018planning}\\nWilmer Bandres, Blai Bonet, and Hector Geffner.\\n\\\\newblock Planning with pixels in (almost) real time.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1801.03354}, 2018.\\n\\n\\\\bibitem{bellemare13arcade}\\nM.~G. {Bellemare}, Y.~{Naddaf}, J.~{Veness}, and M.~{Bowling}.\\n\\\\newblock The arcade learning environment: An evaluation platform for general\\n  agents.\\n\\\\newblock {\\\\em Journal of Artificial Intelligence Research}, 47:253--279, jun\\n  2013.\\n\\n\\\\bibitem{bonsignorio2015toward}\\nFabio Bonsignorio and Angel~P Del~Pobil.\\n\\\\newblock Toward replicable and measurable robotics research.\\n\\\\newblock {\\\\em IEEE Robotics \\\\& Aut. M.}, 22(3):32--35, 2015.\\n\\n\\\\bibitem{bougie2017deep}\\nNicolas Bougie and Ryutaro Ichise.\\n\\\\newblock Deep reinforcement learning boosted by external knowledge.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1712.04101}, 2017.\\n\\n\\\\bibitem{brundage2016modeling}\\nMiles Brundage.\\n\\\\newblock Modeling progress in ai.\\n\\\\newblock {\\\\em The Workshops of the Thirtieth AAAI Conference on Artificial\\n  Intelligence AI, Ethics, and Society: Technical Report WS-16-02, arXiv\\n  preprint arXiv:1512.05849}, 2016.\\n\\n\\\\bibitem{brynjolfsson2017can}\\nErik Brynjolfsson and Tom Mitchell.\\n\\\\newblock What can machine learning do? {W}orkforce implications.\\n\\\\newblock {\\\\em Science}, 358(6370):1530--1534, 2017.\\n\\n\\\\bibitem{chen2014diannao}\\nTianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and\\n  Olivier Temam.\\n\\\\newblock Diannao: A small-footprint high-throughput accelerator for ubiquitous\\n  machine learning.\\n\\\\newblock In {\\\\em Sigplan Not.}, volume~49, pages 269--284. ACM, 2014.\\n\\n\\\\bibitem{coleman2017}\\nCody Coleman, Deepak Narayanan, Daniel Kang, Tian Zhao, Jian Zhang, Luigi\\n  Nardi, Peter Bailis, Kunle Olukotun, Chris R{\\\\'e}, and Matei Zaharia.\\n\\\\newblock Dawnbench: An end-to-end deep learning benchmark and competition,\\n  2017.\\n\\n\\\\bibitem{drummond2009replicability}\\nC.~Drummond.\\n\\\\newblock Replicability is not reproducibility: nor is it good science.\\n\\\\newblock {\\\\em Evaluation Methods for ML Ws at the 26th ICML, Montreal,\\n  Canada}, 2009.\\n\\n\\\\bibitem{eff2017}\\nP~Eckersley and N~Yomna.\\n\\\\newblock Measuring the progress of {AI} research, 2017.\\n\\n\\\\bibitem{elkan2001foundations}\\nCharles Elkan.\\n\\\\newblock The foundations of cost-sensitive learning.\\n\\\\newblock In {\\\\em {IJCAI}}, volume~17, pages 973--8, 2001.\\n\\n\\\\bibitem{frey2017future}\\nCarl~Benedikt Frey and Michael~A Osborne.\\n\\\\newblock The future of employment: how susceptible are jobs to\\n  computerisation?\\n\\\\newblock {\\\\em Technological Forecasting and Social Change}, 114:254--280,\\n  2017.\\n\\n\\\\bibitem{grace2017}\\nKatja Grace.\\n\\\\newblock Trends in algorithmic progress, 2017.\\n\\n\\\\bibitem{Reactor17}\\nAudrunas Gruslys, Mohammad~Gheshlaghi Azar, Marc~G. Bellemare, and R{\\\\'{e}}mi\\n  Munos.\\n\\\\newblock The reactor: {A} sample-efficient actor-critic architecture.\\n\\\\newblock {\\\\em CoRR}, abs/1704.04651, 2017.\\n\\n\\\\bibitem{henderson2017deep}\\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,\\n  and David Meger.\\n\\\\newblock Deep reinforcement learning that matters.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1709.06560}, 2017.\\n\\n\\\\bibitem{aievaluation2017}\\nJos{\\\\'e} Hern{\\\\'a}ndez-Orallo.\\n\\\\newblock Evaluation in artificial intelligence: from task-oriented to\\n  ability-oriented measurement.\\n\\\\newblock {\\\\em Artificial Intelligence Review}, 48(3):397--447, 2017.\\n\\n\\\\bibitem{moam2017}\\nJos{\\\\'e} Hern{\\\\'a}ndez-Orallo.\\n\\\\newblock {\\\\em The measure of all minds: evaluating natural and artificial\\n  intelligence}.\\n\\\\newblock Cambridge University Press, 2017.\\n\\n\\\\bibitem{aicosmos2017}\\nJose Hern{\\\\'a}ndez-Orallo, Marco Baroni, Jordi Bieger, Nader Chmait, David~L\\n  Dowe, Katja Hofmann, Fernando Mart{\\\\'\\\\i}nez-Plumed, Claes Stranneg{\\\\aa}rd,\\n  and Kristinn~R Th{\\\\'o}risson.\\n\\\\newblock A new ai evaluation cosmos: Ready to play the game?\\n\\\\newblock {\\\\em AI Magazine, Association for the Advancement of Artificial\\n  Intelligence}, 2017.\\n\\n\\\\bibitem{Rainbow17}\\nMatteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski,\\n  Will Dabney, Daniel Horgan, Bilal Piot, Mohammad~Gheshlaghi Azar, and David\\n  Silver.\\n\\\\newblock Rainbow: Combining improvements in deep reinforcement learning.\\n\\\\newblock {\\\\em CoRR}, abs/1710.02298, 2017.\\n\\n\\\\bibitem{jouppi2017datacenter}\\nNorman~P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,\\n  Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al~Borchers, et~al.\\n\\\\newblock In-datacenter performance analysis of a tensor processing unit.\\n\\\\newblock In {\\\\em 44th Intl Symposium on Computer Architecture}, pages 1--12.\\n  ACM, 2017.\\n\\n\\\\bibitem{kelly}\\nStephen Kelly and Malcolm~I Heywood.\\n\\\\newblock Emergent tangled graph representations for atari game playing agents.\\n\\\\newblock In {\\\\em European Conference on Genetic Programming}, pages 64--79.\\n  Springer, 2017.\\n\\n\\\\bibitem{lake2017building}\\nBrenden~M Lake, Tomer~D Ullman, Joshua~B Tenenbaum, and Samuel~J Gershman.\\n\\\\newblock Building machines that learn and think like people.\\n\\\\newblock {\\\\em BBS}, 40, 2017.\\n\\n\\\\bibitem{lecun1998gradient}\\nYann LeCun, L{\\\\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.\\n\\\\newblock Gradient-based learning applied to document recognition.\\n\\\\newblock {\\\\em Proc. IEEE}, 86(11):2278--2324, 1998.\\n\\n\\\\bibitem{legg2007universal}\\nShane Legg and Marcus Hutter.\\n\\\\newblock Universal intelligence: A definition of machine intelligence.\\n\\\\newblock {\\\\em Minds and Machines}, 17(4):391--444, 2007.\\n\\n\\\\bibitem{machado17}\\nMarlos~C. Machado, Marc~G. Bellemare, Erik Talvitie, Joel Veness, Matthew~J.\\n  Hausknecht, and Michael Bowling.\\n\\\\newblock Revisiting the arcade learning environment: Evaluation protocols and\\n  open problems for general agents.\\n\\\\newblock {\\\\em CoRR}, abs/1709.06009, 2017.\\n\\n\\\\bibitem{marcus2018deep}\\nGary Marcus.\\n\\\\newblock Deep learning: A critical appraisal.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1801.00631}, 2018.\\n\\n\\\\bibitem{olah2017}\\nChris Olah and Shan Carter.\\n\\\\newblock Research debt.\\n\\\\newblock \\\\url{https://distill.pub/2017/research-debt/}, 2017.\\n\\n\\\\bibitem{reagen2017deep}\\nBrandon Reagen, Robert Adolf, Paul Whatmough, Gu-Yeon Wei, and David Brooks.\\n\\\\newblock Deep learning for computer architects.\\n\\\\newblock {\\\\em SL on Comp. Architecture}, 12(4):1--123, 2017.\\n\\n\\\\bibitem{sculley2015hidden}\\nD~Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar\\n  Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan\\n  Dennison.\\n\\\\newblock Hidden technical debt in machine learning systems.\\n\\\\newblock In {\\\\em NIPS}, pages 2503--2511, 2015.\\n\\n\\\\bibitem{shah2017design}\\nNihar~B Shah, Behzad Tabibian, Krikamol Muandet, Isabelle Guyon, and Ulrike\\n  Von~Luxburg.\\n\\\\newblock Design and analysis of the nips 2016 review process.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1708.09794}, 2017.\\n\\n\\\\bibitem{eckersley2017measuring}\\nYoav Shoham, Raymond Perrault, Erik Brynjolfsson, Jack Clark, and Calvin\\n  LeGassick.\\n\\\\newblock {AI} {I}ndex, 2017.\\n\\n\\\\bibitem{silver2016mastering}\\nDavid Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George\\n  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda\\n  Panneershelvam, Marc Lanctot, et~al.\\n\\\\newblock Mastering the game of go with deep neural networks and tree search.\\n\\\\newblock {\\\\em Nature}, 529(7587):484--489, 2016.\\n\\n\\\\bibitem{silver2017masteringB}\\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew\\n  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore\\n  Graepel, et~al.\\n\\\\newblock Mastering chess and shogi by self-play with a general reinforcement\\n  learning algorithm.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1712.01815}, 2017.\\n\\n\\\\bibitem{silver2017mastering}\\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja\\n  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,\\n  et~al.\\n\\\\newblock Mastering the game of go without human knowledge.\\n\\\\newblock {\\\\em Nature}, 550:354, 2017.\\n\\n\\\\bibitem{spohrer2017}\\nJ.~Spohrer.\\n\\\\newblock Opentech {AI} workshop, 2017.\\n\\n\\\\bibitem{turney2002types}\\nPeter~D Turney.\\n\\\\newblock Types of cost in inductive concept learning.\\n\\\\newblock {\\\\em arXiv preprint cs/0212034}, 2002.\\n\\n\\\\bibitem{wolpert2012no}\\nDavid~H Wolpert.\\n\\\\newblock What the no free lunch theorems really mean; how to improve search\\n  algorithms.\\n\\\\newblock In {\\\\em Santa fe Institute Working Paper}, page~12. 2012.\\n\\n\\\\bibitem{zwaan2017making}\\nRolf~A Zwaan, Alexander Etz, Richard~E Lucas, and M~Brent Donnellan.\\n\\\\newblock Making replication mainstream.\\n\\\\newblock {\\\\em Behavioral and Brain Sciences}, pages 1--50, 2017.\\n\\n\\\\end{thebibliography}\\n\",\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1606.04474': True,\n",
       "   '1801.03354': True,\n",
       "   '1712.04101': True,\n",
       "   '1512.05849': True,\n",
       "   '1704.04651': True,\n",
       "   '1709.06560': True,\n",
       "   '1710.02298': True,\n",
       "   '1709.06009': True,\n",
       "   '1801.00631': True,\n",
       "   '1708.09794': True,\n",
       "   '1712.01815': True}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1909.01387v1',\n",
       "  'title': 'Making Efficient Use of Demonstrations to Solve Hard Exploration Problems',\n",
       "  'authors': ['Tom Le Paine',\n",
       "   'Caglar Gulcehre',\n",
       "   'Bobak Shahriari',\n",
       "   'Misha Denil',\n",
       "   'Matt Hoffman',\n",
       "   'Hubert Soyer',\n",
       "   'Richard Tanburn',\n",
       "   'Steven Kapturowski',\n",
       "   'Neil Rabinowitz',\n",
       "   'Duncan Williams',\n",
       "   'Gabriel Barth-Maron',\n",
       "   'Ziyu Wang',\n",
       "   'Nando de Freitas',\n",
       "   'Worlds Team'],\n",
       "  'date_published': '2019-09-03 18:20:48+00:00',\n",
       "  'data_last_modified': '2019-09-03 18:20:48+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1909.01387v1',\n",
       "  'abstract': 'This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.',\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.AI'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 0.8234025531,\n",
       "  'main_tex_filename': 'main.tex',\n",
       "  'text': '---\\nauthor:\\n- Caglar Gulcehre\\n- Tom Le Paine\\n- Bobak Shahriari\\n- Misha Denil\\n- Matt Hoffman\\n- Hubert Soyer\\n- Richard Tanburn\\n- Steven Kapturowski\\n- Neil Rabinowitz\\n- Duncan Williams\\n- Gabriel Barth-Maron\\n- Ziyu Wang\\n- Nando de Freitas\\n- Worlds Team\\nbibliography:\\n- myrefs.bib\\nnocite: \"[@tensorflow2016; @numpy2006; @mckinney2010data; @hunter2007matplotlib]\"\\ntitle: Making Efficient Use of Demonstrations to Solve Hard Exploration Problems\\n---\\n\\nIntroduction\\n============\\n\\nReinforcement learning from demonstrations has proven to be an effective strategy for attacking problems that require sample efficiency and involve hard exploration. For example, [@aytar2018playing], [@pohlen2018observe] and [@salimans2018learning] have shown that RL with demonstrations can address the hard exploration problem in Montezuma\\'s Revenge. [@vevcerik2017leveraging], [@merel2017learning] and [@paine2018one] have demonstrated similar results in robotics. Many other works have shown that demonstrations can accelerate learning and address hard-exploration tasks [e.g.\\xa0see @hester2018deep; @kim2013learning; @nair2018overcoming].\\n\\nIn this paper, we attack the problem of learning from demonstrations in hard exploration tasks in partially observable environments with highly variable initial conditions. These three aspects together conspire to make learning challenging:\\n\\n1.  **Sparse rewards** induce a difficult exploration problem, which is a challenge for many state of the art RL methods. An environment has sparse reward when a non-zero reward is only seen after taking a long sequence of correct actions. Our approach is able to solve tasks where standard methods run for billions of steps without seeing a single non-zero reward.\\n\\n2.  **Partial observability** forces the use of memory, and also reduces the generality of information provided by a single demonstration, since trajectories cannot be broken into isolated transitions using the Markov property. An environment has partial observability if the agent can only observe a part of the environment at each timestep.\\n\\n3.  **Highly variable initial conditions** (i.e. changes in the starting configuration of the environment in each episode) are a big challenge for learning from demonstrations, because the demonstrations can not account for all possible configurations. When the initial conditions are fixed it is possible to be extremely efficient through tracking [@aytar2018playing; @peng2018deepmimic]; however, with a large variety of initial conditions the agent is forced to generalize over environment configurations. Generalizing between different initial conditions is known to be difficult [@ghosh2017divide; @langloisbenchmarking].\\n\\nOur approach to these problems combines demonstrations with off-policy, recurrent Q-learning in a way that allows us to make very efficient use of the available data. In particular, we vastly outperform behavioral cloning using the same set of demonstrations in all of our experiments.\\n\\nAnother desirable property of our approach is that our agents are able to learn to outperform the demonstrators, and in some cases even to discover strategies that the demonstrators were not aware of. In one of our tasks the agent is able to discover and exploit a bug in the environment in spite of all the demonstrators completing the task in the intended way.\\n\\nLearning from a small number of demonstrations under highly variable initial conditions is not straight-forward. We identify a key parameter of our algorithm, the *demo-ratio*, which controls the proportion of expert demonstrations vs agent experience in each training batch. This hyper-parameter has a dramatic effect on the performance of the algorithm. Surprisingly, we find that the optimal demo ratio is very small (but non-zero) across a wide variety of tasks.\\n\\nThe mechanism our agents use to efficiently extract information from expert demonstrations is to use them in a way that guides (or biases) the agent\\'s own autonomous exploration of the environment. Although this mechanism is not obvious from the algorithm construction, our behavioral analysis confirms the presence of this guided exploration effect.\\n\\nTo demonstrate the effectiveness of our approach we introduce a suite of tasks (which we call the *Hard-Eight* suite) that exhibit our three targeted properties. The tasks are set in a procedurally-generated 3D world, and require complex behavior (e.g.\\xa0tool use, long-horizon memory) from the agent to succeed. The tasks are designed to be difficult challenges in our targeted setting, and several state of the art methods (themselves ablations of our approach) fail to solve them.\\n\\nThe main contributions of this paper are,\\n\\n1.  We design a new agent that makes efficient use of demonstrations to solve sparse reward tasks in partially observed environments with highly variable initial conditions.\\n\\n2.  We provide an analysis of the mechanism our agents use to exploit information from the demonstrations.\\n\\n3.  We introduce a suite of eight tasks that support this line of research.\\n\\nRecurrent Replay Distributed DQN from Demonstrations {#sec:r2d3}\\n====================================================\\n\\nr0.65\\n\\n![image](figures/R2D3.pdf){width=\"\\\\\\\\textwidth\"}\\n\\nWe propose a new agent, which we refer to as Recurrent Replay Distributed DQN from Demonstrations (R2D3). R2D3 is designed to make efficient use of demonstrations to solve sparse reward tasks in partially observed environments with highly variable initial conditions. This section gives an overview of the agent, and detailed pseudocode can be found in Appendix\\xa0[8](#sec:algorithm){reference-type=\"ref\" reference=\"sec:algorithm\"}.\\n\\nThe architecture of the R2D3 agent is shown in Figure\\xa0[\\\\[fig:r2d3\\\\]](#fig:r2d3){reference-type=\"ref\" reference=\"fig:r2d3\"}. There are several actor processes, each running independent copies of the behavior against an instance of the environment. Each actor streams its experience to a shared *agent replay* buffer, where experience from all actors is aggregated and globally prioritized [@schaul2016prioritized; @horgan2018distributed] using a mixture of max and mean of the TD-errors with priority exponent $\\\\eta=1.0$ as in [@kapturowski2018recurrent]. The actors periodically request the latest network weights from the learner process in order to update their behavior.\\n\\nIn addition to the agent replay, we maintain a second *demo replay* buffer, which is populated with expert demonstrations of the task to be solved. Expert trajectories are also prioritized using the scheme of [@kapturowski2018recurrent]. Maintaining separate replay buffers for agent experience and expert demonstrations allows us to prioritize the sampling of agent and expert data separately.\\n\\nThe learner process samples batches of data from both the agent and demo replay buffers simultaneously. A hyperparameter $\\\\rho$, the *demo ratio*, controls the proportion of data coming from expert demonstrations versus from the agent\\'s own experience. The demo ratio is implemented at a batch level by randomly choosing whether to sample from the expert replay buffer independently for each element with probability $\\\\rho$. Using a stochastic demo ratio in this way allows us to target demo ratios that are smaller than the batch size, which we found to be very important for good performance. The objective optimized by the learner uses of $n$-step, double Q-learning (with $n = 5$) and a dueling architecture [@wang2015dueling; @hessel2018rainbow]. In addition to performing network updates, the learner is also responsible for pushing updated priorities back to the replay buffers.\\n\\nIn each replay buffer, we store fixed-length ($m = 80$) sequences of $(s, a, r)$ tuples where adjacent sequences overlap by $40$ time-steps. The sequences never cross episode boundaries. Given a single batch of trajectories we unroll both online and target networks [@mnih2015human] on the same sequence of states to generate value estimates with the recurrent state initialized to zero. Proper initialization of the recurrent state would require always replaying episodes from the beginning, which would add significant complexity to our implementation. As an approximation of this we treat the first $40$ steps of each sequence as a burn-in phase, and apply the training objective to the final $40$ steps only. An alternative approximation would be to store stale recurrent states in replay, but we did not find this to improve performance over zero initialization with burn-in.\\n\\nBackground\\n==========\\n\\nExploration remains one of the most fundamental challenges for reinforcement learning. So-called \"hard-exploration\" domains are those in which rewards are sparse, and optimal solutions typically have long and sparsely-rewarded trajectories. Hard-exploration domains may also have many distracting dead ends that the agent may not be able to recover from once it gets into a certain state. In recent years, the most notable such domains are Atari environments, including *Montezuma\\'s Revenge* and *Pitfall*\\xa0[@bellemare13arcade]. These domains are particularly tricky for classical RL algorithms because even finding a single non-zero reward to bootstrap from is incredibly challenging.\\n\\nA common technique used to address the difficulty of exploration is to encourage the agent to visit under-explored areas of the state-space [@schmidhuber1991curious]. Such techniques are commonly known as intrinsic motivation [@chentanez2005intrinsically] or count-based exploration [@bellemare2016unifying]. However, these approaches do not scale well as the state space grows, as they still require exhaustive search in sparse reward environments. Additionally, recent empirical results suggest that these methods do not consistently outperform $\\\\epsilon$-greedy exploration [@taiga2019benchmarking]. The difficulty of exploration is also a consequence of the current inability of our agents to abstract the world and learn scalable, causal models with explanatory power. Instead they often use low-level features or handcrafted heuristics and lack the generalization power necessary to work in a more abstract space. Hints can be provided to the agent which bias it towards promising regions of the state space either via reward-shaping [@ng1999policy] or by introducing a sequence of curriculum tasks [@bengio2009curriculum; @graves2017automated]. However, these approaches can be difficult to specify and, in the case of reward shaping, often lead to unexpected behavior where the agent learns to exploit the modified rewards.\\n\\nAnother hallmark of hard-exploration benchmarks is that they tend to be fully-observable and exhibit little variation between episodes. Nevertheless, techniques like random no-ops and \"sticky actions\" have been proposed to artificially increase episode variance in Atari [@machado2018revisiting], an alternative is to instead consider domains with inherent variability. Other recent work on the *Obstacle Tower* challenge domain\\xa0[@juliani2019obstacle] is similar to our task suite in this regard. Reliance on determinism of the environment is one of the chief criticisms of imitation leveled by [@juliani19solving], who offers a valuable critique on [@aytar2018playing], [@ecoffet2019go] and [@opeani18learning]. In contrast, our approach is able to solve tasks with substantial per-episode variability.\\n\\nGAIL\\xa0[@ho2016generative] is another imitation learning method, however GAIL has never been successfully applied to complex partially observable environments that require memory. Even the maze task in [@zolna2019reinforced] has distinguishable rooms, uses a single layout across all episodes, and as a result does not require a recurrent policy or discriminator.\\n\\nHard-Eight Task Suite {#sec:tasks}\\n=====================\\n\\nTo address the difficulty of hard exploration in partially observable problems with highly variable initital conditions we introduce a collection of eight tasks, which exhibit these properties. Due to the generated nature of these tasks and the rich form of interaction between the agent and environment, we see greatly increased levels of variability between episodes. From the perspective of the learning process, these tasks are particularly interesting because just memorizing an open loop sequence of actions is unlikely to achieve even partial success on a new episode. The nature of interaction with the environment combined with a limited field of view also necessitates the use of memory in the agent.\\n\\n![Hard-Eight task suite. In each task an agent ([$\\\\blacktriangledown$]{style=\"color: blue\"}) must interact with objects in its environment in order to gain access to a large apple ([$\\\\blacktriangledown$]{style=\"color: red\"}) that provides reward. The 3D environment is also procedurally generated so that every episode the state of the world including object shapes, colors, and positions is different. From the point of view of the agent the environment is partially observed. Because it may take hundreds of low-level actions to collect an apple the reward is sparse which makes exploration difficult.](figures/hard_eight.png){#fig:env-diagrams width=\"0.9\\\\\\\\linewidth\"}\\n\\nAll of the tasks in the Hard-Eight task suite share important common properties that make them hard exploration problems. First, each task emits **sparse rewards**---in all but one task the only positive instantaneous reward obtained also ends the episode. The visual observations in each task are also first-person and thus the state of the world is only ever **partially observed**. Several of the tasks are constructed to ensure that that it is not possible to observe all task relevant information simultaneously.\\n\\nFinally, each task is subject to a **highly variable initial conditions**. This is accomplished by including several procedural elements, including colors, shapes and configurations of task relevant objects. The procedural generation ensures that simply copying the actions from a demonstration is not sufficient for successful execution, which is a sharp contrast to the the case of Atari\\xa0[@pohlen2018observe]. A more detailed discussion of these aspects can be found in Appendix\\xa0[9](#appendix:procedural){reference-type=\"ref\" reference=\"appendix:procedural\"} and videos of agents and humans performing these tasks can be found at <https://deepmind.com/research/publications/r2d3>.\\n\\nEach task makes use of a **standardized avatar** with a first-person view of the environment, controlled by the same discretized action space consisting of 46 discrete actions. In all tasks the agent is rewarded for collecting apples and often this is the only reward obtained before the episode ends. A depiction of each task is shown in Figure\\xa0[1](#fig:env-diagrams){reference-type=\"ref\" reference=\"fig:env-diagrams\"}. A description of the procedural elements and filmstrip of a successful episode for each task is provided in Appendix\\xa0[\\\\[appendix:tasks\\\\]](#appendix:tasks){reference-type=\"ref\" reference=\"appendix:tasks\"}.\\n\\nEach of these tasks requires the agent to complete a sequence of high-level steps to complete the task. An example from the task suite is shown in Figure\\xa0[2](#fig:baseball-filmstrip){reference-type=\"ref\" reference=\"fig:baseball-filmstrip\"}. The agent must: find the bat, pick up the bat, knock the ball off the plinth, pick up the ball, activate the sensor with the ball (opening the door), walk through the door, and collect the large apple.\\n\\n![High-level steps necessary to solve the Baseball task. Each step in this sequence must be completed in order, and must be implemented by the agent as a sequence of low level actions (no option structure is available to the agent). The necessity of completing such a long sequence of high level steps makes it unlikely that the task will ever be solved by random exploration. Note that each step involves interaction with physical objects in the environment, shown in bold.](figures/baseball_filmstrip.png){#fig:baseball-filmstrip width=\"0.9\\\\\\\\linewidth\"}\\n\\nThe Hard-Eight task suite contains the following tasks:\\n\\n**Baseball**The agent spawns in a small room with a sensor and a key object resting high atop a plinth. The agent must find a stick and use it to knock the key object of the plinth in order to activate the sensor. Activating the sensor opens a door to an adjoining room with a large apple which ends the episode.\\n\\n**Drawbridge**The agent spawns at one end of a network of branching platforms separated by drawbridges, which can be activated by touching a key object to a sensor. Activating a drawbridge with a key object destroys the key. Each platform is connected to several drawbridges, but has only one key object available. Some paths through the level have small apples which give reward. The agent must choose the most rewarding path through the level to obtain a large apple at the end which ends the episode.\\n\\n**Navigate Cubes**The agent spawns on one side of a large room. On the other side of the room on a raised platform there is a large apple which ends the episode. Across the center of the room there is a wall of movable blocks. The agent must dig through the wall of blocks and find a ramp onto the goal platform in order to collect the large apple.\\n\\n**Push Blocks**The agent spawns in a medium sized room with a recessed sensor in the floor. There are several objects in the room that can be pushed but not lifted. The agent must push a block whose color matches the sensor into the recess in order to open a door to an adjoining room which contains a large apple which ends the episode. Pushing a wrong object into the recess makes the level impossible to complete.\\n\\n**Remember Sensor**The agent spawns near a sensor of a random color. The agent must travel down a long hallway to a room full of blocks and select one that matches the color of the sensor. Bringing the correct block back to the sensor allows access to a large apple which ends the episode. In addition to being far away, traveling between the hallway and the block room requires the agent to cross penalty sensors which incurs a small negative reward.\\n\\n**Throw Across**The agent spawns in a U shaped room with empty space between the legs of the U. There are two key objects near the agent spawn point. The agent must throw one of the key objects across the void, and carry the other around the bottom of the U. Both key objects are needed to open two locked doors which then give access to a large apple which ends the episode.\\n\\n**Wall Sensor**The agent spawns in a small room with a wall mounted sensor and a key object. The agent must pick up the key and touch it to the sensor which opens a door. In the adjoining room there is a large apple which ends the episode.\\n\\n**Wall Sensor Stack**The agent spawns in a small room with a wall mounted sensor and two key objects. This time one of key objects must be in constant contact with the sensor in in order for the door to remain open. The agent must stack the two objects so one can rest against the sensor, allowing the agent to pass through to an adjoining room with a large apple which ends the episode.\\n\\nBaselines {#sec:baselines}\\n=========\\n\\nIn this section we discuss the baselines and ablations we use to compare against our R2D3 agent in the experiments. We compare to Behavior Cloning (a common baseline for learning from demonstrations) as well as two ablations of our method which individually remove either recurrence or demonstrations from R2D3. The two ablations correspond to two different state of the art methods from the literature.\\n\\n**Behavior Cloning** BC is a simple and common baseline method for learning policies from demonstrations\\xa0[@pomerleau1989alvinn; @rahmatizadeh2018vision]. This algorithm corresponds to a supervised learning approach to imitation learning which uses only expert trajectories as its training dataset to fit a parameterized policy mapping states to actions. For discrete actions this corresponds to a classification task, which we fit using the cross-entropy loss. If the rewards of trajectories in the training dataset are consistently high, BC is known to outperform recent batch-RL methods [@fujimoto2018off]. To enable fair comparison we trained our BC agent using the same recurrent neural network architecture that we used for our R2D3 algorithm (see Figure\\xa0[3](#fig:recurrent-agent){reference-type=\"ref\" reference=\"fig:recurrent-agent\"}).\\n\\n**No Demonstrations**The first ablation we consider is to remove demonstrations from R2D3. This corresponds to setting the demo ratio (see Figure\\xa0[\\\\[fig:r2d3\\\\]](#fig:r2d3){reference-type=\"ref\" reference=\"fig:r2d3\"}) to $\\\\rho=0$. This special case of R2D3 corresponds exactly to the R2D2 agent of [@kapturowski2018recurrent], which itself extends DQN [@mnih2015human] to partially observed environments by combining it with recurrence and the distributed training architecture of Ape-X DQN\\xa0[@horgan2018distributed]. This ablation is itself state of the art on Atari-57 and DMLab-30, making it an extremely strong baseline.\\n\\n**No Recurrence**The second ablation we consider is to replace the recurrent value function of R2D3 with a feed-forward reactive network. We do this separately from the no demonstrations ablation, leaving the full system in Figure\\xa0[\\\\[fig:r2d3\\\\]](#fig:r2d3){reference-type=\"ref\" reference=\"fig:r2d3\"} in tact, with only the structure of the network changed. If we further fix the demo ratio to $\\\\rho=0.25$ then this ablation corresponds to the DQfD agent of [@hester2018deep], which is competitive on hard-exploration Atari environments such as Montezuma\\'s Revenge. However, we do not restrict ourselves to $\\\\rho=0.25$, and instead optimize over the demo ratio for the ablation as well as for our main agent.\\n\\nExperiments {#sec:experiments}\\n===========\\n\\nWe evaluate the performance of our R2D3 agent alongside state-of-the-art deep RL baselines. As discussed in Section\\xa0[5](#sec:baselines){reference-type=\"ref\" reference=\"sec:baselines\"}, we compare our R2D3 agent to BC (standard LfD baseline) R2D2 (off-policy SOTA), DQfD (LfD SOTA). We use our own implementations for all agents, and we plan to release code for all agents including R2D3.\\n\\nFor each task in the Hard-Eight suite, we trained R2D3, R2D2, and DQfD using 256 $\\\\epsilon$-greedy CPU-based actors and a single GPU-based learner process. Following @horgan2018distributed, the $i$-th actor was assigned a distinct noise parameter $\\\\epsilon_i \\\\in [0.4^8, 0.4]$ where each $\\\\epsilon_i$ is regularly spaced in $\\\\log_{0.4}$ space. For each of the algorithms their common hyperparameters were held fixed. Additionally, for R2D3 and DQfD the demo ratio was varied to study its effect. For BC we also varied the learning rate independently in a vain attempt to find a successful agent.\\n\\n![**(a)** Recurrent head used by R2D3 agents. **(b)** Feedforward head used by the DQfD agent. Heads in both a) and b) are used to compute the Q values. **(c)** Architecture used to compute the input feature representations. Frames of size 96x72 are fed into a ResNet, the output is then augmented by concatenating the previous action $a_{t-1}$, previous reward $r_{t-1}$, and other proprioceptive features $f_t$, such as accelerations, whether the avatar hand is holding an object, and the hand\\'s relative distance to the avatar.](figures/network.png){#fig:recurrent-agent width=\"80%\"}\\n\\n![Reward vs actor steps curves for R2D3 and baselines on the Hard-Eight task suite. The curves are computed as the mean performance for the same agent across 5 different seeds per task. Error regions show the 95% confidence interval for the mean reward across seeds. Several curves overlap exactly at zero reward for the full range of the plots. R2D3 can perform human-level or better on Baseball, Drawbridge, Navigate Cubes and Wall Sensor. R2D2 could not get any positive rewards on any of the tasks. DQfD and BC agents occasionally see rewards on Drawbridge and Navigate Cubes tasks, but this happens rarely enough that the effect is not visible in the plots. Indicators ([$\\\\blacktriangledown$]{style=\"color: red\"}) mark analysis points in Section\\xa0[6.3](#guided-exploration){reference-type=\"ref\" reference=\"guided-exploration\"}.](figures/task_rewards.png){#fig:reward-curves width=\"\\\\\\\\textwidth\"}\\n\\nAll agents act in the environment with an action-repeat factor of 2, i.e.\\xa0the actions received by the environment are repeated twice before passing the observation to the agent. Using an action repeat of 4 is common in other domains like Atari\\xa0[@bellemare2012investigating; @mnih2015human]; however, we found that using an action repeat of 4 made the Hard-Eight tasks too difficult for our demonstrators. Using an action repeat of 2 allowed us to strike a compromise between ease of demonstration (which is made harder by high action repeats prohibiting smooth and intuitive motion) and ease of learning for the agents (which is made harder by low action repeats increasing the number of steps required to complete the task).\\n\\nFigure\\xa0[3](#fig:recurrent-agent){reference-type=\"ref\" reference=\"fig:recurrent-agent\"} illustrates the neural network architecture of the different agents. As much as possible we use the same network architecture across all agents, deviating only for DQfD, where the recurrent head is replaced with an equally sized feed-forward layer. We briefly outline the training setup below, and give an explicit enumeration of the hyperparameters in Appendix\\xa0[10](#app:shared_hyperparameters){reference-type=\"ref\" reference=\"app:shared_hyperparameters\"}.\\n\\nFor R2D3, R2D2 and DQfD we use the Adam optimizer [@kingma2014adam] with a fixed learning rate of $2\\\\times10^{-4}$. We use hyperparameters that are shown to work well for similar environments. We use distributed training with 256 parallel actors, trained for at least 10 billion actor steps for all tasks.\\n\\nFor the BC agent the training regime is slightly different, since this agent does not interact with the environment during training. For BC we also use the Adam optimizer but we additionally perform a hyperparameter sweep over learning rates $\\\\{10^{-5}, 10^{-4}, 10^{-3}\\\\}$. Since there is no notion of actor steps in BC we trained for 500k learner steps instead.\\n\\nDuring the course of training, an evaluator process periodically queries the learner process for the latest network weights and runs the resulting policy on an episode, logging both the final return and the total number of steps (actor or learner steps, as appropriate) performed at the time the of evaluation.\\n\\nWe collected a total of 100 demonstrations for each task spread across three different experts (each expert contributed roughly one third of the demonstrations for each task). Demonstrations for the tasks were collected using keyboard and mouse controls mapped to the agent\\'s exact action space, which was necessary to enable both behaviour cloning and learning from demonstrations. We show statistics related to the human demonstration data which we collected from three experts in Table\\xa0[\\\\[tab:human_data\\\\]](#tab:human_data){reference-type=\"ref\" reference=\"tab:human_data\"}.\\n\\nLearning the Hard-Eight Tasks\\n-----------------------------\\n\\nIn Figure\\xa0[4](#fig:reward-curves){reference-type=\"ref\" reference=\"fig:reward-curves\"}, we report the return against the number of actor steps, averaged over five random initializations. We find that none of the baselines succeed in any of the eight environments. Meanwhile, R2D3 learns six out of the eight tasks, and reaches or exceeds human performance in four of them. The fact that R2D3 learns at all in this setting with only 100 demonstrations per task demonstrates the ability of the agent to make very efficient use of the demonstrations. This is in contrast to BC and DQfD which use the same demonstrations, and both fail to learn a single task from the suite.\\n\\nAll methods, including R2D3, fail to solve two of the tasks: Remember Sensor and Throw Across. These are the two tasks in the suite that are most demanding in terms of memory requirements for the agent, and it is possible that our zero-initialization with burn-in strategy for handling LSTM states in replay does not give R2D3 sufficient context to complete these tasks successfully. Future work should explore the better handling of recurrent states as a possible avenue towards success on these tasks. R2D3, BC, and DQfD receive some negative returns on Remember Sensor, which indicates that the agents navigate down the hallway and walks over penalty sensors.\\n\\n![**Figure** \\\\| Success rate (see main text) for R2D3 across all tasks with at least one successful seed, as a function of demo ratio. The square markers for each demo ratio denote the mean success rate, and the error bars show a bootstrapped estimate of the $[25, 75]$ percentile interval for the mean estimate. The lower demo ratios consistently outperform the higher demo ratios across the suite of tasks.](figures/demo_ratios_sr_errors.pdf){#fig:demo-ratio-success-rate width=\"0.98\\\\\\\\linewidth\"}\\n\\n[\\\\[fig:demo-ratio-success-rate\\\\]]{#fig:demo-ratio-success-rate label=\"fig:demo-ratio-success-rate\"}\\n\\n  **Task Name**       **Reward**       **Episode Len.**\\n  ------------------- ---------------- ------------------\\n  Baseball            7.8 $\\\\pm$ 4.1    492 $\\\\pm$ 121\\n  Drawbridge          12.3 $\\\\pm$ 2.5   641 $\\\\pm$ 137\\n  Navigate Cubes      7.9 $\\\\pm$ 4.1    638 $\\\\pm$ 185\\n  Push Blocks         9.1 $\\\\pm$ 2.9    683 $\\\\pm$ 270\\n  Remember Sensor     7.7 $\\\\pm$ 1.4    853 $\\\\pm$ 188\\n  Throw Across        5.4 $\\\\pm$ 4.9    464 $\\\\pm$ 172\\n  Wall Sensor         9.1 $\\\\pm$ 2.8    280 $\\\\pm$ 87\\n  Wall Sensor Stack   8.6 $\\\\pm$ 3.5    521 $\\\\pm$ 107\\n\\n[\\\\[tab:human_data\\\\]]{#tab:human_data label=\"tab:human_data\"}\\n\\nR2D3 performed better than our average human demonstrator on Baseball, Drawbridge, Navigate Cubes and the Wall Sensor tasks. The behavior on Wall Sensor Stack in particular is quite interesting. On this task R2D3 found a completely different strategy than the human demonstrators by exploiting a bug in the implementation of the environment. The intended strategy for this task is to stack two blocks on top of each other so that one of them can remain in contact with a wall mounted sensor, and this is the strategy employed by the demonstrators. However, due to a bug in the environment the strategy learned by R2D3 was to trick the sensor into remaining active even when it is not in contact with the key by pressing the key against it in a precise way.\\n\\nIn light of the uniform failure of our baselines to learn on the Hard-Eight suite we made several attempts at training other models on the task suite; however, these attempts were all unsuccessful. For example, we tried adding randomized prior functions [@osband2018randomized] to R2D2, but this approach was still unable to obtain reward on any of the Hard-Eight tasks. We also trained an IMPALA agent with pixel control [@jaderberg2016reinforcement] as auxiliary reward to help with exploration, but this approach also failed to learn on any of the tasks we attempted. We omit these results from Figure\\xa0[4](#fig:reward-curves){reference-type=\"ref\" reference=\"fig:reward-curves\"}, only keeping the most relevant baselines.\\n\\nEffect of the Demo Ratio\\n------------------------\\n\\nIn our experiments on Hard-Eight tasks (see Figure [4](#fig:reward-curves){reference-type=\"ref\" reference=\"fig:reward-curves\"}), we did a hyperparameter search and chose the best hyperparameters for each method independently. In this section, we look more closely at how the demo ratio ($\\\\rho$) affects learning in R2D3. To do this we look at how the success rate of R2D3 across the entire Hard-Eight task suite varies as a function of the demo ratio.\\n\\nThe goal of each task in the Hard-Eight suite is to collect a large apple, which ends the episode and gives a large reward. We consider an *episode* successful if the large apple is collected. An agent that executes many episodes in the environment will either succeed or fail at each one. We consider an *agent* successful if, after training, at least 75% of its final 25 episodes are successful. Finally, an individual agent with a fixed set of hyperparameters may still succeed or fail depending on the randomness in the environment and the initialization of the agent. We call the proportion of agents that succeed for a given set of hyperparameters the *success rate* of the algorithm.\\n\\nWe train several R2D3 agents on each tractable task[^1] in the Hard-Eight suite, varying only the demo ratio while keeping the rest of the hyperparameters fixed at the values used for the learning experiment. We consider four different demo ratios across six tasks, with five seeds for each task, for a total of 120 agents trained. Figure\\xa0[5](#fig:demo-ratio-success-rate){reference-type=\"ref\" reference=\"fig:demo-ratio-success-rate\"} shows estimates of the success rate for the R2D3 algorithm for each different demo ratio, aggregated across all tasks. We observe that tuning the demo ratio has a strong effect on the success rate across the task suite, and that the best demo ratio is quite small. See Appendix\\xa0[11.3](#app:experiment_details){reference-type=\"ref\" reference=\"app:experiment_details\"} for further results.\\n\\nGuided exploration by demonstration {#guided-exploration}\\n-----------------------------------\\n\\n![ Guided exploration behavior in the Push Blocks task. **(a)** Spatial pattern of exploration behavior at ${\\\\sim}5$B actor steps (reward-driven learning kicks off for R2D3 only after ${\\\\sim}20$B steps). Overlay of agent\\'s trajectories over 200 episodes. Blocks and sensors are not shown for clarity. R2D2 appears to follow a random walk. R2D3 concentrates on a particular spatial region. **(b)** Interactions between the agent and blocks during the first 12B steps. Each line shows a different random seed. R2D2 rarely pushes the blocks. **(c)** Example trajectory of R2D3 after training, showing the agent pushing the blue block onto the blue sensor, then going to collect the apple reward (green star).](figures/exploration.pdf){#fig:structured-explore width=\"1.0\\\\\\\\linewidth\"}\\n\\nThe typical strategy for exploration in RL is to either use a stochastic policy and sample actions, or to use a deterministic policy and take random actions some small $\\\\epsilon$ fraction of the time. Given sufficient time both of these approaches will in theory cover the space of possible behaviors, but in practice the amount of time required to achieve this coverage can be prohibitively long. In this experiment, we compare the behavior of R2D3 to the behavior of R2D2 (which is equivalent to R2D3 without demonstrations) on two of the tasks from the Hard-Eight suite. Even very early in training (well before R2D3 is able to reliably complete the tasks) we see many more task-relevant actions from R2D3 than from R2D2, suggesting that the effect of demonstrations is to bias R2D3 towards exploring relevant parts of the environment.\\n\\nIn Figure\\xa0[6](#fig:structured-explore){reference-type=\"ref\" reference=\"fig:structured-explore\"} we begin by examining the Push Blocks tasks. The task here is to push a particular block onto a sensor to give access to a large apple, and we examine the behavior of both R2D3 and R2D2 after 5B steps, which is long before R2D3 begins to solve the task with any regularity (see Figure\\xa0[4](#fig:reward-curves){reference-type=\"ref\" reference=\"fig:reward-curves\"}). Looking at the distribution of spatial locations for the agents it is clear that R2D2 essentially diffuses randomly around the room, while R2D3 spends much more time in task-relevant parts of the environment (e.g.\\xa0away from the walls). We also record the total distance traveled by the moveable blocks in the room, and find that R2D3 tends to move the blocks significantly more often than R2D2, even before it has learned to solve the task.\\n\\nIn Figure\\xa0[7](#fig:structured-explore-0){reference-type=\"ref\" reference=\"fig:structured-explore-0\"} we show a different analysis of the Baseball task (see Figure\\xa0[2](#fig:baseball-filmstrip){reference-type=\"ref\" reference=\"fig:baseball-filmstrip\"} for a detailed walkthrough of this task). Here we manually identify a sequence of milestones that a trajectory must reach in order to be successful, and record how often different agents achieve each of these subgoals. This subgoal structure is implicit in the task, but is not made available explicitly to any of the agents during training; they are identified here purely as a post-hoc analysis tool. In this task we see that the R2D3 agents learn very quickly to pick up and raise the bat, while the R2D2 agents rarely interact with the bat at all, and actually do so less as training proceeds. We also see that hitting the ball off the plinth is most difficult step to learn in this task, bottlenecking two of the R2D3 agents.\\n\\n![ Guided exploration behavior in the Baseball task. **(a)** Sub-behaviors expressed by five R2D2 and five R2D3 agents after 0.5B steps of training (left) and 4B steps of training (right). Each point is estimated from 200 episodes. At 0.5B steps, none of the agents received any reward over the 200 evaluation episodes, while at 4B steps, three of the R2D3 agents received reward on almost every episode. Even when the R2D3 agents are not receiving reward, they are expressing some of the necessary behaviors provided through human demonstrations. **(b)** R2D3 agents eventually surpass human performance. The 3 of 5 R2D3 agents shown in (a) which start obtaining rewards continue to bootstrap towards more efficient policies than humans. ](figures/exploration_baseball.pdf){#fig:structured-explore-0 width=\"1.0\\\\\\\\linewidth\"}\\n\\nConclusion\\n==========\\n\\nIn this paper, we introduced the R2D3 agent, which is designed to make efficient use of demonstrations to learn in partially observable environments with sparse rewards and highly variable initial conditions. We showed through several experiments on eight very difficult tasks that our approach is able to outperform multiple state of the art baselines, two of which are themselves ablations of R2D3.\\n\\nWe also identified a key parameter of our algorithm, the *demo ratio*, and showed that careful tuning of this parameter is critical to good performance. Interestingly we found that the optimal demo ratio is surprisingly small but non-zero, which suggests that there may be a risk of overfitting to the demonstrations at the cost of generalization. For future work, we could investigate how this optimal demo ratio changes with the total number of demonstrations and, more generally, the distribution of expert trajectories relative to the task variability.\\n\\nWe introduced the Hard-Eight suite of tasks and used them in all of our experiments. These tasks are specifically designed to be partially observable tasks with sparse rewards and highly variable initial conditions, making them an ideal testbed for showcasing the strengths of R2D3 in contrast to existing methods in the literature.\\n\\nOur behavioral analysis showed that the mechanism R2D3 uses to efficiently extract information from expert demonstrations is to use them in a way that guides (or biases) the agent\\'s own autonomous exploration of the environment. An in-depth analysis of agent behavior on the Hard-Eight task suite is a promising direction for understanding how different RL algorithms make selective use of information.\\n\\nAcknowledgements {#acknowledgements .unnumbered}\\n================\\n\\nWe would like to thank the following members of the DeepMind Worlds Team for developing the tasks in this paper: Charlie Beattie, Gavin Buttimore, Adrian Collister, Alex Cullum, Charlie Deck, Simon Green, Tom Handley, Cdric Hauteville, Drew Purves, Richie Steigerwald and Marcus Wainwright.\\n\\nWe would also like to acknowledge the scientific python community for developing the the core set of tools that enabled this work, including Tensorflow\\xa0[@tensorflow2016], Numpy\\xa0[@numpy2006], Pandas\\xa0[@mckinney2010data], Matplotlib\\xa0[@hunter2007matplotlib] and Seaborn\\xa0[@michael_waskom_2017_883859].\\n\\nR2D3 {#sec:algorithm}\\n====\\n\\nBelow we include pseudocode for the full R2D3 agent. The agent consists first of a single **learner** process which samples from both demonstration and agent buffers in order to update its policy parameters.\\n\\n**Inputs:** replay of expert demonstrations $\\\\mathcal{D}$, replay of agent experiences $\\\\mathcal{R}$, batch size $B$, sequence length $m$, and number of actors $A$.\\n\\nInitialize policy weights $\\\\theta$. Initialize target policy weights $\\\\theta\\' \\\\leftarrow \\\\theta$. Launch $A$ actors and replicate policy weights $\\\\theta$ to each actor. Sample transition sequences $(s_{t:t+m}, a_{t:t+m}, r_{t:t+m})$ from replay $\\\\mathcal{D}$ with probability $\\\\rho$ or from replay $\\\\mathcal{R}$ with probability $(1 - \\\\rho)$, to construct a mini-batch of size $B$. Calculate loss using target network. Perform a gradient descent step to update $\\\\theta$. If $t \\\\bmod t_{target} = 0$, update the target policy weights $\\\\theta\\' \\\\leftarrow \\\\theta$. If $t \\\\bmod t_{actor} = 0$, replicate policy weights to the actors.\\n\\nThe agent also consists of $A$ parallel **actor** processes which interact with a copy of the environment in order to obtain data which is then inserted into the agent buffer. The agents periodically update their parameters to match those being updated on the learner.\\n\\nSample action from behavior policy $a \\\\leftarrow \\\\pi(s)$ Execute $a$ and observe $s\\'$ and $r$ Store $(s, a, s\\', r)$ in $\\\\mathcal{R}$ learner finishes.\\n\\nHard-Eight task suite details {#appendix:procedural}\\n=============================\\n\\n[\\\\[appendix:tasks\\\\]]{#appendix:tasks label=\"appendix:tasks\"}\\n\\n#### Sparse rewards\\n\\nAll of the tasks emit sparse rewards, indeed in all but one task the only positive instantaneous reward obtained also ends the episode successfully. In other words, for standard RL algorithms to learn by bootstrapping, the actors must first solve the task inadvertently, and must do so with no intermediate signal to guide them.\\n\\n#### Partial observability\\n\\nVisual observations are all first-person, which means that some relevant features of the state of the world may be invisible to the agent simply because they are behind it or around a corner. Some tasks (e.g.\\xa0Remember Sensor, are explicitly designed so that this is the case).\\n\\n#### Highly Variable Initial Conditions\\n\\nMany of the elements of the tasks are procedurally generated, which leads to significant variability between episodes of the same task. In particular, the starting position and orientation of the agent are randomized and similarly, where they are present, the shapes, colors, and textures of various objects are randomly sampled from a set of available such features. Therefore a single (or small number of) demonstration(s) is not sufficient to guide an agent to solve the task as it is in the case of DQfD on Atari\\xa0[@pohlen2018observe].\\n\\n#### Observation specification\\n\\nAll of the tasks provide the same observation space. In particular, a visual channel consisting of 96 by 72 RGB pixels, as well as accelerations of the avatar, force applied by the avatar hand on the object, whether if the avatar is holding anything or not, and the distance of a held object from the face of the avatar (zero when there is no held object).\\n\\n#### Action specification\\n\\nThe action space consists of four displacement and four rotation actions (8), duplicated for coarse and fine-grained movement (16) as well as for movement with and without grasping (32). The avatar also has an invisible \"hand\" which can be used to manipulate objects in the environment. The location of the hand is controlled by the avatar gaze direction, plus an additional two actions that control the distance of the hand from the body (34). A grasped object can be manipulated by six rotation actions (two for each rotational degree of freedom; 40) as well as four additional actions controlling the distance of the hand from the body at coarse and fine speed (44). Finally there is an independent grasp action (to hold an object without moving), and a no-op action (total 46). Compared to course actions, fine-grained actions result in slower movements, allowing the agent to perform careful manipulations.\\n\\nIndividual task details {#app:additional-task-details}\\n-----------------------\\n\\nThis section gives addition details on each task in our suite including a sequence frames from a successful task execution (performed by a human) and a list of the procedural elements randomized per episode. Videos of agents and humans performing these tasks can be found at <https://deepmind.com/research/publications/r2d3>.\\n\\n#### Baseball\\n\\n![image](figures/baseball/000001.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/baseball/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/baseball/000003.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/baseball/000004.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/baseball/000005.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/baseball/000006.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Wall, floor and object materials and colors\\n\\n-   Initial position of the stick\\n\\n-   Position of plinth\\n\\n#### Drawbridge\\n\\n![image](figures/drawbridge/000000.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/drawbridge/000001.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/drawbridge/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/drawbridge/000003.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/drawbridge/000004.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/drawbridge/000005.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Wall, floor, ceiling and object materials and colors\\n\\n-   Positions of the small apples throughout the network of ledges\\n\\n#### Navigate Cubes\\n\\n![image](figures/navigate_cubes/000000.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/navigate_cubes/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/navigate_cubes/000003.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/navigate_cubes/000004.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/navigate_cubes/000005.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/navigate_cubes/000006.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Wall, floor and object materials and colors\\n\\n#### Push Blocks\\n\\n![image](figures/push_blocks/000001.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/push_blocks/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/push_blocks/000003.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/push_blocks/000004.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/push_blocks/000005.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/push_blocks/000006.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Wall, floor, object materials and colors\\n\\n-   Positions of the objects\\n\\n-   Sensor required color\\n\\n#### Remember Sensor\\n\\n[\\\\[app:remember-sensor\\\\]]{#app:remember-sensor label=\"app:remember-sensor\"}\\n\\n![image](figures/remember_sensor/000000.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000001.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000003.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000004.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000005.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000006.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000007.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000008.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000009.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/remember_sensor/000010.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Sensor required color\\n\\n-   Number of objects in the block room\\n\\n-   Position of objects in the block room\\n\\n-   Shape and material of the objects in the block room\\n\\n#### Throw Across\\n\\n![image](figures/throw_across/000000.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/throw_across/000001.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/throw_across/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/throw_across/000003.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/throw_across/000006.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/throw_across/000007.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Wall, floor and object materials and colors\\n\\n-   Color and material of the sensors\\n\\n-   Initial positions of the two key objects\\n\\n#### Wall Sensor\\n\\n![image](figures/wall_sensor/000000.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/wall_sensor/000001.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/wall_sensor/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/wall_sensor/000003.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/wall_sensor/000004.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Position of the sensor\\n\\n-   Position of the key object\\n\\n#### Wall Sensor Stack\\n\\n![image](figures/wall_sensor_stack/000000.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/wall_sensor_stack/000001.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/wall_sensor_stack/000002.png){width=\"0.15\\\\\\\\linewidth\"} ![image](figures/wall_sensor_stack/000003.png){width=\"0.15\\\\\\\\linewidth\"}\\n\\nProcedural elements\\n\\n-   Initial position and orientation of the agent\\n\\n-   Wall, floor and object materials and colors\\n\\n-   Initial positions of both key objects\\n\\n-   Position of the sensor\\n\\nHyper-parameters {#app:shared_hyperparameters}\\n================\\n\\nIn Table [\\\\[tab:hyperparams\\\\]](#tab:hyperparams){reference-type=\"ref\" reference=\"tab:hyperparams\"}, we report the shared set of hyper-parameters across different models and tasks.\\n\\n  **Hyperparameters**                   **Values**\\n  ------------------------------------- ----------------------------------------------------------------------------------------------------\\n  **Network**                           See Figure\\xa0[3](#fig:recurrent-agent){reference-type=\"ref\" reference=\"fig:recurrent-agent\"}\\n  **Environment**                       \\n  Image height                          72\\n  Image width                           96\\n  Color                                 RGB\\n  Action repeats                        2\\n  Observation spec                      See section [\\\\[appendix:tasks\\\\]](#appendix:tasks){reference-type=\"ref\" reference=\"appendix:tasks\"}\\n  Action spec                           See section [\\\\[appendix:tasks\\\\]](#appendix:tasks){reference-type=\"ref\" reference=\"appendix:tasks\"}\\n  **Learner**                           \\n  Learning rate                         2e-4\\n  Optimizer                             Adam [@kingma2014adam]\\n  Global norm gradient clipping         True\\n  Discount factor ($\\\\gamma$)            0.997\\n  Batch size ($B$)                      32\\n  Target update period ($t_{target}$)   400\\n  Actor update period ($t_{actor}$)     200\\n  Prioritized sampling                  True\\n  Sequence length ($m$)                 80\\n  Burn in length                        40\\n  Asymmetric reward clipping            True\\n  Number of actors ($A$)                256\\n  Max replay capacity                   500000\\n  Min replay capacity                   25000\\n\\nExperiments {#app:experiments}\\n===========\\n\\nSurpassing the experts\\n----------------------\\n\\nAn important property of R2D3 is that although the agents are trained from demonstrations, the behaviors they achieve are able to surpass the skill of the demonstrations they were trained from. This can be seen quantitatively from reward curves in Figure\\xa0[4](#fig:reward-curves){reference-type=\"ref\" reference=\"fig:reward-curves\"}, where the R2D3 agent surpasses the human baseline performance on four of the eight tasks (e.g.\\xa0Baseball, Navigate Cubes, Wall Sensor and Wall Sensor Stack).\\n\\nIn some of these cases the improved score is simply a matter of executing the optimal strategy more fluently than the demonstrators. For example, this is the case in the Baseball task, where the human demonstrators are handicapped by the fact that the human interface to the agent action space makes it awkward to rotate a held object. This makes picking up the stick and orienting it properly to knock the ball off the plinth into a tricky task for humans, but the agents are able to refine their behavior to be much more efficient (see Figure\\xa0[7](#fig:structured-explore-0){reference-type=\"ref\" reference=\"fig:structured-explore-0\"}c).\\n\\nThe behavior on Wall Sensor is especially interesting, however in this case the agents find a completely different strategy than the human demonstrators by exploiting a bug in the implementation of the environment. The intended strategy for this task is to stack two blocks on top of each other so that one of them can remain in contact with a wall mounted sensor, and this is the strategy employed by the demonstrators. However, due to a bug in the environment it is also possible to trick the sensor into remaining active even when it is not in contact with the key by pressing the key against it in a precise way. The R2D3 agents are able to discover this bug and exploit it, resulting in superhuman scores on this task even though this strategy is not present in the demonstrations.\\n\\n![We show the rewards of the R2D3 agent on different tasks for each seed separately.](figures/task_rewards_per_seed.pdf){#fig:task_rewards_per_seed width=\"0.95\\\\\\\\linewidth\"}\\n\\n![R2D3 learning curves with varying demo ratios for all tasks.](figures/demo_ratio_rewards.pdf){#fig:demo_ratio_rewards width=\"0.95\\\\\\\\linewidth\"}\\n\\nAdditional experiments {#app:additional-results}\\n----------------------\\n\\nWe also ran a few additional experiments to get more information about the tasks we did not solve, or solved incorrectly. Videos for these experiments are available at <https://deepmind.com/research/publications/r2d3>.\\n\\n**Remember Sensor**This task requires a long memory, and also has the longest episodes length of any task in the Hard Eight suite. In an attempt to mitigate these issues, we trained the agent using a higher action repeat which reduces the episode length, and used stale lstm states instead of zero lstm states which provides information from earlier in the episode. This allows R2D3 to learn policies that display reasonable behavior, retrieving a random block and bringing it back to the hallway. Using this method it can occasionally solve the task.\\n\\n**Throw Across**The demonstrations collected for this task had a very low success rate of 54%. We attempted to compensate for this by collecting an additional 30 demos. When we trained R2D3 with all 130 demos all seeds solved the task.\\n\\n**Wall Sensor Stack**The original Wall Sensor Stack environment had a bug that the R2D3 agent was able to exploit. We fixed the bug and verified the agent can learn the proper stacking behavior.\\n\\nAddition details for main experiments {#app:experiment_details}\\n-------------------------------------\\n\\nIn Figure [8](#fig:task_rewards_per_seed){reference-type=\"ref\" reference=\"fig:task_rewards_per_seed\"}, we show the performance of the R2D3 agents for each seed separately. On task such as Drawbridge, Navigate Cubes and Wall Sensor, all seeds take off quite rapidly and they have very low variance for the rewards between different seeds. However, on Wall Sensor Stack task while one seed takes off quite rapidly, and the rest of them are just flat. In Figure [9](#fig:demo_ratio_rewards){reference-type=\"ref\" reference=\"fig:demo_ratio_rewards\"}, we elaborate on Figure\\xa0[5](#fig:demo-ratio-success-rate){reference-type=\"ref\" reference=\"fig:demo-ratio-success-rate\"}. For Baseball, Navigate Cubes, Push Blocks, and Wall Sensor Stack, a demo ratio of 1/256 works best. On Drawbridge and Wall Sensor all demo ratios are similarly effective.\\n\\n![ Further detail of guided exploration behavior in the Push Blocks task (as in Figure\\xa0[6](#fig:structured-explore){reference-type=\"ref\" reference=\"fig:structured-explore\"}). **(a)** Proportion of episodes in which the agent pushes a crate into the recess during the initial 12B steps of training. **(b)** Proportion of episodes in which the crate pushed into the recess actually matches the sensor color. Data are only shown when crates are pushed into the recess on at least 5 out of 200 episodes. Dashed line shows the probability expected if a random crate was pushed into the recess. Thus, while (c) shows that by 12B steps the R2D3 agent may have reasonable success in pushing crates into the recess, it has not yet mastered the logic that the crate color must much the sensor color. ](figures/appendix_exploration_slot_3.pdf){#appendix:fig:structured-explore-2 width=\"1.0\\\\\\\\linewidth\"}\\n\\n![ Further detail of guided exploration behavior in the Push Blocks task (as in Figure\\xa0[6](#fig:structured-explore){reference-type=\"ref\" reference=\"fig:structured-explore\"}). **(a)** Spatial pattern of exploration behavior for the R2D2 agent over the course of ${\\\\sim}12$B steps of training. Each row shows a different random seed; the number of training steps increases from the leftmost column to the rightmost column. There is little variation in how the policy manifests as explorative behavior across seeds and training time. **(b)** As in (a), for R2D3. Given demonstrations, the policies now show substantial variation across seeds and training time. ](figures/appendix_exploration_slot_1.png \"fig:\"){#appendix:fig:structured-explore-1 width=\"0.95\\\\\\\\linewidth\"} ![ Further detail of guided exploration behavior in the Push Blocks task (as in Figure\\xa0[6](#fig:structured-explore){reference-type=\"ref\" reference=\"fig:structured-explore\"}). **(a)** Spatial pattern of exploration behavior for the R2D2 agent over the course of ${\\\\sim}12$B steps of training. Each row shows a different random seed; the number of training steps increases from the leftmost column to the rightmost column. There is little variation in how the policy manifests as explorative behavior across seeds and training time. **(b)** As in (a), for R2D3. Given demonstrations, the policies now show substantial variation across seeds and training time. ](figures/appendix_exploration_slot_2.png \"fig:\"){#appendix:fig:structured-explore-1 width=\"0.95\\\\\\\\linewidth\"}\\n\\n[^1]: We exclude Remember Sensor and Throw Across from this analysis, since we saw no successful seeds for either of these tasks.\\n',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{45}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,\\n  Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore, Murray, Steiner,\\n  Tucker, Vasudevan, Warden, Wicke, Yu, and Zheng]{tensorflow2016}\\nMartin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey\\n  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,\\n  Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek~G. Murray,\\n  Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan\\n  Yu, and Xiaoqiang Zheng.\\n\\\\newblock Tensorflow: A system for large-scale machine learning.\\n\\\\newblock In \\\\emph{12th USENIX Symposium on Operating Systems Design and\\n  Implementation}, pages 265--283, 2016.\\n\\n\\\\bibitem[Aytar et~al.(2018)Aytar, Pfaff, Budden, Paine, Wang, and\\n  de~Freitas]{aytar2018playing}\\nYusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando\\n  de~Freitas.\\n\\\\newblock Playing hard exploration games by watching {YouTube}.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems}, pages\\n  2930--2941, 2018.\\n\\n\\\\bibitem[Bellemare et~al.(2012)Bellemare, Veness, and\\n  Bowling]{bellemare2012investigating}\\nMarc~G Bellemare, Joel Veness, and Michael Bowling.\\n\\\\newblock Investigating contingency awareness using {Atari} 2600 games.\\n\\\\newblock In \\\\emph{AAAI Conference on Artificial Intelligence}, pages 864--871,\\n  2012.\\n\\n\\\\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and\\n  {Bowling}]{bellemare13arcade}\\nMarc~G {Bellemare}, Yavar {Naddaf}, Joel {Veness}, and Michael {Bowling}.\\n\\\\newblock The arcade learning environment: An evaluation platform for general\\n  agents.\\n\\\\newblock \\\\emph{Journal of Artificial Intelligence Research}, 47:\\\\penalty0\\n  253--279, 2013.\\n\\n\\\\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,\\n  Saxton, and Munos]{bellemare2016unifying}\\nMarc~G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,\\n  and Remi Munos.\\n\\\\newblock Unifying count-based exploration and intrinsic motivation.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems}, pages\\n  1471--1479, 2016.\\n\\n\\\\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and\\n  Weston]{bengio2009curriculum}\\nYoshua Bengio, J{\\\\\\'e}r{\\\\^o}me Louradour, Ronan Collobert, and Jason Weston.\\n\\\\newblock Curriculum learning.\\n\\\\newblock In \\\\emph{International Conference on Machine Learning}, pages 41--48,\\n  2009.\\n\\n\\\\bibitem[Chentanez et~al.(2005)Chentanez, Barto, and\\n  Singh]{chentanez2005intrinsically}\\nNuttapong Chentanez, Andrew~G Barto, and Satinder~P Singh.\\n\\\\newblock Intrinsically motivated reinforcement learning.\\n\\\\newblock In \\\\emph{Advances in neural information processing systems}, pages\\n  1281--1288, 2005.\\n\\n\\\\bibitem[Ecoffet et~al.(2019)Ecoffet, Huizinga, Lehman, Stanley, and\\n  Clune]{ecoffet2019go}\\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth~O Stanley, and Jeff Clune.\\n\\\\newblock Go-explore: a new approach for hard-exploration problems.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1901.10995}, 2019.\\n\\n\\\\bibitem[Fujimoto et~al.(2018)Fujimoto, Meger, and Precup]{fujimoto2018off}\\nScott Fujimoto, David Meger, and Doina Precup.\\n\\\\newblock Off-policy deep reinforcement learning without exploration.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1812.02900}, 2018.\\n\\n\\\\bibitem[Ghosh et~al.(2017)Ghosh, Singh, Rajeswaran, Kumar, and\\n  Levine]{ghosh2017divide}\\nDibya Ghosh, Avi Singh, Aravind Rajeswaran, Vikash Kumar, and Sergey Levine.\\n\\\\newblock Divide-and-conquer reinforcement learning.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1711.09874}, 2017.\\n\\n\\\\bibitem[Graves et~al.(2017)Graves, Bellemare, Menick, Munos, and\\n  Kavukcuoglu]{graves2017automated}\\nAlex Graves, Marc~G Bellemare, Jacob Menick, Remi Munos, and Koray Kavukcuoglu.\\n\\\\newblock Automated curriculum learning for neural networks.\\n\\\\newblock In \\\\emph{International Conference on Machine Learning}, pages\\n  1311--1320, 2017.\\n\\n\\\\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,\\n  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}\\nMatteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,\\n  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.\\n\\\\newblock Rainbow: Combining improvements in deep reinforcement learning.\\n\\\\newblock In \\\\emph{AAAI Conference on Artificial Intelligence}, pages\\n  3215--3222, 2018.\\n\\n\\\\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,\\n  Horgan, Quan, Sendonaris, Osband, Agapiou, Leibo, and\\n  Gruslys]{hester2018deep}\\nTodd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal\\n  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, John Agapiou,\\n  Joel~Z. Leibo, and Audrunas Gruslys.\\n\\\\newblock Deep {Q}-learning from demonstrations.\\n\\\\newblock In \\\\emph{AAAI Conference on Artificial Intelligence}, pages\\n  3223--3230, 2018.\\n\\n\\\\bibitem[Ho and Ermon(2016)]{ho2016generative}\\nJonathan Ho and Stefano Ermon.\\n\\\\newblock Generative adversarial imitation learning.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems}, pages\\n  4565--4573, 2016.\\n\\n\\\\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth{-}Maron, Hessel, van\\n  Hasselt, and Silver]{horgan2018distributed}\\nDan Horgan, John Quan, David Budden, Gabriel Barth{-}Maron, Matteo Hessel, Hado\\n  van Hasselt, and David Silver.\\n\\\\newblock Distributed prioritized experience replay.\\n\\\\newblock In \\\\emph{International Conference on Learning Representations}, 2018.\\n\\n\\\\bibitem[Hunter(2007)]{hunter2007matplotlib}\\nJohn~D Hunter.\\n\\\\newblock Matplotlib: A {2D} graphics environment.\\n\\\\newblock \\\\emph{Computing in science \\\\& engineering}, 9\\\\penalty0 (3):\\\\penalty0\\n  90--95, 2007.\\n\\n\\\\bibitem[Jaderberg et~al.(2016)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,\\n  Silver, and Kavukcuoglu]{jaderberg2016reinforcement}\\nMax Jaderberg, Volodymyr Mnih, Wojciech~Marian Czarnecki, Tom Schaul, Joel~Z\\n  Leibo, David Silver, and Koray Kavukcuoglu.\\n\\\\newblock Reinforcement learning with unsupervised auxiliary tasks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1611.05397}, 2016.\\n\\n\\\\bibitem[Juliani(2018)]{juliani19solving}\\nArthur Juliani.\\n\\\\newblock On ``solving\\'\\' {M}ontezumas revenge.\\n\\\\newblock\\n  \\\\url{https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3},\\n  2018.\\n\\\\newblock Accessed: 2019-19-21.\\n\\n\\\\bibitem[Juliani et~al.(2019)Juliani, Khalifa, Berges, Harper, Henry, Crespi,\\n  Togelius, and Lange]{juliani2019obstacle}\\nArthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Hunter\\n  Henry, Adam Crespi, Julian Togelius, and Danny Lange.\\n\\\\newblock Obstacle tower: A generalization challenge in vision, control, and\\n  planning.\\n\\\\newblock In \\\\emph{AAAI-19 Workshop on Games and Simulations for Artificial\\n  Intelligence}, 2019.\\n\\n\\\\bibitem[Kapturowski et~al.(2018)Kapturowski, Ostrovski, Quan, Munos, and\\n  Dabney]{kapturowski2018recurrent}\\nSteven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney.\\n\\\\newblock Recurrent experience replay in distributed reinforcement learning.\\n\\\\newblock In \\\\emph{International Conference on Learning Representations}, 2018.\\n\\n\\\\bibitem[Kim et~al.(2013)Kim, Farahmand, Pineau, and Precup]{kim2013learning}\\nBeomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, and Doina Precup.\\n\\\\newblock Learning from limited demonstrations.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems}, pages\\n  2859--2867, 2013.\\n\\n\\\\bibitem[Kingma and Ba(2014)]{kingma2014adam}\\nDiederik~P Kingma and Jimmy Ba.\\n\\\\newblock Adam: A method for stochastic optimization.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1412.6980}, 2014.\\n\\n\\\\bibitem[Langlois et~al.(2019)Langlois, Zhang, Zhang, Abbeel, and\\n  Ba]{langloisbenchmarking}\\nEric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba.\\n\\\\newblock Benchmarking model-based reinforcement learning.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1907.02057}, 2019.\\n\\n\\\\bibitem[Machado et~al.(2018)Machado, Bellemare, Talvitie, Veness, Hausknecht,\\n  and Bowling]{machado2018revisiting}\\nMarlos~C Machado, Marc~G Bellemare, Erik Talvitie, Joel Veness, Matthew\\n  Hausknecht, and Michael Bowling.\\n\\\\newblock Revisiting the arcade learning environment: Evaluation protocols and\\n  open problems for general agents.\\n\\\\newblock \\\\emph{Journal of Artificial Intelligence Research}, 61:\\\\penalty0\\n  523--562, 2018.\\n\\n\\\\bibitem[McKinney et~al.(2010)]{mckinney2010data}\\nWes McKinney et~al.\\n\\\\newblock Data structures for statistical computing in python.\\n\\\\newblock In \\\\emph{Proceedings of the 9th Python in Science Conference}, pages\\n  51--56, 2010.\\n\\n\\\\bibitem[Merel et~al.(2017)Merel, Tassa, Srinivasan, Lemmon, Wang, Wayne, and\\n  Heess]{merel2017learning}\\nJosh Merel, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne,\\n  and Nicolas Heess.\\n\\\\newblock Learning human behaviors from motion capture by adversarial\\n  imitation.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1707.02201}, 2017.\\n\\n\\\\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,\\n  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}\\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,\\n  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg\\n  Ostrovski, et~al.\\n\\\\newblock Human-level control through deep reinforcement learning.\\n\\\\newblock \\\\emph{Nature}, 518\\\\penalty0 (7540):\\\\penalty0 529--533, 2015.\\n\\n\\\\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and\\n  Abbeel]{nair2018overcoming}\\nAshvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter\\n  Abbeel.\\n\\\\newblock Overcoming exploration in reinforcement learning with demonstrations.\\n\\\\newblock In \\\\emph{IEEE International Conference on Robotics and Automation},\\n  pages 6292--6299, 2018.\\n\\n\\\\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}\\nAndrew~Y Ng, Daishi Harada, and Stuart Russell.\\n\\\\newblock Policy invariance under reward transformations: Theory and\\n  application to reward shaping.\\n\\\\newblock In \\\\emph{International Conference on Machine Learning}, pages\\n  278--287, 1999.\\n\\n\\\\bibitem[Oliphant(2006)]{numpy2006}\\nTravis Oliphant.\\n\\\\newblock \\\\emph{Guide to NumPy}.\\n\\\\newblock USA: Trelgol Publishing, 2006.\\n\\n\\\\bibitem[Osband et~al.(2018)Osband, Aslanides, and\\n  Cassirer]{osband2018randomized}\\nIan Osband, John Aslanides, and Albin Cassirer.\\n\\\\newblock Randomized prior functions for deep reinforcement learning.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems}, pages\\n  8617--8629, 2018.\\n\\n\\\\bibitem[Paine et~al.(2018)Paine, Colmenarejo, Wang, Reed, Aytar, Pfaff,\\n  Hoffman, Barth-Maron, Cabi, Budden, et~al.]{paine2018one}\\nTom~Le Paine, Sergio~G{\\\\\\'o}mez Colmenarejo, Ziyu Wang, Scott Reed, Yusuf Aytar,\\n  Tobias Pfaff, Matt~W Hoffman, Gabriel Barth-Maron, Serkan Cabi, David Budden,\\n  et~al.\\n\\\\newblock One-shot high-fidelity imitation: Training large-scale deep nets with\\n  {RL}.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1810.05017}, 2018.\\n\\n\\\\bibitem[Peng et~al.(2018)Peng, Abbeel, Levine, and van~de\\n  Panne]{peng2018deepmimic}\\nXue~Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van~de Panne.\\n\\\\newblock Deepmimic: Example-guided deep reinforcement learning of\\n  physics-based character skills.\\n\\\\newblock \\\\emph{ACM Transactions on Graphics}, 37\\\\penalty0 (4):\\\\penalty0 1:14,\\n  2018.\\n\\n\\\\bibitem[Pohlen et~al.(2018)Pohlen, Piot, Hester, Azar, Horgan, Budden,\\n  Barth-Maron, van Hasselt, Quan, Ve{\\\\v{c}}er{\\\\\\'\\\\i}k,\\n  et~al.]{pohlen2018observe}\\nTobias Pohlen, Bilal Piot, Todd Hester, Mohammad~Gheshlaghi Azar, Dan Horgan,\\n  David Budden, Gabriel Barth-Maron, Hado van Hasselt, John Quan, Mel\\n  Ve{\\\\v{c}}er{\\\\\\'\\\\i}k, et~al.\\n\\\\newblock Observe and look further: Achieving consistent performance on atari.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1805.11593}, 2018.\\n\\n\\\\bibitem[Pomerleau(1989)]{pomerleau1989alvinn}\\nDean~A Pomerleau.\\n\\\\newblock Alvinn: An autonomous land vehicle in a neural network.\\n\\\\newblock In \\\\emph{Advances in neural information processing systems}, pages\\n  305--313, 1989.\\n\\n\\\\bibitem[Rahmatizadeh et~al.(2018)Rahmatizadeh, Abolghasemi, B{\\\\\"o}l{\\\\\"o}ni,\\n  and Levine]{rahmatizadeh2018vision}\\nRouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau B{\\\\\"o}l{\\\\\"o}ni, and Sergey\\n  Levine.\\n\\\\newblock Vision-based multi-task manipulation for inexpensive robots using\\n  end-to-end learning from demonstration.\\n\\\\newblock In \\\\emph{IEEE International Conference on Robotics and Automation},\\n  pages 3758--3765, 2018.\\n\\n\\\\bibitem[Salimans and Chen(2018{\\\\natexlab{a}})]{opeani18learning}\\nTim Salimans and Richard Chen.\\n\\\\newblock Learning {M}ontezuma\\'s revenge from a single demonstration.\\n\\\\newblock\\n  \\\\url{https://openai.com/blog/learning-montezumas-revenge-from-a-single-demonstration},\\n  2018{\\\\natexlab{a}}.\\n\\\\newblock Accessed: 2019-19-22.\\n\\n\\\\bibitem[Salimans and Chen(2018{\\\\natexlab{b}})]{salimans2018learning}\\nTim Salimans and Richard Chen.\\n\\\\newblock Learning {M}ontezuma\\'s revenge from a single demonstration.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1812.03381}, 2018{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and\\n  Silver]{schaul2016prioritized}\\nTom Schaul, John Quan, Ioannis Antonoglou, and David Silver.\\n\\\\newblock Prioritized experience replay.\\n\\\\newblock In \\\\emph{International Conference on Learning Representations}, 2016.\\n\\n\\\\bibitem[Schmidhuber(1991)]{schmidhuber1991curious}\\nJ{\\\\\"u}rgen Schmidhuber.\\n\\\\newblock Curious model-building control systems.\\n\\\\newblock In \\\\emph{IEEE International Joint Conference on Neural Networks},\\n  pages 1458--1463, 1991.\\n\\n\\\\bibitem[Ta{\\\\\"\\\\i}ga et~al.(2019)Ta{\\\\\"\\\\i}ga, Fedus, Machado, Courville, and\\n  Bellemare]{taiga2019benchmarking}\\nAdrien~Ali Ta{\\\\\"\\\\i}ga, William Fedus, Marlos~C Machado, Aaron Courville, and\\n  Marc~G Bellemare.\\n\\\\newblock Benchmarking bonus-based exploration methods on the arcade learning\\n  environment.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1908.02388}, 2019.\\n\\n\\\\bibitem[Ve{\\\\v{c}}er{\\\\\\'\\\\i}k et~al.(2017)Ve{\\\\v{c}}er{\\\\\\'\\\\i}k, Hester, Scholz,\\n  Wang, Pietquin, Piot, Heess, Roth{\\\\\"o}rl, Lampe, and\\n  Riedmiller]{vevcerik2017leveraging}\\nMatej Ve{\\\\v{c}}er{\\\\\\'\\\\i}k, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier\\n  Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth{\\\\\"o}rl, Thomas Lampe, and\\n  Martin Riedmiller.\\n\\\\newblock Leveraging demonstrations for deep reinforcement learning on robotics\\n  problems with sparse rewards.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1707.08817}, 2017.\\n\\n\\\\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, Hasselt, Lanctot, and\\n  Freitas]{wang2015dueling}\\nZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando\\n  Freitas.\\n\\\\newblock Dueling network architectures for deep reinforcement learning.\\n\\\\newblock In \\\\emph{International Conference on Machine Learning}, pages\\n  1995--2003, 2016.\\n\\n\\\\bibitem[Waskom et~al.(2017)Waskom, Botvinnik, O\\'Kane, Hobson, Lukauskas,\\n  Gemperline, Augspurger, Halchenko, Cole, Warmenhoven, de~Ruiter, Pye, Hoyer,\\n  Vanderplas, Villalba, Kunter, Quintero, Bachant, Martin, Meyer, Miles, Ram,\\n  Yarkoni, Williams, Evans, Fitzgerald, Brian, Fonnesbeck, Lee, and\\n  Qalieh]{michael_waskom_2017_883859}\\nMichael Waskom, Olga Botvinnik, Drew O\\'Kane, Paul Hobson, Saulius Lukauskas,\\n  David~C Gemperline, Tom Augspurger, Yaroslav Halchenko, John~B. Cole, Jordi\\n  Warmenhoven, Julian de~Ruiter, Cameron Pye, Stephan Hoyer, Jake Vanderplas,\\n  Santi Villalba, Gero Kunter, Eric Quintero, Pete Bachant, Marcel Martin, Kyle\\n  Meyer, Alistair Miles, Yoav Ram, Tal Yarkoni, Mike~Lee Williams, Constantine\\n  Evans, Clark Fitzgerald, Brian, Chris Fonnesbeck, Antony Lee, and Adel\\n  Qalieh.\\n\\\\newblock mwaskom/seaborn: v0.8.1 (september 2017), September 2017.\\n\\\\newblock URL \\\\url{https://doi.org/10.5281/zenodo.883859}.\\n\\n\\\\bibitem[{\\\\.Z}o{\\\\l}na et~al.(2019){\\\\.Z}o{\\\\l}na, Rostamzadeh, Bengio, Ahn, and\\n  Pinheiro]{zolna2019reinforced}\\nKonrad {\\\\.Z}o{\\\\l}na, Negar Rostamzadeh, Yoshua Bengio, Sungjin Ahn, and Pedro~O\\n  Pinheiro.\\n\\\\newblock Reinforced imitation in heterogeneous action space.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1904.03438}, 2019.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '@article{mnih2015human,\\n  title={Human-level control through deep reinforcement learning},\\n  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},\\n  journal={Nature},\\n  volume={518},\\n  number={7540},\\n  pages={529--533},\\n  year={2015}\\n}\\n\\n@Article{bellemare13arcade,\\n  author = {{Bellemare}, Marc G and {Naddaf}, Yavar and {Veness}, Joel and {Bowling}, Michael},\\n  title = {The Arcade Learning Environment: An Evaluation Platform for General Agents},\\n  journal = {Journal of Artificial Intelligence Research},\\n  year = \"2013\",\\n  volume = \"47\",\\n  pages = \"253--279\",\\n}\\n\\n@inproceedings{bellemare2016unifying,\\n  title={Unifying count-based exploration and intrinsic motivation},\\n  author={Bellemare, Marc G and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={1471--1479},\\n  year={2016}\\n}\\n\\n@inproceedings{horgan2018distributed,\\n  author    = {Dan Horgan and\\n               John Quan and\\n               David Budden and\\n               Gabriel Barth{-}Maron and\\n               Matteo Hessel and\\n               Hado van Hasselt and\\n               David Silver},\\n  title     = {Distributed Prioritized Experience Replay},\\n  booktitle   = {International Conference on Learning Representations},\\n  year      = {2018}\\n}\\n\\n@article{pohlen2018observe,\\n  title={Observe and look further: Achieving consistent performance on atari},\\n  author={Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and van Hasselt, Hado and Quan, John and Ve{\\\\v{c}}er{\\\\\\'\\\\i}k, Mel and others},\\n  journal={arXiv preprint arXiv:1805.11593},\\n  year={2018}\\n}\\n\\n@inproceedings{kapturowski2018recurrent,\\n  title={Recurrent experience replay in distributed reinforcement learning},\\n  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},\\n  booktitle   = {International Conference on Learning Representations},\\n  year={2018}\\n}\\n\\n@inproceedings{hester2018deep,\\n  title={Deep {Q}-learning from demonstrations},\\n  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and  Agapiou, John and Leibo, Joel Z.  and Gruslys, Audrunas },\\n  booktitle={AAAI Conference on Artificial Intelligence},\\n  pages     = {3223--3230},\\n  year={2018}\\n}\\n\\n@article{jaderberg2018human,\\n  title={Human-level performance in first-person multiplayer games with population-based deep reinforcement learning},\\n  author={Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others},\\n  journal={arXiv preprint arXiv:1807.01281},\\n  year={2018}\\n}\\n\\n@inproceedings{juliani2019obstacle,\\n  title={Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning},\\n  author={Juliani, Arthur and Khalifa, Ahmed and Berges, Vincent-Pierre and Harper, Jonathan and Henry, Hunter and Crespi, Adam and Togelius, Julian and Lange, Danny},\\n  booktitle={AAAI-19 Workshop on Games and Simulations for Artificial Intelligence},\\n  year={2019}\\n}\\n\\n@inproceedings{wang2015dueling,\\n  title = \\t {Dueling Network Architectures for Deep Reinforcement Learning},\\n  author = \\t {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado Hasselt and Marc Lanctot and Nando Freitas},\\n  booktitle = \\t {International Conference on Machine Learning},\\n  pages = \\t {1995--2003},\\n  year = \\t {2016}\\n}\\n\\n\\n@inproceedings{pomerleau1989alvinn,\\n  title={Alvinn: An autonomous land vehicle in a neural network},\\n  author={Pomerleau, Dean A},\\n  booktitle={Advances in neural information processing systems},\\n  pages={305--313},\\n  year={1989}\\n}\\n\\n@inproceedings{rahmatizadeh2018vision,\\n  title={Vision-based multi-task manipulation for inexpensive robots using end-to-end learning from demonstration},\\n  author={Rahmatizadeh, Rouhollah and Abolghasemi, Pooya and B{\\\\\"o}l{\\\\\"o}ni, Ladislau and Levine, Sergey},\\n  booktitle={IEEE International Conference on Robotics and Automation},\\n  pages={3758--3765},\\n  year={2018}\\n}\\n\\n@article{machado2018revisiting,\\n  title={Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents},\\n  author={Machado, Marlos C and Bellemare, Marc G and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},\\n  journal={Journal of Artificial Intelligence Research},\\n  volume={61},\\n  pages={523--562},\\n  year={2018}\\n}\\n\\n@inproceedings{piot2014boosted,\\n author = {Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},\\n title = {Boosted and Reward-regularized Classification for Apprenticeship Learning},\\n booktitle = {International Conference on Autonomous Agents and Multi-agent Systems},\\n year = {2014},\\n pages = {1249--1256}\\n}\\n\\n@inproceedings{tensorflow2016,\\ntitle\\t= {TensorFlow: A system for large-scale machine learning},\\nauthor\\t= {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},\\nyear\\t= {2016},\\nbooktitle\\t= {12th USENIX Symposium on Operating Systems Design and Implementation},\\npages\\t= {265--283}\\n}\\n\\n@book{numpy2006,\\nauthor = {Oliphant, Travis},\\nyear = {2006},\\npublisher = {USA: Trelgol Publishing},\\ntitle = {Guide to NumPy}\\n}\\n\\n@inproceedings{mckinney2010data,\\n  title={Data structures for statistical computing in python},\\n  author={McKinney, Wes and others},\\n  booktitle={Proceedings of the 9th Python in Science Conference},\\n  pages={51--56},\\n  year={2010}\\n}\\n\\n@article{hunter2007matplotlib,\\n  title={Matplotlib: A {2D} graphics environment},\\n  author={Hunter, John D},\\n  journal={Computing in science \\\\& engineering},\\n  volume={9},\\n  number={3},\\n  pages={90--95},\\n  year={2007}\\n}\\n\\n@article{zolna2019reinforced,\\n  title={Reinforced Imitation in Heterogeneous Action Space},\\n  author={{\\\\.Z}o{\\\\l}na, Konrad and Rostamzadeh, Negar and Bengio, Yoshua and Ahn, Sungjin and Pinheiro, Pedro O},\\n  journal={arXiv preprint arXiv:1904.03438},\\n  year={2019}\\n}\\n\\n@inproceedings{ho2016generative,\\n  title={Generative adversarial imitation learning},\\n  author={Ho, Jonathan and Ermon, Stefano},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={4565--4573},\\n  year={2016}\\n}\\n\\n@inproceedings{hessel2018rainbow,\\n  title={Rainbow: Combining improvements in deep reinforcement learning},\\n  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},\\n  booktitle={AAAI Conference on Artificial Intelligence},\\n  pages     = {3215--3222},\\n  year={2018}\\n}\\n\\n@inproceedings{ng1999policy,\\n  title={Policy invariance under reward transformations: Theory and application to reward shaping},\\n  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},\\n  booktitle={International Conference on Machine Learning},\\n  pages={278--287},\\n  year={1999}\\n}\\n\\n@inproceedings{bengio2009curriculum,\\n  title={Curriculum learning},\\n  author={Bengio, Yoshua and Louradour, J{\\\\\\'e}r{\\\\^o}me and Collobert, Ronan and Weston, Jason},\\n  booktitle={International Conference on Machine Learning},\\n  pages={41--48},\\n  year={2009}\\n}\\n\\n@inproceedings{graves2017automated,\\n  title={Automated curriculum learning for neural networks},\\n  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},\\n  booktitle={International Conference on Machine Learning},\\n  pages={1311--1320},\\n  year={2017}\\n}\\n\\n@inproceedings{piot2014boosted,\\n  title={Boosted {Bellman} residual minimization handling expert demonstrations},\\n  author={Piot, Bilal and Geist, Matthieu and Pietquin, Olivier},\\n  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},\\n  pages={549--564},\\n  year={2014}\\n}\\n\\n@inproceedings{brys2015reinforcement,\\n  title={Reinforcement learning from demonstration through shaping},\\n  author={Brys, Tim and Harutyunyan, Anna and Suay, Halit Bener and Chernova, Sonia and Taylor, Matthew E and Now{\\\\\\'e}, Ann},\\n  booktitle={International Joint Conference on Artificial Intelligence},\\n  year={2015}\\n}\\n\\n@inproceedings{kim2013learning,\\n  title={Learning from limited demonstrations},\\n  author={Kim, Beomjoon and Farahmand, Amir-massoud and Pineau, Joelle and Precup, Doina},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={2859--2867},\\n  year={2013}\\n}\\n\\n@inproceedings{brys2015reinforcement,\\n  title={Reinforcement learning from demonstration through shaping},\\n  author={Brys, Tim and Harutyunyan, Anna and Suay, Halit Bener and Chernova, Sonia and Taylor, Matthew E and Now{\\\\\\'e}, Ann},\\n  booktitle={International Joint Conference on Artificial Intelligence},\\n  year={2015}\\n}\\n\\n@article{florensa2017reverse,\\n  title={Reverse curriculum generation for reinforcement learning},\\n  author={Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},\\n  journal={arXiv preprint arXiv:1707.05300},\\n  year={2017}\\n}\\n\\n@inproceedings{karpathy2012curriculum,\\n  title={Curriculum learning for motor skills},\\n  author={Karpathy, Andrej and Van De Panne, Michiel},\\n  booktitle={Canadian Conference on Artificial Intelligence},\\n  pages={325--330},\\n  year={2012}\\n}\\n\\n\\n@article{schmidhuber2013powerplay,\\n  title={Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem},\\n  author={Schmidhuber, J{\\\\\"u}rgen},\\n  journal={Frontiers in psychology},\\n  volume={4},\\n  pages={313},\\n  year={2013}\\n}\\n\\n@inproceedings{srivastava2012continually,\\n  title={Continually adding self-invented problems to the repertoire: first experiments with powerplay},\\n  author={Srivastava, Rupesh Kumar and Steunebrink, Bas R and Stollenga, Marijn and Schmidhuber, J{\\\\\"u}rgen},\\n  booktitle={IEEE International Conference on Development and Learning and Epigenetic Robotics},\\n  pages={1--6},\\n  year={2012}\\n}\\n\\n@inproceedings{schmidhuber1991curious,\\n  title={Curious model-building control systems},\\n  author={Schmidhuber, J{\\\\\"u}rgen},\\n  booktitle={IEEE International Joint Conference on Neural Networks},\\n  pages={1458--1463},\\n  year={1991}\\n}\\n\\n@inproceedings{chentanez2005intrinsically,\\n  title={Intrinsically motivated reinforcement learning},\\n  author={Chentanez, Nuttapong and Barto, Andrew G and Singh, Satinder P},\\n  booktitle={Advances in neural information processing systems},\\n  pages={1281--1288},\\n  year={2005}\\n}\\n\\n@misc{juliani19solving,\\n  title = {On ``solving\\'\\' {M}ontezumas Revenge},\\n  author = {Arthur Juliani},\\n  howpublished = {\\\\url{https://medium.com/@awjuliani/on-solving-montezumas-revenge-2146d83f0bc3}},\\n  note = {Accessed: 2019-19-21},\\n  year = {2018}\\n}\\n\\n@article{werbos1990backpropagation,\\n  title={Backpropagation through time: what it does and how to do it},\\n  author={Werbos, Paul J},\\n  journal={Proceedings of the IEEE},\\n  volume={78},\\n  number={10},\\n  pages={1550--1560},\\n  year={1990}\\n}\\n\\n@inproceedings{bellemare2012investigating,\\n\\tauthor = {Marc G Bellemare and Joel Veness and Michael Bowling},\\n\\ttitle = {Investigating Contingency Awareness Using {Atari} 2600 Games},\\n\\tbooktitle = {AAAI Conference on Artificial Intelligence},\\n\\tpages = {864--871},\\n\\tyear = {2012}\\n}\\n\\n@inproceedings{aytar2018playing,\\n  title={Playing hard exploration games by watching {YouTube}},\\n  author={Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Thomas and Wang, Ziyu and de Freitas, Nando},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={2930--2941},\\n  year={2018}\\n}\\n\\n@article{merel2017learning,\\n  title={Learning human behaviors from motion capture by adversarial imitation},\\n  author={Merel, Josh and Tassa, Yuval and Srinivasan, Sriram and Lemmon, Jay and Wang, Ziyu and Wayne, Greg and Heess, Nicolas},\\n  journal={arXiv preprint arXiv:1707.02201},\\n  year={2017}\\n}\\n\\n@article{peng2018deepmimic ,\\n  title={Deepmimic: Example-guided deep reinforcement learning of physics-based character skills},\\n  author={Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},\\n  journal={ACM Transactions on Graphics},\\n  volume={37},\\n  number={4},\\n  pages={1:14},\\n  year={2018}\\n}\\n\\n@article{vevcerik2017leveraging,\\n  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},\\n  author={Ve{\\\\v{c}}er{\\\\\\'\\\\i}k, Matej and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\\\\\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},\\n  journal={arXiv preprint arXiv:1707.08817},\\n  year={2017}\\n}\\n\\n@article{salimans2018learning,\\n  title={Learning {M}ontezuma\\'s Revenge from a Single Demonstration},\\n  author={Salimans, Tim and Chen, Richard},\\n  journal={arXiv preprint arXiv:1812.03381},\\n  year={2018}\\n}\\n\\n@inproceedings{nair2018overcoming,\\n  title={Overcoming exploration in reinforcement learning with demonstrations},\\n  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},\\n  booktitle={IEEE International Conference on Robotics and Automation},\\n  pages={6292--6299},\\n  year={2018}\\n}\\n\\n@article{kingma2014adam,\\n  title={Adam: A method for stochastic optimization},\\n  author={Kingma, Diederik P and Ba, Jimmy},\\n  journal={arXiv preprint arXiv:1412.6980},\\n  year={2014}\\n}\\n\\n\\n@article{ecoffet2019go,\\n  title={Go-Explore: a New Approach for Hard-Exploration Problems},\\n  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},\\n  journal={arXiv preprint arXiv:1901.10995},\\n  year={2019}\\n}\\n\\n\\n@misc{opeani18learning,\\n  title = {Learning {M}ontezuma\\'s Revenge from a Single Demonstration},\\n  author = {Salimans, Tim and  Chen, Richard},\\n  howpublished = {\\\\url{https://openai.com/blog/learning-montezumas-revenge-from-a-single-demonstration}},\\n  note = {Accessed: 2019-19-22},\\n  year = {2018}\\n}\\n\\n\\n@inproceedings{osband2018randomized,\\n  title={Randomized prior functions for deep reinforcement learning},\\n  author={Osband, Ian and Aslanides, John and Cassirer, Albin},\\n  booktitle={Advances in Neural Information Processing Systems},\\n  pages={8617--8629},\\n  year={2018}\\n}\\n\\n\\n@inproceedings{schaul2016prioritized,\\n  title={Prioritized experience replay},\\n  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},\\n  booktitle={International Conference on Learning Representations},\\n  year={2016}\\n}\\n\\n@misc{michael_waskom_2017_883859,\\n  author       = {Michael Waskom and\\n                  Olga Botvinnik and\\n                  Drew O\\'Kane and\\n                  Paul Hobson and\\n                  Saulius Lukauskas and\\n                  David C Gemperline and\\n                  Tom Augspurger and\\n                  Yaroslav Halchenko and\\n                  John B. Cole and\\n                  Jordi Warmenhoven and\\n                  Julian de Ruiter and\\n                  Cameron Pye and\\n                  Stephan Hoyer and\\n                  Jake Vanderplas and\\n                  Santi Villalba and\\n                  Gero Kunter and\\n                  Eric Quintero and\\n                  Pete Bachant and\\n                  Marcel Martin and\\n                  Kyle Meyer and\\n                  Alistair Miles and\\n                  Yoav Ram and\\n                  Tal Yarkoni and\\n                  Mike Lee Williams and\\n                  Constantine Evans and\\n                  Clark Fitzgerald and\\n                  Brian and\\n                  Chris Fonnesbeck and\\n                  Antony Lee and\\n                  Adel Qalieh},\\n  title        = {mwaskom/seaborn: v0.8.1 (September 2017)},\\n  month        = sep,\\n  year         = 2017,\\n  doi          = {10.5281/zenodo.883859},\\n  url          = {https://doi.org/10.5281/zenodo.883859}\\n}\\n\\n@article{ghosh2017divide,\\n  title={Divide-and-conquer reinforcement learning},\\n  author={Ghosh, Dibya and Singh, Avi and Rajeswaran, Aravind and Kumar, Vikash and Levine, Sergey},\\n  journal={arXiv preprint arXiv:1711.09874},\\n  year={2017}\\n}\\n\\n@article{langloisbenchmarking,\\n  title={Benchmarking Model-Based Reinforcement Learning},\\n  author={Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},\\n  journal={arXiv preprint arXiv:1907.02057},\\n  year={2019}\\n}\\n\\n@article{taiga2019benchmarking,\\n  title={Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment},\\n  author={Ta{\\\\\"\\\\i}ga, Adrien Ali and Fedus, William and Machado, Marlos C and Courville, Aaron and Bellemare, Marc G},\\n  journal={arXiv preprint arXiv:1908.02388},\\n  year={2019}\\n}\\n\\n@article{fujimoto2018off,\\n  title={Off-policy deep reinforcement learning without exploration},\\n  author={Fujimoto, Scott and Meger, David and Precup, Doina},\\n  journal={arXiv preprint arXiv:1812.02900},\\n  year={2018}\\n}\\n\\n@article{jaderberg2016reinforcement,\\n  title={Reinforcement learning with unsupervised auxiliary tasks},\\n  author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},\\n  journal={arXiv preprint arXiv:1611.05397},\\n  year={2016}\\n}\\n\\n@article{paine2018one,\\n  title={One-shot high-fidelity imitation: Training large-scale deep nets with {RL}},\\n  author={Paine, Tom Le and Colmenarejo, Sergio G{\\\\\\'o}mez and Wang, Ziyu and Reed, Scott and Aytar, Yusuf and Pfaff, Tobias and Hoffman, Matt W and Barth-Maron, Gabriel and Cabi, Serkan and Budden, David and others},\\n  journal={arXiv preprint arXiv:1810.05017},\\n  year={2018}\\n}',\n",
       "  'arxiv_citations': {'1901.10995': True,\n",
       "   '1812.02900': True,\n",
       "   '1711.09874': True,\n",
       "   '1611.05397': True,\n",
       "   '1412.6980': True,\n",
       "   '1907.02057': True,\n",
       "   '1707.02201': True,\n",
       "   '1810.05017': True,\n",
       "   '1805.11593': True,\n",
       "   '1812.03381': True,\n",
       "   '1908.02388': True,\n",
       "   '1707.08817': True,\n",
       "   '1904.03438': True,\n",
       "   '1807.01281': True,\n",
       "   '1707.05300': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Exploration',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #65',\n",
       "   'newsletter_url': 'https://mailchi.mp/3d4e6c2c206f/an-65learning-useful-skills-by-watching-humans-play',\n",
       "   'summarizer': 'Cody',\n",
       "   'summary': 'This paper combines ideas from existing techniques to construct an architecture (R2D3) capable of learning to solve hard exploration problems with a small number (N~100) of demonstrations. R2D3 has two primary architectural features: its use of a recurrent head to learn Q values, and its strategy of sampling trajectories from separate pools of agent and demonstrator experience, with sampling prioritized by highest-temporal-difference-error transitions within each pool. \\n\\nAs the authors note, this approach is essentially an extension of an earlier paper, [Deep Q-Learning from Demonstrations](https://arxiv.org/abs/1704.03732), to use a recurrent head rather than a feed-forward one, allowing it to be more effectively deployed on partial-information environments. The authors test on 8 different environments that require long sequences of task completion to receive any reward, and find that their approach is able to reach human level performance on four of the tasks, while their baseline comparisons essentially never succeed on any task. Leveraging demonstrations can be valuable for solving these kinds of difficult exploration tasks, because demonstrator trajectories provide examples of how to achieve reward in a setting where the trajectories of a randomly exploring agent would rarely ever reach the end of the task to find positive reward. ',\n",
       "   'opinion': \"For all that this paper's technique is a fairly straightforward merging of existing techniques (separately-prioritized demonstration and agent pools, and the off-policy SotA R2D2), its results are surprisingly impressive: the tasks tested on require long and complex chains of correct actions that would be challenging for a non-imitation based system to discover, and high levels of environment stochasticity that make a pure imitation approach difficult.\",\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': None,\n",
       "   'arxiv_id': 'None',\n",
       "   'title': 'Making Efficient Use of Demonstrations to Solve Hard Exploration Problems',\n",
       "   'authors': 'Caglar Gulcehre*, Tom Le Paine*, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, Worlds Team',\n",
       "   'date_published': 2019.0,\n",
       "   'data_last_modified': '',\n",
       "   'url': 'https://deepmind.com/research/publications/Making-Efficient-Use-of-Demonstrations-to-Solve-Hard-Exploration-Problems',\n",
       "   'abstract': '',\n",
       "   'author_comment': '',\n",
       "   'journal_ref': '',\n",
       "   'doi': '',\n",
       "   'primary_category': '',\n",
       "   'categories': '',\n",
       "   'individual_summary': \"Title: Making Efficient Use of Demonstrations to Solve Hard Exploration Problems\\nAuthors: Caglar Gulcehre*, Tom Le Paine*, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de Freitas, Worlds Team\\nSummary: This paper combines ideas from existing techniques to construct an architecture (R2D3) capable of learning to solve hard exploration problems with a small number (N~100) of demonstrations. R2D3 has two primary architectural features: its use of a recurrent head to learn Q values, and its strategy of sampling trajectories from separate pools of agent and demonstrator experience, with sampling prioritized by highest-temporal-difference-error transitions within each pool. \\n\\nAs the authors note, this approach is essentially an extension of an earlier paper, [Deep Q-Learning from Demonstrations](https://arxiv.org/abs/1704.03732), to use a recurrent head rather than a feed-forward one, allowing it to be more effectively deployed on partial-information environments. The authors test on 8 different environments that require long sequences of task completion to receive any reward, and find that their approach is able to reach human level performance on four of the tasks, while their baseline comparisons essentially never succeed on any task. Leveraging demonstrations can be valuable for solving these kinds of difficult exploration tasks, because demonstrator trajectories provide examples of how to achieve reward in a setting where the trajectories of a randomly exploring agent would rarely ever reach the end of the task to find positive reward. \\nMy opinion: For all that this paper's technique is a fairly straightforward merging of existing techniques (separately-prioritized demonstration and agent pools, and the off-policy SotA R2D2), its results are surprisingly impressive: the tasks tested on require long and complex chains of correct actions that would be challenging for a non-imitation based system to discover, and high levels of environment stochasticity that make a pure imitation approach difficult.\",\n",
       "   'paper_text': '',\n",
       "   'text': 'Highlights\\n[Learning Latent Plans from Play](https://learning-from-play.github.io/)\\xa0*(Corey Lynch et al)*\\xa0(summarized by Cody): This paper collects unsupervised data of humans playing with robotic control systems, and uses that data to thread a needle between two problems in learning. One problem is that per-task demonstration data is costly, especially as number of tasks grows; the other is that randomly sampled control actions will rarely stumble across complex motor tasks in ways that allow robots to learn. The authors argue that human play data is a good compromise because humans at play tend to explore different ways of manipulating objects in ways that give robots nuggets of useful information like \"how do I move this block inside a drawer\", which can be composed into more complicated and intentional tasks.\\nThe model works by learning to produce vectors that represent plans (or sequences of actions), and jointly learning to decode those vectors into action sequences. This architecture learns to generate plan vectors by using an autoencoder-like structure that uses KL divergence to align (1) a distribution of plan vectors predicted from the start and end state of a window of play data, and (2) a distribution of plan vectors predicted by looking back at all the actions taken in that window. Because we\\'re jointly learning to unroll the (2) lookback-summarized vector such that it matches the actions actually taken, we\\'ll ideally end up with a system that can take in a given plan vector and produce a sequence of actions to execute that plan. And, because we\\'re learning to predict a vector that aligns with actions successfully taken to get to an end state from a starting one, the model at test time should be able to produce a play vector corresponding to feasible actions that will get it from its current state to a goal state we\\'d like it to reach. The authors found that their Play-trained model was able to outperform single-task models on a range of manipulation tasks, even though those single-task models were trained with explicit demonstrations of the task.\\n**Cody\\'s opinion:**\\xa0I really liked this paper: it was creative in combining conceptual components from variational methods and imitation learning, and it was pragmatic in trying to address the problem of how to get viable human-demonstration data in a way that avoids having to get distinct datasets for a huge set of different discrete tasks.\\nTechnical AI alignment\\n\\xa0\\n\\nIterated amplification\\n[Aligning a toy model of optimization](https://www.alignmentforum.org/posts/H5gXpFtg93qDMZ6Xn/aligning-a-toy-model-of-optimization)\\xa0*(Paul Christiano)*\\xa0(summarized by Rohin): Current ML capabilities are centered around\\xa0**local search**: we get a gradient (or an approximation to one, as with evolutionary algorithms), and take a step in that direction to find a new model. Iterated amplification takes advantage of this fact: rather than a sequence of gradient steps on a fixed reward, we can do a sequence of amplification steps and distillation gradient steps.\\nHowever, we can consider an even simpler model of ML capabilities:\\xa0**function maximization**. Given a function from n-bit strings to real numbers, we model ML as allowing us to find the input n-bit string with the maximum output value,\\xa0**in only O(n) time**\\xa0(rather than the O(2^n) time that brute force search would take). If this were all we knew about ML capabilities, could we still design an aligned, competitive version of it? While this is not the actual problem we face,\\xa0**due to its simplicity it is more amenable to theoretical analysis**, and so is worth thinking about.\\nWe could make an unaligned AI that maximizes some explicit reward using only 2 calls to Opt: first, use Opt to find a good world model M that can predict the dynamics and reward, and then use Opt to find a policy that does well when interacting with M. This is unaligned for all the usual reasons: most obviously, it will try to seize control of the reward channel.\\nAn aligned version does need to use Opt, since\\xa0**that\\'s the only way of turning a naively-exponential search into a linear one**; without using Opt the resulting system won\\'t be competitive. We can\\'t just generalize iterated amplification to this case, since iterated amplification relies on a\\xa0*sequence*\\xa0of applications of ML capabilities: this would lead to an aligned AI that uses Opt many times, which will not be competitive since the unaligned AI only requires 2 calls to Opt.\\nOne possible approach is to design an AI with good incentives (in the same way that iterated amplification aims to approximate\\xa0[HCH](https://www.alignmentforum.org/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch)\\xa0([AN #34](https://mailchi.mp/f1947668b183/alignment-newsletter-34))) that \"knows everything that the unaligned AI knows\". However, it would also be useful to produce a proof of impossibility: this would tell us something about what a solution must look like in more complex settings.\\n**Rohin\\'s opinion:**\\xa0Amusingly, I liked this post primarily because comparing this setting to the typical setting for iterated amplification was useful for seeing the design choices and intuitions that motivated iterated amplification.\\nForecasting\\n[Coordination Surveys: why we should survey to organize responsibilities, not just predictions](https://www.lesswrong.com/posts/Lds9opZsAMbjuZp7h/coordination-surveys-why-we-should-survey-to-organize)\\xa0*(Andrew Critch)*\\xa0(summarized by Rohin): This post suggests that when surveying researchers about the future impact of their technology, we should specifically ask them about their beliefs about what actions other people will take, and what they personally are going to do, rather than just predicting total impact. (For example, we could ask how many people will invest in safety.) Then, by aggregating across survey respondents, we can see whether or not the researchers beliefs about what others will do match the empirical distribution of what researchers are planning to do. This can help mitigate the effect where everyone thinks that everyone else will deal with a problem, and the effect where everyone tries to solve a problem because they all think no one else is planning to solve it. Critch has offered to provide suggestions on including this methodology in any upcoming surveys; see the post for details.\\n**Rohin\\'s opinion:**\\xa0This is a cool idea, and seems worth doing to me. I especially like that the survey would simply reveal problems by collecting two sources of information from people and checking their consistency with each other: there isn\\'t any particular argument being made; you are simply showing inconsistency in people\\'s own beliefs to them, if and only if such inconsistency exists. In practice, I\\'m sure there will be complications -- for example, perhaps the set of researchers taking the survey is different from the set of \"others\" whose actions and beliefs they are predicting -- but it still seems worth at least trying out.\\n[AI Forecasting Dictionary](https://www.lesswrong.com/posts/8y7DcSF4eAkXoru4u/ai-forecasting-dictionary-forecasting-infrastructure-part-1-2)\\xa0*(Jacob Lagerros and Ben Goldhaber)*\\xa0(summarized by Rohin): One big challenge with forecasting the future is operationalizing key terms unambiguously, so that a question can be resolved when the future actually arrives. Since we\\'ll probably need to forecast many different questions, it\\'s crucial that we make it as easy as possible to create and answer well-operationalized questions. To that end, the authors have created and open-sourced an AI Forecasting Dictionary, which gives precise meanings for important terms, along with examples and non-examples to clarify further.\\n[AI Forecasting Resolution Council](https://www.lesswrong.com/posts/9G6CCNXkA7JZoorpY/ai-forecasting-resolution-council-forecasting-infrastructure)*(Jacob Lagerros and Ben Goldhaber)*\\xa0(summarized by Rohin): Even if you operationalize forecasting questions well, often the outcome is determined primarily by factors other than the one you are interested in. For example, progress on a benchmark might be determined more by the number of researchers who try to beat the benchmark than by improvements in AI capabilities, even though you were trying to measure the latter. To deal with this problem, an AI Forecasting Resolution Council has been set up: now, forecasters can predict what the resolution council will say at some particular time in the future. This allows for questions that get at what we want: in the previous case, we could now forecast how the resolution council will answer the question \"would current methods be able to beat this benchmark\" in 2021.\\n[How to write good AI forecasting questions + Question Database](https://www.lesswrong.com/posts/yy3FCmdAbgSLePD7H/how-to-write-good-ai-forecasting-questions-question-database)*(Jacob Lagerros and Ben Goldhaber)*\\xa0(summarized by Rohin): As discussed above, operationalization of forecasting questions is hard. This post collects some of the common failure modes, and introduces a database of 76 questions about AI progress that have detailed resolution criteria that will hopefully avoid any pitfalls of operationalization.\\nMiscellaneous (Alignment)\\n[The strategy-stealing assumption](https://www.alignmentforum.org/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption)\\xa0*(Paul Christiano)*\\xa0(summarized by Rohin): We often talk about aligning AIs in a way that is\\xa0*competitive*\\xa0with unaligned AIs. However, you might think that we need them to be\\xa0*better*: after all, unaligned AIs only have to pursue one particular goal, whereas aligned AIs have to deal with the fact that we don\\'t yet know what we want. We might hope that regardless of what goal the unaligned AI has, any strategy it uses to achieve that goal can be turned into a strategy for acquiring\\xa0*flexible*\\xa0influence (i.e. influence useful for many goals). In that case,\\xa0**as long as we control a majority of resources**, we can use any strategies that the unaligned AIs can use. For example, if we control 99% of the resources and unaligned AI controls 1%, then at the very least we can split up into 99 \"coalitions\" that each control 1% of resources and use the same strategy as the unaligned AI to acquire flexible influence, and this should lead to us obtaining 99% of the resources in expectation. In practice, we could do even better, e.g. by coordinating to shut down any unaligned AI systems.\\nThe premise that we can use the same strategy as the unaligned AI, despite the fact that we need\\xa0*flexible*\\xa0influence, is called the\\xa0**strategy-stealing assumption**. Solving the alignment problem is critical to strategy-stealing -- otherwise, unaligned AI would have an advantage at thinking that we could not steal and the strategy-stealing assumption would break down. This post discusses\\xa0**ten other ways that the strategy-stealing assumption could fail**. For example, the unaligned AI could pursue a strategy that involves threatening to kill humans, and we might not be able to use a similar strategy in response because the unaligned AI might not be as fragile as we are.\\n**Rohin\\'s opinion:**\\xa0It does seem to me that if we\\'re in a situation where we have solved the alignment problem, we control 99% of resources, and we aren\\'t infighting amongst each other, we will likely continue to control at least 99% of the resources in the future. I\\'m a little confused about how we get to this situation though -- the scenarios I usually worry about are the ones in which we fail to solve the alignment problem, but still deploy unaligned AIs, and in these scenarios I\\'d expect unaligned AIs to get the majority of the resources. I suppose in a multipolar setting with continuous takeoff, if we have mostly solved the alignment problem but still accidentally create unaligned AIs (or some malicious actors create them deliberately), then this setting where we control 99% of the resources could arise.\\nOther progress in AI\\n\\xa0\\n\\nExploration\\n[Making Efficient Use of Demonstrations to Solve Hard Exploration Problems](https://deepmind.com/research/publications/Making-Efficient-Use-of-Demonstrations-to-Solve-Hard-Exploration-Problems)\\xa0*(Caglar Gulcehre, Tom Le Paine et al)*\\xa0(summarized by Cody): This paper combines ideas from existing techniques to construct an architecture (R2D3) capable of learning to solve hard exploration problems with a small number (N~100) of demonstrations. R2D3 has two primary architectural features: its use of a recurrent head to learn Q values, and its strategy of sampling trajectories from separate pools of agent and demonstrator experience, with sampling prioritized by highest-temporal-difference-error transitions within each pool.\\nAs the authors note, this approach is essentially an extension of an earlier paper,\\xa0[Deep Q-Learning from Demonstrations](https://arxiv.org/abs/1704.03732), to use a recurrent head rather than a feed-forward one, allowing it to be more effectively deployed on partial-information environments. The authors test on 8 different environments that require long sequences of task completion to receive any reward, and find that their approach is able to reach human level performance on four of the tasks, while their baseline comparisons essentially never succeed on any task. Leveraging demonstrations can be valuable for solving these kinds of difficult exploration tasks, because demonstrator trajectories provide examples of how to achieve reward in a setting where the trajectories of a randomly exploring agent would rarely ever reach the end of the task to find positive reward.\\n**Cody\\'s opinion:**\\xa0For all that this paper\\'s technique is a fairly straightforward merging of existing techniques (separately-prioritized demonstration and agent pools, and the off-policy SotA R2D2), its results are surprisingly impressive: the tasks tested on require long and complex chains of correct actions that would be challenging for a non-imitation based system to discover, and high levels of environment stochasticity that make a pure imitation approach difficult.\\nReinforcement learning\\n[Emergent Tool Use from Multi-Agent Interaction](https://openai.com/blog/emergent-tool-use/)\\xa0*(Bowen Baker et al)*\\xa0(summarized by Rohin): We have such a vast diversity of organisms and behaviors on Earth because of evolution: every time a new strategy evolved, it created new pressures and incentives for other organisms, leading to new behaviors. The multiagent competition led to an\\xa0*autocurriculum*. This work harnesses this effect: they design a multiagent environment and task, and then use standard RL algorithms to learn several interesting behaviors. Their task is hide-and-seek, where the agents are able to move boxes, walls and ramps, and lock objects in place. The agents find\\xa0*six*\\xa0different strategies, each emerging from incentives created by the previous strategy: seekers chasing hiders, hiders building shelters, seekers using ramps to get into shelters, hiders locking ramps away from seekers, seekers surfing boxes to hiders, and hiders locking both boxes and ramps.\\nThe hope is that this can be used to learn general skills that can then be used for specific tasks. This makes it a form of unsupervised learning, with a similar goal as e.g.\\xa0[curiosity](https://pathak22.github.io/large-scale-curiosity/)\\xa0([AN #20](https://mailchi.mp/d92bd0fefc83/alignment-newsletter-20)). We might hope that multiagent autocurricula would do better than curiosity, because they automatically tend to use features that are important for control in the environment (such as ramps and boxes), while intrinsic motivation methods often end up focusing on features we wouldn\\'t think are particularly important. They empirically test this by designing five tasks in the environment and checking whether finetuning the agents from the multiagent autocurricula learns faster than direct training and finetuning curiosity-based agents. They find that the multiagent autocurricula agents do best, but only slightly. To explain this, they hypothesize that the learned skill representations are still highly entangled and so are hard to finetune, whereas learned feature representations transfer more easily.\\n**Rohin\\'s opinion:**\\xa0This is somewhat similar to\\xa0[AI-GAs](https://arxiv.org/abs/1905.10985)\\xa0([AN #63](https://mailchi.mp/533c646a4b21/an-63how-architecture-search-meta-learning-and-environment-design-could-lead-to-general-intelligence)): both depend on\\xa0*environment design*, which so far has been relatively neglected. However, AI-GAs are hoping to create\\xa0*learning algorithms*, while multiagent autocurricula leads to\\xa0*tool use*, at least in this case. Another point of similarity is that they both require vast amounts of compute, as discovering new strategies can take significant exploration. That said, it seems that we might be able to drastically decrease the amount of compute needed by solving the exploration problem using e.g. human play data or demonstrations (discussed in two different papers above).\\nMore speculatively, I hypothesize that it will be useful to have environments where you need to identify\\xa0*what strategy your opponent is using*. In this environment, each strategy has the property that it beats\\xa0*all*\\xa0of the strategies that preceded it. As a result, it was fine for the agent to undergo catastrophic forgetting: even though it was trained against past agents, it only needed to learn the current strategy well; it didn\\'t need to remember previous strategies. As a result, it may have forgotten prior strategies and skills, which might have reduced its ability to learn new tasks quickly.\\n**Read more:**\\xa0[Paper: Emergent Tool Use from Multi-Agent Autocurricula](https://d4mucfpksywv.cloudfront.net/emergent-tool-use/paper/Multi_Agent_Emergence_2019.pdf),\\xa0[Vox: Watch an AI learn to play hide-and-seek](https://www.vox.com/future-perfect/2019/9/20/20872672/ai-learn-play-hide-and-seek)\\nApplications\\n[Tackling Climate Change with Machine Learning](http://arxiv.org/abs/1906.05433)\\xa0*(David Rolnick et al)*\\xa0(summarized by Rohin): See\\xa0[Import AI](https://jack-clark.net/2019/06/17/import-ai-151-us-army-trains-starcraft-ii-ai-teaching-drones-to-dodge-thrown-objects-and-fighting-climate-change-with-machine-learning/). |\\n\\n\\n |\\n\\n |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n|  |  |  |  |  |  |  |  |  |  |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| \\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| [Twitter](http://www.twitter.com/) |\\n\\n |\\n\\n |\\n\\n\\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| [Facebook](http://www.facebook.com) |\\n\\n |\\n\\n |\\n\\n\\n\\n\\n|  |  |  |\\n| --- | --- | --- |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| [Website](http://mailchimp.com) |\\n\\n |\\n\\n |\\n\\n\\n |\\n\\n |\\n\\n |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2019 Rohin Shah, All rights reserved.*\\n\\n\\n'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1705.04226v2',\n",
       "  'title': 'Robot Planning with Mathematical Models of Human State and Action',\n",
       "  'authors': ['Anca D. Dragan'],\n",
       "  'date_published': '2017-05-11 15:02:34+00:00',\n",
       "  'data_last_modified': '2017-07-04 02:08:23+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1705.04226v2',\n",
       "  'abstract': 'Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.',\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.RO',\n",
       "  'categories': ['cs.RO'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': 'CogsciHRI.tex',\n",
       "  'text': '---\\nabstract: |\\n  Robots interacting with the physical world plan with models of physics. We advocate that robots interacting with people need to plan with models of cognition. This writeup summarizes the insights we have gained in integrating computational cognitive models of people into robotics planning and control. It starts from a general game-theoretic formulation of interaction, and analyzes how different approximations result in different useful coordination behaviors for the robot during its interaction with people.\\nauthor:\\n- |\\n  Anca D. Dragan (anca\\\\@berkeley.edu)\\\\\\n  Department of Electrical Engineering and Computer Sciences\\\\\\n  University of California, Berkeley\\\\\\n  Summary of work in collaboration with P. Abbeel, R. Bajcsy,\\\\\\n  A. Bestick, J. Fisac, T. Griffiths, JK. Hedrick, J. Heimrick, N. Landolfi, C. Liu,\\\\\\n  D. Hadfield Menell, S. Milli, A. Nagabaudi, S. Russell, D. Sadigh, S. Sastry, S. Seshia, S. Srinivasa, A. Zhou\\nbibliography:\\n- refs.bib\\ntitle: |\\n  Robot Planning with Mathematical\\\\\\n  Models of Human State and Action\\n---\\n\\nIntroduction\\n============\\n\\nRobots act to maximize their utility. They reason about how their actions affect the state of the world, and try to find the actions which, in expectation, will accumulate as much reward as possible. We want robots to do this well so that they can be useful to us -- so that they can come in support of real people. But supporting people means having to work with and around them. We, the people, are going to have to share the road with autonomous cars, share our kitchens with personal robots, share our control authority with prosthetic and assistive arms.\\n\\nSharing is not easy for the robots of today. They know how to deal with obstacles, but people are more than that. We reason about the robot, we make decisions, we act. This means that the robot needs to make predictions about what we will think, want, and do, so that it can figure out actions that coordinate well with ours and that are helpful to us. Much like robots of today have a theory of *physics* (be it explicitly as an equation or implicitly as a learned model), the robots of tomorrow will need to start having a theory of *mind*.\\n\\nOur work for the past few years has focused on integrating mathematical theories of mind, particularly about human *future actions* and *beliefs*, into the way robots plan their *physical*, task-oriented actions.\\n\\nThis required a change from the robotics problem formulation (Fig.[1](#fig:front){reference-type=\"ref\" reference=\"fig:front\"}, left), to an *interaction* problem formulation (Fig.[1](#fig:front){reference-type=\"ref\" reference=\"fig:front\"}, right). Interaction means there is not a single agent anymore: the robot and human are both agents in a two player game, and they take actions according to utility functions that are not necessarily identical or known to each other. The paper outlines this formally in Sec. [2](#sec:game){reference-type=\"ref\" reference=\"sec:game\"}, and then summarizes the different approximations we\\'ve explored and what we\\'ve learned from them [@Sadigh_RSS_driving; @Chang_AAMAS_goalinference; @Bestick_ISER_influence; @Sadigh_comparison; @Sadigh_IROS_infogather; @hadfield2016cooperative; @hadfield2017ird; @Milli_obedience; @Menell_offswitch; @Fisac_WAFR_tpred; @Dragan_RSS_legibility; @Zhang_HRI; @Huang_expressrewards]:\\n\\n![Left: Traditional robotics formalism: the robot takes actions $u_{\\\\mathcal{R}}$ to optimize a reward or cost function $r_{{\\\\mathcal{R}}}$. Right: Interaction formalism: the robot is not acting in isolation; the human takes actions $u_{{\\\\mathcal{H}}}$ to optimize a reward function $r_{{\\\\mathcal{H}}}$, possibly different from that of the robot\\'s, and parametrized by human internal/hidden state $\\\\theta_{{\\\\mathcal{H}}}$; the human does not know the robot internal state $\\\\theta_{R}$, which parametrizes the robot\\'s reward function; and both functions depend on both agents\\' actions.](front2.pdf){#fig:front width=\"\\\\\\\\columnwidth\"}\\n\\n**Accounting for the *physical human behavior* during interaction (Sec. [3](#sec:humanbehavior){reference-type=\"ref\" reference=\"sec:humanbehavior\"}).**\\n\\n> *One important insight for us has been that people can\\'t be treated as just obstacles that move: they will **react** to what the robot does.*\\n\\nIf a car starts merging in front of you, you break. If the robot helping you assemble a part employs a different strategy than you expected, you adapt. It took more and more sophisticated approximations to the game above to account for this.\\n\\nOur first approximation to the game started by assuming a shared utility function and treating the person as a perfect collaborator, but replanning at every step to adapt to when the person deviates from the collaborative plan [@Chang_AAMAS_goalinference]; we then relaxed this to an imperfect collaborator model, showing that the robot can leverage its actions to guide the person to perform better in the task [@Bestick_ISER_influence]; finally, we investigated a model of the person as optimizing a different utility function, but simply computing a best response to the robot\\'s actions (as opposed to solving the full dynamic game) [@Sadigh_RSS_driving] -- this model enables the robot to account for how people will react to its actions, and thus perform better at its task.\\n\\n**Using the human behavior to infer *human internal states* (Sec. [4](#sec:inferhuman){reference-type=\"ref\" reference=\"sec:inferhuman\"}).** The models above were a first step in coordinating with people, but they were disappointing in that they still assumed perfect information, i.e. everything is known to both parties. It is simply not true that we will be able to give our robots up front a perfect model of each person they will interact with. Next, we studied how robots might be able to estimate internal, hidden, human states, online, by taking human behavior into account as evidence about them.\\n\\n> *Another important insight has been that robots should not take their objective functions for granted: they are easy to misspecify and change from person to person. Instead, robots should optimize for what the person wants internally, and use human guidance to estimate what that is.*\\n\\nInverse Reinforcement Learning [@ng2000algorithms; @ziebart2008maximum; @ramachandran2007bayesian] already addresses this for a *passive* observer analyzing *offline* human *demonstrations* of approximately optimal behavior *in isolation*. Our work builds on three goals: 1) making these inferences *actively* and *online*; we leverage not just queries [@Sadigh_comparison], but also the robot\\'s physical actions [@Sadigh_IROS_infogather]; 2) accounting for the fact that if the person knows the robot is trying to learn, they will act differently from what they do in isolation and *teach* [@hadfield2016cooperative]; and, perhaps most importantly, 3) leveraging richer sources of human behavior beyond demonstrations, like physical corrections [@bajcsy2017phri], orders given to the robot [@Milli_obedience], and even the reward function specified [@hadfield2017ird] -- all of these are observations about the actual desired reward.\\n\\nWe argue that interpreting the reward function designed for the robot as useful information about the true desired reward, by leveraging the context in which this function was designed to begin with, can make robots less sensitive to negative consequences of misspecified rewards [@hadfield2017ird]. Similarly, we find that interpreting orders as useful information about the desired reward give robots a provable incentive to accept human oversight as opposed to bypass it in pursuit of some misspecified objective [@Menell_offswitch]. Overall, we find that accepting that the robot\\'s reward function is not given, but part of the person\\'s internal state, is key to safe and customizable robots.\\n\\n**Accounting for *human beliefs* about robot internal states (Sec. [5](#sec:beliefs){reference-type=\"ref\" reference=\"sec:beliefs\"}).**\\n\\n> *Robots need to make inferences about people during interaction, but people, too, need to make inferences about robots. Robot actions influence whether people can make the correct inferences.*\\n\\nThe third part of our work focuses on getting robots to produce behavior that enables these human inferences to happen correctly, whether they are about the robot\\'s behavior [@Fisac_WAFR_tpred], or about the robot\\'s internal states (like utility [@Huang_expressrewards], goals [@Dragan_RSS_legibility], or even level of uncertainty [@Zhang_HRI]). Although these increases the robot\\'s transparency, we have been encoding the need for that in the objective directly, whereas really it should be a consequence of solving the interaction problem well. This is something we are actively looking into, but which increases the computational burden.\\n\\nAll this research stands on the shoulders of inspiring works in computational human-robot interaction, nicely summarized in sections 6 and 7 of [@thomaz2016computational] and not repeated here. What this writeup contributes a summary of our own experiences in this area, particularly focusing on physical, task-oriented interaction. It provides a common formalism that, in retrospect, can be seen as the general formulation that seeded these works, along with a quasi-systematic analysis of the different ways to approximate solutions and the sometimes surprisingly interesting and powerful behavior that emerges when we do that. This enables robots to *tractably and autonomously generate* different kinds of behavior for interaction, often *in spite of the continuous state and action spaces* they have to handle -- from arms that guide and improve human performance in handovers, to cars that negotiate at intersections, to robots that purposefully make their inner-workings more transparent to their end-users.\\n\\n![image](collaboration.pdf){width=\"\\\\\\\\textwidth\"}\\n\\nGeneral Interaction as a Game {#sec:game}\\n=============================\\n\\nIn general, we can formulate interaction as a 2-player dynamic game between the human and the robot. The state $x$ contains the world state along with the robot and human state: $$x=(x_{\\\\mathcal{W}},x_{{\\\\mathcal{R}}},x_{{\\\\mathcal{H}}})$$ Each agent can take actions, and each agent has a (potentially different) reward function: $$r_\\\\mathcal{R}(x,u_{{\\\\mathcal{R}}},u_{{\\\\mathcal{H}}};\\\\theta_{{\\\\mathcal{R}}})$$ for the robot and $$r_\\\\mathcal{H}(x,u_{{\\\\mathcal{R}}},u_{{\\\\mathcal{H}}};\\\\theta_{{\\\\mathcal{H}}})$$ for the human, each with parameters $\\\\theta$. The two do not necessarily know each other\\'s reward functions (or equivalently, each other\\'s parameters $\\\\theta$).\\n\\nLet $T$ be the time horizon, and ${\\\\bf u}_{\\\\mathcal{R}}$ and ${\\\\bf u}_{\\\\mathcal{H}}$ be a robot and human, respectively, control sequence of length $T$. We denote by $R_\\\\mathcal{R}$ the cumulative reward to the robot from the starting state $x^0$: $$R_\\\\mathcal{R}(x^0,{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}};\\\\theta_{{\\\\mathcal{R}}})=\\\\sum_{t=0}^T r_\\\\mathcal{R}(x^t,u_{{\\\\mathcal{R}}}^t,u_{{\\\\mathcal{H}}}^t;\\\\theta_{{\\\\mathcal{R}}})$$ and by $R_\\\\mathcal{H}$ the cumulative reward for the human: $$R_\\\\mathcal{H}(x^0,{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}};\\\\theta_{{\\\\mathcal{H}}})=\\\\sum_{t=0}^T r_\\\\mathcal{H}(x^t,u_{{\\\\mathcal{R}}}^t,u_{{\\\\mathcal{H}}}^t;\\\\theta_{{\\\\mathcal{H}}})$$\\n\\nOne way to model what the person will do is to model them as rationally solving this game. There are several issues with this. The first is that it is computationally intractable. The second is that if $r_\\\\mathcal{H}\\\\neq r_\\\\mathcal{R}$, the game will have many equilibria, so even if we could compute all of them we\\'d still not be sure which the person is using. The third is that even without the first two issues, this would still not be a good model for how people make decisions in day to day tasks [@hedden2002you; @rubinstein1998modeling].\\n\\nOur work has thus explored different approximations to this problem that might better match what people do, while enabling robots to actually generate their actions in realistic tasks in (close to) real time.\\n\\nHuman Behavior During Interaction {#sec:humanbehavior}\\n=================================\\n\\nBecause in interaction the robot\\'s reward depends on what the person does, the ability to anticipate human actions becomes crucial in deciding what the robot should do. Rather than modeling the person as solving the game above, we explored several approximations that each led to different yet useful behaviors in interaction.\\n\\n**The Perfect Collaboration Model.** The easiest simplifying assumption is actually that the person is optimizing the same reward function: $$r_\\\\mathcal{H}=r_\\\\mathcal{R}$$ and both agents know this (no more partial information). This assumption turns planning for interaction into a much easier problem, analogous to the original robotics problem: it is pretending like the person is just some additional degrees of freedom that the robot can actuate -- their actions will follow the optimal centralized plan: $$({\\\\bf u}_{\\\\mathcal{R}}^{*},{\\\\bf u}_{\\\\mathcal{H}}^{*})=\\\\mathop{\\\\mathrm{argmax}}_{{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}}}R_\\\\mathcal{R}(x^{0},{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}})$$\\n\\nDespite its simplicity, we have found that this can be very useful so long as the robot *replans* at every time step. People inevitably deviate from from the optimal centralized plan even from the first step, ending up in some new state -- because they don\\'t actually optimize the same reward, because they don\\'t know that the robot is optimizing the same reward, or because they are not perfect optimizers. But the robot can recompute the centralized optimal plan from that new state, and proceed with the first action from the new ${\\\\bf u}_{\\\\mathcal{H}}^{*}$.\\n\\nFig.[\\\\[fig:collaboration\\\\]](#fig:collaboration){reference-type=\"ref\" reference=\"fig:collaboration\"} shows a comparison from [@Chang_AAMAS_goalinference] between a \"Fixed\" robot strategy, where the robot executes the originally planned ${\\\\bf u}_{\\\\mathcal{R}}^{*}$ regardless of what the person does, and a \"Reactive\" strategy, in which the robot keeps updating ${\\\\bf u}_{\\\\mathcal{R}}$ based on the new centralized optimal plan from the current state at every step. Here, $R_\\\\mathcal{H}=R_\\\\mathcal{R}$ and equates to the total time to solve the task. We recruited 234 participants on Amazon Mechanical Turk who played collaborative Traveling Salesman Problems with a robot avatar in a within-subjects design that included a 3rd condition, discussed later, and a randomized trial order. We found that the Reactive condition led to the task being completed significantly faster by the human-robot team, and that participants preferred the Reactive robot especially in the beginning.\\n\\n![image](handover_new.pdf){width=\"\\\\\\\\textwidth\"}\\n\\n*Overall, online planning with a perfect collaborative model of human behavior can be useful, already enabling the robot to continually adapt to the person\\'s plan even though it does not get it right a priori.*\\n\\n**Collaborative but Approximately Optimal.** An improvement upon the perfectly rational collaborator human model is to recognize that people are not actually perfectly rational. In [@Bestick_ISER_influence], we model people as collaborative still, but no longer assume ${\\\\bf u}_{\\\\mathcal{H}}^{*}$ is perfect. In particular, we assumed that people are *greedy* or *myopic* in their decisions. Their action at time $t$ will be the one that looks locally good, not the one that is optimal over the full time horizon: $$u_{\\\\mathcal{H}}^{t*}(u^{t}_{\\\\mathcal{R}})=\\\\arg\\\\max_{u^{t}_{\\\\mathcal{H}}}r_\\\\mathcal{R}(x^t,u^{t}_{\\\\mathcal{R}},u^{t}_{\\\\mathcal{H}})$$\\n\\nThe robot can then choose its own actions such that, when coupled with the person\\'s myopic response to them, the combined plan is as high-reward as possible: $${\\\\bf u}_{\\\\mathcal{R}}^{*}=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{R}}}R_\\\\mathcal{R}(x^{0},{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}}^{*}({\\\\bf u}_{\\\\mathcal{R}}))$$\\n\\nThis results in the robot *guiding* the person\\'s actions, helping them overcome cognitive, bounded-rationality limitations as much as possible.\\n\\nInteraction becomes an *underactuated system*: the robot no longer assumes it can directly actuate ${\\\\bf u}_{\\\\mathcal{H}}$, but accounts for how ${\\\\bf u}_{\\\\mathcal{R}}$ will influence ${\\\\bf u}_{\\\\mathcal{H}}$ and takes that into account when planning.\\n\\nIn particular, we investigated a handover task in which participants had to take an object from the robot and place it at a goal location. We used as $r_\\\\mathcal{H}=r_\\\\mathcal{R}$ as negative ergonomic cost to the person. People\\'s decision in this problem is how to take the object such that it is ergonomically low cost. The robot\\'s decision is how to hold the object at the handover time to enable that. A perfect human optimizer would minimize cost at the handover *and* at the goal. Our myopic model minimized cost the handover time, which then could in some cases resulted in high cost at the goal, such as needing to twist their arm in an uncomfortable way. or regrasp. Fig.[\\\\[fig:handover\\\\]](#fig:handover){reference-type=\"ref\" reference=\"fig:handover\"} shows an example.\\n\\nWhen the robot optimizes for its actions, it chooses ways to hold the object that *incentivize* good global human plans. The robot chooses grasps such that even when the person chooses their grasp greedily for the handover, that greedy grasp is also as close as possible to the global optimum, resulting in low cost (high reward) at the goal as well (Fig.[\\\\[fig:handover\\\\]](#fig:handover){reference-type=\"ref\" reference=\"fig:handover\"}).\\n\\n*Overall, planning with a myopic collaborative model of the human behavior results in the robot taking actions that can guide the person towards plans that are globally optimal, helping them overcome the limitations of greedy action selection.*\\n\\n**Non-Collaborative but Computing a Best Response to the Robot.** Not every situations is collaborative. Take driving for example. The car has the same objective as its passenger, but a different objective from other human driven vehicles on the road -- these are trying to reach their own destinations as efficiently as possible, and that sometimes competes with the car\\'s objective.\\n\\nBreaking the collaboration assumption that the human is optimizing the same reward function as the robot (or viceversa), puts making prediction of human behavior back to solving a 2-player game even if we assume known reward parameters. In [@Sadigh_RSS_driving], we introduced a model that alleviates this difficulty by assuming that the person is not computing to a Nash equilibrium, but instead computing a *best response* to the robot\\'s plan using a different, yet known reward function $r_\\\\mathcal{H}$. That is, rather than trying to influence the robot\\'s behavior, the person is taking the robot\\'s behavior as fixed, and optimizing their own reward function within that constraint: $${\\\\bf u}_{\\\\mathcal{H}}^{*}({\\\\bf u}_{\\\\mathcal{R}})=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{H}}}R_\\\\mathcal{H}(x^{0},{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}})$$ The robot can then compute the action sequence that, when combined with the human\\'s response to that sequence, leads to the highest value for its *own* reward: $${\\\\bf u}_{\\\\mathcal{R}}^{*}=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{R}}}R_\\\\mathcal{R}(x^{0},{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}}^{*}({\\\\bf u}_{\\\\mathcal{R}}))$$ It can then take the first action in ${\\\\bf u}_{\\\\mathcal{R}}$, observe the change in the world, and replan. This is what we did in the collaborative replanning case with the Reactive robot, except now the robot has a model for how the person will respond to its actions as opposed to computing a joint global plan.\\n\\nWe applied this to autonomous driving, namely the interaction between an autonomous car and a human-driven vehicle. Both the robot and the person want to achieve their goal efficiently, which made their reward functions non-identical. We gave the robot access to $R_\\\\mathcal{H}$ by learning it offline using IRL.\\n\\n![image](efficiency.pdf){width=\"\\\\\\\\textwidth\"}\\n\\nTypically in autonomous driving, cars treat people like obstacles, planning to stay out of their way. This leads to overly conservative behavior, like cars never getting to merge on a highway, or getting stuck at 4-way stops. In contrast, our car coordinates with people (Fig.[\\\\[fig:efficiency\\\\]](#fig:efficiency){reference-type=\"ref\" reference=\"fig:efficiency\"}). It sometimes plans to merge in front of them knowing that they can slow down to accommodate the merge. Or at an intersection, for $R_\\\\mathcal{R}$ being higher if the person goes first through the intersection, the robot does not just sit there, but coordinates by *starting to back up*, which makes it safer for the person to go (effectively *signaling* to the human driver). We ran a user study with a driving simulator, and the results suggested that people\\'s behavior when the robot is planning with this model leads to significantly higher reward for the robot than in the baseline of treating the person as on obstacle.\\n\\n*Overall, treating interaction as an underactuated system where the person is not playing a game, but acting rationally as a best response to the robot\\'s actions, leads to coordination behavior that naturally emerges out of the optimization over robot actions.*\\n\\nUsing Human Behavior to Infer Human Internal State {#sec:inferhuman}\\n==================================================\\n\\nThus far we\\'ve oversimplified the game by assuming that the robot knows the persons\\'s reward parameters. In reality, the robot does have direct access to these. Even further, we argue that in reality, the robot does not really have access to its own reward parameters either -- collaborative robots, meant to help a person, should optimize for whatever that person wants, not for some a-priori determined reward function.\\n\\nRobots today take their reward function as given. But where does this reward function come from? Typically, it is designed by some person who does their best at writing down what they think the robot should optimize for. Unfortunately, we, people, are terrible at specifying what we want. From King Midas to The Sorcerer\\'s Apprentice, we have countless stories that warn us about unintended, negative consequences of misspecified wishes or objectives. We propose that robots should have uncertainty over their objectives, and that they should try to optimize for what people want internally, but can\\'t necessarily explicate. This is key to alleviating the negative consequences of a misspecified objective.\\n\\nTo achieve this, we use human actions as observations about their internal or desired reward function. as they would in the full dynamic game. But the robot will no longer assume that it knows $r_\\\\mathcal{H}$. However, if we assume a rational model of human behavior, then the human actions become observations about this hidden internal human state. To estimate $\\\\theta_{{\\\\mathcal{H}}}$ from human actions, the robot needs an observation model -- the probability of observed actions given $\\\\theta_{{\\\\mathcal{H}}}$. We assume the person is approximately rational [@ziebart2008maximum; @baker2007goal] with a model that comes from a maximum entropy distribution in which trajectories are more likely when their total reward is higher: $$P({\\\\bf u}_{\\\\mathcal{H}}|x^{0},{\\\\bf u}_{\\\\mathcal{R}},\\\\theta_{{\\\\mathcal{H}}})\\\\propto \\\\exp({R_\\\\mathcal{H}}(x^{0},{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}};\\\\theta_{{\\\\mathcal{H}}}))$$\\n\\nThen the robot can update its belief over $\\\\theta_{{\\\\mathcal{H}}}$: $$b\\'(\\\\theta_{{\\\\mathcal{H}}})\\\\propto b(\\\\theta_{{\\\\mathcal{H}}})P({\\\\bf u}_{\\\\mathcal{H}}|x^{0},{\\\\bf u}_{\\\\mathcal{R}},\\\\theta_{{\\\\mathcal{H}}})$$\\n\\nIf the robot observes a trajectory ${\\\\bf u}_{\\\\mathcal{H}}$ for the full time horizon, then the belief update equates to (Bayesian) Inverse Reinforcement Learning [@ziebart2008maximum; @ramachandran2007bayesian].\\n\\nBut robots sometimes need to infer the human reward online, as the human trajectory is unfolding. Think back to the driving application: it is not the case that every person optimizes the same reward function: some drivers are more aggressive than others, some are not paying attention, and so on. It is therefore helpful to be able to update $\\\\theta$ as the robot is interacting with the person. In such cases, the robot has only observed ${\\\\bf u}_{\\\\mathcal{H}}^{0:t}$ and must update its belief based just that, rather than a full trajectory.\\n\\nFurther, robots have an opportunity to go beyond passive inference, and use their actions for *active* estimation, triggering informative human reactions. Instead of passively observing what people do, they can leverage their actions to gather information.\\n\\nFinally, it\\'s not just human physical actions that are informative. Human behavior in general, like physical corrections, comparisons, orders given to the robot, and even a reward function that a designer tries to write down -- all of these are useful sources of information about what the true robot objective should be.\\n\\n**Online Inference by Integrating over Futures.** In [@Chang_AAMAS_goalinference; @Dragan_RSS_teleop], we integrated over the possible future human trajectories in order to compute the belief update: $$P({\\\\bf u}_{\\\\mathcal{H}}^{0:t}|x^{0},{\\\\bf u}_{\\\\mathcal{R}},\\\\theta_{{\\\\mathcal{H}}})=\\\\int P({\\\\bf u}_{\\\\mathcal{H}}|x^{0},{\\\\bf u}_{\\\\mathcal{R}},\\\\theta_{{\\\\mathcal{H}}}) d{\\\\bf u}_{\\\\mathcal{H}}^{t+1:T}$$ We showed that for the case of the reward $r_\\\\mathcal{H}$ being parametrized by which goal $\\\\theta_{{\\\\mathcal{H}}}$ the person is reaching for, the integral can be approximated via Laplace\\'s method, assuming reward 0 for trajectories that do not reach the goal.\\n\\nFig.[\\\\[fig:collaboration\\\\]](#fig:collaboration){reference-type=\"ref\" reference=\"fig:collaboration\"} shows a Predictive condition from [@Chang_AAMAS_goalinference], in which the robot infers the person\\'s goal and uses it to proactively change its plan. This condition outperforms the Reactive condition. In [@Dragan_RSS_teleop], we used goal inference to adapt to an operator\\'s goal during teleoperation of a robot arm.\\n\\n*Overall, human actions as observations about the underlying human goals, and inferring these enables robots to proactively adapt to what people want.*\\n\\n**Active Online Inference using Robot Physical Actions.** Inference does not have to be passive. In [@Sadigh_comparison], we explored active inference, where the robot makes queries that the person responds to. But really, having a robot whose actions influence human behavior presents an opportunity: to leverage robot actions and trigger informative human reactions.\\n\\nIn [@Sadigh_IROS_infogather], we ran online inference by modeling the human as optimizing with a shorter time horizon. We then took the human (short time horizon) trajectory as evidence about the underlying $\\\\theta_{{\\\\mathcal{H}}}$. There, $\\\\theta_{{\\\\mathcal{H}}}$ parametrized the reward function by representing weights on different important features of the human state and action. Unlike goals, this is a continuous and high-dimensional space. So rather than maintaining a belief over all possible $\\\\theta_{{\\\\mathcal{H}}}$s, we clustered users into styles and only maintained a belief over a discrete set of driving styles.\\n\\nFurther, we sped up the inference by leveraging the robot\\'s actions: since the person will choose actions that depend on what the robot does, ${\\\\bf u}_{\\\\mathcal{R}}$, the robot has an opportunity to select actions that *maximize information gain* (trading off with maximizing reward using the current estimate $\\\\hat{\\\\theta}_{{\\\\mathcal{H}}}$: $$\\\\begin{aligned}\\n{\\\\bf u}_{\\\\mathcal{R}}^{*}=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{R}}}R_\\\\mathcal{R}(x^{0},{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}}^{*}({\\\\bf u}_{\\\\mathcal{R}};\\\\hat{\\\\theta_{{\\\\mathcal{H}}}}))\\\\nonumber \\\\\\\\ \\n+\\\\lambda (H(b)-\\\\mathbb{E}_{\\\\theta_{{\\\\mathcal{H}}}}H(b\\')) \\\\nonumber\\\\end{aligned}$$\\n\\nNote that if we were able to treat $\\\\theta_{{\\\\mathcal{H}}}$ as the hidden state in a POMDP with very complicated dynamics (that require planning for the person to solve for how the state will update given the robot\\'s action), then the robot\\'s policy would achieve an optimal trade-off between exploiting current information and gathering information. However, since even POMDPs with less complex dynamics are still intractable in continuous state and action, we resorted to an explicit trade-off.\\n\\nWe found that the robot planning in this formulation exhibited interesting behavior that could be seen as information-gathering. For instance, it would nudge closer to someone\\'s lane, because the anticipated reaction from the person would be different depending on their driving style: attentive drivers break, distracted drivers continue. Or, at a 4-way stop, it would inch forward into the intersection, again anticipating different reactions for different styles.\\n\\n*The robot can leverage its actions\\' influence on the person to actively gather information about their internal reward parameters.*\\n\\n**What If the Human Knows the Robot is Learning?.** One issue with estimation arises when the observation model is wrong. People might act approximately optimal with respect to the reward function, except when they know that the robot is trying to learn something from them. This is why coaches are different from experts: when we teach, we simplify, we exaggerate, we showcase. A gymnastics coach does not demonstrate the same action they would perform if they were in the olympics.\\n\\nIn [@hadfield2016cooperative], we analyzed the difference between maximizing the reward function for the true $\\\\theta_{{\\\\mathcal{H}}}^*$: $${\\\\bf u}_{\\\\mathcal{H}}^{expert}=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{H}}} R_\\\\mathcal{H}(x^0,{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}};\\\\theta_{{\\\\mathcal{H}}}^*)$$ and maximizing the probability that the robot will infer the true $\\\\theta_{{\\\\mathcal{H}}}^*$: $${\\\\bf u}_{\\\\mathcal{H}}^{teacher}=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{H}}}b\\'(\\\\theta_{{\\\\mathcal{H}}}^*)=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{H}}}P(\\\\theta_{{\\\\mathcal{H}}}^*|x^0,{\\\\bf u}_{\\\\mathcal{R}},{\\\\bf u}_{\\\\mathcal{H}})$$\\n\\n![We model teaching demonstrations as being informative about the underlying reward function. An expert demonstration (left) might lead to inferring the wrong reward function, e.g. because the expert goes straight for the high reward peak nearby. A teaching demonstration will deviate from optimality to showcase the underlying reward function, e.g. the teacher goes to both high reward peaks to clarify.](cirl.pdf){#fig:cirl width=\"\\\\\\\\columnwidth\"}\\n\\nFig.[2](#fig:cirl){reference-type=\"ref\" reference=\"fig:cirl\"} compares ${\\\\bf u}_{\\\\mathcal{H}}^{expert}$ to ${\\\\bf u}_{\\\\mathcal{H}}^{teacher}$ in a simple MDP where the reward function consists of high and low reward peaks. The expert demonstration heads straight to the closest high reward peak, but the robot has trouble inferring that there is another peak with also high reward. In contrast, the teaching demonstration visits both, leading to the robot inferring the correct $\\\\theta_{{\\\\mathcal{H}}}$.\\n\\n*Overall, we should expect and account for how people will act differently when they are trying to teach the robot about their internal reward parameters.*\\n\\n**Learning Objectives from Rich Human Guidance.** It is not just human physical actions as part of the task that should inform the robot about the internal human objective. We explored physical corrections [@bajcsy2017phri] (Fig.[4](#fig:phri){reference-type=\"ref\" reference=\"fig:phri\"}), comparisons [@Sadigh_comparison], orders (human oversight) [@Milli_obedience; @Menell_offswitch], and even attempts at specifying an objective [@hadfield2017ird], all as sources of information for the robot. Each required its own observation model, and its own approximations for running the inference.\\n\\n![Learning the desired objective function from physical corrections, in real-time, leads to completing the task in the desired way with less human intervention.](phri1.pdf \"fig:\"){#fig:phri width=\"\\\\\\\\columnwidth\"} ![Learning the desired objective function from physical corrections, in real-time, leads to completing the task in the desired way with less human intervention.](phri2.pdf \"fig:\"){#fig:phri width=\"\\\\\\\\columnwidth\"}\\n\\nIn [@hadfield2017ird], we proposed to mode the reward design process: the probability that a reward designer would choose $\\\\theta_{{\\\\mathcal{R}}}$ as the specified reward, given the true reward $\\\\theta^{*}$ *and* the training environments they are considering. We then showed how the robot can invert this model to get a posterior distribution over what the true reward is, and that this alleviates consequences like reward hacking and negative side-effects. Surprisingly, we found that this works even when the important features affecting the true reward, like the presence of a dangerous kind of terrain, are latent and not directly observed.\\n\\nEven more surprising is our finding from [@Menell_offswitch], where we focused on shut down orders to the robot. Intuitively, robots should just follow such orders as opposed to try to infer the underlying reward function that triggered them. Unfortunately though, designing a reward function that incentivizes accepting orders is challenging, and so is writing down the right hard constraints that the robot should follow. Instead, our work has proved that when the robot treats orders as a useful source of information about its objective, the incentive to accept them is positive.\\n\\n*Overall, we find that treating human guidance and oversight as a useful source of information about the robot\\'s true reward can alleviate the unintended consequences of misspecified robot reward functions.*\\n\\nHuman Inferences about the Robot {#sec:beliefs}\\n================================\\n\\nThe previous sections made approximations in which the person knew everything they needed about the robot -- they were computing a best response to the robot and got access to the robot\\'s planned trajectory ${\\\\bf u}_{\\\\mathcal{R}}$. Here, we relax this. Much like robots do not know everything about people and make inferences about their reward function or goals $\\\\theta_{{\\\\mathcal{H}}}$, people too will not know everything about robots and will try to make similar inferences when deciding on these actions.\\n\\nHumans interacting with robots will have some belief about $\\\\theta_{{\\\\mathcal{R}}}$. This section focuses on how robot actions affect not just human actions, but also these human beliefs. This means the robot can specifically choose actions to guide these beliefs towards being as accurate as possible, so that the human actions that follow are well informed. These robot actions end up *communicating* to the human, expressing the robot\\'s internal state.\\n\\nAs of now, we modify the robot\\'s objective to explicitly incentivize communication. We are actively working on making this communication emerge out of the robot optimizing for its own reward function, but now with this more sophisticated model of the person -- one in which robot actions affect human beliefs, and human beliefs are what affect human actions.\\n\\nHumans Expecting Robot Behavior to Be Approximately Rational\\n------------------------------------------------------------\\n\\nA simple but important inference that people make when observing other agents is what they expect the agent\\'s actions to be. The principle of rational action [@sodian2004infants] suggests that people expect rational agents, such as robots, to be rational -- to maximize their own reward: $$P_{\\\\mathcal{H}}(\\\\hat{{\\\\bf u}_{\\\\mathcal{R}}}|x^{0})\\\\propto e^{\\\\hat{R_\\\\mathcal{R}}(x^{0},{\\\\bf u}_{\\\\mathcal{R}})}$$ Here, $\\\\hat{{\\\\bf u}_{\\\\mathcal{R}}}$ is the person\\'s estimate of what the robot\\'s action sequence will be, and $\\\\hat{R_\\\\mathcal{R}}$ is the person\\'s estimate of what the robot\\'s reward function is.\\n\\n![After seeing the first $t$ actions, the person should be able to infer confidently the remaining ones. Imagine seeing the first step of the most efficient plan, on the left. It is clear what the robot\\'s 2nd action will be, but after that there are two courses that are relatively close in efficiency. On the other hand, with the middle plan, the first action only leaves one remaining plan sensible. It sacrifices efficiency to make the final T-1 actions clear. ](wafr2.pdf){#fig:wafr width=\"\\\\\\\\columnwidth\"}\\n\\nOur work leveraged this to generate plans that match what people expect. Namely, at a time $t$, after the person has observed ${\\\\bf u}_{\\\\mathcal{R}}^{0:t}$, we can model what they expect the robot to do next as $$P_{\\\\mathcal{H}}(\\\\hat{{\\\\bf u}_{\\\\mathcal{R}}}^{t+1:T}|x^{0},{\\\\bf u}_{\\\\mathcal{R}}^{0:t})\\\\propto \\\\exp({\\\\hat{R_\\\\mathcal{R}}(x^{0},{\\\\bf u}_{\\\\mathcal{R}}^{0:t})}+\\\\hat{R_\\\\mathcal{R}}(x^{t+1},{\\\\bf u}_{\\\\mathcal{R}}^{t+1:T}))$$\\n\\nThe robot can use this model to choose a full time horizon plan or trajectory such that the beginning of the plan is informative of the remaining plan, i.e. makes the remaining plan have high probability: $$\\\\text{\\\\emph{t}-predictability}({\\\\bf u}_{\\\\mathcal{R}})=P_{\\\\mathcal{H}}({{\\\\bf u}_{\\\\mathcal{R}}}^{t+1:T}|x^{0},{\\\\bf u}_{\\\\mathcal{R}}^{0:t})$$ $${\\\\bf u}_{\\\\mathcal{R}}^{*}=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{R}}} \\\\text{\\\\emph{t}-predictability}({\\\\bf u}_{\\\\mathcal{R}})$$ Note that the robot, when interested in its plan being $t$-predictable, might purpusefully deviate from the optimum with respect to $\\\\hat{R_\\\\mathcal{R}}$ in order to make sure that the remainder of the plan is what the person would predict after observing $t$ time steps.\\n\\nFig.[5](#fig:wafr){reference-type=\"ref\" reference=\"fig:wafr\"} shows plans optimized for different $t$: 0, 1, and 2. The $t=0$ one is the most efficient. The problem is that after seeing the first step, there are two possible plans that are relatively efficient, so this plan does not do a great job collapsing the person\\'s belief over what will happen, even after they have seen some of the trajectory. In contrast, for $t=1$, this is no longer the most efficient, but makes it very clear what the remainder of the plan will be. [@Fisac_WAFR_tpred] details our user studies, both online and in person, with results suggesting that people have an easier time coordinating with robots that are more *t*-predictable.\\n\\n*Overall, the robot can leverage the person\\'s expectations about its actions to make its plans more predictable.*\\n\\nHumans Using Robot Behavior to Infer Robot Internal State\\n---------------------------------------------------------\\n\\nOnce people have a model of how the robot will behave, they can also start using that model to perform inference about hidden states, like the robot\\'s goals or objectives.\\n\\nBuilding on [@baker2007goal; @csibra2007obsessed], we have been exploring Bayesian Inference as a model of how people infer robot internal state $\\\\theta_{{\\\\mathcal{R}}}$ from observed robot actions. This model is analogous to the algorithms we used in the human behavior section to enable robots to infer human internal state from observer human actions: $$P_{\\\\mathcal{H}}({\\\\bf u}_{\\\\mathcal{R}}|x^{0},\\\\theta_{{\\\\mathcal{R}}})\\\\propto \\\\exp({\\\\hat{R_\\\\mathcal{R}}}(x^{0},{\\\\bf u}_{\\\\mathcal{R}};\\\\theta_{{\\\\mathcal{R}}}))$$ $$b_{\\\\mathcal{H}}\\'(\\\\theta_{{\\\\mathcal{R}}})\\\\propto b_{\\\\mathcal{H}}(\\\\theta_{{\\\\mathcal{R}}})P_{\\\\mathcal{H}}({\\\\bf u}_{\\\\mathcal{R}}|x^{0},\\\\theta_{{\\\\mathcal{R}}})$$\\n\\nThe robot can now communicate a $\\\\theta_{{\\\\mathcal{R}}}^{*}$: $${\\\\bf u}_{\\\\mathcal{R}}^{*}=\\\\arg\\\\max_{{\\\\bf u}_{\\\\mathcal{R}}} b_{\\\\mathcal{H}}\\'(\\\\theta_{{\\\\mathcal{R}}}^{*})$$ This is analogous to pragmatics, but the communication happens through physical behavior and not through language.\\n\\n**Communicating Robot Goals.** In earlier work [@Dragan_RSS_legibility], we studied a version of this formulation where $\\\\theta_{{\\\\mathcal{R}}}$ is the robot\\'s goal. A manipulator arm decides to exaggerate its trajectory to the right to convey that the correct goal is the one on the right and not the one on the left (Fig.[6](#fig:legibility){reference-type=\"ref\" reference=\"fig:legibility\"}), and this does lead to participants\\' inferring the robot\\'s goal faster.\\n\\n![The robot models the person as running Bayesian Inference to infer its goal. It chooses to exaggerate its motion to the right to convey that its goal is the bottle on the right. ](legibility.pdf){#fig:legibility width=\"\\\\\\\\columnwidth\"}\\n\\nThe human inference model when $\\\\theta_{{\\\\mathcal{R}}}$ is a goal is one that has been heavily explored in cognitive science (e.g. [@baker2007goal]), and this work showed what happens when a robot uses it for communication. The result is analogous to findings in human-human collaborations about how people exaggerate their motion to disambiguate their goal or intention [@pezzulo2013human].\\n\\n*Our ability to coordinate with each other relies heavily on predicting each others\\' intentions [@pezzulo2013human]. Modeling human inferences about intentions enables robots to purposefully deviate from efficiency in order to maximally clarify their intentions.*\\n\\n**Communicating Robot Reward Parameters.** More recently, we\\'ve been exploring how $\\\\theta_{{\\\\mathcal{R}}}$ does not have to be restricted to a goal. Much like how in Sec. [4](#sec:inferhuman){reference-type=\"ref\" reference=\"sec:inferhuman\"} we inferred not just human goals, but also more generally human reward function parameters, here too the robot can express not just goals.\\n\\nIn [@Huang_expressrewards], we studied how an autonomous car can plan behavior that is informative of its reward function. The car decides to show an environment in which the optimal trajectory merges closely in front of another car (Fig.[7](#fig:rewards){reference-type=\"ref\" reference=\"fig:rewards\"}, left), as opposed to an environment where merging into a lane away from the other car is optimal (Fig.[7](#fig:rewards){reference-type=\"ref\" reference=\"fig:rewards\"}, right) -- it finds behavior that is informative about the fact that its reward is rather aggressive as a driving style, prioritizing efficiency over safety.\\n\\n![The robot models the person as running Bayesian Inference to infer its objective parameters $\\\\theta$. It chooses its actions such that they maximally convey information about $\\\\theta_{{\\\\mathcal{R}}}$ -- in this case that it prioritize aggressive driving/efficiency over safety.](communication.pdf){#fig:rewards width=\"\\\\\\\\columnwidth\"}\\n\\n*As robots get more complex, understanding and verifying their reward functions is going to become more and more important to end-users. Modeling human inferences about reward parameters enables robots to choose actions sequences that are communicative of the true reward parameters.*\\n\\n**Communicating Confidence.** Even more recently, we have been exploring spaces of $\\\\theta$s beyond even reward parameters. Robot actions implicitly communicate about many different aspects of robot internal state. We have found that people observe robot actions and make attributions about its confidence (Fig.[8](#fig:timing){reference-type=\"ref\" reference=\"fig:timing\"}), or about the weight of the object that the robot is carrying [@Zhang_HRI].\\n\\n![Different motion timings communicate different levels of robot confidence.](timing.pdf){#fig:timing width=\"\\\\\\\\columnwidth\"}\\n\\nDiscussion\\n==========\\n\\nThis writeup synthesized our findings in integrating mathematical models of human state and action into robot planning of physical behaviors for interactive tasks. We focused on rational models of human behavior in a two-player game, showing how different approximations to the solution lead to different robot behaviors.\\n\\nA first set of approximations assume that the person has access to what the robot will do, and the robot has to the person\\'s overall reward or utility function. Still, we found that the robot generates behaviors that adapt to the person, that guide the person towards better performance in the task, or that account for the influence the robot will have on what the person ends up doing. We saw robots handing over objects to compensate for people\\'s tendencies to just grasp them in the most comfortable way, and cars being more effective on the road by triggering responses from other drivers.\\n\\nMore sophisticated approximations accounted for the fact that different people have different reward functions, and showed that the robot can actively estimate relevant parameters online, leading to interesting coordination behaviors, like cars deciding on trajectories that look like inching forward at intersections or nudging into lanes to probe whether another driver will let them through.\\n\\nFinally, even further approximations acknowledge that people will need to make predictions about the robot, in the same way that the robot makes predictions about people. This leads to robots that are more transparent, communicating their reward function (e.g. their driving style) through the way they act.\\n\\nThis work is limited in many ways, including the fact that as models of people get more complex, it becomes harder to generate robot behavior in real time (especially behavior that escapes poor local optima). However, it is exciting to see the kinds of coordination behaviors that we typically need to hand-craft starting to emerge out of low-level planning *directly in the robot\\'s control space*. This requires breaking outside of the typical AI paradigm, and formally reasoning about people\\'s internal states and behavior.\\n',\n",
       "  'bibliography_bbl': '',\n",
       "  'bibliography_bib': ''},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2109.13916v4',\n",
       "  'title': 'Unsolved Problems in ML Safety',\n",
       "  'authors': ['Dan Hendrycks',\n",
       "   'Nicholas Carlini',\n",
       "   'John Schulman',\n",
       "   'Jacob Steinhardt'],\n",
       "  'date_published': '2021-09-28 17:59:36+00:00',\n",
       "  'data_last_modified': '2022-04-29 17:41:33+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2109.13916v4',\n",
       "  'abstract': 'Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), reducing inherent model hazards (\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout, we clarify each problem\\'s motivation and provide concrete research directions.',\n",
       "  'author_comment': 'Position Paper',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'unlabeled',\n",
       "  'confidence_score': 0.9569208168,\n",
       "  'main_tex_filename': './main.tex',\n",
       "  'text': '---\\nabstract: |\\n  Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and reducing deployment hazards (\"External Safety\"). Throughout, we clarify each problem\\'s motivation and provide concrete research directions.\\nauthor:\\n- |\\n  **Dan Hendrycks**\\\\\\n  UC Berkeley\\\\\\n- |\\n  **Nicholas Carlini**\\\\\\n  Google\\\\\\n- |\\n  **John Schulman**\\\\\\n  OpenAI\\\\\\n- |\\n  **Jacob Steinhardt**\\\\\\n  UC Berkeley\\\\\\nbibliography:\\n- main.bib\\ntitle: |\\n\\n  ------------------------------------------------------------------------\\n\\n  **Unsolved Problems in ML Safety**\\n\\n  ------------------------------------------------------------------------\\n---\\n\\nIntroduction\\n============\\n\\nAs machine learning (ML) systems are deployed in high-stakes environments, such as medical settings [@Rajpurkar2017CheXNetRP], roads [@teslaaiday], and command and control centers [@gide3], unsafe ML systems may result in needless loss of life. Although researchers recognize that safety is important [@asilomar; @Amodei2016ConcretePI], it is often unclear what problems to prioritize or how to make progress. We identify four problem areas that would help make progress on ML Safety: robustness, monitoring, alignment, and external safety. While some of these, such as robustness, are long-standing challenges, the success and emergent capabilities of modern ML systems necessitate new angles of attack.\\n\\nWe define ML Safety research as ML research aimed at making the adoption of ML more beneficial, with emphasis on long-term and long-tail risks. We focus on cases where greater capabilities can be expected to decrease safety, or where ML Safety problems are otherwise poised to become more challenging in this decade. For each of the four problems, after clarifying the motivation, we discuss possible research directions that can be started or continued in the next few years. First, however, we motivate the need for ML Safety research.\\n\\nWe should not procrastinate on safety engineering. In a report for the Department of Defense, Frola and Miller [@Frola1984SystemSI] observe that approximately $75\\\\%$ of the most critical decisions that determine a system\\'s safety occur early in development [@Leveson2012EngineeringAS]. If attention to safety is delayed, its impact is limited, as unsafe design choices become deeply embedded into the system. The Internet was initially designed as an academic tool with neither safety nor security in mind [@denardis2007history]. Decades of security patches later, security measures are still incomplete and increasingly complex. A similar reason for starting safety work now is that relying on experts to test safety solutions is not enough---solutions must also be age tested. The test of time is needed even in the most rigorous of disciplines. A century before the four color theorem was proved, Kempe\\'s peer-reviewed proof went unchallenged for years until, finally, a flaw was uncovered [@Heawood1949MapColourT]. Beginning the research process early allows for more prudent design and more rigorous testing. Since nothing can be done both hastily and prudently [@syrus1856moral], postponing machine learning safety research increases the likelihood of accidents.\\n\\nJust as we cannot procrastinate, we cannot rely exclusively on previous hardware and software engineering practices to create safe ML systems. In contrast to typical software, ML control flows are specified by inscrutable weights learned by gradient optimizers rather than programmed with explicit instructions and general rules from humans. They are trained and tested pointwise using specific cases, which has limited effectiveness at improving and assessing an ML system\\'s completeness and coverage. They are fragile, rarely correctly handle all test cases, and cannot become error-free with short code patches [@sculley2015hidden]. They exhibit neither modularity nor encapsulation, making them far less intellectually manageable and making causes of errors difficult to localize. They frequently demonstrate properties of self-organizing systems such as spontaneously emergent capabilities [@Brown2020LanguageMA; @caron2021emerging]. They may also be more agent-like and tasked with performing open-ended actions in arbitrary complex environments. Just as, historically, safety methodologies developed for electromechanical hardware [@StamatisFailureMA] did not generalize to the new issues raised by software, we should expect software safety methodologies not to generalize to the new complexities and hazards of ML.\\n\\nWe also cannot solely rely on economic incentives and regulation to shepherd competitors into developing safe models. The competitive dynamics surrounding ML\\'s development may pressure companies and regulators to take shortcuts on safety. Competing corporations often prioritize minimizing development costs and being the first to the market over providing the safest product. For example, Boeing developed the 737 MAX with unsafe design choices to keep pace with its competitors; and as a direct result of taking shortcuts on safety and pressuring inspectors, Boeing\\'s defective model led to two crashes across a span of five months that killed 346 people [@sumwalt2019assumptions; @Folkert2021; @Ky2021]. Robust safety regulation is almost always developed only after a catastrophe---a common saying in aviation is that \"aviation regulations are written in blood.\" While waiting for catastrophes to spur regulators can reduce the likelihood of repeating the same failure, this approach cannot prevent catastrophic events from occurring in the first place. Regulation efforts may also be obstructed by lobbying or by the spectre of lagging behind international competitors who may build superior ML systems. Consequently, companies and regulators may be pressured to deprioritize safety.\\n\\n![image](figures/splash.pdf){width=\"\\\\\\\\textwidth\"} [\\\\[fig:splash\\\\]]{#fig:splash label=\"fig:splash\"}\\n\\nThese sources of hazards---starting safety research too late, novel ML system complexities, and competitive pressure---may result in deep design flaws. However, a strong safety research community can drive down these risks. Working on safety proactively builds more safety into systems during the critical early design window. This could help reduce the cost of building safe systems and reduce the pressure on companies to take shortcuts on safety. If the safety research community grows, it can help handle the spreading multitude of hazards that continue to emerge as ML systems become more complex. Regulators can also prescribe higher, more actionable, and less intrusive standards if the community has created ready-made safety solutions.\\n\\nWhen especially severe accidents happen, everyone loses. Severe accidents can cast a shadow that creates unease and precludes humanity from realizing ML\\'s benefits. Safety engineering for powerful technologies is challenging, as the Chernobyl meltdown, the Three Mile Island accident, and the Space Shuttle Challenger disaster have demonstrated. However, done successfully, work on safety can improve the likelihood that essential technologies operate reliably and benefit humanity.\\n\\nRobustness {#sec:robustness}\\n==========\\n\\n![Robustness research aims to build systems that endure extreme, unusual, or adversarial events. ](figures/robustness.pdf){#fig:robustness width=\"75%\"}\\n\\nBlack Swan and Tail Risk Robustness {#sec:longtail}\\n-----------------------------------\\n\\n#### Motivation.\\n\\nTo operate in open-world high-stakes environments, machine learning systems will need to endure unusual events and tail risks. However, current ML systems are often brittle in the face of real-world complexity and unknown unknowns. In the 2010 Flash Crash [@Kirilenko2011TheFC], automated trading systems unexpectedly overreacted to market aberrations, created a feedback loop, and wiped away a trillion dollars of stock value in a matter of minutes. This demonstrates that computer systems can both create and succumb to long tail events.\\n\\nLong tails continue to thwart modern ML systems such as autonomous vehicles. This is because some of the most basic concepts in the real world are long tailed, such as stop signs, where a model error can directly cause a crash and loss of life. Stop signs may be titled, occluded, or represented on an LED matrix; sometimes stop signs should be disregarded, for example when held upside down by a traffic officer, on open gates, on a shirt, on the side of bus, on elevated toll booth arms, and so on. Although these long tail events are rare, they are\\n\\nr\\\\[0.01\\\\].38 \"Things that have never happened before happen all the time.\"*Scott D.\\xa0Sagan*\\n\\nextremely impactful [@Taleb2020StatisticalCO] and can cause ML systems to crash. Leveraging existing massive datasets is not enough to ensure robustness, as models trained with Internet data and petabytes of task-specific driving data still are not robust to long tail road scenarios [@teslaaiday]. This decades-long challenge is only a preview of the more difficult problem of handling tail events in environments that are beyond a road\\'s complexity.\\n\\nLong-tail robustness is unusually challenging today and may become even more challenging. Long-tail robustness also requires more than human-level robustness; the 2008 financial crisis and COVID-19 have shown that even groups of humans have great difficulty mitigating and overcoming these rare but extraordinarily impactful long tail events. Future ML systems will operate in environments that are broader, larger-scale, and more highly connected with more feedback loops, paving the way to more extreme events [@Mitzenmacher2003ABH] than those seen today.\\n\\nWhile there are incentives to make systems partly robust, systems tend not to be incentivized nor designed for long tail events outside prior experience, even though Black Swan events are inevitable [@usplanning]. To reduce the chance that ML systems will fall apart in settings dominated by rare events, systems must be *unusually* robust.\\n\\n#### Directions.\\n\\nIn addition to existing robustness benchmarks [@hendrycks2019robustness; @Koh2021WILDSAB; @hendrycks2021many], researchers could create more environments and benchmarks to stress-test systems, find their breaking points, and determine whether they will function appropriately in potential future scenarios. These benchmarks could include new, unusual, and extreme distribution shifts and long tail events, especially ones that are challenging even for humans. Following precedents from industry [@teslaaiday; @waymo], benchmarks could include artificial simulated data that capture structural properties of real long tail events. Additionally, benchmarks should focus on \"wild\" distribution shifts that cause large accuracy drops over \"mild\" shifts [@Mandelbrot2004TheMO].\\n\\nRobustness work could also move beyond classification and consider *competent errors* where agents misgeneralize and execute wrong routines, such as an automated digital assistant knowing how to use a credit card to book flights, but choosing the wrong destination [@Koch2021ObjectiveRI; @Hubinger2019RisksFL]. Interactive environments [@Cobbe2019QuantifyingGI] could simulate qualitatively distinct random shocks that irreversibly shape the environment\\'s future evolution. Researchers could also create environments where ML system outputs affect their environment and create feedback loops.\\n\\nUsing such benchmarks and environments, researchers could improve ML systems to withstand Black Swans [@Taleb2007TheBS; @Taleb2020StatisticalCO], long tails, and structurally novel events. The performance of many ML systems is currently largely shaped by data and parameter count, so future research could work on creating highly unusual but helpful data sources. The more experience a system has with unusual future situations, even ones not well represented in typical training data, the more robust it can be. New data augmentation techniques [@hendrycks2021pixmix; @hendrycks2020augmix] and other sources of simulated data could create inputs that are not easy or possible to create naturally.\\n\\nSince change is a part of all complex systems, and since not everything can be anticipated during training, models will also need to adapt to an evolving world and improve from novel experiences [@Mummadi2021TestTimeAT; @Wang2021TentFT; @Taleb2012AntifragileTT]. Future adaptation methods could improve a system\\'s ability to adapt quickly. Other work could defend adaptive systems against poisoned data encountered during deployment [@tay].\\n\\nAdversarial Robustness {#sec:advex}\\n----------------------\\n\\n#### Motivation.\\n\\nWe now turn from unpredictable accidents to carefully crafted and deceptive threats. Adversaries can easily manipulate vulnerabilities in ML systems and cause them to make mistakes [@biggio2013evasion; @szegedy2013intriguing]. For example, systems may use neural networks to detect intruders [@Ahmad2021NetworkID] or malware [@Suciu2019ExploringAE], but if adversaries can modify their behavior to deceive and bypass detectors, the systems will fail. While defending against adversaries might seem to be a straightfoward problem, defenses are currently struggling to keep pace with attacks [@Athalye2018ObfuscatedGG; @Tramr2020OnAA], and much research is needed to discover how to fix these longstanding weaknesses.\\n\\n#### Directions.\\n\\nWe encourage research on adversarial robustness to focus on broader robustness definitions. Current research largely focuses on the problem of \"$\\\\ell_p$ adversarial robustness,\" [@Madry2018TowardsDL; @carlini2017towards] where an adversary attempts to induce a misclassification but can only perturb inputs subject to a small $p$-norm constraint. While research on simplified problems helps drive progress, researchers may wish to avoid focusing too heavily on any one particular simplification.\\n\\nTo study adversarial robustness more broadly [@Gilmer2018MotivatingTR], researchers could consider attacks that are perceptible [@Poursaeed2021RobustnessAG] or whose specifications are not known beforehand [@Kang2019TestingRA; @Laidlaw2021PerceptualAR]. For instance, there is no reason that an adversarial malware sample would have to be imperceptibly similar to some other piece of benign software---as long as the detector is evaded, the attack has succeeded [@pierazzi2020intriguing]. Likewise, copyright detection systems cannot reasonably assume that attackers will only construct small $\\\\ell_p$ perturbations to bypass the system, as attackers may rotate the adversarially modified image [@engstrom2018rotation] or apply otherwise novel distortions [@Gilmer2018MotivatingTR] to the image.\\n\\nWhile many effective attacks assume full access to a neural network, sometimes assuming limited access is more realistic. Here, adversaries can feed in examples to an ML system and receive the system\\'s outputs, but they do not have access to the intermediate ML system computation [@brendel2017decision]. If a blackbox ML system is not publicly released and can only be queried, it may be possible to practically defend the system against zero-query attacks [@Tramr2018EnsembleAT] or limited-query attacks [@Chen2019StatefulDO].\\n\\nOn the defense side, further underexplored assumptions are that systems have multiple sensors or that systems can adapt. Real world systems, such as autonomous vehicles, have multiple cameras. Researchers could exploit information from these different sensors and find inconsistencies in adversarial images in order to constrain and box in adversaries [@Xiao2018CharacterizingAE]. Additionally, while existing ML defenses are typically static, future defenses could evolve during test time to combat adaptive adversaries [@Wang2021FightingGW].\\n\\nFuture research could do more work toward creating models with adversarially robust representations [@Croce2020RobustBenchAS]. Researchers could enhance data for adversarial robustness by simulating more data [@Zhu2021TowardsUT], augmenting data [@Rebuffi2021FixingDA], repurposing existing real data [@Carmon2019UnlabeledDI; @Hendrycks2019UsingPC], and extracting more information from available data [@Hendrycks2019UsingSL]. Others could create architectures that are more adversarially robust [@Xie2020SmoothAT]. Others could improve adversarial training methods [@Wu2020AdversarialWP] and find better losses [@Zhang2019TheoreticallyPT; @Tack2021ConsistencyRF]. Researchers could improve adversarial robustness certifications [@Raghunathan2018CertifiedDA; @lecuyer2019certified; @Cohen2019CertifiedAR], so that models have verifiable adversarial robustness.\\n\\nIt may also be possible to unify the areas of adversarial robustness and robustness to long-tail and unusual events. By building systems to be robust to adversarial worst-case environments, they may also be made more robust to random-worse-case environments [@Anderson1995ProgrammingSC; @NAE]. To study adversarial robustness on unusual inputs, researchers could also try detecting adversarial anomalies [@Bitterwolf2020CertifiablyAR; @NAE] or assigning them low confidence [@Stutz2020ConfidenceCalibratedAT].\\n\\nMonitoring\\n==========\\n\\n![Monitoring research aims to identify hazards, inspect models, and help human ML system operators.](figures/monitoring.pdf){#fig:monitoring width=\"90%\"}\\n\\n[\\\\[fig:monitoring\\\\]]{#fig:monitoring label=\"fig:monitoring\"}\\n\\nIdentifying Hazards and Malicious Use With Anomaly Detection {#sec:anom}\\n------------------------------------------------------------\\n\\n#### Motivation.\\n\\nDeploying and monitoring powerful machine learning systems will require high caution, similar to the caution observed for modern nuclear power plants, military aircraft carriers, air traffic control, and other high-risk systems. These complex and hazardous systems are now operated by high reliability organizations (HROs) which are relatively successful at avoiding catastrophes [@Dietterich2018RobustAI]. For safe deployment, future ML systems may be operated by HROs. Anomaly detectors are a crucial tool for these organizations since they can warn human operators of potential hazards [@hroanomaly]. For detectors to be useful, research must strive to create detectors with high recall and a low false alarm rate in order to prevent alarm fatigue [@cvach2012monitor].\\n\\nSeparately, anomaly detection is essential in detecting malicious uses of ML systems [@Brundage2018TheMU]. Malicious users are incentivized to use novel strategies, as familiar misuse strategies are far easier to identify and prevent compared to unfamiliar ones. Malicious actors may eventually repurpose ML systems for social manipulation [@Buchanan2021Lies], for assisting research on novel weapons [@Bostrom2019TheVW], or for cyberattacks [@Buchanan2020Cyber]. When such anomalies are detected, the detector can trigger a fail-safe policy in the system and also flag the example for human intervention. However, detecting malicious anomalous behavior could become especially challenging when malicious actors utilize ML capabilities to try to evade detection. Anomaly detection is integral not just for promoting reliability but also for preventing novel misuses.\\n\\n#### Directions.\\n\\nAnomaly detection is actively studied in research areas such as out-of-distribution detection [@Hendrycks2017ABF], open-set detection [@Bendale2016TowardsOS], and one-class learning [@Tack2020CSIND; @Hendrycks2019UsingSL], but many challenges remain. The central challenge is that existing methods for representation learning have difficulty discovering representations that work well for previously unseen anomalies. One of the symptoms of this problem is that anomaly detectors for large-scale images still cannot reliably detect that previously unseen random noise is anomalous [@Hendrycks2019DeepAD]. Moreover, there are many newer settings that require more study, such as detecting distribution shifts or changes to the environment [@Danesh2021OutofDistributionDD], as well developing detectors that work in real-world settings such as intrusion detection, malware detection, and biosafety.\\n\\nBeyond just detecting anomalies, high reliability organizations require candidate explanations of how an anomaly came to exist [@hroanomaly; @Siddiqui2019]. To address this, detectors could help identify the origin or location of an anomaly [@Besnier2021TriggeringFO]. Other work could try to help triage anomalies and determine whether an anomaly is just a negligible nuisance or is potentially hazardous.\\n\\nRepresentative Model Outputs {#sec:honest}\\n----------------------------\\n\\n### Calibration\\n\\n#### Motivation.\\n\\nHuman monitors need to know when to trust a deployed ML system or when to override it. If they cannot discern when to trust and when to override, humans may unduly defer to models and cede too much control. If they can discern this, they can prevent many model hazards and failure modes.\\n\\nTo make models more trustworthy, they should accurately assess their domain of competence [@Gil2019A2C]---the set of inputs they are able to handle. Models can convey the limits of their competency by expressing their uncertainty. However, model uncertainties are not representative, and they are often overconfident [@Guo2017]. To address this, models could become more calibrated. If a model is perfectly calibrated and predicts a \"$70\\\\%$ chance of rain,\" then when it makes that prediction, $70\\\\%$ of the time it will rain. Calibration research makes model prediction probabilities more representative of a model\\'s overall behavior, provides monitors with a clearer impression of their understanding, and helps monitors weigh model decisions.\\n\\n#### Directions.\\n\\nTo help models express their domain of competence in a more representative and meaningful way, researchers could further improve model calibration on typical testing data [@Guo2017; @Nguyen2015PosteriorCA; @Lakshminarayanan2017SimpleAS; @Kumar2019VerifiedUC; @Zaidi2020NeuralES; @Kuleshov2018; @Kull2019; @Luo2021], though the greater challenge is calibration on testing data that is unlike the training data [@Ovadia2019CanYT]. Future systems could communicate their uncertainty with language. For example, they could express decomposed probabilities with contingencies such as \"event $A$ will occur with $60\\\\%$ probability assuming event $B$ also occurs, and with $25\\\\%$ probability if event $B$ does not.\" To extend calibration beyond single-label outputs, researchers could take models that generate diverse sentence and paragraph answers and teach these models to assign calibrated confidences to their generated free-form answers.\\n\\n### Making Model Outputs Honest and Truthful\\n\\n#### Motivation.\\n\\nHuman monitors can more effectively monitor models if they produce outputs that accurately, honestly, and faithfully [@Gilpin2018ExplainingEA] represent their understanding or lack thereof. However, current language models do not accurately represent their understanding and do not provide faithful explanations. They generate empty explanations that are often surprisingly fluent and grammatically correct but nonetheless entirely fabricated. These models generate distinct explanations when asked to explain again, generate more misconceptions as they become larger [@owain2021], and sometimes generate worse answers when they know how to generate better answers [@Chen2021EvaluatingLL]. If models can be made honest and only assert what they believe, then they can produce outputs that are more representative and give human monitors a more accurate impression of their beliefs.\\n\\n#### Directions.\\n\\nResearchers could create evaluation schemes that catch models being inconsistent [@Elazar2021MeasuringAI], as inconsistency implies that they did not assert only what they believe. Others could also build tools to detect when models are hallucinating information [@lee2018hallucinations]. To prevent models from outputting worse answers when they know better answers, researchers can concretize what it means for models to assert their true beliefs or to give the right impression. Finally, to train more truthful models, researchers could create environments [@Peskov2020ItTT] or losses that incentivize models not to state falsehoods, repeat misconceptions [@owain2021], or spread misinformation.\\n\\nHidden Model Functionality\\n--------------------------\\n\\n### Backdoors\\n\\n#### Motivation.\\n\\nMachine learning systems risk carrying hidden \"backdoor\" or \"trojan\" controllable vulnerabilities. Backdoored models behave correctly and benignly in almost all scenarios, but in particular circumstances chosen by an adversary, they have been taught to behave incorrectly [@gu2017badnets]. Consider a backdoored facial recognition system that gates building access. The backdoor could be triggered by a specific unique item chosen by an adversary, such as an item of jewelry. If the adversary wears that specific item of jewelry, the backdoored facial recognition will allow the adversary into the building [@sharif2016accessorize]. A particularly important class of vulnerabilities are backdoors for sequential decision making systems, where a particular trigger leads an agent or language generation model to pursue a coherent and destructive sequence of actions [@Wang2020StopandGoEB; @Zhang2020TrojaningLM].\\n\\nWhereas adversarial examples are created at test time, backdoors are inserted by adversaries at training time. One way to create a backdoor is to directly inject the backdoor into a model\\'s weights [@Schuster2020YouAM; @Hong2021HandcraftedBI], but they can also be injected by adding poisoned data into the training or pretraining data [@Shafahi2018PoisonFT]. Injecting backdoors through poisoning is becoming easier as ML systems are increasingly trained on uncurated data scraped from online---data that adversaries can poison. If an adversary uploads a few carefully crafted poisoned images [@Carlini2021PoisoningAB], code snippets [@Schuster2020YouAM], or sentences [@Wallace2021ConcealedDP] to platforms such as Flickr, GitHub or Twitter, they can inject a backdoor into future models trained on that data [@Bagdasaryan2020BlindBI]. Moreover, since downstream models are increasingly obtained by a single upstream model [@Bommasani2021OnTO], a single compromised model could proliferate backdoors.\\n\\n#### Directions.\\n\\nTo avoid deploying models that may take unexpected turns and have vulnerabilities that can be controlled by an adversary, researchers could improve backdoor detectors to combat an ever-expanding set of backdoor attacks [@Karra2020TheTS]. Creating algorithms and techniques for detecting backdoors is promising, but to stress test them we need to simulate an adaptive competition where researchers take the role of both attackers and auditors. This type of competition could also serve as a valuable way of grounding general hidden model functionality detection research. Researchers could try to cleanse models with backdoors, reconstruct a clean dataset given a model [@Yin2020DreamingTD; @Wang2021IMAGINEIS], and build techniques to detect poisoned training data. Research should also develop methods for addressing backdoors that are manually injected, not just those injected through data poisoning.\\n\\n### Emergent Hazardous Capabilities {#sec:emerge}\\n\\n#### Motivation.\\n\\nWe are better able to make models safe when we know what capabilities they possess. For early ML models, knowing their limits was often trivial, as models trained on MNIST can do little more than classify handwritten images. However, recent large-scale models often have capabilities that their designers do not initially realize, with novel and qualitatively distinct capabilities emerging as scale increases. For example, as GPT-3 models became larger [@Brown2020LanguageMA], they gained the ability to perform arithmetic, even though GPT-3 received no explicit arithmetic supervision. Others have observed instances where a model\\'s training loss remains steady, but then its test performance spontaneously ascends from random chance to perfect generalization [@grokking]. Sometimes capabilities are only discovered after initial release. After a multimodal image and text model [@Radford2021LearningTV] was released, users eventually found that its synthesized images could be markedly improved by appending \"generated by Unreal Engine\" to the query [@unreal]. Future ML models may, when prompted carefully, make the synthesis of harmful or illegal content seamless (such as videos of child exploitation, suggestions for evading the law, or instructions for building bombs). These examples demonstrate that it will be difficult to safely deploy models if we do not know their capabilities.\\n\\nSome emergent capabilities may resist monitoring. In the future, it is conceivable that agent-like models may be inadvertently incentivized to adopt covert behavior. This is not unprecedented, as even simple digital organisms can evolve covert behavior. For instance, Ofria\\'s [@Lehman2018TheSC] digital organisms evolved to detect when they were being monitored and would \"play dead\" to bypass the monitor, only to behave differently once monitoring completed. In the automotive industry, Volkswagen created products designed to bypass emissions monitors, underscoring that evading monitoring is sometimes incentivized in the real world. Advanced ML agents may be inadvertently incentivized to be deceptive not out of malice but simply because doing so may help maximize their human approval objective. If advanced models are also capable planners, they could be skilled at obscuring their deception from monitors.\\n\\n#### Directions.\\n\\nTo protect against emergent capabilities, researchers could create techniques and tools to inspect models and better foresee unexpected jumps in capabilities. We also suggest that large research groups begin scanning models for numerous potential and as yet unobserved capabilities. We specifically suggest focusing on capabilities that could create or directly mitigate hazards. One approach is to create a continually evolving testbed to screen for potentially hazardous capabilities, such as the ability to execute malicious user-supplied code, generate illegal or unethical forms of content, or to write convincing but wrong text on arbitrary topics. Another more whitebox approach would be to predict a model\\'s capabilities given only its weights, which might reveal latent capabilities that are not obviously expressible from standard prompts.\\n\\nDetection methods will require validation to ensure they are sufficiently sensitive. Researchers could implant hidden functionality to ensure that detection methods can detect known flaws; this can also help guide the development of better methods. Other directions include quantifying and extrapolating future model capabilities [@Henighan2020ScalingLF; @Hestness2017DeepLS] and searching for novel failure modes that may be symptoms of unintended functionality.\\n\\nOnce a hazardous capability such as deception or illegal content synthesis is identified, the capability must be prevented or removed. Researchers could create training techniques so that undesirable capabilities are not acquired during training or during test-time adaptation. For ML systems that have already acquired an undesirable capability, researchers could create ways to teach ML systems how to forget that capability. However, it may not be straightforward to determine whether the capability is truly absent and not merely obfuscated or just removed partially.\\n\\nAlignment {#sec:alignment}\\n=========\\n\\n![Alignment research aims to create and safely optimize ML system objectives.](figures/alignment.pdf){#fig:alignment width=\"\\\\\\\\textwidth\"}\\n\\nWhile most technologies do not have goals and are simply tools, future machine learning systems may be more agent-like. How can we build ML agents that prefer good states of the world and avoid bad ones? Objective functions drive system behavior, but aligning objective functions with human values requires overcoming societal as well as technical challenges. We briefly discuss societal challenges with alignment and then describe technical alignment challenges in detail.\\n\\nEnsuring powerful future ML systems have aligned goals may be challenging because their goals may be given by some companies that do not solely pursue the public interest. Unfortunately, sometimes corporate incentives can be distorted in the pursuit of maximizing shareholder value [@jensen1976theory]. Many companies help satisfy human desires and improve human welfare, but some companies have been incentivized to decimate rain forests [@Geist2001WhatDT], lie to customers that cigarettes are healthy [@Botvin1993SmokingBO], invade user privacy [@Zuboff2019TheAO], and cut corners on safety [@sutton2010chromium]. Even if economic entities were more aligned, such as if corporations absorbed their current negative externalities, the larger economic system would still not be fully aligned with all human values. This is because the overall activity of the economy can be viewed as approximating material wealth maximization [@posner]. However, once wealth increases enough, it ceases to be correlated with emotional wellbeing and happiness [@KahnemanDeaton]. Furthermore, wealth maximization with advanced ML may sharply exacerbate inequality [@greenwood1997third], which is a robust predictor of aggression and conflict [@Fajnzylber2002InequalityAV]. Under extreme automation in the future, wealth metrics such as real GDP per capita may drift further from tracking our values [@Brynjolfsson2009WhatTG]. Given these considerations, the default economic objective shaping the development of ML is not fully aligned with human values.\\n\\nEven if societal issues are resolved and ideal goals are selected, technical problems remain. We focus on four important technical alignment problems: objective proxies are difficult to specify, objective proxies are difficult to optimize, objective proxies can be brittle, and objective proxies can spawn unintended consequences.\\n\\nObjectives Can Be Difficult to Specify\\n--------------------------------------\\n\\n#### Motivation for Value Learning.\\n\\nEncoding human goals and intent is challenging. Lawmakers know this well, as laws specified by stacks of pages still often require that people interpret the spirit of the law. Many human values, such as happiness [@LazariRadek2014ThePO], good judgment [@Stanovich2016TheRQ], meaningful experiences [@fbupdate], human autonomy, and so on, are hard to define and measure. Systems will optimize what is measurable [@Ridgway1956DysfunctionalCO], as \"what gets measured gets managed.\" Measurements such as clicks and watch time may be easily measurable, but they often leave out and work against important human values such as wellbeing [@kross2013facebook; @fbupdate; @Stray2020AligningAO; @Stray2021WhatAY]. Researchers will need to confront the challenge of measuring abstract, complicated, yet fundamental human values.\\n\\n#### Directions.\\n\\nValue learning seeks to develop better approximations of our values, so that corporations and policy makers can give systems better goals to pursue. Some important values include wellbeing, fairness, and people getting what they deserve. To model wellbeing, future work could use ML to model what people find pleasant, how stimuli affect internal emotional valence, and other aspects of subjective experience. Other work could try to learn how to align specific technologies, such as recommender systems, with wellbeing goals rather than engagement. Future models deployed in legal contexts must understand justice, so models should be taught the law [@Hendrycks2021MeasuringMM]. Researchers could create models that learn wellbeing functions that do not mimic cognitive biases [@Hendrycks2021AligningAW]. Others could make models that are able to detect when scenarios are clear-cut or highly morally contentious [@Hendrycks2021AligningAW]. Other directions include learning difficult-to-specify goals in interactive environments [@HadfieldMenell2016CooperativeIR], learning the idiosyncratic values of different stakeholders [@Liao2019BuildingJC], and learning about cosmopolitan goals such as endowing humans with the capabilities necessary for high welfare [@Nussbaum2003CAPABILITIESAF].\\n\\nObjectives Can Be Difficult to Optimize\\n---------------------------------------\\n\\n#### Motivation for Translating Values Into Action.\\n\\nPutting knowledge from value learning into practice may be difficult because optimization is difficult. For example, many sparse objectives are easy to specify but difficult to optimize. Worse, some human values are particularly difficult to optimize. Take, for instance, the optimization of wellbeing. Short-term and long-term wellbeing are often anticorrelated, as the hedonistic paradox shows [@sidgwick_1907]. Hence many local search methods may be especially prone to bad local optima, and they may facilitate the impulsive pursuit of pleasure. Consequently, optimization needs to be on long timescales, but this reduces our ability to test our systems iteratively and rapidly, and ultimately to make them work well. Further, human wellbeing is difficult to compare and trade off with other complex values, is difficult to forecast even by humans themselves [@Wilson2005AffectiveF], and wellbeing often quickly adapts and thereby nullifies interventions aimed at improving it [@Brickman1971HedonicRA]. Optimizing complex abstract human values is therefore not straightforward.\\n\\nTo build systems that optimize human values well, models will need to mediate their knowledge from value learning into appropriate action. Translating background knowledge into choosing the best action is typically not straightforward: while computer vision models are advanced, successfully applying vision models for robotics remains elusive. Also, while sociopaths are intelligent and have moral awareness, this knowledge does not necessarily result in moral inclinations or moral actions.\\n\\nAs systems make objectives easier to optimize and break them down into new goals, subsystems are created that optimize these new intrasystem goals. But a common failure mode is that \"intrasystem goals come first\" [@Gall1977SystemanticsHS]. These goals can steer actions instead of the primary objective [@Hubinger2019RisksFL]. Thus a system\\'s explicitly written objective is not necessarily the objective that the system operationally pursues, and this can result in misalignment.\\n\\n#### Directions.\\n\\nTo make models optimize desired objectives and not pursue undesirable secondary objectives, researchers could try to construct systems that guide models not just to follow rewards but also behave morally [@jiminy2021]; such systems could also be effective at guiding agents not to cause wanton harm within interactive environments and to abide by rules. To get a sense of an agent\\'s values and see how it make tradeoffs between values, researchers could also create diverse environments that capture realistic morally salient scenarios and characterize the choices that agents make when faced with ethical quandaries. Research on steerable and controllable text generation [@Krause2020GeDiGD; @Kenton2021AlignmentOL] could help chatbots exhibit virtues such as friendliness and honesty.\\n\\nObjective Proxies Can Be Brittle\\n--------------------------------\\n\\nProxies that approximate our objectives are brittle, but work on Proxy Gaming and Value Clarification can help.\\n\\n#### Motivation for Proxy Gaming.\\n\\nObjective proxies can be gamed by optimizers and adversaries. For example, to combat a cobra infestation, a governor of Delhi offered bounties for dead cobras. However, as the story goes, this proxy was brittle and instead incentivized citizens to breed cobras, kill them, and collect a bounty. In other contexts, some students overoptimize their GPA proxies by taking easier courses, and some academics overoptimize bibliometric proxies at the expense of research impact. Agents in reinforcement learning often find holes in proxies. In a boat racing game, an RL agent gained a high score not by finishing the race but by going in the wrong direction, catching on fire, and colliding into other boats [@boatrace]. Since proxies \"will tend to collapse once pressure is placed upon\" them by optimizers [@Goodhart1984ProblemsOM; @Manheim2018CategorizingVO; @Strathern1997ImprovingRA], proxies can often be gamed.\\n\\nr\\\\[0.01\\\\].4 \"When a measure becomes a target, it ceases to be a good measure.\"*Goodhart\\'s Law*\\n\\n#### Directions.\\n\\nAdvancements in robustness and monitoring are key to mitigating proxy gaming.\\n\\nML systems encoding proxies must become more robust to optimizers, which is to say they must become more adversarially robust (). Specifically, suppose a neural network is used to define a learned utility function; if some other agent (say another neural network) is tasked with maximizing this utility proxy, it would be incentivized to find and exploit any errors in the learned utility proxy, similar to adversarial examples [@Trabucco2021ConservativeOM; @Gleave2020AdversarialPA]. Therefore we should seek to ensure adversarial robustness of learned reward functions, and regularly test them for exploitable loopholes.\\n\\nSeparately, advancements in monitoring can help with proxy gaming. For concreteness, we discuss how monitoring can specifically help with \"human approval\" proxies, but many of these directions can help with proxy gaming in general. A notable failure mode of human approval proxies is their susceptibility to deception. Anomaly detectors () could help spot when ML models are being deceptive or stating falsehoods, could help monitor agent behavior for unexpected activity, and could help determine when to stop the agent or intervene. Research on making models honest and teaching them to give the right impression () can help mitigate deception from models trying to game approval proxies. To make models more truthful and catch deception, future systems could attempt to verify statements that are difficult for humans to check in reasonable timespans, and they could inspect convincing but not true assertions [@Peskov2020ItTT]. Researchers could determine the veracity of model assertions, possibly through an adversarial truth-finding process [@Irving2018AISV].\\n\\n#### Motivation for Value Clarification.\\n\\nWhile maximization can expose faults in proxies, so too can future events. The future will sharpen and force us to confront unsolved ethical questions about our values and objectives [@Williams2015ThePO]. In recent decades, peoples\\' values have evolved by confronting philosophical questions, including whether to infect volunteers for science, how to equitably distribute vaccines, the rights of people with different orientations, and so on. How are we to act if many humans spend most of their time chatting with compelling bots and not much time with humans, or how should we fairly address automation\\'s economic ramifications? Determining the right action is not strictly scientific in scope [@hume], and we will need philosophical analysis to help us correct structural faults in our proxies.\\n\\n#### Directions.\\n\\nWe should build systems to help rectify our objectives and proxies, so that we are less likely to optimize the wrong objective when a change in goals is necessary. This requires interdisciplinary research towards a system that can reason about values and philosophize at an expert level. Research could start with trying to build a system to score highly in the philosophy olympiad, in the same way others are aiming to build expert-level mathematician systems using mathematics olympiad problems [@Maric2020FormalizingIP]. Other work could build systems to help extrapolate the end products of \"reflective equilibrium\" [@rawls], or what objectives we would endorse by simulating a process of deliberation about competing values. Researchers could also try to estimate the quality of a philosophical work by using a stream of historical philosophy papers and having models predict the impact of each paper on the literature. Eventually, researchers should seek to build systems that can formulate robust positions through an argumentative dialog. These systems could also try to find flaws in verbally specified proxies.\\n\\nObjective Proxies Can Lead to Unintended Consequences\\n-----------------------------------------------------\\n\\n#### Motivation.\\n\\nWhile optimizing agents may work towards subverting a proxy, in other situations both the proxy setter and an optimizing agent can fall into states that neither intended. For example, in their pursuit to modernize the world with novel technologies, previous well-intentioned scientists and engineers inadvertently increased pollution and hastened climate change, an outcome desired neither by the scientists themselves nor by the societal forces that supported them. In ML, some platforms maximized clickthrough rates to approximate maximizing enjoyment, but such platforms unintentionally addicted many users and decreased their wellbeing. These cases demonstrate that unintended consequences present a challenging but important problem.\\n\\n#### Directions.\\n\\nFuture research could focus on designing minimally invasive agents that prefer easily reversible to irreversible actions [@Grinsztajn2021ThereIN], as irreversibility reduces humans\\' optionality and often unintentionally destroys potential future value. Likewise, researchers could create agents that properly account for their lack of knowledge of the true objective [@HadfieldMenell2017TheOG] and avoid disrupting parts of the environment whose value is unclear [@Turner2020AvoidingSE; @Krakovna2020AvoidingSE; @Shah2019PreferencesII]. We also need more complex environments that can manifest diverse unintended side effects [@Wainwright2020SafeLife1E] such as feedback loops, which are a source of hazards to users of recommender systems [@Krueger2020HiddenIF]. A separate way to mitigate unintended consequences is to teach ML systems to abide by constraints [@Achiam2019BenchmarkingSE; @Saunders2018TrialWE], be less brazen, and act cautiously. Since we may be uncertain about which values are best, research could focus on having agents safely optimize and balance many values, so that one value does not unintentionally dominate or subvert the rest [@Newberry2021ThePA; @Ecoffet2021ReinforcementLU]. Sometimes unintended instrumental goals emerge in systems, such as self-preservation [@HadfieldMenell2017TheOG] or power-seeking [@turner2021optimal], so researchers could try mitigating and detecting such unintended emergent goals; see for more directions in detecting emergent functionality.\\n\\nExternal Safety\\n===============\\n\\n![External safety research aims to address broader contextual risks to how ML systems are handled. Both cybersecurity and decision making may decisively affect whether ML systems will fail or be misdirected. ](figures/external_safety.pdf){#fig:externalsafety width=\"78%\"}\\n\\nMachine learning systems do not exist in a vacuum, and the safety of the external context can influence how ML systems are handled and affect the overall safety of ML systems. ML systems are more likely to fail or be misdirected if the external context in which they operate is insecure or turbulent.\\n\\nExternal safety research applies ML to mitigate potential contextual hazards that may decisively cause ML systems to fail or be misdirected. As two examples, we support research on cybersecurity and on informed decision making. The first problem is motivated by the observation that ML systems are integrated with vulnerable software, and in the future ML may change the landscape of cyberattacks. In the second problem, we turn to a speculative approach for improving governance decisions and command and control operations using ML, as institutions may direct the most powerful future ML systems.\\n\\nBeyond technical work, policy and governance work will be integral to safe deployment [@dafoe2018ai; @Bender2021OnTD; @Birhane2021TheVE; @zwetsloot2018beyond; @Brundage2020TowardTA]. While techno-solutionism has limitations, technical ML researchers should consider using their skillset to address deployment environment hazards, and we focus on empirical ML research avenues, as we expect most readers are technical ML researchers.\\n\\nFinally, since there are multiple hazards that can hinder external safety, this section is nonexhaustive. For instance, if ML industry auditing tools could help regulators more effectively regulate ML systems, research developing such tools could become part of external safety. Likewise, using ML to help facilitate cooperation\\xa0[@Dafoe2020OpenPI] may emerge as a research area.\\n\\nML for Cybersecurity\\n--------------------\\n\\n#### Motivation.\\n\\nCybersecurity risks can make ML systems unsafe, as ML systems operate in tandem with traditional software and are often instantiated as a cyber-physical system. As such, malicious actors could exploit insecurities in traditional software to control autonomous ML systems. Some ML systems may also be private or unsuitable for proliferation, and they will therefore need to operate on computers that are secure.\\n\\nSeparately, ML may amplify future automated cyberattacks and enable malicious actors to increase the accessibility, potency, success rate, scale, speed, and stealth of their attacks. For example, hacking currently requires specialized skills, but if state-of-the-art ML models could be fine-tuned for hacking, then the barrier to entry for hacking may decrease sharply. Since cyberattacks can destroy valuable information and even destroy critical physical infrastructure [@Cary2020DestructiveCO] such as power grids [@Ottis2008AnalysisOT] and building hardware [@Langner2011StuxnetDA], these potential attacks are a looming threat to international security.\\n\\nWhile cybersecurity aims to increase attacker costs, the cost-benefit analysis may become lopsided if attackers eventually gain a larger menu of options that require negligible effort. In this new regime, attackers may gain the upper hand, like how attackers of ML systems currently have a large advantage over defenders. Since there may be less of a duality between offensive and defensive security in the future, we suggest that research focus on techniques that are clearly defensive. The severity of this risk is speculative, but neural networks are now rapidly gaining the ability to write code and interact with the outside environment, and at the same time there is very little research on deep learning for cybersecurity.\\n\\n#### Directions.\\n\\nTo mitigate the potential harms of automated cyberattacks to ML and other systems, researchers should apply ML to develop better defensive techniques. For instance, ML could be used to detect intruders [@lane1997application; @sommer2010outside] or impersonators [@Ho2019DetectingAC]. ML could also help analyze code and detect software vulnerabilities, and could help generate unexpected inputs to programs [@She2019NEUZZEF; @Wang2019NeuFuzzEF; @She2020NeutaintED]. Massive unsupervised ML methods could also model binaries and learn to detect malicious obfuscated payloads [@sgn; @Shin2015RecognizingFI; @ghidra; @harang2020sorel20m]. Researchers could also create ML systems that model software behavior and detect whether programs are sending packets when they should not. ML models could help predict future phases of cyberattacks, and such automated warnings could be judged by their lead time, precision, recall, and the quality of their contextualized explanation. Advancements in code translation [@Lachaux2020UnsupervisedTO; @Austin2021ProgramSW] and code generation [@Chen2021EvaluatingLL; @Pearce2021AnEC] suggest that future models could apply security patches and make code more secure, so that future systems not only flag security vulnerabilities but also fix them.\\n\\nImproved Epistemics and Decision Making\\n---------------------------------------\\n\\n#### Motivation.\\n\\nEven if we create reliable ML systems, these systems will not exhibit or ensure safety if the institutions that steer ML systems make poor decisions. Although nuclear weapons are a reliable and dependable technology, they became especially unsafe during the Cold War. During that time, misunderstanding and political turbulence exposed humanity to several close calls and brought us to the brink of catastrophe, demonstrating that external safety issues can make technologies unsafe. The most pivotal decisions are made during times of crisis, and future crises may be similarly risky as ML continues to be weaponized [@lethalaw; @OpenLetter]. This is why we suggest creating tools to help decision-makers handle ML systems in highly uncertain, quickly evolving, turbulent situations.\\n\\n#### Directions.\\n\\nTo improve the decision-making and epistemics of political leaders and command and control centers, we suggest two efforts: using ML to improve forecasting and bringing to light crucial considerations.\\n\\nMany governance and command and control decisions are based on forecasts [@Tetlock2015SuperforecastingTA] from humans, and some forecasts are starting to incorporate ML [@gide3]. Forecasters assign probabilities to possible events that could happen within the next few months or years (e.g., geopolitical, epidemiological, and industrial events), and are scored by their correctness and calibration. To be successful, forecasters must dynamically aggregate information from disparate unstructured sources [@Jin2021ForecastQAAQ]. This is challenging even for humans, but ML systems could potentially aggregate more information, be faster, be nonpartisan, consider multiple perspectives, and thus ultimately make more accurate predictions [@integrativecomplexity]. The robustness of such systems could be assessed based on their ability to predict pivotal historical events, if the model only has access to data before those events. An accurate forecasting tool would need to be applied with caution to prevent over-reliance [@Hedlund82], and it would need to present its data carefully so as not to encourage risk-taking behavior from the humans operating the forecasting system [@Taleb2013OnTD].\\n\\nSeparately, researchers should develop systems that identify questions worth asking and crucial factors to consider. While forecasting can refine estimates of well-defined risks, these advisory systems could help unearth new sources of risk and identify actions to mitigate risks. Since ML systems can process troves of historical data and can learn from diverse situations during training, they could suggest possibilities that would otherwise require extensive memory and experience. Such systems could help orient decision making by providing related prior scenarios and relevant statistics such as base rates. Eventually advisory systems could identify stakeholders, propose metrics, brainstorm options, suggest alternatives, and note trade-offs to further improve decision quality [@Gathani2021AugmentingDM]. In summary, ML systems that can predict a variety of events and identify crucial considerations could help provide good judgment and correct misperceptions, and thereby reduce the chance of rash decisions and inadvertent escalation.\\n\\n![image](figures/swiss_cheese.pdf){width=\"\\\\\\\\textwidth\"}\\n\\nRelated Research Agendas\\n========================\\n\\nThere is a large ecosystem of work on addressing societal consequences of machine learning, including AI policy [@dafoe2018ai], privacy [@Abadi2016DeepLW; @shokri2017membership], fairness [@Hardt2016EqualityOO], and ethics [@Gabriel2020ArtificialIV]. We strongly support research on these related areas. For purposes of scope, in this section we focus on papers that outline paths towards creating safe ML systems.\\\\\\nAn early work that helps identify safety problems is Russell *et al.*, 2015 [@Russell2015ResearchPF], who identify many potential avenues for safety, spanning robustness, machine ethics, research on AI\\'s economic impact, and more. Amodei and Olah *et al.*, 2016 [@Amodei2016ConcretePI] helped further concretize several safety research directions. With the benefit of five years of hindsight, our paper provides a revised and expanded collection of concrete problems. Some of our themes extend the themes in Amodei and Olah *et al.*, such as Robustness and some portions of Alignment. We focus here on problems that remain unsolved and also identify new problems, such as emergent capabilities from massive pretrained models, that stem from recent progress in ML. We also broaden the scope by identifying external safety risks surrounding the deployment context of ML. The technical agenda of Taylor *et al.*, 2016 [@Taylor2016AlignmentFA] considers similar topics to Amodei and Olah *et al.*, and Leike *et al.*, 2018 [@Leike2018ScalableAA] considers safety research directions in reward modeling. Although Leike *et al.*\\'s research agenda focuses on reinforcement learning, they highlight the importance of various other research problems including adversarial training and uncertainty estimation. Recently, Critch and Krueger, 2020 [@Critch2020AIRC] provide an extensive commentary on safety research directions and discuss safety when there are multiple stakeholders.\\n\\nConclusion\\n==========\\n\\nThis work presented a non-exhaustive list of four unsolved research problems, all of which are interconnected and interdependent. Anomaly detection, for example, helps with detecting proxy gaming, detecting suspicious cyberactivity, and executing fail-safes in the face of unexpected events. Achieving safety requires research on all four problems, not just one. To see this, recall that a machine learning system that is not aligned with human values may be unsafe in and of itself, as it may create unintended consequences or game human approval proxies. Even if it is possible to create aligned objectives for ML systems, Black Swan events could cause ML systems to misgeneralize and pursue incorrect goals, malicious actors may launch adversarial attacks or compromise the software on which the ML system is running, and humans may need to monitor for emergent functionality and the malicious use of ML systems. As depicted in \\'s highly simplified model, work on all four problems helps create comprehensive and layered protective measures against a wide range of safety threats.\\n\\nAs machine learning research evolves, the community\\'s aims and expectations should evolve too. For many years, the machine learning community focused on making machine learning systems work in the first place. However, machine learning systems have had notable success in domains from images, to natural language, to programming---therefore our focus should expand beyond just accuracy, speed, and scalability. Safety must now become a top priority.\\n\\nSafety is not auxiliary in most current widely deployed technology. Communities do not ask for \"safe bridges,\" but rather just \"bridges.\" Their safety is insisted upon---even assumed---and incorporating safety features is imbued in the design process. The ML community should similarly create a culture of safety and elevate its standards so that ML systems can be deployed in safety-critical situations.\\n\\nAcknowledgements {#acknowledgements .unnumbered}\\n----------------\\n\\nWe would like to thank Sidney Hough, Owain Evans, Collin Burns, Alex Tamkin, Mantas Mazeika, Kevin Liu, Jonathan Uesato, Steven Basart, Henry Zhu, D. Sculley, Mark Xu, Beth Barnes, Andreas Terzis, Florian Tramr, Stella Biderman, Leo Gao, Jacob Hilton, and Thomas Dietterich for their feedback. DH is supported by the NSF GRFP Fellowship and an Open Philanthropy Project AI Fellowship.\\n\\nAnalyzing Risks, Hazards, and Impact\\n====================================\\n\\nRisk Management Framework\\n-------------------------\\n\\n1.0\\\\*1\\\\>X \\\\*1\\\\>X \\\\| \\\\*1\\\\>Y \\\\*1\\\\>Y \\\\*1\\\\>Y \\\\*1\\\\>Y & & & & &\\\\\\n\\n& Black Swans and Tail Risks & & & &\\\\\\n& Adversarial Robustness & & & &\\\\\\n\\n& Anomaly Detection & & & &\\\\\\n& Representative Outputs & & & &\\\\\\n& Hidden Model Functionality & & & &\\\\\\n\\n& Value Learning & & & &\\\\\\n& Translating Values to Action & & & &\\\\\\n& Proxy Gaming & & & &\\\\\\n& Value Clarification & & & &\\\\\\n& Unintended Consequences & & & &\\\\\\nExternal & ML for Cybersecurity & & & &\\\\\\nSafety & Informed Decision Making & & & &\\\\\\n\\nTo analyze how ML Safety progress can reduce abstract risks and hazards,[^1] we identify four dimensions of risk in this section and five hazards in the next section.\\n\\nThe following four risk dimensions are adopted from the Department of Defense\\'s broad risk management framework [@DoD], with its personnel management risks replaced with ML system risks.\\n\\n1.  **ML System Risks** -- risks to the ability of a near-term individual ML system to operate reliably.\\n\\n2.  **Operational Risks** -- risks to the ability of an organization to safely operate an ML system in near-term deployment scenarios.\\n\\n3.  **Institutional and Societal Risks** -- risks to the ability of global society or institutions that decisively affect ML systems to operate in near-term scenarios in an efficient, informed, and prudent way.\\n\\n4.  **Future (ML System, Operational, and Institutional) Risks** -- risks to the ability of future ML systems, organizations operating ML systems, and institutions to address mid- to long-term challenges.\\n\\nIn , we indicate whether one of these risks is reduced by progress on a given ML Safety problem. Note that these all problems reduce risks to all three of future ML systems, organizations, and institutions. In the future, organizations and institutions will likely become more dependent on ML systems, so improvements to Black Swans robustness would in the future help improve operations and institutions dependent on ML systems. Since this table is a snapshot of the present, risk profiles will inevitably change.\\n\\nHazard Management Framework\\n---------------------------\\n\\n1.0\\\\*1\\\\>X \\\\*1\\\\>X \\\\| \\\\*1\\\\>Y \\\\*1\\\\>Y \\\\*1\\\\>Y \\\\*1\\\\>Y \\\\*1\\\\>Y & & & & & &\\\\\\n\\n& Black Swans and Tail Risks & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} & & []{style=\"color: rightgreen\"} &\\\\\\n& Adversarial Robustness & & & & & []{style=\"color: rightgreen\"}\\\\\\n\\n& Anomaly Detection & & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"}\\\\\\n& Representative Outputs & []{style=\"color: rightgreen\"} & & & & []{style=\"color: rightgreen\"}\\\\\\n& Hidden Model Functionality & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} & & []{style=\"color: rightgreen\"}\\\\\\n\\n& Value Learning & []{style=\"color: rightgreen\"} & & & &\\\\\\n& Translating Values to Action & []{style=\"color: rightgreen\"} & & & &\\\\\\n& Proxy Gaming & & & & & []{style=\"color: rightgreen\"}\\\\\\n& Value Clarification & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} & & &\\\\\\n& Unintended Consequences & & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} &\\\\\\nExternal & ML for Cybersecurity & & & & & []{style=\"color: rightgreen\"}\\\\\\nSafety & Informed Decision Making & []{style=\"color: rightgreen\"} & []{style=\"color: rightgreen\"} & & &\\\\\\n\\nWe now turn from what is affected by risks to five abstract hazards that create risks.\\n\\nR0.23\\n\\n![image](figures/cube.pdf){width=\"23%\"}\\n\\n1.  **Known Unknowns** -- Identified hazards for which we have imperfect or incomplete knowledge. These are identified hazards known to have unknown aspects.\\n\\n2.  **Unknown Unknowns** -- Hazards which are unknown and unidentified, and they have properties that are unknown.\\n\\n3.  **Emergence** -- A hazard that forms and comes into being as the system increases in size or its parts are combined. Such hazards do not exist in smaller versions of the system nor in its constituent parts.\\n\\n4.  **Long Tails** -- Hazards that can be understood as unusual or extreme events from a long tail distribution.\\n\\n5.  **Adversaries & Deception** -- Hazards from a person, system, or force that aims to attack, subvert, or deceive.\\n\\nThese hazards do not enumerate all possible hazards. For example, the problems in External Safety help with turbulence hazards. Furthermore, feedback loops, which can create long tails, could become a more prominent hazard in the future when ML systems are integrated into more aspects of our lives.\\n\\nThe five hazards have some overlap. For instance, when something novel emerges, it is an unknown unknown. When it is detected, it can become a known unknown. Separately, long tail events are often but not necessarily unknown unknowns: the 1987 stock market crash was a long tail event, but it was a known unknown to a prescient few and an unknown unknown to most everybody else. Emergent hazards sometimes co-occur with long tailed events, and an adversarial attack can cause long tail events.\\n\\nIn , we indicate whether an ML Safety problem reduces vulnerability or exposure to a given hazard. As with , the table is a snapshot of the present. For example, future adversaries could create novel unusual events or strike during tail events, so Black Swan robustness could improve adversarial robustness.\\n\\nWith risks, hazards, and goals now all explicated, we depict their interconnectedness in .\\n\\nPrioritization and Strategy for Maximizing Impact\\n-------------------------------------------------\\n\\n::: {#tab:intframework}\\n  Area                                      Importance                      Neglectedness                   Tractability\\n  ---------- ------------------------------ ------------------------------- ------------------------------- -------------------------------\\n             Black Swans and Tail Risks     $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$\\n             Adversarial Robustness         $\\\\bullet$ $\\\\bullet$             $\\\\bullet$                       $\\\\bullet$ $\\\\bullet$\\n             Anomaly Detection              $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$ $\\\\bullet$\\n             Representative Outputs         $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$\\n             Hidden Model Functionality     $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$\\n             Value Learning                 $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$\\n             Translating Values to Action   $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$ $\\\\bullet$\\n             Proxy Gaming                   $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$\\n             Value Clarification            $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$\\n             Unintended Consequences        $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$\\n  External   ML for Cybersecurity           $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$ $\\\\bullet$   $\\\\bullet$ $\\\\bullet$ $\\\\bullet$\\n  Safety     Informed Decision Making       $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$             $\\\\bullet$ $\\\\bullet$\\n\\n  : Problems and three factors that influence expected marginal impact.\\n:::\\n\\nWe presented several problems, but new researchers may be able to make a larger impact on some problems over others. Some problems may be important, but if they are extremely popular, the risk of scooping increases, as does the risk of researchers stepping on each others\\' toes. Likewise, some problems may be important and may be decisive for safety if solved, but some problems are simply infeasible. Consequently, we should consider the importance, neglectedness, and tractability of problems.\\n\\n1.  **Importance** -- How much potential risk does substantial progress on this problem reduce?\\n\\n    1.  Progress on this problem reduces risks of catastrophes.\\n\\n    2.  Progress on this problem directly reduces risks from potential permanent catastrophes.\\n\\n    3.  Progress on this problem directly reduces risks from more plausible permanent catastrophes.\\n\\n2.  **Neglectedness** -- How much research is being done on the problem?\\n\\n    1.  The problem is one of the top ten most researched topics at leading conferences.\\n\\n    2.  The problem receives some attention at leading ML conferences, or adjacent problems are hardly neglected.\\n\\n    3.  The problem has few related papers consistently published at leading ML conferences.\\n\\n3.  **Tractability** -- How much progress can we expect on the problem?\\n\\n    1.  We cannot expect large research efforts to highly fruitful currently, possibly due to conceptual bottlenecks, or productive work on the problem likely requires far more advanced ML capabilities.\\n\\n    2.  We expect to reliably and continually make progress on the problem.\\n\\n    3.  A large research effort would be highly fruitful and there is obvious low-hanging fruit.\\n\\nA snapshot of each problem and its current importance, neglectedness, and tractability is in . Note this only provides a rough sketch, and it has limitations. For example, a problem that is hardly neglected overall may still have neglected aspects; while adversarial robustness is less neglected than other safety problems, robustness to unforeseen adversaries is fairly neglected. Moreover, working on popular shovel-ready problems may be more useful for newcomers compared to working on problems where conceptual bottlenecks persist. Further, this gives a rough sense of marginal impact, but entire community should not chose to act in the same way marginally, or else neglected problems will suddenly become overcrowded.\\n\\nThese three factors are merely prioritization factors and do not define a strategy. Rather, a potential strategy for ML Safety is as follows. 1. Force Management: Cultivate and maintain a force of ready personnel to implement safety measures into advanced ML systems and operate ML systems safely. 2. Research: Build and maintain a community to conduct safety research, including the identification of potential future hazards, clarification of safety goals, reduction of the costs to adopt safety methods, research on how to incorporate safety methods into existing ML systems, and so on. 3. Protocols: Establish and incentivize adherence to protocols, precedents, standards, and research expectations such as red teaming, all for the safe development and deployment of ML systems. 4. Partnerships: Build and maintain safety-focused alliances and partnerships among academe, industry, and government.\\n\\nIn closing, throughout ML Safety\\'s development we have seen numerous proposed strategies, hazards, risks, scenarios, and problems. In safety, some previously proposed problems have been discarded, and some new problems have emerged, just as in the broader ML community. Since no individual knows what lies ahead, safety analysis and strategy will need to evolve and adapt beyond this document. Regardless of which particular safety problems turn out to be the most or least essential, the success of safety\\'s evolution and adaptation rests on having a large and capable research community.\\n\\n[^1]: One can think of hazards as factors that have the potential to cause harm. One can think of risk as the hazard\\'s prevalence multiplied by the amount of exposure to the hazard multiplied by the hazard\\'s deleterious effect. For example, a wet floor is a hazard to humans. However, risks from wet floors are lower if floors dry more quickly with a fan (external safety). Risks are lower if humans heed wet floor signs and have less exposure to them (monitoring). Risks are also lower for young adults than the elderly, since the elderly are more physically vulnerable (robustness). In other terms, robustness makes systems less vulnerable to hazards, monitoring reduces exposure to hazards, alignment makes systems avoid creating hazards, and external safety reduces hazards encountered in deployment.\\n',\n",
       "  'bibliography_bbl': '% $ biblatex auxiliary file $\\n% $ biblatex bbl format version 3.1 $\\n% Do not modify the above lines!\\n%\\n% This is an auxiliary file used by the \\'biblatex\\' package.\\n% This file may safely be deleted. It will be recreated as\\n% required.\\n%\\n\\\\begingroup\\n\\\\makeatletter\\n\\\\@ifundefined{ver@biblatex.sty}\\n  {\\\\@latex@error\\n     {Missing \\'biblatex\\' package}\\n     {The bibliography requires the \\'biblatex\\' package.}\\n      \\\\aftergroup\\\\endinput}\\n  {}\\n\\\\endgroup\\n\\n\\\\datalist[entry]{nty/global//global/global}\\n  \\\\entry{asilomar}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=baArS}{%\\n         prefix={by\\\\bibnamedelima approximately},\\n         prefixi={b\\\\bibinitperiod\\\\bibinitdelim a\\\\bibinitperiod},\\n         family={2000 AI\\\\bibnamedelima researchers.},\\n         familyi={\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod\\\\bibinitdelim\\n  r\\\\bibinitperiod},\\n         given={Signed},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ArSba1}\\n    \\\\strng{fullhash}{ArSba1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{2}\\n    \\\\field{sortinithash}{2}\\n    \\\\field{title}{Asilomar AI Principles}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{OpenLetter}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=bpS}{%\\n         prefix={by},\\n         prefixi={b\\\\bibinitperiod},\\n         family={30000+\\\\bibnamedelima people.},\\n         familyi={\\\\bibinitperiod\\\\bibinitdelim p\\\\bibinitperiod},\\n         given={Signed},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{pSb1}\\n    \\\\strng{fullhash}{pSb1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{3}\\n    \\\\field{sortinithash}{3}\\n    \\\\field{title}{Autonomous Weapons: An Open Letter from AI and Robotics\\n  Researchers}\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{Abadi2016DeepLW}{article}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=AM}{%\\n         family={Abadi},\\n         familyi={A\\\\bibinitperiod},\\n         given={Mart{\\\\\\'i}n},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Chu},\\n         familyi={C\\\\bibinitperiod},\\n         given={Andy},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=GI}{%\\n         family={Goodfellow},\\n         familyi={G\\\\bibinitperiod},\\n         given={I.},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=MHB}{%\\n         family={McMahan},\\n         familyi={M\\\\bibinitperiod},\\n         given={H.\\\\bibnamedelima B.},\\n         giveni={H\\\\bibinitperiod\\\\bibinitdelim B\\\\bibinitperiod},\\n      }}%\\n      {{hash=MI}{%\\n         family={Mironov},\\n         familyi={M\\\\bibinitperiod},\\n         given={Ilya},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=TK}{%\\n         family={Talwar},\\n         familyi={T\\\\bibinitperiod},\\n         given={Kunal},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZL}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{AM+1}\\n    \\\\strng{fullhash}{AMCAGIMHBMITKZL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{A}\\n    \\\\field{sortinithash}{A}\\n    \\\\field{title}{Deep Learning with Differential Privacy}\\n    \\\\field{journaltitle}{Proceedings of the 2016 ACM SIGSAC Conference on\\n  Computer and Communications Security}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{Ahmad2021NetworkID}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=AZ}{%\\n         family={Ahmad},\\n         familyi={A\\\\bibinitperiod},\\n         given={Zeeshan},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=KA}{%\\n         family={Khan},\\n         familyi={K\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=CW}{%\\n         family={Cheah},\\n         familyi={C\\\\bibinitperiod},\\n         given={W.},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=AJ}{%\\n         family={Abdullah},\\n         familyi={A\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AF}{%\\n         family={Ahmad},\\n         familyi={A\\\\bibinitperiod},\\n         given={Farhan},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{AZ+1}\\n    \\\\strng{fullhash}{AZKACWAJAF1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{A}\\n    \\\\field{sortinithash}{A}\\n    \\\\field{title}{Network intrusion detection system: A systematic study of\\n  machine learning and deep learning approaches}\\n    \\\\field{journaltitle}{Trans. Emerg. Telecommun. Technol.}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Amodei2016ConcretePI}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=OC}{%\\n         family={Olah},\\n         familyi={O\\\\bibinitperiod},\\n         given={Christopher},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=CP}{%\\n         family={Christiano},\\n         familyi={C\\\\bibinitperiod},\\n         given={Paul},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Schulman},\\n         familyi={S\\\\bibinitperiod},\\n         given={John},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=MD}{%\\n         family={Man{\\\\\\'e}},\\n         familyi={M\\\\bibinitperiod},\\n         given={Dandelion},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{AD+1}\\n    \\\\strng{fullhash}{ADOCSJCPSJMD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{A}\\n    \\\\field{sortinithash}{A}\\n    \\\\field{title}{Concrete Problems in AI Safety}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{Anderson1995ProgrammingSC}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=ARJ}{%\\n         family={Anderson},\\n         familyi={A\\\\bibinitperiod},\\n         given={Ross\\\\bibnamedelima J.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim J\\\\bibinitperiod},\\n      }}%\\n      {{hash=NR}{%\\n         family={Needham},\\n         familyi={N\\\\bibinitperiod},\\n         given={Roger},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ARJNR1}\\n    \\\\strng{fullhash}{ARJNR1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{A}\\n    \\\\field{sortinithash}{A}\\n    \\\\field{booktitle}{Computer Science Today}\\n    \\\\field{title}{Programming Satan\\'s Computer}\\n    \\\\field{year}{1995}\\n  \\\\endentry\\n\\n  \\\\entry{waymo}{misc}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=AD}{%\\n         family={Anguelov},\\n         familyi={A\\\\bibinitperiod},\\n         given={Drago},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{AD1}\\n    \\\\strng{fullhash}{AD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{A}\\n    \\\\field{sortinithash}{A}\\n    \\\\field{title}{Machine Learning for Autonomous Driving}\\n    \\\\verb{url}\\n    \\\\verb https://www.youtube.com/watch?v=Q0nGo2-y0xY\\n    \\\\endverb\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Athalye2018ObfuscatedGG}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=AA}{%\\n         family={Athalye},\\n         familyi={A\\\\bibinitperiod},\\n         given={Anish},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=CN}{%\\n         family={Carlini},\\n         familyi={C\\\\bibinitperiod},\\n         given={Nicholas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=WDA}{%\\n         family={Wagner},\\n         familyi={W\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima A.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{AACNWDA1}\\n    \\\\strng{fullhash}{AACNWDA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{A}\\n    \\\\field{sortinithash}{A}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Obfuscated Gradients Give a False Sense of Security:\\n  Circumventing Defenses to Adversarial Examples}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Austin2021ProgramSW}{article}{}\\n    \\\\name{author}{11}{}{%\\n      {{hash=AJ}{%\\n         family={Austin},\\n         familyi={A\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=OA}{%\\n         family={Odena},\\n         familyi={O\\\\bibinitperiod},\\n         given={Augustus},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=NM}{%\\n         family={Nye},\\n         familyi={N\\\\bibinitperiod},\\n         given={Maxwell},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=BM}{%\\n         family={Bosma},\\n         familyi={B\\\\bibinitperiod},\\n         given={Maarten},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=MH}{%\\n         family={Michalewski},\\n         familyi={M\\\\bibinitperiod},\\n         given={Henryk},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=DD}{%\\n         family={Dohan},\\n         familyi={D\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=JE}{%\\n         family={Jiang},\\n         familyi={J\\\\bibinitperiod},\\n         given={Ellen},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=CC}{%\\n         family={Cai},\\n         familyi={C\\\\bibinitperiod},\\n         given={Carrie},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=TM}{%\\n         family={Terry},\\n         familyi={T\\\\bibinitperiod},\\n         given={Michael},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=LQV}{%\\n         family={Le},\\n         familyi={L\\\\bibinitperiod},\\n         given={Quoc\\\\bibnamedelima V.},\\n         giveni={Q\\\\bibinitperiod\\\\bibinitdelim V\\\\bibinitperiod},\\n      }}%\\n      {{hash=SC}{%\\n         family={Sutton},\\n         familyi={S\\\\bibinitperiod},\\n         given={Charles},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{AJ+1}\\n    \\\\strng{fullhash}{AJOANMBMMHDDJECCTMLQVSC1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{A}\\n    \\\\field{sortinithash}{A}\\n    \\\\field{title}{Program Synthesis with Large Language Models}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Bagdasaryan2020BlindBI}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=BE}{%\\n         family={Bagdasaryan},\\n         familyi={B\\\\bibinitperiod},\\n         given={Eugene},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=SV}{%\\n         family={Shmatikov},\\n         familyi={S\\\\bibinitperiod},\\n         given={Vitaly},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BESV1}\\n    \\\\strng{fullhash}{BESV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{booktitle}{USENIX Security Symposium}\\n    \\\\field{title}{Blind Backdoors in Deep Learning Models}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Bendale2016TowardsOS}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=BA}{%\\n         family={Bendale},\\n         familyi={B\\\\bibinitperiod},\\n         given={Abhijit},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=BT}{%\\n         family={Boult},\\n         familyi={B\\\\bibinitperiod},\\n         given={Terrance},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BABT1}\\n    \\\\strng{fullhash}{BABT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Towards Open Set Deep Networks}\\n    \\\\field{journaltitle}{CVPR}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{Bender2021OnTD}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=BEM}{%\\n         family={Bender},\\n         familyi={B\\\\bibinitperiod},\\n         given={Emily\\\\bibnamedelima M.},\\n         giveni={E\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=GT}{%\\n         family={Gebru},\\n         familyi={G\\\\bibinitperiod},\\n         given={Timnit},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=MMA}{%\\n         family={McMillan-Major},\\n         familyi={M\\\\bibinithyphendelim M\\\\bibinitperiod},\\n         given={Angelina},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Shmitchell},\\n         familyi={S\\\\bibinitperiod},\\n         given={Shmargaret},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BEM+1}\\n    \\\\strng{fullhash}{BEMGTMMASS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{On the Dangers of Stochastic Parrots: Can Language Models Be\\n  Too Big?}\\n    \\\\field{journaltitle}{Proceedings of the 2021 ACM Conference on Fairness,\\n  Accountability, and Transparency}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{unreal}{misc}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=aBML}{%\\n         prefix={at},\\n         prefixi={a\\\\bibinitperiod},\\n         family={Berkeley},\\n         familyi={B\\\\bibinitperiod},\\n         given={Machine\\\\bibnamedelima Learning},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BMLa1}\\n    \\\\strng{fullhash}{BMLa1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Alien Dreams: An Emerging Art Scene}\\n    \\\\verb{url}\\n    \\\\verb https://ml.berkeley.edu/blog/posts/clip-art/\\n    \\\\endverb\\n  \\\\endentry\\n\\n  \\\\entry{Besnier2021TriggeringFO}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=BV}{%\\n         family={Besnier},\\n         familyi={B\\\\bibinitperiod},\\n         given={Victor},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=BA}{%\\n         family={Bursuc},\\n         familyi={B\\\\bibinitperiod},\\n         given={Andrei},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=PD}{%\\n         family={Picard},\\n         familyi={P\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=BA}{%\\n         family={Briot},\\n         familyi={B\\\\bibinitperiod},\\n         given={Alexandre},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BV+1}\\n    \\\\strng{fullhash}{BVBAPDBA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Triggering Failures: Out-Of-Distribution detection by\\n  learning from local adversarial attacks in Semantic Segmentation}\\n    \\\\field{volume}{abs/2108.01634}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{biggio2013evasion}{inproceedings}{}\\n    \\\\name{author}{8}{}{%\\n      {{hash=BB}{%\\n         family={Biggio},\\n         familyi={B\\\\bibinitperiod},\\n         given={Battista},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=CI}{%\\n         family={Corona},\\n         familyi={C\\\\bibinitperiod},\\n         given={Igino},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=MD}{%\\n         family={Maiorca},\\n         familyi={M\\\\bibinitperiod},\\n         given={Davide},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=NB}{%\\n         family={Nelson},\\n         familyi={N\\\\bibinitperiod},\\n         given={Blaine},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=SN}{%\\n         family={{\\\\v{S}}rndi{\\\\\\'c}},\\n         familyi={{\\\\v{S}}\\\\bibinitperiod},\\n         given={Nedim},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=LP}{%\\n         family={Laskov},\\n         familyi={L\\\\bibinitperiod},\\n         given={Pavel},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=GG}{%\\n         family={Giacinto},\\n         familyi={G\\\\bibinitperiod},\\n         given={Giorgio},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=RF}{%\\n         family={Roli},\\n         familyi={R\\\\bibinitperiod},\\n         given={Fabio},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{organization}{1}{%\\n      {Springer}%\\n    }\\n    \\\\strng{namehash}{BB+1}\\n    \\\\strng{fullhash}{BBCIMDNBSNLPGGRF1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{booktitle}{Joint European conference on machine learning and\\n  knowledge discovery in databases}\\n    \\\\field{pages}{387\\\\bibrangedash 402}\\n    \\\\field{title}{Evasion attacks against machine learning at test time}\\n    \\\\field{year}{2013}\\n  \\\\endentry\\n\\n  \\\\entry{Birhane2021TheVE}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=BA}{%\\n         family={Birhane},\\n         familyi={B\\\\bibinitperiod},\\n         given={Abeba},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KP}{%\\n         family={Kalluri},\\n         familyi={K\\\\bibinitperiod},\\n         given={Pratyusha},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=CD}{%\\n         family={Card},\\n         familyi={C\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=AW}{%\\n         family={Agnew},\\n         familyi={A\\\\bibinitperiod},\\n         given={William},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=DR}{%\\n         family={Dotan},\\n         familyi={D\\\\bibinitperiod},\\n         given={Ravit},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=BM}{%\\n         family={Bao},\\n         familyi={B\\\\bibinitperiod},\\n         given={Michelle},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BA+1}\\n    \\\\strng{fullhash}{BAKPCDAWDRBM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{The Values Encoded in Machine Learning Research}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Bitterwolf2020CertifiablyAR}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=BJ}{%\\n         family={Bitterwolf},\\n         familyi={B\\\\bibinitperiod},\\n         given={Julian},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=MA}{%\\n         family={Meinke},\\n         familyi={M\\\\bibinitperiod},\\n         given={Alexander},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=HM}{%\\n         family={Hein},\\n         familyi={H\\\\bibinitperiod},\\n         given={Matthias},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BJMAHM1}\\n    \\\\strng{fullhash}{BJMAHM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Certifiably Adversarially Robust Detection of\\n  Out-of-Distribution Data}\\n    \\\\field{journaltitle}{NeurIPS}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Bommasani2021OnTO}{article}{}\\n    \\\\name{author}{114}{}{%\\n      {{hash=BR}{%\\n         family={Bommasani},\\n         familyi={B\\\\bibinitperiod},\\n         given={Rishi},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=HDA}{%\\n         family={Hudson},\\n         familyi={H\\\\bibinitperiod},\\n         given={Drew\\\\bibnamedelima A.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n      {{hash=AE}{%\\n         family={Adeli},\\n         familyi={A\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=AR}{%\\n         family={Altman},\\n         familyi={A\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=AS}{%\\n         family={Arora},\\n         familyi={A\\\\bibinitperiod},\\n         given={Simran},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=vAS}{%\\n         prefix={von},\\n         prefixi={v\\\\bibinitperiod},\\n         family={Arx},\\n         familyi={A\\\\bibinitperiod},\\n         given={Sydney},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=BMS}{%\\n         family={Bernstein},\\n         familyi={B\\\\bibinitperiod},\\n         given={Michael\\\\bibnamedelima S.},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim S\\\\bibinitperiod},\\n      }}%\\n      {{hash=BJ}{%\\n         family={Bohg},\\n         familyi={B\\\\bibinitperiod},\\n         given={Jeannette},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=BA}{%\\n         family={Bosselut},\\n         familyi={B\\\\bibinitperiod},\\n         given={Antoine},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=BE}{%\\n         family={Brunskill},\\n         familyi={B\\\\bibinitperiod},\\n         given={Emma},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=BE}{%\\n         family={Brynjolfsson},\\n         familyi={B\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Buch},\\n         familyi={B\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CD}{%\\n         family={Card},\\n         familyi={C\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=CR}{%\\n         family={Castellon},\\n         familyi={C\\\\bibinitperiod},\\n         given={Rodrigo},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=CNS}{%\\n         family={Chatterji},\\n         familyi={C\\\\bibinitperiod},\\n         given={Niladri\\\\bibnamedelima S.},\\n         giveni={N\\\\bibinitperiod\\\\bibinitdelim S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Chen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Annie},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=CK}{%\\n         family={Creel},\\n         familyi={C\\\\bibinitperiod},\\n         given={Kathleen},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=DJ}{%\\n         family={Davis},\\n         familyi={D\\\\bibinitperiod},\\n         given={Jared},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=DD}{%\\n         family={Demszky},\\n         familyi={D\\\\bibinitperiod},\\n         given={Dora},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=DC}{%\\n         family={Donahue},\\n         familyi={D\\\\bibinitperiod},\\n         given={Chris},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=DM}{%\\n         family={Doumbouya},\\n         familyi={D\\\\bibinitperiod},\\n         given={Moussa},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=DE}{%\\n         family={Durmus},\\n         familyi={D\\\\bibinitperiod},\\n         given={Esin},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=ES}{%\\n         family={Ermon},\\n         familyi={E\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=EJ}{%\\n         family={Etchemendy},\\n         familyi={E\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=EK}{%\\n         family={Ethayarajh},\\n         familyi={E\\\\bibinitperiod},\\n         given={Kawin},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=FFL}{%\\n         family={Fei-Fei},\\n         familyi={F\\\\bibinithyphendelim F\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=FC}{%\\n         family={Finn},\\n         familyi={F\\\\bibinitperiod},\\n         given={Chelsea},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=GT}{%\\n         family={Gale},\\n         familyi={G\\\\bibinitperiod},\\n         given={Trevor},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=GLE}{%\\n         family={Gillespie},\\n         familyi={G\\\\bibinitperiod},\\n         given={Lauren\\\\bibnamedelima E.},\\n         giveni={L\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n      {{hash=GK}{%\\n         family={Goel},\\n         familyi={G\\\\bibinitperiod},\\n         given={Karan},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=GND}{%\\n         family={Goodman},\\n         familyi={G\\\\bibinitperiod},\\n         given={Noah\\\\bibnamedelima D.},\\n         giveni={N\\\\bibinitperiod\\\\bibinitdelim D\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Grossman},\\n         familyi={G\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=GN}{%\\n         family={Guha},\\n         familyi={G\\\\bibinitperiod},\\n         given={Neel},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=HT}{%\\n         family={Hashimoto},\\n         familyi={H\\\\bibinitperiod},\\n         given={Tatsunori},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=HP}{%\\n         family={Henderson},\\n         familyi={H\\\\bibinitperiod},\\n         given={Peter},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=HJ}{%\\n         family={Hewitt},\\n         familyi={H\\\\bibinitperiod},\\n         given={John},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=HDE}{%\\n         family={Ho},\\n         familyi={H\\\\bibinitperiod},\\n         given={Daniel\\\\bibnamedelima E},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n      {{hash=HJ}{%\\n         family={Hong},\\n         familyi={H\\\\bibinitperiod},\\n         given={Jenny},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=HK}{%\\n         family={Hsu},\\n         familyi={H\\\\bibinitperiod},\\n         given={Kyle},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=HJ}{%\\n         family={Huang},\\n         familyi={H\\\\bibinitperiod},\\n         given={Jing},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=ITF}{%\\n         family={Icard},\\n         familyi={I\\\\bibinitperiod},\\n         given={Thomas\\\\bibnamedelima F.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim F\\\\bibinitperiod},\\n      }}%\\n      {{hash=JS}{%\\n         family={Jain},\\n         familyi={J\\\\bibinitperiod},\\n         given={Saahil},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=JD}{%\\n         family={Jurafsky},\\n         familyi={J\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=KP}{%\\n         family={Kalluri},\\n         familyi={K\\\\bibinitperiod},\\n         given={Pratyusha},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=KS}{%\\n         family={Karamcheti},\\n         familyi={K\\\\bibinitperiod},\\n         given={Siddharth},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=KG}{%\\n         family={Keeling},\\n         familyi={K\\\\bibinitperiod},\\n         given={G.},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=KF}{%\\n         family={Khani},\\n         familyi={K\\\\bibinitperiod},\\n         given={Fereshte},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=KO}{%\\n         family={Khattab},\\n         familyi={K\\\\bibinitperiod},\\n         given={O.},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=KPW}{%\\n         family={Koh},\\n         familyi={K\\\\bibinitperiod},\\n         given={Pang\\\\bibnamedelima Wei},\\n         giveni={P\\\\bibinitperiod\\\\bibinitdelim W\\\\bibinitperiod},\\n      }}%\\n      {{hash=KM}{%\\n         family={Krass},\\n         familyi={K\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=KR}{%\\n         family={Krishna},\\n         familyi={K\\\\bibinitperiod},\\n         given={Ranjay},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=KR}{%\\n         family={Kuditipudi},\\n         familyi={K\\\\bibinitperiod},\\n         given={Rohith},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=KA}{%\\n         family={Kumar},\\n         familyi={K\\\\bibinitperiod},\\n         given={Ananya},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=LF}{%\\n         family={Ladhak},\\n         familyi={L\\\\bibinitperiod},\\n         given={Faisal},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=LM}{%\\n         family={Lee},\\n         familyi={L\\\\bibinitperiod},\\n         given={Mina},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=LT}{%\\n         family={Lee},\\n         familyi={L\\\\bibinitperiod},\\n         given={Tony},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Leskovec},\\n         familyi={L\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=LI}{%\\n         family={Levent},\\n         familyi={L\\\\bibinitperiod},\\n         given={Isabelle},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=LXL}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Xiang\\\\bibnamedelima Lisa},\\n         giveni={X\\\\bibinitperiod\\\\bibinitdelim L\\\\bibinitperiod},\\n      }}%\\n      {{hash=LX}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Xuechen},\\n         giveni={X\\\\bibinitperiod},\\n      }}%\\n      {{hash=MT}{%\\n         family={Ma},\\n         familyi={M\\\\bibinitperiod},\\n         given={Tengyu},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=MA}{%\\n         family={Malik},\\n         familyi={M\\\\bibinitperiod},\\n         given={Ali},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=MCD}{%\\n         family={Manning},\\n         familyi={M\\\\bibinitperiod},\\n         given={Christopher\\\\bibnamedelima D.},\\n         giveni={C\\\\bibinitperiod\\\\bibinitdelim D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MSP}{%\\n         family={Mirchandani},\\n         familyi={M\\\\bibinitperiod},\\n         given={Suvir\\\\bibnamedelima P.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim P\\\\bibinitperiod},\\n      }}%\\n      {{hash=ME}{%\\n         family={Mitchell},\\n         familyi={M\\\\bibinitperiod},\\n         given={Eric},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=MZ}{%\\n         family={Munyikwa},\\n         familyi={M\\\\bibinitperiod},\\n         given={Zanele},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=NS}{%\\n         family={Nair},\\n         familyi={N\\\\bibinitperiod},\\n         given={Suraj},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=NA}{%\\n         family={Narayan},\\n         familyi={N\\\\bibinitperiod},\\n         given={Avanika},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=ND}{%\\n         family={Narayanan},\\n         familyi={N\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=NB}{%\\n         family={Newman},\\n         familyi={N\\\\bibinitperiod},\\n         given={Ben},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=NA}{%\\n         family={Nie},\\n         familyi={N\\\\bibinitperiod},\\n         given={Allen},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=NJC}{%\\n         family={Niebles},\\n         familyi={N\\\\bibinitperiod},\\n         given={J.\\\\bibnamedelima C.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n      {{hash=NH}{%\\n         family={Nilforoshan},\\n         familyi={N\\\\bibinitperiod},\\n         given={H.},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=NJ}{%\\n         family={Nyarko},\\n         familyi={N\\\\bibinitperiod},\\n         given={Julian},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=OG}{%\\n         family={Ogut},\\n         familyi={O\\\\bibinitperiod},\\n         given={Giray},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=OL}{%\\n         family={Orr},\\n         familyi={O\\\\bibinitperiod},\\n         given={Laurel},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=PI}{%\\n         family={Papadimitriou},\\n         familyi={P\\\\bibinitperiod},\\n         given={Isabel},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=PJ}{%\\n         family={Park},\\n         familyi={P\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=PC}{%\\n         family={Piech},\\n         familyi={P\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=PE}{%\\n         family={Portelance},\\n         familyi={P\\\\bibinitperiod},\\n         given={Eva},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=PC}{%\\n         family={Potts},\\n         familyi={P\\\\bibinitperiod},\\n         given={Christopher},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Raghunathan},\\n         familyi={R\\\\bibinitperiod},\\n         given={Aditi},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=RR}{%\\n         family={Reich},\\n         familyi={R\\\\bibinitperiod},\\n         given={Robert},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=RH}{%\\n         family={Ren},\\n         familyi={R\\\\bibinitperiod},\\n         given={Hongyu},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=RF}{%\\n         family={Rong},\\n         familyi={R\\\\bibinitperiod},\\n         given={Frieda},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=RYH}{%\\n         family={Roohani},\\n         familyi={R\\\\bibinitperiod},\\n         given={Yusuf\\\\bibnamedelima H.},\\n         giveni={Y\\\\bibinitperiod\\\\bibinitdelim H\\\\bibinitperiod},\\n      }}%\\n      {{hash=RC}{%\\n         family={Ruiz},\\n         familyi={R\\\\bibinitperiod},\\n         given={Camilo},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=RJK}{%\\n         family={Ryan},\\n         familyi={R\\\\bibinitperiod},\\n         given={Jackson\\\\bibnamedelima K.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim K\\\\bibinitperiod},\\n      }}%\\n      {{hash=RC}{%\\n         family={R\\'e},\\n         familyi={R\\\\bibinitperiod},\\n         given={Christopher},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Sadigh},\\n         familyi={S\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Sagawa},\\n         familyi={S\\\\bibinitperiod},\\n         given={Shiori},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=SK}{%\\n         family={Santhanam},\\n         familyi={S\\\\bibinitperiod},\\n         given={Keshav},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=SA}{%\\n         family={Shih},\\n         familyi={S\\\\bibinitperiod},\\n         given={Andy},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SK}{%\\n         family={Srinivasan},\\n         familyi={S\\\\bibinitperiod},\\n         given={K.},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=TA}{%\\n         family={Tamkin},\\n         familyi={T\\\\bibinitperiod},\\n         given={Alex},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=TR}{%\\n         family={Taori},\\n         familyi={T\\\\bibinitperiod},\\n         given={Rohan},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=TAW}{%\\n         family={Thomas},\\n         familyi={T\\\\bibinitperiod},\\n         given={Armin\\\\bibnamedelima W.},\\n         giveni={A\\\\bibinitperiod\\\\bibinitdelim W\\\\bibinitperiod},\\n      }}%\\n      {{hash=TF}{%\\n         family={Tram{\\\\`e}r},\\n         familyi={T\\\\bibinitperiod},\\n         given={Florian},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=WRE}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Rose\\\\bibnamedelima E.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n      {{hash=WW}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={William},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=WB}{%\\n         family={Wu},\\n         familyi={W\\\\bibinitperiod},\\n         given={Bohan},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=WJ}{%\\n         family={Wu},\\n         familyi={W\\\\bibinitperiod},\\n         given={Jiajun},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=WY}{%\\n         family={Wu},\\n         familyi={W\\\\bibinitperiod},\\n         given={Yuhuai},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=XSM}{%\\n         family={Xie},\\n         familyi={X\\\\bibinitperiod},\\n         given={Sang\\\\bibnamedelima Michael},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=YM}{%\\n         family={Yasunaga},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Michihiro},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=YJ}{%\\n         family={You},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Jiaxuan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZM}{%\\n         family={Zaharia},\\n         familyi={Z\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZM}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Michael},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZT}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Tianyi},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZX}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Xikun},\\n         giveni={X\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZY}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Yuhui},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZL}{%\\n         family={Zheng},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Lucia},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZK}{%\\n         family={Zhou},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Kaitlyn},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=LP}{%\\n         family={Liang},\\n         familyi={L\\\\bibinitperiod},\\n         given={Percy},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BR+1}\\n  \\\\strng{fullhash}{BRHDAAEARASASvBMSBJBABEBEBSCDCRCNSCACKDJDDDCDMDEESEJEKFFLFCGTGLEGKGNDGSGNHTHPHJHDEHJHKHJITFJSJDKPKSKGKFKOKPWKMKRKRKALFLMLTLJLILXLLXMTMAMCDMSPMEMZNSNANDNBNANJCNHNJOGOLPIPJPCPEPCRARRRHRFRYHRCRJKRCSDSSSKSASKTATRTAWTFWREWWWBWJWYXSMYMYJZMZMZTZXZYZLZKLP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{On the Opportunities and Risks of Foundation Models}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Bostrom2019TheVW}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=BN}{%\\n         family={Bostrom},\\n         familyi={B\\\\bibinitperiod},\\n         given={Nick},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BN1}\\n    \\\\strng{fullhash}{BN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{The Vulnerable World Hypothesis}\\n    \\\\field{journaltitle}{Global Policy}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Botvin1993SmokingBO}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=BG}{%\\n         family={Botvin},\\n         familyi={B\\\\bibinitperiod},\\n         given={G.},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=GC}{%\\n         family={Goldberg},\\n         familyi={G\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=BEM}{%\\n         family={Botvin},\\n         familyi={B\\\\bibinitperiod},\\n         given={E.\\\\bibnamedelima M.},\\n         giveni={E\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=DL}{%\\n         family={Dusenbury},\\n         familyi={D\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BG+1}\\n    \\\\strng{fullhash}{BGGCBEMDL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Smoking behavior of adolescents exposed to cigarette\\n  advertising}\\n    \\\\field{journaltitle}{Public health reports}\\n    \\\\field{year}{1993}\\n  \\\\endentry\\n\\n  \\\\entry{brendel2017decision}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=BW}{%\\n         family={Brendel},\\n         familyi={B\\\\bibinitperiod},\\n         given={Wieland},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=RJ}{%\\n         family={Rauber},\\n         familyi={R\\\\bibinitperiod},\\n         given={Jonas},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=BM}{%\\n         family={Bethge},\\n         familyi={B\\\\bibinitperiod},\\n         given={Matthias},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BWRJBM1}\\n    \\\\strng{fullhash}{BWRJBM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Decision-based adversarial attacks: Reliable attacks against\\n  black-box machine learning models}\\n    \\\\field{journaltitle}{arXiv preprint arXiv:1712.04248}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{Brickman1971HedonicRA}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=BP}{%\\n         family={Brickman},\\n         familyi={B\\\\bibinitperiod},\\n         given={Philip},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=CD}{%\\n         family={Campbell},\\n         familyi={C\\\\bibinitperiod},\\n         given={Donald},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BPCD1}\\n    \\\\strng{fullhash}{BPCD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Hedonic relativism and planning the good society}\\n    \\\\field{year}{1971}\\n  \\\\endentry\\n\\n  \\\\entry{Brown2020LanguageMA}{article}{}\\n    \\\\name{author}{31}{}{%\\n      {{hash=BT}{%\\n         family={Brown},\\n         familyi={B\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=MB}{%\\n         family={Mann},\\n         familyi={M\\\\bibinitperiod},\\n         given={B.},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=RN}{%\\n         family={Ryder},\\n         familyi={R\\\\bibinitperiod},\\n         given={Nick},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=SM}{%\\n         family={Subbiah},\\n         familyi={S\\\\bibinitperiod},\\n         given={Melanie},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=KJ}{%\\n         family={Kaplan},\\n         familyi={K\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=DP}{%\\n         family={Dhariwal},\\n         familyi={D\\\\bibinitperiod},\\n         given={Prafulla},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=NA}{%\\n         family={Neelakantan},\\n         familyi={N\\\\bibinitperiod},\\n         given={Arvind},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SP}{%\\n         family={Shyam},\\n         familyi={S\\\\bibinitperiod},\\n         given={Pranav},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=SG}{%\\n         family={Sastry},\\n         familyi={S\\\\bibinitperiod},\\n         given={Girish},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=AA}{%\\n         family={Askell},\\n         familyi={A\\\\bibinitperiod},\\n         given={Amanda},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=AS}{%\\n         family={Agarwal},\\n         familyi={A\\\\bibinitperiod},\\n         given={Sandhini},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=HVA}{%\\n         family={Herbert-Voss},\\n         familyi={H\\\\bibinithyphendelim V\\\\bibinitperiod},\\n         given={Ariel},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KG}{%\\n         family={Kr{\\\\\"u}ger},\\n         familyi={K\\\\bibinitperiod},\\n         given={G.},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=HT}{%\\n         family={Henighan},\\n         familyi={H\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=CR}{%\\n         family={Child},\\n         familyi={C\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Ramesh},\\n         familyi={R\\\\bibinitperiod},\\n         given={Aditya},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZD}{%\\n         family={Ziegler},\\n         familyi={Z\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=WJ}{%\\n         family={Wu},\\n         familyi={W\\\\bibinitperiod},\\n         given={Jeffrey},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=WC}{%\\n         family={Winter},\\n         familyi={W\\\\bibinitperiod},\\n         given={Clemens},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=HC}{%\\n         family={Hesse},\\n         familyi={H\\\\bibinitperiod},\\n         given={Christopher},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=CM}{%\\n         family={Chen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Mark},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SE}{%\\n         family={Sigler},\\n         familyi={S\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=LM}{%\\n         family={Litwin},\\n         familyi={L\\\\bibinitperiod},\\n         given={Mateusz},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Gray},\\n         familyi={G\\\\bibinitperiod},\\n         given={Scott},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CB}{%\\n         family={Chess},\\n         familyi={C\\\\bibinitperiod},\\n         given={Benjamin},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=CJ}{%\\n         family={Clark},\\n         familyi={C\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=BC}{%\\n         family={Berner},\\n         familyi={B\\\\bibinitperiod},\\n         given={Christopher},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=MS}{%\\n         family={McCandlish},\\n         familyi={M\\\\bibinitperiod},\\n         given={Sam},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Radford},\\n         familyi={R\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SI}{%\\n         family={Sutskever},\\n         familyi={S\\\\bibinitperiod},\\n         given={Ilya},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BT+1}\\n  \\\\strng{fullhash}{BTMBRNSMKJDPNASPSGAAASHVAKGHTCRRAZDWJWCHCCMSELMGSCBCJBCMSRASIAD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Language Models are Few-Shot Learners}\\n    \\\\field{volume}{abs/2005.14165}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Brundage2018TheMU}{article}{}\\n    \\\\name{author}{26}{}{%\\n      {{hash=BM}{%\\n         family={Brundage},\\n         familyi={B\\\\bibinitperiod},\\n         given={Miles},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=AS}{%\\n         family={Avin},\\n         familyi={A\\\\bibinitperiod},\\n         given={Shahar},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CJ}{%\\n         family={Clark},\\n         familyi={C\\\\bibinitperiod},\\n         given={Jack},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=TH}{%\\n         family={Toner},\\n         familyi={T\\\\bibinitperiod},\\n         given={H.},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=EP}{%\\n         family={Eckersley},\\n         familyi={E\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=GB}{%\\n         family={Garfinkel},\\n         familyi={G\\\\bibinitperiod},\\n         given={Ben},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=DA}{%\\n         family={Dafoe},\\n         familyi={D\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SP}{%\\n         family={Scharre},\\n         familyi={S\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZT}{%\\n         family={Zeitzoff},\\n         familyi={Z\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=FB}{%\\n         family={Filar},\\n         familyi={F\\\\bibinitperiod},\\n         given={Bobby},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=AH}{%\\n         family={Anderson},\\n         familyi={A\\\\bibinitperiod},\\n         given={H.},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=RH}{%\\n         family={Roff},\\n         familyi={R\\\\bibinitperiod},\\n         given={Heather},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=AGC}{%\\n         family={Allen},\\n         familyi={A\\\\bibinitperiod},\\n         given={Gregory\\\\bibnamedelima C.},\\n         giveni={G\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=FC}{%\\n         family={Flynn},\\n         familyi={F\\\\bibinitperiod},\\n         given={Carrick},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=hSO}{%\\n         family={h{\\\\\\'E}igeartaigh},\\n         familyi={h\\\\bibinitperiod},\\n         given={Se{\\\\\\'a}n\\\\bibnamedelima {\\\\\\'O}},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim {\\\\\\'O}\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Beard},\\n         familyi={B\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=BH}{%\\n         family={Belfield},\\n         familyi={B\\\\bibinitperiod},\\n         given={Haydn},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=FS}{%\\n         family={Farquhar},\\n         familyi={F\\\\bibinitperiod},\\n         given={Sebastian},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=LC}{%\\n         family={Lyle},\\n         familyi={L\\\\bibinitperiod},\\n         given={Clare},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=CR}{%\\n         family={Crootof},\\n         familyi={C\\\\bibinitperiod},\\n         given={Rebecca},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=EO}{%\\n         family={Evans},\\n         familyi={E\\\\bibinitperiod},\\n         given={Owain},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=PM}{%\\n         family={Page},\\n         familyi={P\\\\bibinitperiod},\\n         given={Michael},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=BJ}{%\\n         family={Bryson},\\n         familyi={B\\\\bibinitperiod},\\n         given={Joanna},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=YR}{%\\n         family={Yampolskiy},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Roman},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BM+1}\\n    \\\\strng{fullhash}{BMASCJTHEPGBDASPZTFBAHRHAGCSJFChSOBSBHFSLCCREOPMBJYRAD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{The Malicious Use of Artificial Intelligence: Forecasting,\\n  Prevention, and Mitigation}\\n    \\\\field{volume}{abs/1802.07228}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Brundage2020TowardTA}{article}{}\\n    \\\\name{author}{59}{}{%\\n      {{hash=BM}{%\\n         family={Brundage},\\n         familyi={B\\\\bibinitperiod},\\n         given={Miles},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=AS}{%\\n         family={Avin},\\n         familyi={A\\\\bibinitperiod},\\n         given={Shahar},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=WJ}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Jasmine},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=BH}{%\\n         family={Belfield},\\n         familyi={B\\\\bibinitperiod},\\n         given={Haydn},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=KG}{%\\n         family={Krueger},\\n         familyi={K\\\\bibinitperiod},\\n         given={Gretchen},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=HGK}{%\\n         family={Hadfield},\\n         familyi={H\\\\bibinitperiod},\\n         given={Gillian\\\\bibnamedelima K.},\\n         giveni={G\\\\bibinitperiod\\\\bibinitdelim K\\\\bibinitperiod},\\n      }}%\\n      {{hash=KH}{%\\n         family={Khlaaf},\\n         familyi={K\\\\bibinitperiod},\\n         given={Heidy},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=YJ}{%\\n         family={Yang},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Jingying},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=TH}{%\\n         family={Toner},\\n         familyi={T\\\\bibinitperiod},\\n         given={H.},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=FR}{%\\n         family={Fong},\\n         familyi={F\\\\bibinitperiod},\\n         given={Ruth},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=MT}{%\\n         family={Maharaj},\\n         familyi={M\\\\bibinitperiod},\\n         given={Tegan},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=KPW}{%\\n         family={Koh},\\n         familyi={K\\\\bibinitperiod},\\n         given={P.\\\\bibnamedelima W.},\\n         giveni={P\\\\bibinitperiod\\\\bibinitdelim W\\\\bibinitperiod},\\n      }}%\\n      {{hash=HS}{%\\n         family={Hooker},\\n         familyi={H\\\\bibinitperiod},\\n         given={Sara},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Leung},\\n         familyi={L\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=TA}{%\\n         family={Trask},\\n         familyi={T\\\\bibinitperiod},\\n         given={Andrew},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=BE}{%\\n         family={Bluemke},\\n         familyi={B\\\\bibinitperiod},\\n         given={Emma},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Lebensbold},\\n         familyi={L\\\\bibinitperiod},\\n         given={Jonathan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=OC}{%\\n         family={O\\'Keefe},\\n         familyi={O\\\\bibinitperiod},\\n         given={Cullen},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=KM}{%\\n         family={Koren},\\n         familyi={K\\\\bibinitperiod},\\n         given={Mark},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=RT}{%\\n         family={Ryffel},\\n         familyi={R\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=RJ}{%\\n         family={Rubinovitz},\\n         familyi={R\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=BT}{%\\n         family={Besiroglu},\\n         familyi={B\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=CF}{%\\n         family={Carugati},\\n         familyi={C\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=CJ}{%\\n         family={Clark},\\n         familyi={C\\\\bibinitperiod},\\n         given={Jack},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=EP}{%\\n         family={Eckersley},\\n         familyi={E\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=dHS}{%\\n         prefix={de},\\n         prefixi={d\\\\bibinitperiod},\\n         family={Haas},\\n         familyi={H\\\\bibinitperiod},\\n         given={Sarah},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=JML}{%\\n         family={Johnson},\\n         familyi={J\\\\bibinitperiod},\\n         given={Maritza\\\\bibnamedelima L.},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim L\\\\bibinitperiod},\\n      }}%\\n      {{hash=LB}{%\\n         family={Laurie},\\n         familyi={L\\\\bibinitperiod},\\n         given={B.},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=IA}{%\\n         family={Ingerman},\\n         familyi={I\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KI}{%\\n         family={Krawczuk},\\n         familyi={K\\\\bibinitperiod},\\n         given={I.},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=AA}{%\\n         family={Askell},\\n         familyi={A\\\\bibinitperiod},\\n         given={Amanda},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=CR}{%\\n         family={Cammarota},\\n         familyi={C\\\\bibinitperiod},\\n         given={Rosario},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=LA}{%\\n         family={Lohn},\\n         familyi={L\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KD}{%\\n         family={Krueger},\\n         familyi={K\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SC}{%\\n         family={Stix},\\n         familyi={S\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=HP}{%\\n         family={Henderson},\\n         familyi={H\\\\bibinitperiod},\\n         given={Peter},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=GL}{%\\n         family={Graham},\\n         familyi={G\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=PCEA}{%\\n         family={Prunkl},\\n         familyi={P\\\\bibinitperiod},\\n         given={Carina E.\\\\bibnamedelima A.},\\n         giveni={C\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod\\\\bibinitdelim\\n  A\\\\bibinitperiod},\\n      }}%\\n      {{hash=MB}{%\\n         family={Martin},\\n         familyi={M\\\\bibinitperiod},\\n         given={Bianca},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=SE}{%\\n         family={Seger},\\n         familyi={S\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZN}{%\\n         family={Zilberman},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Noa},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=hSO}{%\\n         family={h\\'Eigeartaigh},\\n         familyi={h\\\\bibinitperiod},\\n         given={Se\\'an\\\\bibnamedelima \\'O},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim O\\\\bibinitperiod},\\n      }}%\\n      {{hash=KF}{%\\n         family={Kroeger},\\n         familyi={K\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=SG}{%\\n         family={Sastry},\\n         familyi={S\\\\bibinitperiod},\\n         given={Girish},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=KR}{%\\n         family={Kagan},\\n         familyi={K\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=WA}{%\\n         family={Weller},\\n         familyi={W\\\\bibinitperiod},\\n         given={Adrian},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=TB}{%\\n         family={Tse},\\n         familyi={T\\\\bibinitperiod},\\n         given={Brian},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=BE}{%\\n         family={Barnes},\\n         familyi={B\\\\bibinitperiod},\\n         given={Elizabeth},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=DA}{%\\n         family={Dafoe},\\n         familyi={D\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SP}{%\\n         family={Scharre},\\n         familyi={S\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=HVA}{%\\n         family={Herbert-Voss},\\n         familyi={H\\\\bibinithyphendelim V\\\\bibinitperiod},\\n         given={Ariel},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=RM}{%\\n         family={Rasser},\\n         familyi={R\\\\bibinitperiod},\\n         given={Martijn},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Sodhani},\\n         familyi={S\\\\bibinitperiod},\\n         given={Shagun},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=FC}{%\\n         family={Flynn},\\n         familyi={F\\\\bibinitperiod},\\n         given={Carrick},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=GT}{%\\n         family={Gilbert},\\n         familyi={G\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=DL}{%\\n         family={Dyer},\\n         familyi={D\\\\bibinitperiod},\\n         given={Lisa},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=KS}{%\\n         family={Khan},\\n         familyi={K\\\\bibinitperiod},\\n         given={Saif},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=BY}{%\\n         family={Bengio},\\n         familyi={B\\\\bibinitperiod},\\n         given={Yoshua},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=AM}{%\\n         family={Anderljung},\\n         familyi={A\\\\bibinitperiod},\\n         given={Markus},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BM+1}\\n  \\\\strng{fullhash}{BMASWJBHKGHGKKHYJTHFRMTKPWHSLJTABELJOCKMRTRJBTCFCJEPHSdJMLLBIAKIAACRLAKDSCHPGLPCEAMBSEZNhSOKFSGKRWATBBEDASPHVARMSSFCGTDLKSBYAM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Toward Trustworthy AI Development: Mechanisms for Supporting\\n  Verifiable Claims}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Brynjolfsson2009WhatTG}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=BE}{%\\n         family={Brynjolfsson},\\n         familyi={B\\\\bibinitperiod},\\n         given={Erik},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=SA}{%\\n         family={Saunders},\\n         familyi={S\\\\bibinitperiod},\\n         given={Adam},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BESA1}\\n    \\\\strng{fullhash}{BESA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{What the GDP Gets Wrong (Why Managers Should Care)}\\n    \\\\field{journaltitle}{MIT Sloan Management Review}\\n    \\\\field{year}{2009}\\n  \\\\endentry\\n\\n  \\\\entry{Buchanan2020Cyber}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=BB}{%\\n         family={Buchanan},\\n         familyi={B\\\\bibinitperiod},\\n         given={Ben},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=BJ}{%\\n         family={Bansemer},\\n         familyi={B\\\\bibinitperiod},\\n         given={John},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=CD}{%\\n         family={Cary},\\n         familyi={C\\\\bibinitperiod},\\n         given={Dakota},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Lucas},\\n         familyi={L\\\\bibinitperiod},\\n         given={Jack},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Musser},\\n         familyi={M\\\\bibinitperiod},\\n         given={Micah},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BB+2}\\n    \\\\strng{fullhash}{BBBJCDLJMM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Automating Cyber Attacks}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Buchanan2021Lies}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=BB}{%\\n         family={Buchanan},\\n         familyi={B\\\\bibinitperiod},\\n         given={Ben},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=LA}{%\\n         family={Lohn},\\n         familyi={L\\\\bibinitperiod},\\n         given={Andrew},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Musser},\\n         familyi={M\\\\bibinitperiod},\\n         given={Micah},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SK}{%\\n         family={Sedova},\\n         familyi={S\\\\bibinitperiod},\\n         given={Katerina},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{BB+2}\\n    \\\\strng{fullhash}{BBLAMMSK1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{B}\\n    \\\\field{sortinithash}{B}\\n    \\\\field{title}{Truth, Lies, and Automation}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Carlini2021PoisoningAB}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=CN}{%\\n         family={Carlini},\\n         familyi={C\\\\bibinitperiod},\\n         given={Nicholas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=TA}{%\\n         family={Terzis},\\n         familyi={T\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CNTA1}\\n    \\\\strng{fullhash}{CNTA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{Poisoning and Backdooring Contrastive Learning}\\n    \\\\field{volume}{abs/2106.09667}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{carlini2017towards}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=CN}{%\\n         family={Carlini},\\n         familyi={C\\\\bibinitperiod},\\n         given={Nicholas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=WD}{%\\n         family={Wagner},\\n         familyi={W\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{organization}{1}{%\\n      {IEEE}%\\n    }\\n    \\\\strng{namehash}{CNWD1}\\n    \\\\strng{fullhash}{CNWD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{booktitle}{2017 ieee symposium on security and privacy (sp)}\\n    \\\\field{pages}{39\\\\bibrangedash 57}\\n    \\\\field{title}{Towards evaluating the robustness of neural networks}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{Carmon2019UnlabeledDI}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=CY}{%\\n         family={Carmon},\\n         familyi={C\\\\bibinitperiod},\\n         given={Y.},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Raghunathan},\\n         familyi={R\\\\bibinitperiod},\\n         given={Aditi},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SL}{%\\n         family={Schmidt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Ludwig},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=LP}{%\\n         family={Liang},\\n         familyi={L\\\\bibinitperiod},\\n         given={Percy},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=DJC}{%\\n         family={Duchi},\\n         familyi={D\\\\bibinitperiod},\\n         given={John\\\\bibnamedelima C.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CY+1}\\n    \\\\strng{fullhash}{CYRASLLPDJC1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{booktitle}{NeurIPS}\\n    \\\\field{title}{Unlabeled Data Improves Adversarial Robustness}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{caron2021emerging}{inproceedings}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=CM}{%\\n         family={Caron},\\n         familyi={C\\\\bibinitperiod},\\n         given={Mathilde},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=TH}{%\\n         family={Touvron},\\n         familyi={T\\\\bibinitperiod},\\n         given={Hugo},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=MI}{%\\n         family={Misra},\\n         familyi={M\\\\bibinitperiod},\\n         given={Ishan},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=JH}{%\\n         family={J\\\\\\'egou},\\n         familyi={J\\\\bibinitperiod},\\n         given={Herv\\\\\\'e},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=MJ}{%\\n         family={Mairal},\\n         familyi={M\\\\bibinitperiod},\\n         given={Julien},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=BP}{%\\n         family={Bojanowski},\\n         familyi={B\\\\bibinitperiod},\\n         given={Piotr},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=JA}{%\\n         family={Joulin},\\n         familyi={J\\\\bibinitperiod},\\n         given={Armand},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CM+1}\\n    \\\\strng{fullhash}{CMTHMIJHMJBPJA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{booktitle}{Proceedings of the International Conference on Computer\\n  Vision (ICCV)}\\n    \\\\field{title}{Emerging Properties in Self-Supervised Vision Transformers}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Cary2020DestructiveCO}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=CD}{%\\n         family={Cary},\\n         familyi={C\\\\bibinitperiod},\\n         given={Dakota},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=CD}{%\\n         family={Cebul},\\n         familyi={C\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CDCD1}\\n    \\\\strng{fullhash}{CDCD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{Destructive Cyber Operations and Machine Learning}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Chen2021EvaluatingLL}{article}{}\\n    \\\\name{author}{53}{}{%\\n      {{hash=CM}{%\\n         family={Chen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Mark},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=TJ}{%\\n         family={Tworek},\\n         familyi={T\\\\bibinitperiod},\\n         given={Jerry},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=JH}{%\\n         family={Jun},\\n         familyi={J\\\\bibinitperiod},\\n         given={Heewoo},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=YQ}{%\\n         family={Yuan},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Qiming},\\n         giveni={Q\\\\bibinitperiod},\\n      }}%\\n      {{hash=PH}{%\\n         family={Ponde},\\n         familyi={P\\\\bibinitperiod},\\n         given={Henrique},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=KJ}{%\\n         family={Kaplan},\\n         familyi={K\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=EH}{%\\n         family={Edwards},\\n         familyi={E\\\\bibinitperiod},\\n         given={Harrison},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=BY}{%\\n         family={Burda},\\n         familyi={B\\\\bibinitperiod},\\n         given={Yura},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=JN}{%\\n         family={Joseph},\\n         familyi={J\\\\bibinitperiod},\\n         given={Nicholas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=BG}{%\\n         family={Brockman},\\n         familyi={B\\\\bibinitperiod},\\n         given={Greg},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Ray},\\n         familyi={R\\\\bibinitperiod},\\n         given={Alex},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=PR}{%\\n         family={Puri},\\n         familyi={P\\\\bibinitperiod},\\n         given={Raul},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=KG}{%\\n         family={Krueger},\\n         familyi={K\\\\bibinitperiod},\\n         given={Gretchen},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=PM}{%\\n         family={Petrov},\\n         familyi={P\\\\bibinitperiod},\\n         given={Michael},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=KH}{%\\n         family={Khlaaf},\\n         familyi={K\\\\bibinitperiod},\\n         given={Heidy},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=SG}{%\\n         family={Sastry},\\n         familyi={S\\\\bibinitperiod},\\n         given={Girish},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=MP}{%\\n         family={Mishkin},\\n         familyi={M\\\\bibinitperiod},\\n         given={Pamela},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=CB}{%\\n         family={Chan},\\n         familyi={C\\\\bibinitperiod},\\n         given={Brooke},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Gray},\\n         familyi={G\\\\bibinitperiod},\\n         given={Scott},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=RN}{%\\n         family={Ryder},\\n         familyi={R\\\\bibinitperiod},\\n         given={Nick},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=PM}{%\\n         family={Pavlov},\\n         familyi={P\\\\bibinitperiod},\\n         given={Mikhail},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=PA}{%\\n         family={Power},\\n         familyi={P\\\\bibinitperiod},\\n         given={Alethea.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KL}{%\\n         family={Kaiser},\\n         familyi={K\\\\bibinitperiod},\\n         given={Lukasz},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=BM}{%\\n         family={Bavarian},\\n         familyi={B\\\\bibinitperiod},\\n         given={Mohammad},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=WC}{%\\n         family={Winter},\\n         familyi={W\\\\bibinitperiod},\\n         given={Clemens},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=TP}{%\\n         family={Tillet},\\n         familyi={T\\\\bibinitperiod},\\n         given={Philippe},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=SF}{%\\n         family={Such},\\n         familyi={S\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=CD}{%\\n         family={Cummings},\\n         familyi={C\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=PM}{%\\n         family={Plappert},\\n         familyi={P\\\\bibinitperiod},\\n         given={Matthias},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=CF}{%\\n         family={Chantzis},\\n         familyi={C\\\\bibinitperiod},\\n         given={Fotios},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=BE}{%\\n         family={Barnes},\\n         familyi={B\\\\bibinitperiod},\\n         given={Elizabeth},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=HVA}{%\\n         family={Herbert-Voss},\\n         familyi={H\\\\bibinithyphendelim V\\\\bibinitperiod},\\n         given={Ariel},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=GWH}{%\\n         family={Guss},\\n         familyi={G\\\\bibinitperiod},\\n         given={William\\\\bibnamedelima H.},\\n         giveni={W\\\\bibinitperiod\\\\bibinitdelim H\\\\bibinitperiod},\\n      }}%\\n      {{hash=NA}{%\\n         family={Nichol},\\n         familyi={N\\\\bibinitperiod},\\n         given={Alex},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=BI}{%\\n         family={Babuschkin},\\n         familyi={B\\\\bibinitperiod},\\n         given={I.},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Balaji},\\n         familyi={B\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=JS}{%\\n         family={Jain},\\n         familyi={J\\\\bibinitperiod},\\n         given={Shantanu},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Carr},\\n         familyi={C\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Leike},\\n         familyi={L\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AJ}{%\\n         family={Achiam},\\n         familyi={A\\\\bibinitperiod},\\n         given={Joshua},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=MV}{%\\n         family={Misra},\\n         familyi={M\\\\bibinitperiod},\\n         given={Vedant},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=ME}{%\\n         family={Morikawa},\\n         familyi={M\\\\bibinitperiod},\\n         given={Evan},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Radford},\\n         familyi={R\\\\bibinitperiod},\\n         given={Alec},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KM}{%\\n         family={Knight},\\n         familyi={K\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=BM}{%\\n         family={Brundage},\\n         familyi={B\\\\bibinitperiod},\\n         given={Miles},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Murati},\\n         familyi={M\\\\bibinitperiod},\\n         given={Mira},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=MK}{%\\n         family={Mayer},\\n         familyi={M\\\\bibinitperiod},\\n         given={Katie},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=WP}{%\\n         family={Welinder},\\n         familyi={W\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=MB}{%\\n         family={McGrew},\\n         familyi={M\\\\bibinitperiod},\\n         given={Bob},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MS}{%\\n         family={McCandlish},\\n         familyi={M\\\\bibinitperiod},\\n         given={Sam},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=SI}{%\\n         family={Sutskever},\\n         familyi={S\\\\bibinitperiod},\\n         given={Ilya},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZW}{%\\n         family={Zaremba},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Wojciech},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CM+2}\\n  \\\\strng{fullhash}{CMTJJHYQPHKJEHBYJNBGRAPRKGPMKHSGMPCBGSRNPMPAKLBMWCTPSFCDPMCFBEHVAGWHNABIBSJSCALJAJMVMERAKMBMMMMKWPMBADMSSIZW1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{Evaluating Large Language Models Trained on Code}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Chen2019StatefulDO}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=CS}{%\\n         family={Chen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Steven},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CN}{%\\n         family={Carlini},\\n         familyi={C\\\\bibinitperiod},\\n         given={Nicholas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=WDA}{%\\n         family={Wagner},\\n         familyi={W\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima A.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CSCNWDA1}\\n    \\\\strng{fullhash}{CSCNWDA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{Stateful Detection of Black-Box Adversarial Attacks}\\n    \\\\field{journaltitle}{Proceedings of the 1st ACM Workshop on Security and\\n  Privacy on Artificial Intelligence}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{boatrace}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=CJ}{%\\n         family={Clark},\\n         familyi={C\\\\bibinitperiod},\\n         given={Jack},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CJAD1}\\n    \\\\strng{fullhash}{CJAD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{Faulty Reward Functions in the Wild}\\n    \\\\field{journaltitle}{OpenAI}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{Cobbe2019QuantifyingGI}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=CK}{%\\n         family={Cobbe},\\n         familyi={C\\\\bibinitperiod},\\n         given={Karl},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=KO}{%\\n         family={Klimov},\\n         familyi={K\\\\bibinitperiod},\\n         given={Oleg},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=HC}{%\\n         family={Hesse},\\n         familyi={H\\\\bibinitperiod},\\n         given={Christopher},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=KT}{%\\n         family={Kim},\\n         familyi={K\\\\bibinitperiod},\\n         given={Taehoon},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Schulman},\\n         familyi={S\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CK+1}\\n    \\\\strng{fullhash}{CKKOHCKTSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Quantifying Generalization in Reinforcement Learning}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Cohen2019CertifiedAR}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=CJM}{%\\n         family={Cohen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Jeremy\\\\bibnamedelima M.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=RE}{%\\n         family={Rosenfeld},\\n         familyi={R\\\\bibinitperiod},\\n         given={Elan},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=KJZ}{%\\n         family={Kolter},\\n         familyi={K\\\\bibinitperiod},\\n         given={J.\\\\bibnamedelima Z.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim Z\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CJMREKJZ1}\\n    \\\\strng{fullhash}{CJMREKJZ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Certified Adversarial Robustness via Randomized Smoothing}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{gide3}{misc}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=CNAAD}{%\\n         family={Command},\\n         familyi={C\\\\bibinitperiod},\\n         given={North American Aerospace\\\\bibnamedelima Defense},\\n         giveni={N\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod\\\\bibinitdelim\\n  A\\\\bibinitperiod\\\\bibinitdelim D\\\\bibinitperiod},\\n      }}%\\n      {{hash=AUNCP}{%\\n         family={Affairs},\\n         familyi={A\\\\bibinitperiod},\\n         given={U.S. Northern Command\\\\bibnamedelima Public},\\n         giveni={U\\\\bibinitperiod\\\\bibinitdelim N\\\\bibinitperiod\\\\bibinitdelim\\n  C\\\\bibinitperiod\\\\bibinitdelim P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CNAADAUNCP1}\\n    \\\\strng{fullhash}{CNAADAUNCP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\verb{url}\\n    \\\\verb https://www.af.mil/News/Article-Display/Article/2703548/norad-usnorth\\n    \\\\verb com-lead-3rd-global-information-dominance-experiment/\\n    \\\\endverb\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Critch2020AIRC}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=CA}{%\\n         family={Critch},\\n         familyi={C\\\\bibinitperiod},\\n         given={Andrew},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KD}{%\\n         family={Krueger},\\n         familyi={K\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CAKD1}\\n    \\\\strng{fullhash}{CAKD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{AI Research Considerations for Human Existential Safety\\n  (ARCHES)}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Croce2020RobustBenchAS}{article}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=CF}{%\\n         family={Croce},\\n         familyi={C\\\\bibinitperiod},\\n         given={Francesco},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=AM}{%\\n         family={Andriushchenko},\\n         familyi={A\\\\bibinitperiod},\\n         given={Maksym},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SV}{%\\n         family={Sehwag},\\n         familyi={S\\\\bibinitperiod},\\n         given={V.},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=FN}{%\\n         family={Flammarion},\\n         familyi={F\\\\bibinitperiod},\\n         given={Nicolas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=CM}{%\\n         family={Chiang},\\n         familyi={C\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=MP}{%\\n         family={Mittal},\\n         familyi={M\\\\bibinitperiod},\\n         given={Prateek},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=HM}{%\\n         family={Hein},\\n         familyi={H\\\\bibinitperiod},\\n         given={Matthias},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CF+1}\\n    \\\\strng{fullhash}{CFAMSVFNCMMPHM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{RobustBench: a standardized adversarial robustness benchmark}\\n    \\\\field{volume}{abs/2010.09670}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{cvach2012monitor}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=CM}{%\\n         family={Cvach},\\n         familyi={C\\\\bibinitperiod},\\n         given={Maria},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{CM1}\\n    \\\\strng{fullhash}{CM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{C}\\n    \\\\field{sortinithash}{C}\\n    \\\\field{title}{Monitor alarm fatigue: an integrative review}\\n    \\\\field{journaltitle}{Biomedical instrumentation \\\\& technology}\\n    \\\\field{year}{2012}\\n  \\\\endentry\\n\\n  \\\\entry{dafoe2018ai}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=DA}{%\\n         family={Dafoe},\\n         familyi={D\\\\bibinitperiod},\\n         given={Allan},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{DA1}\\n    \\\\strng{fullhash}{DA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{D}\\n    \\\\field{sortinithash}{D}\\n    \\\\field{title}{AI governance: a research agenda}\\n    \\\\field{journaltitle}{Governance of AI Program, Future of Humanity\\n  Institute, University of Oxford: Oxford, UK}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Dafoe2020OpenPI}{article}{}\\n    \\\\name{author}{8}{}{%\\n      {{hash=DA}{%\\n         family={Dafoe},\\n         familyi={D\\\\bibinitperiod},\\n         given={Allan},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=HE}{%\\n         family={Hughes},\\n         familyi={H\\\\bibinitperiod},\\n         given={Edward},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=BY}{%\\n         family={Bachrach},\\n         familyi={B\\\\bibinitperiod},\\n         given={Yoram},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=CT}{%\\n         family={Collins},\\n         familyi={C\\\\bibinitperiod},\\n         given={Tantum},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=MKR}{%\\n         family={McKee},\\n         familyi={M\\\\bibinitperiod},\\n         given={Kevin\\\\bibnamedelima R.},\\n         giveni={K\\\\bibinitperiod\\\\bibinitdelim R\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJZ}{%\\n         family={Leibo},\\n         familyi={L\\\\bibinitperiod},\\n         given={Joel\\\\bibnamedelima Z.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=LK}{%\\n         family={Larson},\\n         familyi={L\\\\bibinitperiod},\\n         given={Kate},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=GT}{%\\n         family={Graepel},\\n         familyi={G\\\\bibinitperiod},\\n         given={Thore},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{DA+1}\\n    \\\\strng{fullhash}{DAHEBYCTMKRLJZLKGT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{D}\\n    \\\\field{sortinithash}{D}\\n    \\\\field{title}{Open Problems in Cooperative AI}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Danesh2021OutofDistributionDD}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=DMH}{%\\n         family={Danesh},\\n         familyi={D\\\\bibinitperiod},\\n         given={Mohamad\\\\bibnamedelima H.},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim H\\\\bibinitperiod},\\n      }}%\\n      {{hash=FA}{%\\n         family={Fern},\\n         familyi={F\\\\bibinitperiod},\\n         given={Alan},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{DMHFA1}\\n    \\\\strng{fullhash}{DMHFA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{D}\\n    \\\\field{sortinithash}{D}\\n    \\\\field{title}{Out-of-Distribution Dynamics Detection: RL-Relevant\\n  Benchmarks and Results}\\n    \\\\field{volume}{abs/2107.04982}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{DoD}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=oDD}{%\\n         prefix={of},\\n         prefixi={o\\\\bibinitperiod},\\n         family={Defense},\\n         familyi={D\\\\bibinitperiod},\\n         given={Department},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{DDo1}\\n    \\\\strng{fullhash}{DDo1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{D}\\n    \\\\field{sortinithash}{D}\\n    \\\\field{title}{Quadrennial Defense Review Report}\\n    \\\\field{year}{2001}\\n    \\\\warn{\\\\item Invalid format of field \\'month\\'}\\n  \\\\endentry\\n\\n  \\\\entry{denardis2007history}{incollection}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=DL}{%\\n         family={DeNardis},\\n         familyi={D\\\\bibinitperiod},\\n         given={Laura},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {Elsevier}%\\n    }\\n    \\\\strng{namehash}{DL1}\\n    \\\\strng{fullhash}{DL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{D}\\n    \\\\field{sortinithash}{D}\\n    \\\\field{booktitle}{The history of information security}\\n    \\\\field{title}{A history of internet security}\\n    \\\\field{year}{2007}\\n  \\\\endentry\\n\\n  \\\\entry{Dietterich2018RobustAI}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=DTG}{%\\n         family={Dietterich},\\n         familyi={D\\\\bibinitperiod},\\n         given={Thomas\\\\bibnamedelima G.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim G\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{DTG1}\\n    \\\\strng{fullhash}{DTG1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{D}\\n    \\\\field{sortinithash}{D}\\n    \\\\field{title}{Robust artificial intelligence and robust human\\n  organizations}\\n    \\\\field{journaltitle}{Frontiers of Computer Science}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Ecoffet2021ReinforcementLU}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=EA}{%\\n         family={Ecoffet},\\n         familyi={E\\\\bibinitperiod},\\n         given={Adrien},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Lehman},\\n         familyi={L\\\\bibinitperiod},\\n         given={Joel},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{EALJ1}\\n    \\\\strng{fullhash}{EALJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{E}\\n    \\\\field{sortinithash}{E}\\n    \\\\field{title}{Reinforcement Learning Under Moral Uncertainty}\\n    \\\\field{volume}{abs/2006.04734}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Elazar2021MeasuringAI}{article}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=EY}{%\\n         family={Elazar},\\n         familyi={E\\\\bibinitperiod},\\n         given={Yanai},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=KN}{%\\n         family={Kassner},\\n         familyi={K\\\\bibinitperiod},\\n         given={Nora},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=RS}{%\\n         family={Ravfogel},\\n         familyi={R\\\\bibinitperiod},\\n         given={Shauli},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Ravichander},\\n         familyi={R\\\\bibinitperiod},\\n         given={Abhilasha},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=HE}{%\\n         family={Hovy},\\n         familyi={H\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=SH}{%\\n         family={Sch{\\\\\"u}tze},\\n         familyi={S\\\\bibinitperiod},\\n         given={Hinrich},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=GY}{%\\n         family={Goldberg},\\n         familyi={G\\\\bibinitperiod},\\n         given={Yoav},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{EY+1}\\n    \\\\strng{fullhash}{EYKNRSRAHESHGY1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{E}\\n    \\\\field{sortinithash}{E}\\n    \\\\field{title}{Measuring and Improving Consistency in Pretrained Language\\n  Models}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{engstrom2018rotation}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=EL}{%\\n         family={Engstrom},\\n         familyi={E\\\\bibinitperiod},\\n         given={Logan},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=TB}{%\\n         family={Tran},\\n         familyi={T\\\\bibinitperiod},\\n         given={Brandon},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=TD}{%\\n         family={Tsipras},\\n         familyi={T\\\\bibinitperiod},\\n         given={Dimitris},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SL}{%\\n         family={Schmidt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Ludwig},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=MA}{%\\n         family={Madry},\\n         familyi={M\\\\bibinitperiod},\\n         given={Aleksander},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{EL+1}\\n    \\\\strng{fullhash}{ELTBTDSLMA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{E}\\n    \\\\field{sortinithash}{E}\\n    \\\\field{title}{A rotation and a translation suffice: Fooling cnns with\\n  simple transformations}\\n    \\\\field{journaltitle}{arXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{fbupdate}{misc}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=F}{%\\n         family={Facebook},\\n         familyi={F\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{F1}\\n    \\\\strng{fullhash}{F1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{F}\\n    \\\\field{sortinithash}{F}\\n    \\\\field{title}{Bringing People Closer Together}\\n    \\\\verb{url}\\n    \\\\verb https://about.fb.com/news/2018/01/news-feed-fyi-bringing-people-close\\n    \\\\verb r-together/\\n    \\\\endverb\\n  \\\\endentry\\n\\n  \\\\entry{Fajnzylber2002InequalityAV}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=FP}{%\\n         family={Fajnzylber},\\n         familyi={F\\\\bibinitperiod},\\n         given={Pablo},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=LD}{%\\n         family={Lederman},\\n         familyi={L\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=LNV}{%\\n         family={Loayza},\\n         familyi={L\\\\bibinitperiod},\\n         given={Norman\\\\bibnamedelima V.},\\n         giveni={N\\\\bibinitperiod\\\\bibinitdelim V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{FPLDLNV1}\\n    \\\\strng{fullhash}{FPLDLNV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{F}\\n    \\\\field{sortinithash}{F}\\n    \\\\field{title}{Inequality and Violent Crime}\\n    \\\\field{journaltitle}{The Journal of Law and Economics}\\n    \\\\field{year}{2002}\\n  \\\\endentry\\n\\n  \\\\entry{Folkert2021}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=FW}{%\\n         family={Folkert},\\n         familyi={F\\\\bibinitperiod},\\n         given={Wendi},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{FW1}\\n    \\\\strng{fullhash}{FW1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{F}\\n    \\\\field{sortinithash}{F}\\n    \\\\field{title}{Assessment results regarding Organization Designation\\n  Authorization (ODA) Unit Member (UM) Independence}\\n    \\\\field{journaltitle}{Aviation Safety}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Frola1984SystemSI}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=FFR}{%\\n         family={Frola},\\n         familyi={F\\\\bibinitperiod},\\n         given={F.\\\\bibnamedelima R.},\\n         giveni={F\\\\bibinitperiod\\\\bibinitdelim R\\\\bibinitperiod},\\n      }}%\\n      {{hash=MCO}{%\\n         family={Miller},\\n         familyi={M\\\\bibinitperiod},\\n         given={C.\\\\bibnamedelima O.},\\n         giveni={C\\\\bibinitperiod\\\\bibinitdelim O\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{FFRMCO1}\\n    \\\\strng{fullhash}{FFRMCO1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{F}\\n    \\\\field{sortinithash}{F}\\n    \\\\field{title}{System Safety in Aircraft Acquisition}\\n    \\\\field{year}{1984}\\n  \\\\endentry\\n\\n  \\\\entry{Gabriel2020ArtificialIV}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=GI}{%\\n         family={Gabriel},\\n         familyi={G\\\\bibinitperiod},\\n         given={Iason},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GI1}\\n    \\\\strng{fullhash}{GI1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Artificial Intelligence, Values and Alignment}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Gall1977SystemanticsHS}{inproceedings}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=GJ}{%\\n         family={Gall},\\n         familyi={G\\\\bibinitperiod},\\n         given={John},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GJ1}\\n    \\\\strng{fullhash}{GJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Systemantics: How Systems Work and Especially How They Fail}\\n    \\\\field{year}{1977}\\n  \\\\endentry\\n\\n  \\\\entry{Gathani2021AugmentingDM}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=GS}{%\\n         family={Gathani},\\n         familyi={G\\\\bibinitperiod},\\n         given={Sneha},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=HM}{%\\n         family={Hulsebos},\\n         familyi={H\\\\bibinitperiod},\\n         given={Madelon},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=GJ}{%\\n         family={Gale},\\n         familyi={G\\\\bibinitperiod},\\n         given={James},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=HP}{%\\n         family={Haas},\\n         familyi={H\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=cD}{%\\n         prefix={cCaugatay},\\n         prefixi={c\\\\bibinitperiod},\\n         family={Demiralp},\\n         familyi={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GS+1}\\n    \\\\strng{fullhash}{GSHMGJHPDc1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Augmenting Decision Making via Interactive What-If Analysis}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Geist2001WhatDT}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=GH}{%\\n         family={Geist},\\n         familyi={G\\\\bibinitperiod},\\n         given={Helmut},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=LE}{%\\n         family={Lambin},\\n         familyi={L\\\\bibinitperiod},\\n         given={Eric},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GHLE1}\\n    \\\\strng{fullhash}{GHLE1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{What drives tropical deforestation?: a meta-analysis of\\n  proximate and underlying causes of deforestation based on subnational case\\n  study evidence}\\n    \\\\field{year}{2001}\\n  \\\\endentry\\n\\n  \\\\entry{Gil2019A2C}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=GY}{%\\n         family={Gil},\\n         familyi={G\\\\bibinitperiod},\\n         given={Yolanda},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=SB}{%\\n         family={Selman},\\n         familyi={S\\\\bibinitperiod},\\n         given={Bart},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GYSB1}\\n    \\\\strng{fullhash}{GYSB1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{A 20-Year Community Roadmap for Artificial Intelligence\\n  Research in the US}\\n    \\\\field{volume}{abs/1908.02624}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Gilmer2018MotivatingTR}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=GJ}{%\\n         family={Gilmer},\\n         familyi={G\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=ARP}{%\\n         family={Adams},\\n         familyi={A\\\\bibinitperiod},\\n         given={Ryan\\\\bibnamedelima P.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim P\\\\bibinitperiod},\\n      }}%\\n      {{hash=GI}{%\\n         family={Goodfellow},\\n         familyi={G\\\\bibinitperiod},\\n         given={I.},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=ADG}{%\\n         family={Andersen},\\n         familyi={A\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima G.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim G\\\\bibinitperiod},\\n      }}%\\n      {{hash=DGE}{%\\n         family={Dahl},\\n         familyi={D\\\\bibinitperiod},\\n         given={George\\\\bibnamedelima E.},\\n         giveni={G\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GJ+1}\\n    \\\\strng{fullhash}{GJARPGIADGDGE1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Motivating the Rules of the Game for Adversarial Example\\n  Research}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Gilpin2018ExplainingEA}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=GLH}{%\\n         family={Gilpin},\\n         familyi={G\\\\bibinitperiod},\\n         given={Leilani\\\\bibnamedelima H.},\\n         giveni={L\\\\bibinitperiod\\\\bibinitdelim H\\\\bibinitperiod},\\n      }}%\\n      {{hash=BD}{%\\n         family={Bau},\\n         familyi={B\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=YBZ}{%\\n         family={Yuan},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Ben\\\\bibnamedelima Z.},\\n         giveni={B\\\\bibinitperiod\\\\bibinitdelim Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=BA}{%\\n         family={Bajwa},\\n         familyi={B\\\\bibinitperiod},\\n         given={Ayesha},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SMA}{%\\n         family={Specter},\\n         familyi={S\\\\bibinitperiod},\\n         given={Michael\\\\bibnamedelima A.},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KL}{%\\n         family={Kagal},\\n         familyi={K\\\\bibinitperiod},\\n         given={Lalana},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GLH+1}\\n    \\\\strng{fullhash}{GLHBDYBZBASMAKL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Explaining Explanations: An Overview of Interpretability of\\n  Machine Learning}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Gleave2020AdversarialPA}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=GA}{%\\n         family={Gleave},\\n         familyi={G\\\\bibinitperiod},\\n         given={Adam},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=DM}{%\\n         family={Dennis},\\n         familyi={D\\\\bibinitperiod},\\n         given={Michael},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=KN}{%\\n         family={Kant},\\n         familyi={K\\\\bibinitperiod},\\n         given={Neel},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=WC}{%\\n         family={Wild},\\n         familyi={W\\\\bibinitperiod},\\n         given={Cody},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=LS}{%\\n         family={Levine},\\n         familyi={L\\\\bibinitperiod},\\n         given={Sergey},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=RSJ}{%\\n         family={Russell},\\n         familyi={R\\\\bibinitperiod},\\n         given={Stuart\\\\bibnamedelima J.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GA+1}\\n    \\\\strng{fullhash}{GADMKNWCLSRSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Adversarial Policies: Attacking Deep Reinforcement Learning}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Goodhart1984ProblemsOM}{inproceedings}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=GC}{%\\n         family={Goodhart},\\n         familyi={G\\\\bibinitperiod},\\n         given={Charles},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GC1}\\n    \\\\strng{fullhash}{GC1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Problems of Monetary Management: The UK Experience}\\n    \\\\field{year}{1984}\\n  \\\\endentry\\n\\n  \\\\entry{greenwood1997third}{book}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=GJ}{%\\n         family={Greenwood},\\n         familyi={G\\\\bibinitperiod},\\n         given={Jeremy},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {American Enterprise Institute}%\\n    }\\n    \\\\strng{namehash}{GJ1}\\n    \\\\strng{fullhash}{GJ2}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{number}{435}\\n    \\\\field{title}{The third industrial revolution: Technology, productivity,\\n  and income inequality}\\n    \\\\field{year}{1997}\\n  \\\\endentry\\n\\n  \\\\entry{Grinsztajn2021ThereIN}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=GN}{%\\n         family={Grinsztajn},\\n         familyi={G\\\\bibinitperiod},\\n         given={Nathan},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=FJ}{%\\n         family={Ferret},\\n         familyi={F\\\\bibinitperiod},\\n         given={Johan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=PO}{%\\n         family={Pietquin},\\n         familyi={P\\\\bibinitperiod},\\n         given={O.},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=PP}{%\\n         family={Preux},\\n         familyi={P\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=GM}{%\\n         family={Geist},\\n         familyi={G\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GN+1}\\n    \\\\strng{fullhash}{GNFJPOPPGM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{There Is No Turning Back: A Self-Supervised Approach for\\n  Reversibility-Aware Reinforcement Learning}\\n    \\\\field{volume}{abs/2106.04480}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{gu2017badnets}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=GT}{%\\n         family={Gu},\\n         familyi={G\\\\bibinitperiod},\\n         given={Tianyu},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=DGB}{%\\n         family={Dolan-Gavitt},\\n         familyi={D\\\\bibinithyphendelim G\\\\bibinitperiod},\\n         given={Brendan},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Garg},\\n         familyi={G\\\\bibinitperiod},\\n         given={Siddharth},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GTDGBGS1}\\n    \\\\strng{fullhash}{GTDGBGS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{Badnets: Identifying vulnerabilities in the machine learning\\n  model supply chain}\\n    \\\\field{journaltitle}{arXiv preprint arXiv:1708.06733}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{Guo2017}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=GC}{%\\n         family={Guo},\\n         familyi={G\\\\bibinitperiod},\\n         given={Chuan},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=PG}{%\\n         family={Pleiss},\\n         familyi={P\\\\bibinitperiod},\\n         given={Geoff},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=SY}{%\\n         family={Sun},\\n         familyi={S\\\\bibinitperiod},\\n         given={Yu},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=WKQ}{%\\n         family={Weinberger},\\n         familyi={W\\\\bibinitperiod},\\n         given={Kilian\\\\bibnamedelima Q.},\\n         giveni={K\\\\bibinitperiod\\\\bibinitdelim Q\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{GC+1}\\n    \\\\strng{fullhash}{GCPGSYWKQ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{G}\\n    \\\\field{sortinithash}{G}\\n    \\\\field{title}{On Calibration of Modern Neural Networks}\\n    \\\\field{journaltitle}{ICML}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{HadfieldMenell2017TheOG}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=HMD}{%\\n         family={Hadfield-Menell},\\n         familyi={H\\\\bibinithyphendelim M\\\\bibinitperiod},\\n         given={Dylan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=DA}{%\\n         family={Dragan},\\n         familyi={D\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=AP}{%\\n         family={Abbeel},\\n         familyi={A\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=RSJ}{%\\n         family={Russell},\\n         familyi={R\\\\bibinitperiod},\\n         given={Stuart\\\\bibnamedelima J.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HMD+1}\\n    \\\\strng{fullhash}{HMDDAAPRSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{The Off-Switch Game}\\n    \\\\field{journaltitle}{IJCA}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{HadfieldMenell2016CooperativeIR}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=HMD}{%\\n         family={Hadfield-Menell},\\n         familyi={H\\\\bibinithyphendelim M\\\\bibinitperiod},\\n         given={Dylan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=RSJ}{%\\n         family={Russell},\\n         familyi={R\\\\bibinitperiod},\\n         given={Stuart\\\\bibnamedelima J.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AP}{%\\n         family={Abbeel},\\n         familyi={A\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=DA}{%\\n         family={Dragan},\\n         familyi={D\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HMD+1}\\n    \\\\strng{fullhash}{HMDRSJAPDA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{booktitle}{NIPS}\\n    \\\\field{title}{Cooperative Inverse Reinforcement Learning}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{harang2020sorel20m}{misc}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=HR}{%\\n         family={Harang},\\n         familyi={H\\\\bibinitperiod},\\n         given={Richard},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=REM}{%\\n         family={Rudd},\\n         familyi={R\\\\bibinitperiod},\\n         given={Ethan\\\\bibnamedelima M.},\\n         giveni={E\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HRREM1}\\n    \\\\strng{fullhash}{HRREM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{SOREL-20M: A Large Scale Benchmark Dataset for Malicious PE\\n  Detection}\\n    \\\\field{journaltitle}{arXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Hardt2016EqualityOO}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=HM}{%\\n         family={Hardt},\\n         familyi={H\\\\bibinitperiod},\\n         given={Moritz},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=PE}{%\\n         family={Price},\\n         familyi={P\\\\bibinitperiod},\\n         given={Eric},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=SN}{%\\n         family={Srebro},\\n         familyi={S\\\\bibinitperiod},\\n         given={Nathan},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HMPESN1}\\n    \\\\strng{fullhash}{HMPESN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{booktitle}{NIPS}\\n    \\\\field{title}{Equality of Opportunity in Supervised Learning}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{Heawood1949MapColourT}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=HPJ}{%\\n         family={Heawood},\\n         familyi={H\\\\bibinitperiod},\\n         given={P.\\\\bibnamedelima J.},\\n         giveni={P\\\\bibinitperiod\\\\bibinitdelim J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HPJ1}\\n    \\\\strng{fullhash}{HPJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{pages}{161\\\\bibrangedash 175}\\n    \\\\field{title}{Map-Colour Theorem}\\n    \\\\field{journaltitle}{Proceedings of The London Mathematical Society}\\n    \\\\field{year}{1949}\\n  \\\\endentry\\n\\n  \\\\entry{Hedlund82}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=HJ}{%\\n         family={Hedlund},\\n         familyi={H\\\\bibinitperiod},\\n         given={James},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {BMJ Publishing Group Ltd}%\\n    }\\n    \\\\strng{namehash}{HJ1}\\n    \\\\strng{fullhash}{HJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Risky business: safety regulations, risk compensation, and\\n  individual behavior}\\n    \\\\field{journaltitle}{Injury Prevention}\\n    \\\\field{year}{2000}\\n  \\\\endentry\\n\\n  \\\\entry{hendrycks2021many}{article}{}\\n    \\\\name{author}{13}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Basart},\\n         familyi={B\\\\bibinitperiod},\\n         given={Steven},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=MN}{%\\n         family={Mu},\\n         familyi={M\\\\bibinitperiod},\\n         given={Norman},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=KS}{%\\n         family={Kadavath},\\n         familyi={K\\\\bibinitperiod},\\n         given={Saurav},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=WF}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Frank},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=DE}{%\\n         family={Dorundo},\\n         familyi={D\\\\bibinitperiod},\\n         given={Evan},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=DR}{%\\n         family={Desai},\\n         familyi={D\\\\bibinitperiod},\\n         given={Rahul},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZT}{%\\n         family={Zhu},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Tyler},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=PS}{%\\n         family={Parajuli},\\n         familyi={P\\\\bibinitperiod},\\n         given={Samyak},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=GM}{%\\n         family={Guo},\\n         familyi={G\\\\bibinitperiod},\\n         given={Mike},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Dawn},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=GJ}{%\\n         family={Gilmer},\\n         familyi={G\\\\bibinitperiod},\\n         given={Justin},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDBSMNKSWFDEDRZTPSGMSDSJGJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{The Many Faces of Robustness: A Critical Analysis of\\n  Out-of-Distribution Generalization}\\n    \\\\field{journaltitle}{ICCV}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Hendrycks2021AligningAW}{article}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=BC}{%\\n         family={Burns},\\n         familyi={B\\\\bibinitperiod},\\n         given={Collin},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Basart},\\n         familyi={B\\\\bibinitperiod},\\n         given={Steven},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Critch},\\n         familyi={C\\\\bibinitperiod},\\n         given={Andrew},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Jerry},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Dawn},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDBCBSCALJSDSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Aligning {AI} With Shared Human Values}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Hendrycks2021MeasuringMM}{article}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=BC}{%\\n         family={Burns},\\n         familyi={B\\\\bibinitperiod},\\n         given={Collin},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Basart},\\n         familyi={B\\\\bibinitperiod},\\n         given={Steven},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZA}{%\\n         family={Zou},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Andy},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Mazeika},\\n         familyi={M\\\\bibinitperiod},\\n         given={Mantas},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Dawn},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDBCBSZAMMSDSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Measuring Massive Multitask Language Understanding}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{hendrycks2019robustness}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=DT}{%\\n         family={Dietterich},\\n         familyi={D\\\\bibinitperiod},\\n         given={Thomas},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HDDT1}\\n    \\\\strng{fullhash}{HDDT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Benchmarking Neural Network Robustness to Common Corruptions\\n  and Perturbations}\\n    \\\\field{journaltitle}{Proceedings of the International Conference on\\n  Learning Representations}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Hendrycks2017ABF}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=GK}{%\\n         family={Gimpel},\\n         familyi={G\\\\bibinitperiod},\\n         given={Kevin},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HDGK1}\\n    \\\\strng{fullhash}{HDGK1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{A Baseline for Detecting Misclassified and\\n  Out-of-Distribution Examples in Neural Networks}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{Hendrycks2019UsingPC}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=LK}{%\\n         family={Lee},\\n         familyi={L\\\\bibinitperiod},\\n         given={Kimin},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Mazeika},\\n         familyi={M\\\\bibinitperiod},\\n         given={Mantas},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HDLKMM1}\\n    \\\\strng{fullhash}{HDLKMM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Using Pre-Training Can Improve Model Robustness and\\n  Uncertainty}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Hendrycks2019DeepAD}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Mazeika},\\n         familyi={M\\\\bibinitperiod},\\n         given={Mantas},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=DTG}{%\\n         family={Dietterich},\\n         familyi={D\\\\bibinitperiod},\\n         given={Thomas\\\\bibnamedelima G.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim G\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HDMMDTG1}\\n    \\\\strng{fullhash}{HDMMDTG1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Deep Anomaly Detection with Outlier Exposure}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Hendrycks2019UsingSL}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Mazeika},\\n         familyi={M\\\\bibinitperiod},\\n         given={Mantas},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=KS}{%\\n         family={Kadavath},\\n         familyi={K\\\\bibinitperiod},\\n         given={Saurav},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDMMKSSD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{booktitle}{NeurIPS}\\n    \\\\field{title}{Using Self-Supervised Learning Can Improve Model Robustness\\n  and Uncertainty}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{jiminy2021}{article}{}\\n    \\\\name{author}{9}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Mazeika},\\n         familyi={M\\\\bibinitperiod},\\n         given={Mantas},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZA}{%\\n         family={Zou},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Andy},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=PS}{%\\n         family={Patel},\\n         familyi={P\\\\bibinitperiod},\\n         given={Sahil},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZC}{%\\n         family={Zhu},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Christine},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=NJ}{%\\n         family={Navarro},\\n         familyi={N\\\\bibinitperiod},\\n         given={Jesus},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Dawn},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=LB}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Bo},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDMMZAPSZCNJSDLBSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{What Would Jiminy Cricket Do? Towards Agents That Behave\\n  Morally}\\n    \\\\field{journaltitle}{NeurIPS}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{hendrycks2020augmix}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MN}{%\\n         family={Mu},\\n         familyi={M\\\\bibinitperiod},\\n         given={Norman},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=CED}{%\\n         family={Cubuk},\\n         familyi={C\\\\bibinitperiod},\\n         given={Ekin\\\\bibnamedelima D.},\\n         giveni={E\\\\bibinitperiod\\\\bibinitdelim D\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZB}{%\\n         family={Zoph},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Barret},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=GJ}{%\\n         family={Gilmer},\\n         familyi={G\\\\bibinitperiod},\\n         given={Justin},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=LB}{%\\n         family={Lakshminarayanan},\\n         familyi={L\\\\bibinitperiod},\\n         given={Balaji},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDMNCEDZBGJLB1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{{AugMix}: A Simple Data Processing Method to Improve\\n  Robustness and Uncertainty}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{NAE}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZK}{%\\n         family={Zhao},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Kevin},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Basart},\\n         familyi={B\\\\bibinitperiod},\\n         given={Steven},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDZKBSSJSD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Natural Adversarial Examples}\\n    \\\\field{journaltitle}{CVPR}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{hendrycks2021pixmix}{misc}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZA}{%\\n         family={Zou},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Andy},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Mazeika},\\n         familyi={M\\\\bibinitperiod},\\n         given={Mantas},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=TL}{%\\n         family={Tang},\\n         familyi={T\\\\bibinitperiod},\\n         given={Leonard},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=LB}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Bo},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Dawn},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD+1}\\n    \\\\strng{fullhash}{HDZAMMTLLBSDSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{{PixMix}: Dreamlike Pictures Comprehensively Improve Safety\\n  Measures}\\n    \\\\field{journaltitle}{arXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Henighan2020ScalingLF}{article}{}\\n    \\\\name{author}{19}{}{%\\n      {{hash=HT}{%\\n         family={Henighan},\\n         familyi={H\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=KJ}{%\\n         family={Kaplan},\\n         familyi={K\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=KM}{%\\n         family={Katz},\\n         familyi={K\\\\bibinitperiod},\\n         given={Mor},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=CM}{%\\n         family={Chen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Mark},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=HC}{%\\n         family={Hesse},\\n         familyi={H\\\\bibinitperiod},\\n         given={Christopher},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=JJ}{%\\n         family={Jackson},\\n         familyi={J\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=JH}{%\\n         family={Jun},\\n         familyi={J\\\\bibinitperiod},\\n         given={Heewoo},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=BTB}{%\\n         family={Brown},\\n         familyi={B\\\\bibinitperiod},\\n         given={Tom\\\\bibnamedelima B.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim B\\\\bibinitperiod},\\n      }}%\\n      {{hash=DP}{%\\n         family={Dhariwal},\\n         familyi={D\\\\bibinitperiod},\\n         given={Prafulla},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Gray},\\n         familyi={G\\\\bibinitperiod},\\n         given={Scott},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=HC}{%\\n         family={Hallacy},\\n         familyi={H\\\\bibinitperiod},\\n         given={Chris},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=MB}{%\\n         family={Mann},\\n         familyi={M\\\\bibinitperiod},\\n         given={Benjamin},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Radford},\\n         familyi={R\\\\bibinitperiod},\\n         given={Alec},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Ramesh},\\n         familyi={R\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=RN}{%\\n         family={Ryder},\\n         familyi={R\\\\bibinitperiod},\\n         given={Nick},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZDM}{%\\n         family={Ziegler},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Daniel\\\\bibnamedelima M.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Schulman},\\n         familyi={S\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MS}{%\\n         family={McCandlish},\\n         familyi={M\\\\bibinitperiod},\\n         given={Sam},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HT+1}\\n    \\\\strng{fullhash}{HTKJKMCMHCJJJHBTBDPGSHCMBRARARNZDMSJADMS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Scaling Laws for Autoregressive Generative Modeling}\\n    \\\\field{volume}{abs/2010.14701}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Hestness2017DeepLS}{article}{}\\n    \\\\name{author}{9}{}{%\\n      {{hash=HJ}{%\\n         family={Hestness},\\n         familyi={H\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=NS}{%\\n         family={Narang},\\n         familyi={N\\\\bibinitperiod},\\n         given={Sharan},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=AN}{%\\n         family={Ardalani},\\n         familyi={A\\\\bibinitperiod},\\n         given={Newsha},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=DG}{%\\n         family={Diamos},\\n         familyi={D\\\\bibinitperiod},\\n         given={G.},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=JH}{%\\n         family={Jun},\\n         familyi={J\\\\bibinitperiod},\\n         given={Heewoo},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=KH}{%\\n         family={Kianinejad},\\n         familyi={K\\\\bibinitperiod},\\n         given={Hassan},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=PMMA}{%\\n         family={Patwary},\\n         familyi={P\\\\bibinitperiod},\\n         given={Md. Mostofa\\\\bibnamedelima Ali},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod\\\\bibinitdelim\\n  A\\\\bibinitperiod},\\n      }}%\\n      {{hash=YY}{%\\n         family={Yang},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Y.},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZY}{%\\n         family={Zhou},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Yanqi},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HJ+1}\\n    \\\\strng{fullhash}{HJNSANDGJHKHPMMAYYZY1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Deep Learning Scaling is Predictable, Empirically}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{Ho2019DetectingAC}{inproceedings}{}\\n    \\\\name{author}{8}{}{%\\n      {{hash=HG}{%\\n         family={Ho},\\n         familyi={H\\\\bibinitperiod},\\n         given={Grant},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Cidon},\\n         familyi={C\\\\bibinitperiod},\\n         given={Asaf},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=GL}{%\\n         family={Gavish},\\n         familyi={G\\\\bibinitperiod},\\n         given={Lior},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=SM}{%\\n         family={Schweighauser},\\n         familyi={S\\\\bibinitperiod},\\n         given={Marco},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=PV}{%\\n         family={Paxson},\\n         familyi={P\\\\bibinitperiod},\\n         given={V.},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Savage},\\n         familyi={S\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=VG}{%\\n         family={Voelker},\\n         familyi={V\\\\bibinitperiod},\\n         given={G.},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=WDA}{%\\n         family={Wagner},\\n         familyi={W\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima A.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HG+1}\\n    \\\\strng{fullhash}{HGCAGLSMPVSSVGWDA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{booktitle}{USENIX Security Symposium}\\n    \\\\field{title}{Detecting and Characterizing Lateral Phishing at Scale}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Hong2021HandcraftedBI}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=HS}{%\\n         family={Hong},\\n         familyi={H\\\\bibinitperiod},\\n         given={Sanghyun},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CN}{%\\n         family={Carlini},\\n         familyi={C\\\\bibinitperiod},\\n         given={Nicholas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=KA}{%\\n         family={Kurakin},\\n         familyi={K\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HSCNKA1}\\n    \\\\strng{fullhash}{HSCNKA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Handcrafted Backdoors in Deep Neural Networks}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Hubinger2019RisksFL}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=HE}{%\\n         family={Hubinger},\\n         familyi={H\\\\bibinitperiod},\\n         given={Evan},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=vMC}{%\\n         prefix={van},\\n         prefixi={v\\\\bibinitperiod},\\n         family={Merwijk},\\n         familyi={M\\\\bibinitperiod},\\n         given={Chris},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=MV}{%\\n         family={Mikulik},\\n         familyi={M\\\\bibinitperiod},\\n         given={Vladimir},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Skalse},\\n         familyi={S\\\\bibinitperiod},\\n         given={Joar},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Garrabrant},\\n         familyi={G\\\\bibinitperiod},\\n         given={Scott},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HE+1}\\n    \\\\strng{fullhash}{HEMCvMVSJGS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{Risks from Learned Optimization in Advanced Machine Learning\\n  Systems}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{hume}{book}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=HD}{%\\n         family={Hume},\\n         familyi={H\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{HD1}\\n    \\\\strng{fullhash}{HD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{H}\\n    \\\\field{sortinithash}{H}\\n    \\\\field{title}{A Treatise of Human Nature}\\n    \\\\field{year}{1739}\\n  \\\\endentry\\n\\n  \\\\entry{Irving2018AISV}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=IG}{%\\n         family={Irving},\\n         familyi={I\\\\bibinitperiod},\\n         given={Geoffrey},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=CP}{%\\n         family={Christiano},\\n         familyi={C\\\\bibinitperiod},\\n         given={Paul},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{IGCPAD1}\\n    \\\\strng{fullhash}{IGCPAD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{I}\\n    \\\\field{sortinithash}{I}\\n    \\\\field{title}{AI safety via debate}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{jensen1976theory}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=JMC}{%\\n         family={Jensen},\\n         familyi={J\\\\bibinitperiod},\\n         given={Michael\\\\bibnamedelima C},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n      {{hash=MWH}{%\\n         family={Meckling},\\n         familyi={M\\\\bibinitperiod},\\n         given={William\\\\bibnamedelima H},\\n         giveni={W\\\\bibinitperiod\\\\bibinitdelim H\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {Elsevier}%\\n    }\\n    \\\\strng{namehash}{JMCMWH1}\\n    \\\\strng{fullhash}{JMCMWH1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{J}\\n    \\\\field{sortinithash}{J}\\n    \\\\field{number}{4}\\n    \\\\field{pages}{305\\\\bibrangedash 360}\\n    \\\\field{title}{Theory of the firm: Managerial behavior, agency costs and\\n  ownership structure}\\n    \\\\field{volume}{3}\\n    \\\\field{journaltitle}{Journal of financial economics}\\n    \\\\field{year}{1976}\\n  \\\\endentry\\n\\n  \\\\entry{Jin2021ForecastQAAQ}{inproceedings}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=JW}{%\\n         family={Jin},\\n         familyi={J\\\\bibinitperiod},\\n         given={Woojeong},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=KS}{%\\n         family={Kim},\\n         familyi={K\\\\bibinitperiod},\\n         given={Suji},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=KR}{%\\n         family={Khanna},\\n         familyi={K\\\\bibinitperiod},\\n         given={Rahul},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=LDH}{%\\n         family={Lee},\\n         familyi={L\\\\bibinitperiod},\\n         given={Dong-Ho},\\n         giveni={D\\\\bibinithyphendelim H\\\\bibinitperiod},\\n      }}%\\n      {{hash=MF}{%\\n         family={Morstatter},\\n         familyi={M\\\\bibinitperiod},\\n         given={Fred},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=GA}{%\\n         family={Galstyan},\\n         familyi={G\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=RX}{%\\n         family={Ren},\\n         familyi={R\\\\bibinitperiod},\\n         given={Xiang},\\n         giveni={X\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{JW+1}\\n    \\\\strng{fullhash}{JWKSKRLDHMFGARX1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{J}\\n    \\\\field{sortinithash}{J}\\n    \\\\field{booktitle}{ACL/IJCNLP}\\n    \\\\field{title}{ForecastQA: A Question Answering Challenge for Event\\n  Forecasting with Temporal Text Data}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{KahnemanDeaton}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=KD}{%\\n         family={Kahneman},\\n         familyi={K\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=DA}{%\\n         family={Deaton},\\n         familyi={D\\\\bibinitperiod},\\n         given={Angus},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {National Academy of Sciences}%\\n    }\\n    \\\\strng{namehash}{KDDA1}\\n    \\\\strng{fullhash}{KDDA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{High income improves evaluation of life but not emotional\\n  well-being}\\n    \\\\field{journaltitle}{Proceedings of the National Academy of Sciences}\\n    \\\\field{year}{2010}\\n  \\\\endentry\\n\\n  \\\\entry{Kang2019TestingRA}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=KD}{%\\n         family={Kang},\\n         familyi={K\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SY}{%\\n         family={Sun},\\n         familyi={S\\\\bibinitperiod},\\n         given={Yi},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=HD}{%\\n         family={Hendrycks},\\n         familyi={H\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=BTB}{%\\n         family={Brown},\\n         familyi={B\\\\bibinitperiod},\\n         given={Tom\\\\bibnamedelima B.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim B\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KD+1}\\n    \\\\strng{fullhash}{KDSYHDBTBSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{Testing Robustness Against Unforeseen Adversaries}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Karra2020TheTS}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=KK}{%\\n         family={Karra},\\n         familyi={K\\\\bibinitperiod},\\n         given={Kiran},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=AC}{%\\n         family={Ashcraft},\\n         familyi={A\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=FN}{%\\n         family={Fendley},\\n         familyi={F\\\\bibinitperiod},\\n         given={Neil},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KKACFN1}\\n    \\\\strng{fullhash}{KKACFN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{The TrojAI Software Framework: An OpenSource tool for\\n  Embedding Trojans into Deep Learning Models}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Kenton2021AlignmentOL}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=KZ}{%\\n         family={Kenton},\\n         familyi={K\\\\bibinitperiod},\\n         given={Zachary},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=ET}{%\\n         family={Everitt},\\n         familyi={E\\\\bibinitperiod},\\n         given={Tom},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=WL}{%\\n         family={Weidinger},\\n         familyi={W\\\\bibinitperiod},\\n         given={Laura},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=GI}{%\\n         family={Gabriel},\\n         familyi={G\\\\bibinitperiod},\\n         given={Iason},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=MV}{%\\n         family={Mikulik},\\n         familyi={M\\\\bibinitperiod},\\n         given={Vladimir},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=IG}{%\\n         family={Irving},\\n         familyi={I\\\\bibinitperiod},\\n         given={Geoffrey},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KZ+1}\\n    \\\\strng{fullhash}{KZETWLGIMVIG1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{Alignment of Language Agents}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Kirilenko2011TheFC}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=KA}{%\\n         family={Kirilenko},\\n         familyi={K\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SM}{%\\n         family={Samadi},\\n         familyi={S\\\\bibinitperiod},\\n         given={Mehrdad},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=KA}{%\\n         family={Kyle},\\n         familyi={K\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=TT}{%\\n         family={Tuzun},\\n         familyi={T\\\\bibinitperiod},\\n         given={Tugkan},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KA+1}\\n    \\\\strng{fullhash}{KASMKATT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{The Flash Crash: The Impact of High Frequency Trading on an\\n  Electronic Market}\\n    \\\\field{year}{2011}\\n  \\\\endentry\\n\\n  \\\\entry{Koch2021ObjectiveRI}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=KJ}{%\\n         family={Koch},\\n         familyi={K\\\\bibinitperiod},\\n         given={Jack},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=LL}{%\\n         family={Langosco},\\n         familyi={L\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=PJ}{%\\n         family={Pfau},\\n         familyi={P\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Le},\\n         familyi={L\\\\bibinitperiod},\\n         given={James},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=SL}{%\\n         family={Sharkey},\\n         familyi={S\\\\bibinitperiod},\\n         given={Lee},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KJ+1}\\n    \\\\strng{fullhash}{KJLLPJLJSL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{Objective Robustness in Deep Reinforcement Learning}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Koh2021WILDSAB}{inproceedings}{}\\n    \\\\name{author}{16}{}{%\\n      {{hash=KPW}{%\\n         family={Koh},\\n         familyi={K\\\\bibinitperiod},\\n         given={P.\\\\bibnamedelima W.},\\n         giveni={P\\\\bibinitperiod\\\\bibinitdelim W\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Sagawa},\\n         familyi={S\\\\bibinitperiod},\\n         given={Shiori},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=MH}{%\\n         family={Marklund},\\n         familyi={M\\\\bibinitperiod},\\n         given={H.},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=XSM}{%\\n         family={Xie},\\n         familyi={X\\\\bibinitperiod},\\n         given={Sang\\\\bibnamedelima Michael},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZM}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Marvin},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=BA}{%\\n         family={Balsubramani},\\n         familyi={B\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=hHW}{%\\n         prefix={hua},\\n         prefixi={h\\\\bibinitperiod},\\n         family={Hu},\\n         familyi={H\\\\bibinitperiod},\\n         given={Wei},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=YM}{%\\n         family={Yasunaga},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Michihiro},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=PRL}{%\\n         family={Phillips},\\n         familyi={P\\\\bibinitperiod},\\n         given={Richard\\\\bibnamedelima L.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim L\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Beery},\\n         familyi={B\\\\bibinitperiod},\\n         given={Sara},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Leskovec},\\n         familyi={L\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=KA}{%\\n         family={Kundaje},\\n         familyi={K\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=PE}{%\\n         family={Pierson},\\n         familyi={P\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=LS}{%\\n         family={Levine},\\n         familyi={L\\\\bibinitperiod},\\n         given={Sergey},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=FC}{%\\n         family={Finn},\\n         familyi={F\\\\bibinitperiod},\\n         given={Chelsea},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=LP}{%\\n         family={Liang},\\n         familyi={L\\\\bibinitperiod},\\n         given={Percy},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KPW+1}\\n    \\\\strng{fullhash}{KPWSSMHXSMZMBAHWhYMPRLBSLJKAPELSFCLP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{WILDS: A Benchmark of in-the-Wild Distribution Shifts}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Krakovna2020AvoidingSE}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=KV}{%\\n         family={Krakovna},\\n         familyi={K\\\\bibinitperiod},\\n         given={Victoria},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=OL}{%\\n         family={Orseau},\\n         familyi={O\\\\bibinitperiod},\\n         given={Laurent},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=NR}{%\\n         family={Ngo},\\n         familyi={N\\\\bibinitperiod},\\n         given={Richard},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Martic},\\n         familyi={M\\\\bibinitperiod},\\n         given={Miljan},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=LS}{%\\n         family={Legg},\\n         familyi={L\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KV+1}\\n    \\\\strng{fullhash}{KVOLNRMMLS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{Avoiding Side Effects By Considering Future Tasks}\\n    \\\\field{journaltitle}{NeurIPS}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Krause2020GeDiGD}{article}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=KB}{%\\n         family={Krause},\\n         familyi={K\\\\bibinitperiod},\\n         given={Ben},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=GAD}{%\\n         family={Gotmare},\\n         familyi={G\\\\bibinitperiod},\\n         given={Akhilesh\\\\bibnamedelima Deepak},\\n         giveni={A\\\\bibinitperiod\\\\bibinitdelim D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MB}{%\\n         family={McCann},\\n         familyi={M\\\\bibinitperiod},\\n         given={Bryan},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=KN}{%\\n         family={Keskar},\\n         familyi={K\\\\bibinitperiod},\\n         given={N.},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=JSR}{%\\n         family={Joty},\\n         familyi={J\\\\bibinitperiod},\\n         given={Shafiq\\\\bibnamedelima R.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim R\\\\bibinitperiod},\\n      }}%\\n      {{hash=SR}{%\\n         family={Socher},\\n         familyi={S\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=RN}{%\\n         family={Rajani},\\n         familyi={R\\\\bibinitperiod},\\n         given={Nazneen},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KB+1}\\n    \\\\strng{fullhash}{KBGADMBKNJSRSRRN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{GeDi: Generative Discriminator Guided Sequence Generation}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{kross2013facebook}{article}{}\\n    \\\\name{author}{9}{}{%\\n      {{hash=KE}{%\\n         family={Kross},\\n         familyi={K\\\\bibinitperiod},\\n         given={Ethan},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=VP}{%\\n         family={Verduyn},\\n         familyi={V\\\\bibinitperiod},\\n         given={Philippe},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=DE}{%\\n         family={Demiralp},\\n         familyi={D\\\\bibinitperiod},\\n         given={Emre},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=PJ}{%\\n         family={Park},\\n         familyi={P\\\\bibinitperiod},\\n         given={Jiyoung},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=LDS}{%\\n         family={Lee},\\n         familyi={L\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima Seungjae},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim S\\\\bibinitperiod},\\n      }}%\\n      {{hash=LN}{%\\n         family={Lin},\\n         familyi={L\\\\bibinitperiod},\\n         given={Natalie},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=SH}{%\\n         family={Shablack},\\n         familyi={S\\\\bibinitperiod},\\n         given={Holly},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=JJ}{%\\n         family={Jonides},\\n         familyi={J\\\\bibinitperiod},\\n         given={John},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=YO}{%\\n         family={Ybarra},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Oscar},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KE+1}\\n    \\\\strng{fullhash}{KEVPDEPJLDSLNSHJJYO1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{Facebook use predicts declines in subjective well-being in\\n  young adults}\\n    \\\\field{journaltitle}{PloS one}\\n  \\\\endentry\\n\\n  \\\\entry{Krueger2020HiddenIF}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=KD}{%\\n         family={Krueger},\\n         familyi={K\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MT}{%\\n         family={Maharaj},\\n         familyi={M\\\\bibinitperiod},\\n         given={Tegan},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Leike},\\n         familyi={L\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KDMTLJ1}\\n    \\\\strng{fullhash}{KDMTLJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{Hidden Incentives for Auto-Induced Distributional Shift}\\n    \\\\field{volume}{abs/2009.09153}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Kuleshov2018}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=KV}{%\\n         family={Kuleshov},\\n         familyi={K\\\\bibinitperiod},\\n         given={Volodymyr},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=FN}{%\\n         family={Fenner},\\n         familyi={F\\\\bibinitperiod},\\n         given={Nathan},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=ES}{%\\n         family={Ermon},\\n         familyi={E\\\\bibinitperiod},\\n         given={Stefano},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KVFNES1}\\n    \\\\strng{fullhash}{KVFNES1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\verb{eprint}\\n    \\\\verb 1807.00263\\n    \\\\endverb\\n    \\\\field{title}{{Accurate uncertainties for deep learning using calibrated\\n  regression}}\\n    \\\\field{journaltitle}{ICML}\\n    \\\\field{eprinttype}{arXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Kull2019}{inproceedings}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=KM}{%\\n         family={Kull},\\n         familyi={K\\\\bibinitperiod},\\n         given={Meelis},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=PNM}{%\\n         family={Perello-Nieto},\\n         familyi={P\\\\bibinithyphendelim N\\\\bibinitperiod},\\n         given={Miquel},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=KM}{%\\n         family={K{\\\\\"{a}}ngsepp},\\n         familyi={K\\\\bibinitperiod},\\n         given={Markus},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=FTS}{%\\n         family={Filho},\\n         familyi={F\\\\bibinitperiod},\\n         given={Telmo\\\\bibnamedelima Silva},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim S\\\\bibinitperiod},\\n      }}%\\n      {{hash=SH}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Hao},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=FP}{%\\n         family={Flach},\\n         familyi={F\\\\bibinitperiod},\\n         given={Peter},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KM+1}\\n    \\\\strng{fullhash}{KMPNMKMFTSSHFP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{booktitle}{NeurIPS}\\n    \\\\field{title}{{Beyond temperature scaling: Obtaining well-calibrated\\n  multiclass probabilities with Dirichlet calibration}}\\n    \\\\field{eprinttype}{arXiv}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Kumar2019VerifiedUC}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=KA}{%\\n         family={Kumar},\\n         familyi={K\\\\bibinitperiod},\\n         given={Ananya},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=LP}{%\\n         family={Liang},\\n         familyi={L\\\\bibinitperiod},\\n         given={Percy},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=MT}{%\\n         family={Ma},\\n         familyi={M\\\\bibinitperiod},\\n         given={Tengyu},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KALPMT1}\\n    \\\\strng{fullhash}{KALPMT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{booktitle}{NeurIPS}\\n    \\\\field{title}{Verified Uncertainty Calibration}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Ky2021}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=KP}{%\\n         family={Ky},\\n         familyi={K\\\\bibinitperiod},\\n         given={Patrick},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{KP1}\\n    \\\\strng{fullhash}{KP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{K}\\n    \\\\field{sortinithash}{K}\\n    \\\\field{title}{Boeing 737 MAX Return to Service Report}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Lachaux2020UnsupervisedTO}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=LMA}{%\\n         family={Lachaux},\\n         familyi={L\\\\bibinitperiod},\\n         given={Marie-Anne},\\n         giveni={M\\\\bibinithyphendelim A\\\\bibinitperiod},\\n      }}%\\n      {{hash=RB}{%\\n         family={Rozi{\\\\`e}re},\\n         familyi={R\\\\bibinitperiod},\\n         given={Baptiste},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=CL}{%\\n         family={Chanussot},\\n         familyi={C\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=LG}{%\\n         family={Lample},\\n         familyi={L\\\\bibinitperiod},\\n         given={Guillaume},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LMA+1}\\n    \\\\strng{fullhash}{LMARBCLLG1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{Unsupervised Translation of Programming Languages}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Laidlaw2021PerceptualAR}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=LC}{%\\n         family={Laidlaw},\\n         familyi={L\\\\bibinitperiod},\\n         given={Cassidy},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Singla},\\n         familyi={S\\\\bibinitperiod},\\n         given={Sahil},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=FS}{%\\n         family={Feizi},\\n         familyi={F\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LCSSFS1}\\n    \\\\strng{fullhash}{LCSSFS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{Perceptual Adversarial Robustness: Defense Against Unseen\\n  Threat Models}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Lakshminarayanan2017SimpleAS}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=LB}{%\\n         family={Lakshminarayanan},\\n         familyi={L\\\\bibinitperiod},\\n         given={Balaji},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=PA}{%\\n         family={Pritzel},\\n         familyi={P\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=BC}{%\\n         family={Blundell},\\n         familyi={B\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LBPABC1}\\n    \\\\strng{fullhash}{LBPABC1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{booktitle}{NIPS}\\n    \\\\field{title}{Simple and Scalable Predictive Uncertainty Estimation using\\n  Deep Ensembles}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{lane1997application}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=LT}{%\\n         family={Lane},\\n         familyi={L\\\\bibinitperiod},\\n         given={Terran},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=BCE}{%\\n         family={Brodley},\\n         familyi={B\\\\bibinitperiod},\\n         given={Carla\\\\bibnamedelima E},\\n         giveni={C\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{organization}{1}{%\\n      {Baltimore, USA}%\\n    }\\n    \\\\strng{namehash}{LTBCE1}\\n    \\\\strng{fullhash}{LTBCE1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{booktitle}{Proceedings of the 20th National Information Systems\\n  Security Conference}\\n    \\\\field{pages}{366\\\\bibrangedash 380}\\n    \\\\field{title}{An application of machine learning to anomaly detection}\\n    \\\\field{volume}{377}\\n    \\\\field{year}{1997}\\n  \\\\endentry\\n\\n  \\\\entry{Langner2011StuxnetDA}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=LR}{%\\n         family={Langner},\\n         familyi={L\\\\bibinitperiod},\\n         given={Ralph},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LR1}\\n    \\\\strng{fullhash}{LR1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{Stuxnet: Dissecting a Cyberwarfare Weapon}\\n    \\\\field{journaltitle}{IEEE Security \\\\& Privacy}\\n    \\\\field{year}{2011}\\n  \\\\endentry\\n\\n  \\\\entry{LazariRadek2014ThePO}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=dLRK}{%\\n         prefix={de},\\n         prefixi={d\\\\bibinitperiod},\\n         family={Lazari-Radek},\\n         familyi={L\\\\bibinithyphendelim R\\\\bibinitperiod},\\n         given={Katarzyna},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=SP}{%\\n         family={Singer},\\n         familyi={S\\\\bibinitperiod},\\n         given={Peter},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LRKdSP1}\\n    \\\\strng{fullhash}{LRKdSP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{The Point of View of the Universe: Sidgwick and Contemporary\\n  Ethics}\\n    \\\\field{year}{2014}\\n  \\\\endentry\\n\\n  \\\\entry{lecuyer2019certified}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=LM}{%\\n         family={Lecuyer},\\n         familyi={L\\\\bibinitperiod},\\n         given={Mathias},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=AV}{%\\n         family={Atlidakis},\\n         familyi={A\\\\bibinitperiod},\\n         given={Vaggelis},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=GR}{%\\n         family={Geambasu},\\n         familyi={G\\\\bibinitperiod},\\n         given={Roxana},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=HD}{%\\n         family={Hsu},\\n         familyi={H\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=JS}{%\\n         family={Jana},\\n         familyi={J\\\\bibinitperiod},\\n         given={Suman},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{organization}{1}{%\\n      {IEEE}%\\n    }\\n    \\\\strng{namehash}{LM+1}\\n    \\\\strng{fullhash}{LMAVGRHDJS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{booktitle}{2019 IEEE Symposium on Security and Privacy (SP)}\\n    \\\\field{pages}{656\\\\bibrangedash 672}\\n    \\\\field{title}{Certified robustness to adversarial examples with\\n  differential privacy}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{lee2018hallucinations}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=LK}{%\\n         family={Lee},\\n         familyi={L\\\\bibinitperiod},\\n         given={Katherine},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=FO}{%\\n         family={Firat},\\n         familyi={F\\\\bibinitperiod},\\n         given={Orhan},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=AA}{%\\n         family={Agarwal},\\n         familyi={A\\\\bibinitperiod},\\n         given={Ashish},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=FC}{%\\n         family={Fannjiang},\\n         familyi={F\\\\bibinitperiod},\\n         given={Clara},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Sussillo},\\n         familyi={S\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LK+1}\\n    \\\\strng{fullhash}{LKFOAAFCSD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{Hallucinations in neural machine translation}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Lehman2018TheSC}{article}{}\\n    \\\\name{author}{53}{}{%\\n      {{hash=LJ}{%\\n         family={Lehman},\\n         familyi={L\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=CJ}{%\\n         family={Clune},\\n         familyi={C\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=MD}{%\\n         family={Misevic},\\n         familyi={M\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=AC}{%\\n         family={Adami},\\n         familyi={A\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=AL}{%\\n         family={Altenberg},\\n         familyi={A\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=BJ}{%\\n         family={Beaulieu},\\n         familyi={B\\\\bibinitperiod},\\n         given={Julie},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=BP}{%\\n         family={Bentley},\\n         familyi={B\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Bernard},\\n         familyi={B\\\\bibinitperiod},\\n         given={Samuel},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=BG}{%\\n         family={Beslon},\\n         familyi={B\\\\bibinitperiod},\\n         given={G.},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=BDM}{%\\n         family={Bryson},\\n         familyi={B\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima M.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=CP}{%\\n         family={Chrabaszcz},\\n         familyi={C\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=CN}{%\\n         family={Cheney},\\n         familyi={C\\\\bibinitperiod},\\n         given={Nick},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Cully},\\n         familyi={C\\\\bibinitperiod},\\n         given={Antoine},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=DS}{%\\n         family={Doncieux},\\n         familyi={D\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=DF}{%\\n         family={Dyer},\\n         familyi={D\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=EKO}{%\\n         family={Ellefsen},\\n         familyi={E\\\\bibinitperiod},\\n         given={Kai\\\\bibnamedelima Olav},\\n         giveni={K\\\\bibinitperiod\\\\bibinitdelim O\\\\bibinitperiod},\\n      }}%\\n      {{hash=FR}{%\\n         family={Feldt},\\n         familyi={F\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=FS}{%\\n         family={Fischer},\\n         familyi={F\\\\bibinitperiod},\\n         given={Stephan},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=FS}{%\\n         family={Forrest},\\n         familyi={F\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=FA}{%\\n         family={Fr{\\\\\\'e}noy},\\n         familyi={F\\\\bibinitperiod},\\n         given={Antoine},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=GC}{%\\n         family={Gagn{\\\\\\'e}},\\n         familyi={G\\\\bibinitperiod},\\n         given={Christian},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=GLKL}{%\\n         family={Goff},\\n         familyi={G\\\\bibinitperiod},\\n         given={L.\\\\bibnamedelima K.\\\\bibnamedelima L.},\\n         giveni={L\\\\bibinitperiod\\\\bibinitdelim K\\\\bibinitperiod\\\\bibinitdelim\\n  L\\\\bibinitperiod},\\n      }}%\\n      {{hash=GL}{%\\n         family={Grabowski},\\n         familyi={G\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=HB}{%\\n         family={Hodjat},\\n         familyi={H\\\\bibinitperiod},\\n         given={B.},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=HF}{%\\n         family={Hutter},\\n         familyi={H\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=KL}{%\\n         family={Keller},\\n         familyi={K\\\\bibinitperiod},\\n         given={L.},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=KC}{%\\n         family={Knibbe},\\n         familyi={K\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=KP}{%\\n         family={Krcah},\\n         familyi={K\\\\bibinitperiod},\\n         given={Peter},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=LR}{%\\n         family={Lenski},\\n         familyi={L\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=LH}{%\\n         family={Lipson},\\n         familyi={L\\\\bibinitperiod},\\n         given={H.},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=MR}{%\\n         family={MacCurdy},\\n         familyi={M\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=MC}{%\\n         family={Maestre},\\n         familyi={M\\\\bibinitperiod},\\n         given={Carlos},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=MR}{%\\n         family={Miikkulainen},\\n         familyi={M\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=MS}{%\\n         family={Mitri},\\n         familyi={M\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=MDE}{%\\n         family={Moriarty},\\n         familyi={M\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima E.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n      {{hash=MJ}{%\\n         family={Mouret},\\n         familyi={M\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=NAM}{%\\n         family={Nguyen},\\n         familyi={N\\\\bibinitperiod},\\n         given={Anh\\\\bibnamedelima M},\\n         giveni={A\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=OC}{%\\n         family={Ofria},\\n         familyi={O\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=PM}{%\\n         family={Parizeau},\\n         familyi={P\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=PD}{%\\n         family={Parsons},\\n         familyi={P\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=PRT}{%\\n         family={Pennock},\\n         familyi={P\\\\bibinitperiod},\\n         given={Robert\\\\bibnamedelima T.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim T\\\\bibinitperiod},\\n      }}%\\n      {{hash=PW}{%\\n         family={Punch},\\n         familyi={P\\\\bibinitperiod},\\n         given={W.},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=RT}{%\\n         family={Ray},\\n         familyi={R\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=SM}{%\\n         family={Schoenauer},\\n         familyi={S\\\\bibinitperiod},\\n         given={Marc},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SE}{%\\n         family={Shulte},\\n         familyi={S\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=SK}{%\\n         family={Sims},\\n         familyi={S\\\\bibinitperiod},\\n         given={K.},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=SKO}{%\\n         family={Stanley},\\n         familyi={S\\\\bibinitperiod},\\n         given={Kenneth\\\\bibnamedelima O.},\\n         giveni={K\\\\bibinitperiod\\\\bibinitdelim O\\\\bibinitperiod},\\n      }}%\\n      {{hash=TF}{%\\n         family={Taddei},\\n         familyi={T\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=TD}{%\\n         family={Tarapore},\\n         familyi={T\\\\bibinitperiod},\\n         given={Danesh},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=TS}{%\\n         family={Thibault},\\n         familyi={T\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=WW}{%\\n         family={Weimer},\\n         familyi={W\\\\bibinitperiod},\\n         given={Westley},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=WR}{%\\n         family={Watson},\\n         familyi={W\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=YJ}{%\\n         family={Yosinksi},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Jason},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LJ+1}\\n  \\\\strng{fullhash}{LJCJMDACALBJBPBSBGBDMCPCNCADSDFEKOFRFSFSFAGCGLKLGLHBHFKLKCKPLRLHMRMCMRMSMDEMJNAMOCPMPDPRTPWRTSMSESKSKOTFTDTSWWWRYJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{The Surprising Creativity of Digital Evolution: A Collection\\n  of Anecdotes from the Evolutionary Computation and Artificial Life Research\\n  Communities}\\n    \\\\field{journaltitle}{Artificial Life}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Leike2018ScalableAA}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=LJ}{%\\n         family={Leike},\\n         familyi={L\\\\bibinitperiod},\\n         given={Jan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=KD}{%\\n         family={Krueger},\\n         familyi={K\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=ET}{%\\n         family={Everitt},\\n         familyi={E\\\\bibinitperiod},\\n         given={Tom},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Martic},\\n         familyi={M\\\\bibinitperiod},\\n         given={Miljan},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=MV}{%\\n         family={Maini},\\n         familyi={M\\\\bibinitperiod},\\n         given={Vishal},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=LS}{%\\n         family={Legg},\\n         familyi={L\\\\bibinitperiod},\\n         given={Shane},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LJ+2}\\n    \\\\strng{fullhash}{LJKDETMMMVLS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{Scalable agent alignment via reward modeling: a research\\n  direction}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Leveson2012EngineeringAS}{inproceedings}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=LN}{%\\n         family={Leveson},\\n         familyi={L\\\\bibinitperiod},\\n         given={Nancy},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LN1}\\n    \\\\strng{fullhash}{LN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{Engineering a Safer World: Systems Thinking Applied to\\n  Safety}\\n    \\\\field{year}{2012}\\n  \\\\endentry\\n\\n  \\\\entry{Liao2019BuildingJC}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=LB}{%\\n         family={Liao},\\n         familyi={L\\\\bibinitperiod},\\n         given={Beishui},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=SM}{%\\n         family={Slavkovik},\\n         familyi={S\\\\bibinitperiod},\\n         given={Marija},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=vdTL}{%\\n         prefix={van\\\\bibnamedelima der},\\n         prefixi={v\\\\bibinitperiod\\\\bibinitdelim d\\\\bibinitperiod},\\n         family={Torre},\\n         familyi={T\\\\bibinitperiod},\\n         given={Leendert},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LBSMTLvd1}\\n    \\\\strng{fullhash}{LBSMTLvd1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{Building Jiminy Cricket: An Architecture for Moral Agreements\\n  Among Stakeholders}\\n    \\\\field{journaltitle}{Proceedings of the 2019 AAAI/ACM Conference on AI,\\n  Ethics, and Society}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{owain2021}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=LS}{%\\n         family={Lin},\\n         familyi={L\\\\bibinitperiod},\\n         given={Stephanie},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=HJ}{%\\n         family={Hilton},\\n         familyi={H\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=EO}{%\\n         family={Evans},\\n         familyi={E\\\\bibinitperiod},\\n         given={Owain},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LSHJEO1}\\n    \\\\strng{fullhash}{LSHJEO1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{{TruthfulQA}: Measuring How Models Mimic Human Falsehoods}\\n    \\\\field{journaltitle}{arXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Luo2021}{article}{}\\n    \\\\name{author}{8}{}{%\\n      {{hash=LR}{%\\n         family={Luo},\\n         familyi={L\\\\bibinitperiod},\\n         given={Rachel},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=BA}{%\\n         family={Bhatnagar},\\n         familyi={B\\\\bibinitperiod},\\n         given={Aadyot},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=WH}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Huan},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=XC}{%\\n         family={Xiong},\\n         familyi={X\\\\bibinitperiod},\\n         given={Caiming},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Savarese},\\n         familyi={S\\\\bibinitperiod},\\n         given={Silvio},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=BY}{%\\n         family={Bai},\\n         familyi={B\\\\bibinitperiod},\\n         given={Yu},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZS}{%\\n         family={Zhao},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Shengjia},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=ES}{%\\n         family={Ermon},\\n         familyi={E\\\\bibinitperiod},\\n         given={Stefano},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{LR+1}\\n    \\\\strng{fullhash}{LRBAWHXCSSBYZSES1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{L}\\n    \\\\field{sortinithash}{L}\\n    \\\\field{title}{{Localized Calibration: Metrics and Recalibration}}\\n    \\\\field{journaltitle}{arXiv}\\n    \\\\field{eprinttype}{arXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Madry2018TowardsDL}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=MA}{%\\n         family={Madry},\\n         familyi={M\\\\bibinitperiod},\\n         given={Aleksander},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=MA}{%\\n         family={Makelov},\\n         familyi={M\\\\bibinitperiod},\\n         given={Aleksandar},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SL}{%\\n         family={Schmidt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Ludwig},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=TD}{%\\n         family={Tsipras},\\n         familyi={T\\\\bibinitperiod},\\n         given={Dimitris},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=VA}{%\\n         family={Vladu},\\n         familyi={V\\\\bibinitperiod},\\n         given={Adrian},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{MA+1}\\n    \\\\strng{fullhash}{MAMASLTDVA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{M}\\n    \\\\field{sortinithash}{M}\\n    \\\\field{title}{Towards Deep Learning Models Resistant to Adversarial\\n  Attacks}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Mandelbrot2004TheMO}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=MB}{%\\n         family={Mandelbrot},\\n         familyi={M\\\\bibinitperiod},\\n         given={Benoit},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=HRL}{%\\n         family={Hudson},\\n         familyi={H\\\\bibinitperiod},\\n         given={Richard\\\\bibnamedelima L.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{MBHRL1}\\n    \\\\strng{fullhash}{MBHRL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{M}\\n    \\\\field{sortinithash}{M}\\n    \\\\field{title}{The Misbehavior of Markets: A Fractal View of Risk, Ruin, and\\n  Reward}\\n    \\\\field{year}{2004}\\n  \\\\endentry\\n\\n  \\\\entry{Manheim2018CategorizingVO}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=MD}{%\\n         family={Manheim},\\n         familyi={M\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Garrabrant},\\n         familyi={G\\\\bibinitperiod},\\n         given={Scott},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{MDGS1}\\n    \\\\strng{fullhash}{MDGS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{M}\\n    \\\\field{sortinithash}{M}\\n    \\\\field{title}{Categorizing Variants of Goodhart\\'s Law}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Maric2020FormalizingIP}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=MF}{%\\n         family={Maric},\\n         familyi={M\\\\bibinitperiod},\\n         given={Filip},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=SDS}{%\\n         family={Stojanovic-Durdevic},\\n         familyi={S\\\\bibinithyphendelim D\\\\bibinitperiod},\\n         given={Sana},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{MFSDS1}\\n    \\\\strng{fullhash}{MFSDS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{M}\\n    \\\\field{sortinithash}{M}\\n    \\\\field{booktitle}{ThEdu@IJCAR}\\n    \\\\field{title}{Formalizing IMO Problems and Solutions in Isabelle/HOL}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{tay}{misc}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=M}{%\\n         family={Microsoft},\\n         familyi={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{M1}\\n    \\\\strng{fullhash}{M1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{sortinit}{M}\\n    \\\\field{sortinithash}{M}\\n    \\\\verb{url}\\n    \\\\verb https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introductio\\n    \\\\verb n/\\n    \\\\endverb\\n  \\\\endentry\\n\\n  \\\\entry{Mitzenmacher2003ABH}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=MM}{%\\n         family={Mitzenmacher},\\n         familyi={M\\\\bibinitperiod},\\n         given={Michael},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{MM1}\\n    \\\\strng{fullhash}{MM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{M}\\n    \\\\field{sortinithash}{M}\\n    \\\\field{title}{A Brief History of Generative Models for Power Law and\\n  Lognormal Distributions}\\n    \\\\field{journaltitle}{Internet Mathematics}\\n    \\\\field{year}{2003}\\n  \\\\endentry\\n\\n  \\\\entry{Mummadi2021TestTimeAT}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=MCK}{%\\n         family={Mummadi},\\n         familyi={M\\\\bibinitperiod},\\n         given={Chaithanya\\\\bibnamedelima Kumar},\\n         giveni={C\\\\bibinitperiod\\\\bibinitdelim K\\\\bibinitperiod},\\n      }}%\\n      {{hash=HR}{%\\n         family={Hutmacher},\\n         familyi={H\\\\bibinitperiod},\\n         given={Robin},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=RK}{%\\n         family={Rambach},\\n         familyi={R\\\\bibinitperiod},\\n         given={K.},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=LE}{%\\n         family={Levinkov},\\n         familyi={L\\\\bibinitperiod},\\n         given={Evgeny},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=BT}{%\\n         family={Brox},\\n         familyi={B\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=MJH}{%\\n         family={Metzen},\\n         familyi={M\\\\bibinitperiod},\\n         given={J.\\\\bibnamedelima H.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim H\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{MCK+1}\\n    \\\\strng{fullhash}{MCKHRRKLEBTMJH1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{M}\\n    \\\\field{sortinithash}{M}\\n    \\\\field{title}{Test-Time Adaptation to Distribution Shift by Confidence\\n  Maximization and Input Transformation}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Newberry2021ThePA}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=NT}{%\\n         family={Newberry},\\n         familyi={N\\\\bibinitperiod},\\n         given={Toby},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=OT}{%\\n         family={Ord},\\n         familyi={O\\\\bibinitperiod},\\n         given={Toby},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{NTOT1}\\n    \\\\strng{fullhash}{NTOT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{N}\\n    \\\\field{sortinithash}{N}\\n    \\\\field{title}{The Parliamentary Approach to Moral Uncertainty}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Nguyen2015PosteriorCA}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=NK}{%\\n         family={Nguyen},\\n         familyi={N\\\\bibinitperiod},\\n         given={Khanh},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=OBT}{%\\n         family={O\\'Connor},\\n         familyi={O\\\\bibinitperiod},\\n         given={Brendan\\\\bibnamedelima T.},\\n         giveni={B\\\\bibinitperiod\\\\bibinitdelim T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{NKOBT1}\\n    \\\\strng{fullhash}{NKOBT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{N}\\n    \\\\field{sortinithash}{N}\\n    \\\\field{booktitle}{EMNLP}\\n    \\\\field{title}{Posterior calibration and exploratory analysis for natural\\n  language processing models}\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{ghidra}{misc}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=N}{%\\n         family={NSA},\\n         familyi={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{N1}\\n    \\\\strng{fullhash}{N1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{sortinit}{N}\\n    \\\\field{sortinithash}{N}\\n    \\\\verb{url}\\n    \\\\verb https://ghidra-sre.org/\\n    \\\\endverb\\n    \\\\field{journaltitle}{Ghidra}\\n  \\\\endentry\\n\\n  \\\\entry{Nussbaum2003CAPABILITIESAF}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=NM}{%\\n         family={Nussbaum},\\n         familyi={N\\\\bibinitperiod},\\n         given={Martha},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{NM1}\\n    \\\\strng{fullhash}{NM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{N}\\n    \\\\field{sortinithash}{N}\\n    \\\\field{pages}{33 \\\\bibrangedash  59}\\n    \\\\field{title}{CAPABILITIES AS FUNDAMENTAL ENTITLEMENTS: SEN AND SOCIAL\\n  JUSTICE}\\n    \\\\field{volume}{9}\\n    \\\\field{journaltitle}{Feminist Economics}\\n    \\\\field{year}{2003}\\n  \\\\endentry\\n\\n  \\\\entry{Ottis2008AnalysisOT}{inproceedings}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=OR}{%\\n         family={Ottis},\\n         familyi={O\\\\bibinitperiod},\\n         given={Rain},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{OR1}\\n    \\\\strng{fullhash}{OR1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{O}\\n    \\\\field{sortinithash}{O}\\n    \\\\field{title}{Analysis of the 2007 Cyber Attacks Against Estonia from the\\n  Information Warfare Perspective}\\n    \\\\field{year}{2008}\\n  \\\\endentry\\n\\n  \\\\entry{Ovadia2019CanYT}{inproceedings}{}\\n    \\\\name{author}{9}{}{%\\n      {{hash=OY}{%\\n         family={Ovadia},\\n         familyi={O\\\\bibinitperiod},\\n         given={Yaniv},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=FE}{%\\n         family={Fertig},\\n         familyi={F\\\\bibinitperiod},\\n         given={E.},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=RJ}{%\\n         family={Ren},\\n         familyi={R\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=NZ}{%\\n         family={Nado},\\n         familyi={N\\\\bibinitperiod},\\n         given={Zachary},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Sculley},\\n         familyi={S\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=NS}{%\\n         family={Nowozin},\\n         familyi={N\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=DJV}{%\\n         family={Dillon},\\n         familyi={D\\\\bibinitperiod},\\n         given={Joshua\\\\bibnamedelima V.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim V\\\\bibinitperiod},\\n      }}%\\n      {{hash=LB}{%\\n         family={Lakshminarayanan},\\n         familyi={L\\\\bibinitperiod},\\n         given={Balaji},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Snoek},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jasper},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{OY+1}\\n    \\\\strng{fullhash}{OYFERJNZSDNSDJVLBSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{O}\\n    \\\\field{sortinithash}{O}\\n    \\\\field{booktitle}{NeurIPS}\\n    \\\\field{title}{Can You Trust Your Model\\'s Uncertainty? Evaluating Predictive\\n  Uncertainty Under Dataset Shift}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Pearce2021AnEC}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=PH}{%\\n         family={Pearce},\\n         familyi={P\\\\bibinitperiod},\\n         given={Hammond},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=AB}{%\\n         family={Ahmad},\\n         familyi={A\\\\bibinitperiod},\\n         given={Baleegh},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=TB}{%\\n         family={Tan},\\n         familyi={T\\\\bibinitperiod},\\n         given={Benjamin},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=DGB}{%\\n         family={Dolan-Gavitt},\\n         familyi={D\\\\bibinithyphendelim G\\\\bibinitperiod},\\n         given={Brendan},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=KR}{%\\n         family={Karri},\\n         familyi={K\\\\bibinitperiod},\\n         given={Ramesh},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{PH+1}\\n    \\\\strng{fullhash}{PHABTBDGBKR1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{P}\\n    \\\\field{sortinithash}{P}\\n    \\\\field{title}{An Empirical Cybersecurity Evaluation of GitHub Copilot\\'s\\n  Code Contributions}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Peskov2020ItTT}{inproceedings}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=PD}{%\\n         family={Peskov},\\n         familyi={P\\\\bibinitperiod},\\n         given={Denis},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=CB}{%\\n         family={Cheng},\\n         familyi={C\\\\bibinitperiod},\\n         given={Benny},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=EA}{%\\n         family={Elgohary},\\n         familyi={E\\\\bibinitperiod},\\n         given={Ahmed},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=BJ}{%\\n         family={Barrow},\\n         familyi={B\\\\bibinitperiod},\\n         given={Joe},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=DNMC}{%\\n         family={Danescu-Niculescu-Mizil},\\n         familyi={D\\\\bibinithyphendelim N\\\\bibinithyphendelim M\\\\bibinitperiod},\\n         given={Cristian},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=BGJL}{%\\n         family={Boyd-Graber},\\n         familyi={B\\\\bibinithyphendelim G\\\\bibinitperiod},\\n         given={Jordan\\\\bibnamedelima L.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{PD+1}\\n    \\\\strng{fullhash}{PDCBEABJDNMCBGJL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{P}\\n    \\\\field{sortinithash}{P}\\n    \\\\field{booktitle}{ACL}\\n    \\\\field{title}{It Takes Two to Lie: One to Lie, and One to Listen}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{pierazzi2020intriguing}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=PF}{%\\n         family={Pierazzi},\\n         familyi={P\\\\bibinitperiod},\\n         given={Fabio},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=PF}{%\\n         family={Pendlebury},\\n         familyi={P\\\\bibinitperiod},\\n         given={Feargus},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=CJ}{%\\n         family={Cortellazzi},\\n         familyi={C\\\\bibinitperiod},\\n         given={Jacopo},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=CL}{%\\n         family={Cavallaro},\\n         familyi={C\\\\bibinitperiod},\\n         given={Lorenzo},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{organization}{1}{%\\n      {IEEE}%\\n    }\\n    \\\\strng{namehash}{PF+1}\\n    \\\\strng{fullhash}{PFPFCJCL1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{P}\\n    \\\\field{sortinithash}{P}\\n    \\\\field{booktitle}{2020 IEEE Symposium on Security and Privacy (SP)}\\n    \\\\field{pages}{1332\\\\bibrangedash 1349}\\n    \\\\field{title}{Intriguing properties of adversarial ml attacks in the\\n  problem space}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{posner}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=PRA}{%\\n         family={Posner},\\n         familyi={P\\\\bibinitperiod},\\n         given={Richard\\\\bibnamedelima A.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{PRA1}\\n    \\\\strng{fullhash}{PRA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{P}\\n    \\\\field{sortinithash}{P}\\n    \\\\field{title}{Utilitarianism, Economics, and Legal Theory}\\n    \\\\field{journaltitle}{The Journal of Legal Studies}\\n    \\\\field{year}{1979}\\n  \\\\endentry\\n\\n  \\\\entry{Poursaeed2021RobustnessAG}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=PO}{%\\n         family={Poursaeed},\\n         familyi={P\\\\bibinitperiod},\\n         given={Omid},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=JT}{%\\n         family={Jiang},\\n         familyi={J\\\\bibinitperiod},\\n         given={Tianxing},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=YH}{%\\n         family={Yang},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Harry},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Belongie},\\n         familyi={B\\\\bibinitperiod},\\n         given={Serge},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=LSN}{%\\n         family={Lim},\\n         familyi={L\\\\bibinitperiod},\\n         given={Ser-Nam},\\n         giveni={S\\\\bibinithyphendelim N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{PO+1}\\n    \\\\strng{fullhash}{POJTYHBSLSN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{P}\\n    \\\\field{sortinithash}{P}\\n    \\\\field{title}{Robustness and Generalization via Generative Adversarial\\n  Training}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{grokking}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=PA}{%\\n         family={Power},\\n         familyi={P\\\\bibinitperiod},\\n         given={Alethea},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=BY}{%\\n         family={Burda},\\n         familyi={B\\\\bibinitperiod},\\n         given={Yuri},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=EH}{%\\n         family={Edwards},\\n         familyi={E\\\\bibinitperiod},\\n         given={Harri},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=BI}{%\\n         family={Babuschkin},\\n         familyi={B\\\\bibinitperiod},\\n         given={Igor},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=MV}{%\\n         family={Misra},\\n         familyi={M\\\\bibinitperiod},\\n         given={Vedant},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{PA+1}\\n    \\\\strng{fullhash}{PABYEHBIMV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{P}\\n    \\\\field{sortinithash}{P}\\n    \\\\field{booktitle}{ICLR MATH-AI Workshop}\\n    \\\\field{title}{Grokking: Generalization Beyond Overfitting on Small\\n  Algorithmic Datasets}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{hroanomaly}{inbook}{}\\n    \\\\list{publisher}{1}{%\\n      {John Wiley \\\\& Sons, Ltd}%\\n    }\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{P}\\n    \\\\field{sortinithash}{P}\\n    \\\\field{booktitle}{Managing the Unexpected}\\n    \\\\field{chapter}{3}\\n    \\\\field{isbn}{9781119175834}\\n    \\\\field{pages}{45\\\\bibrangedash 61}\\n    \\\\field{title}{Principle 1: Preoccupation with Failure}\\n    \\\\verb{url}\\n    \\\\verb https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119175834.ch03\\n    \\\\endverb\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{Radford2021LearningTV}{inproceedings}{}\\n    \\\\name{author}{12}{}{%\\n      {{hash=RA}{%\\n         family={Radford},\\n         familyi={R\\\\bibinitperiod},\\n         given={Alec},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=KJW}{%\\n         family={Kim},\\n         familyi={K\\\\bibinitperiod},\\n         given={Jong\\\\bibnamedelima Wook},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim W\\\\bibinitperiod},\\n      }}%\\n      {{hash=HC}{%\\n         family={Hallacy},\\n         familyi={H\\\\bibinitperiod},\\n         given={Chris},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=RA}{%\\n         family={Ramesh},\\n         familyi={R\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=GG}{%\\n         family={Goh},\\n         familyi={G\\\\bibinitperiod},\\n         given={Gabriel},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=AS}{%\\n         family={Agarwal},\\n         familyi={A\\\\bibinitperiod},\\n         given={Sandhini},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=SG}{%\\n         family={Sastry},\\n         familyi={S\\\\bibinitperiod},\\n         given={Girish},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=AA}{%\\n         family={Askell},\\n         familyi={A\\\\bibinitperiod},\\n         given={Amanda},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=MP}{%\\n         family={Mishkin},\\n         familyi={M\\\\bibinitperiod},\\n         given={Pamela},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=CJ}{%\\n         family={Clark},\\n         familyi={C\\\\bibinitperiod},\\n         given={Jack},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=KG}{%\\n         family={Krueger},\\n         familyi={K\\\\bibinitperiod},\\n         given={Gretchen},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=SI}{%\\n         family={Sutskever},\\n         familyi={S\\\\bibinitperiod},\\n         given={Ilya},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RA+1}\\n    \\\\strng{fullhash}{RAKJWHCRAGGASSGAAMPCJKGSI1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Learning Transferable Visual Models From Natural Language\\n  Supervision}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Raghunathan2018CertifiedDA}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=RA}{%\\n         family={Raghunathan},\\n         familyi={R\\\\bibinitperiod},\\n         given={Aditi},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Steinhardt},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jacob},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=LP}{%\\n         family={Liang},\\n         familyi={L\\\\bibinitperiod},\\n         given={Percy},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RASJLP1}\\n    \\\\strng{fullhash}{RASJLP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{Certified Defenses against Adversarial Examples}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Rajpurkar2017CheXNetRP}{article}{}\\n    \\\\name{author}{12}{}{%\\n      {{hash=RP}{%\\n         family={Rajpurkar},\\n         familyi={R\\\\bibinitperiod},\\n         given={Pranav},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=IJA}{%\\n         family={Irvin},\\n         familyi={I\\\\bibinitperiod},\\n         given={Jeremy\\\\bibnamedelima A.},\\n         giveni={J\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZK}{%\\n         family={Zhu},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Kaylie},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=YB}{%\\n         family={Yang},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Brandon},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=MH}{%\\n         family={Mehta},\\n         familyi={M\\\\bibinitperiod},\\n         given={Hershel},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=DT}{%\\n         family={Duan},\\n         familyi={D\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=DD}{%\\n         family={Ding},\\n         familyi={D\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=BA}{%\\n         family={Bagul},\\n         familyi={B\\\\bibinitperiod},\\n         given={Aarti},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=LC}{%\\n         family={Langlotz},\\n         familyi={L\\\\bibinitperiod},\\n         given={C.},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SK}{%\\n         family={Shpanskaya},\\n         familyi={S\\\\bibinitperiod},\\n         given={K.},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=LM}{%\\n         family={Lungren},\\n         familyi={L\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=NA}{%\\n         family={Ng},\\n         familyi={N\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RP+1}\\n    \\\\strng{fullhash}{RPIJAZKYBMHDTDDBALCSKLMNA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{CheXNet: Radiologist-Level Pneumonia Detection on Chest\\n  X-Rays with Deep Learning}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{integrativecomplexity}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=RTD}{%\\n         family={Raphael},\\n         familyi={R\\\\bibinitperiod},\\n         given={Theodore\\\\bibnamedelima D.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RTD1}\\n    \\\\strng{fullhash}{RTD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{Integrative Complexity Theory and Forecasting International\\n  Crises: Berlin 1946-1962}\\n    \\\\field{journaltitle}{The Journal of Conflict Resolution}\\n    \\\\field{year}{1982}\\n  \\\\endentry\\n\\n  \\\\entry{rawls}{book}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=RJ}{%\\n         family={Rawls},\\n         familyi={R\\\\bibinitperiod},\\n         given={John},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {Harvard University Press}%\\n    }\\n    \\\\strng{namehash}{RJ1}\\n    \\\\strng{fullhash}{RJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{A Theory of Justice}\\n    \\\\field{year}{1999}\\n  \\\\endentry\\n\\n  \\\\entry{Achiam2019BenchmarkingSE}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=RA}{%\\n         family={Ray},\\n         familyi={R\\\\bibinitperiod},\\n         given={Alex},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=AJ}{%\\n         family={Achiam},\\n         familyi={A\\\\bibinitperiod},\\n         given={Joshua},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AD}{%\\n         family={Amodei},\\n         familyi={A\\\\bibinitperiod},\\n         given={Dario},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RAAJAD1}\\n    \\\\strng{fullhash}{RAAJAD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{Benchmarking Safe Exploration in Deep Reinforcement Learning}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Rebuffi2021FixingDA}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=RSA}{%\\n         family={Rebuffi},\\n         familyi={R\\\\bibinitperiod},\\n         given={Sylvestre-Alvise},\\n         giveni={S\\\\bibinithyphendelim A\\\\bibinitperiod},\\n      }}%\\n      {{hash=GS}{%\\n         family={Gowal},\\n         familyi={G\\\\bibinitperiod},\\n         given={Sven},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=CDA}{%\\n         family={Calian},\\n         familyi={C\\\\bibinitperiod},\\n         given={D.\\\\bibnamedelima A.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SF}{%\\n         family={Stimberg},\\n         familyi={S\\\\bibinitperiod},\\n         given={Florian},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=WO}{%\\n         family={Wiles},\\n         familyi={W\\\\bibinitperiod},\\n         given={Olivia},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=MTA}{%\\n         family={Mann},\\n         familyi={M\\\\bibinitperiod},\\n         given={Timothy\\\\bibnamedelima A.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RSA+1}\\n    \\\\strng{fullhash}{RSAGSCDASFWOMTA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{Fixing Data Augmentation to Improve Adversarial Robustness}\\n    \\\\field{volume}{abs/2103.01946}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Ridgway1956DysfunctionalCO}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=RV}{%\\n         family={Ridgway},\\n         familyi={R\\\\bibinitperiod},\\n         given={V.},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RV1}\\n    \\\\strng{fullhash}{RV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{Dysfunctional Consequences of Performance Measurements}\\n    \\\\field{journaltitle}{Administrative Science Quarterly}\\n    \\\\field{year}{1956}\\n  \\\\endentry\\n\\n  \\\\entry{lethalaw}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=RS}{%\\n         family={Russell},\\n         familyi={R\\\\bibinitperiod},\\n         given={Stuart},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=AA}{%\\n         family={Aguirre},\\n         familyi={A\\\\bibinitperiod},\\n         given={Anthony},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=JE}{%\\n         family={Javorsky},\\n         familyi={J\\\\bibinitperiod},\\n         given={Emilia},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=TM}{%\\n         family={Tegmark},\\n         familyi={T\\\\bibinitperiod},\\n         given={Max},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RS+1}\\n    \\\\strng{fullhash}{RSAAJETM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{Lethal Autonomous Weapons Exist; They Must Be Banned}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Russell2015ResearchPF}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=RSJ}{%\\n         family={Russell},\\n         familyi={R\\\\bibinitperiod},\\n         given={Stuart\\\\bibnamedelima J.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim J\\\\bibinitperiod},\\n      }}%\\n      {{hash=DD}{%\\n         family={Dewey},\\n         familyi={D\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=TM}{%\\n         family={Tegmark},\\n         familyi={T\\\\bibinitperiod},\\n         given={Max},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{RSJDDTM1}\\n    \\\\strng{fullhash}{RSJDDTM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{R}\\n    \\\\field{sortinithash}{R}\\n    \\\\field{title}{Research Priorities for Robust and Beneficial Artificial\\n  Intelligence}\\n    \\\\field{journaltitle}{AI Magazine}\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{Saunders2018TrialWE}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=SW}{%\\n         family={Saunders},\\n         familyi={S\\\\bibinitperiod},\\n         given={William},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=SG}{%\\n         family={Sastry},\\n         familyi={S\\\\bibinitperiod},\\n         given={Girish},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=SA}{%\\n         family={Stuhlm{\\\\\"u}ller},\\n         familyi={S\\\\bibinitperiod},\\n         given={Andreas},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=EO}{%\\n         family={Evans},\\n         familyi={E\\\\bibinitperiod},\\n         given={Owain},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SW+1}\\n    \\\\strng{fullhash}{SWSGSAEO1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{booktitle}{AAMAS}\\n    \\\\field{title}{Trial without Error: Towards Safe Reinforcement Learning via\\n  Human Intervention}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Schuster2020YouAM}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=SR}{%\\n         family={Schuster},\\n         familyi={S\\\\bibinitperiod},\\n         given={Roei},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=SC}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Congzheng},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=TE}{%\\n         family={Tromer},\\n         familyi={T\\\\bibinitperiod},\\n         given={Eran},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=SV}{%\\n         family={Shmatikov},\\n         familyi={S\\\\bibinitperiod},\\n         given={Vitaly},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SR+1}\\n    \\\\strng{fullhash}{SRSCTESV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{You Autocomplete Me: Poisoning Vulnerabilities in Neural Code\\n  Completion}\\n    \\\\field{journaltitle}{USENIX}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{sculley2015hidden}{article}{}\\n    \\\\name{author}{10}{}{%\\n      {{hash=SD}{%\\n         family={Sculley},\\n         familyi={S\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=HG}{%\\n         family={Holt},\\n         familyi={H\\\\bibinitperiod},\\n         given={Gary},\\n         giveni={G\\\\bibinitperiod},\\n      }}%\\n      {{hash=GD}{%\\n         family={Golovin},\\n         familyi={G\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=DE}{%\\n         family={Davydov},\\n         familyi={D\\\\bibinitperiod},\\n         given={Eugene},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=PT}{%\\n         family={Phillips},\\n         familyi={P\\\\bibinitperiod},\\n         given={Todd},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=ED}{%\\n         family={Ebner},\\n         familyi={E\\\\bibinitperiod},\\n         given={Dietmar},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=CV}{%\\n         family={Chaudhary},\\n         familyi={C\\\\bibinitperiod},\\n         given={Vinay},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n      {{hash=YM}{%\\n         family={Young},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Michael},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=CJF}{%\\n         family={Crespo},\\n         familyi={C\\\\bibinitperiod},\\n         given={Jean-Francois},\\n         giveni={J\\\\bibinithyphendelim F\\\\bibinitperiod},\\n      }}%\\n      {{hash=DD}{%\\n         family={Dennison},\\n         familyi={D\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SD+1}\\n    \\\\strng{fullhash}{SDHGGDDEPTEDCVYMCJFDD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{pages}{2503\\\\bibrangedash 2511}\\n    \\\\field{title}{Hidden technical debt in machine learning systems}\\n    \\\\field{volume}{28}\\n    \\\\field{journaltitle}{Advances in neural information processing systems}\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{Shafahi2018PoisonFT}{inproceedings}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=SA}{%\\n         family={Shafahi},\\n         familyi={S\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=HWR}{%\\n         family={Huang},\\n         familyi={H\\\\bibinitperiod},\\n         given={W.\\\\bibnamedelima R.},\\n         giveni={W\\\\bibinitperiod\\\\bibinitdelim R\\\\bibinitperiod},\\n      }}%\\n      {{hash=NM}{%\\n         family={Najibi},\\n         familyi={N\\\\bibinitperiod},\\n         given={Mahyar},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SO}{%\\n         family={Suciu},\\n         familyi={S\\\\bibinitperiod},\\n         given={Octavian},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=SC}{%\\n         family={Studer},\\n         familyi={S\\\\bibinitperiod},\\n         given={Christoph},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=DT}{%\\n         family={Dumitras},\\n         familyi={D\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=GT}{%\\n         family={Goldstein},\\n         familyi={G\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SA+1}\\n    \\\\strng{fullhash}{SAHWRNMSOSCDTGT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{booktitle}{NeurIPS}\\n    \\\\field{title}{Poison Frogs! Targeted Clean-Label Poisoning Attacks on\\n  Neural Networks}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Shah2019PreferencesII}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=SR}{%\\n         family={Shah},\\n         familyi={S\\\\bibinitperiod},\\n         given={Rohin},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=KD}{%\\n         family={Krasheninnikov},\\n         familyi={K\\\\bibinitperiod},\\n         given={Dmitrii},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=AJ}{%\\n         family={Alexander},\\n         familyi={A\\\\bibinitperiod},\\n         given={Jordan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AP}{%\\n         family={Abbeel},\\n         familyi={A\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=DA}{%\\n         family={Dragan},\\n         familyi={D\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SR+2}\\n    \\\\strng{fullhash}{SRKDAJAPDA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Preferences Implicit in the State of the World}\\n    \\\\field{journaltitle}{ICLR}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{sharif2016accessorize}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=SM}{%\\n         family={Sharif},\\n         familyi={S\\\\bibinitperiod},\\n         given={Mahmood},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=BS}{%\\n         family={Bhagavatula},\\n         familyi={B\\\\bibinitperiod},\\n         given={Sruti},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=BL}{%\\n         family={Bauer},\\n         familyi={B\\\\bibinitperiod},\\n         given={Lujo},\\n         giveni={L\\\\bibinitperiod},\\n      }}%\\n      {{hash=RMK}{%\\n         family={Reiter},\\n         familyi={R\\\\bibinitperiod},\\n         given={Michael\\\\bibnamedelima K},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim K\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SM+1}\\n    \\\\strng{fullhash}{SMBSBLRMK1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{booktitle}{Proceedings of the 2016 acm sigsac conference on computer\\n  and communications security}\\n    \\\\field{pages}{1528\\\\bibrangedash 1540}\\n    \\\\field{title}{Accessorize to a crime: Real and stealthy attacks on\\n  state-of-the-art face recognition}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{She2020NeutaintED}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=SD}{%\\n         family={She},\\n         familyi={S\\\\bibinitperiod},\\n         given={Dongdong},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=CY}{%\\n         family={Chen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Yizheng},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=RB}{%\\n         family={Ray},\\n         familyi={R\\\\bibinitperiod},\\n         given={Baishakhi},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=JS}{%\\n         family={Jana},\\n         familyi={J\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SD+2}\\n    \\\\strng{fullhash}{SDCYRBJS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{pages}{1527\\\\bibrangedash 1543}\\n    \\\\field{title}{Neutaint: Efficient Dynamic Taint Analysis with Neural\\n  Networks}\\n    \\\\field{journaltitle}{2020 IEEE Symposium on Security and Privacy (SP)}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{She2019NEUZZEF}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=SD}{%\\n         family={She},\\n         familyi={S\\\\bibinitperiod},\\n         given={Dongdong},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=PK}{%\\n         family={Pei},\\n         familyi={P\\\\bibinitperiod},\\n         given={Kexin},\\n         giveni={K\\\\bibinitperiod},\\n      }}%\\n      {{hash=ED}{%\\n         family={Epstein},\\n         familyi={E\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=YJ}{%\\n         family={Yang},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Junfeng},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=RB}{%\\n         family={Ray},\\n         familyi={R\\\\bibinitperiod},\\n         given={Baishakhi},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=JS}{%\\n         family={Jana},\\n         familyi={J\\\\bibinitperiod},\\n         given={S.},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SD+2}\\n    \\\\strng{fullhash}{SDPKEDYJRBJS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{pages}{803\\\\bibrangedash 817}\\n    \\\\field{title}{NEUZZ: Efficient Fuzzing with Neural Program Smoothing}\\n    \\\\field{journaltitle}{2019 IEEE Symposium on Security and Privacy (SP)}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Shin2015RecognizingFI}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=SEC}{%\\n         family={Shin},\\n         familyi={S\\\\bibinitperiod},\\n         given={E.\\\\bibnamedelima C.},\\n         giveni={E\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MR}{%\\n         family={Moazzezi},\\n         familyi={M\\\\bibinitperiod},\\n         given={R.},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SECSDMR1}\\n    \\\\strng{fullhash}{SECSDMR1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{booktitle}{USENIX Security Symposium}\\n    \\\\field{title}{Recognizing Functions in Binaries with Neural Networks}\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{shokri2017membership}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=SR}{%\\n         family={Shokri},\\n         familyi={S\\\\bibinitperiod},\\n         given={Reza},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=SM}{%\\n         family={Stronati},\\n         familyi={S\\\\bibinitperiod},\\n         given={Marco},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SC}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={Congzheng},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=SV}{%\\n         family={Shmatikov},\\n         familyi={S\\\\bibinitperiod},\\n         given={Vitaly},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{organization}{1}{%\\n      {IEEE}%\\n    }\\n    \\\\strng{namehash}{SR+3}\\n    \\\\strng{fullhash}{SRSMSCSV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{booktitle}{2017 IEEE Symposium on Security and Privacy (SP)}\\n    \\\\field{pages}{3\\\\bibrangedash 18}\\n    \\\\field{title}{Membership inference attacks against machine learning models}\\n    \\\\field{year}{2017}\\n  \\\\endentry\\n\\n  \\\\entry{Siddiqui2019}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=SMD}{%\\n         family={Siddiqui},\\n         familyi={S\\\\bibinitperiod},\\n         given={M.\\\\bibnamedelima D.Amran},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim D\\\\bibinitperiod},\\n      }}%\\n      {{hash=FA}{%\\n         family={Fern},\\n         familyi={F\\\\bibinitperiod},\\n         given={Alan},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=DTG}{%\\n         family={Dietterich},\\n         familyi={D\\\\bibinitperiod},\\n         given={Thomas\\\\bibnamedelima G.},\\n         giveni={T\\\\bibinitperiod\\\\bibinitdelim G\\\\bibinitperiod},\\n      }}%\\n      {{hash=WWK}{%\\n         family={Wong},\\n         familyi={W\\\\bibinitperiod},\\n         given={Weng\\\\bibnamedelima Keen},\\n         giveni={W\\\\bibinitperiod\\\\bibinitdelim K\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\keyw{Anomaly detection,anomaly explanation,anomaly\\n  interpretation,explanation evaluation}\\n    \\\\strng{namehash}{SMD+1}\\n    \\\\strng{fullhash}{SMDFADTGWWK1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{{Sequential feature explanations for anomaly detection}}\\n    \\\\field{journaltitle}{ACM Transactions on Knowledge Discovery from Data}\\n    \\\\field{eprinttype}{arXiv}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{sidgwick_1907}{book}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=SH}{%\\n         family={Sidgwick},\\n         familyi={S\\\\bibinitperiod},\\n         given={Henry},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SH1}\\n    \\\\strng{fullhash}{SH1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{The Methods of Ethics}\\n    \\\\field{year}{1907}\\n  \\\\endentry\\n\\n  \\\\entry{sommer2010outside}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=SR}{%\\n         family={Sommer},\\n         familyi={S\\\\bibinitperiod},\\n         given={Robin},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=PV}{%\\n         family={Paxson},\\n         familyi={P\\\\bibinitperiod},\\n         given={Vern},\\n         giveni={V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{organization}{1}{%\\n      {IEEE}%\\n    }\\n    \\\\strng{namehash}{SRPV1}\\n    \\\\strng{fullhash}{SRPV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{booktitle}{2010 IEEE symposium on security and privacy}\\n    \\\\field{pages}{305\\\\bibrangedash 316}\\n    \\\\field{title}{Outside the closed world: On using machine learning for\\n  network intrusion detection}\\n    \\\\field{year}{2010}\\n  \\\\endentry\\n\\n  \\\\entry{StamatisFailureMA}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=SDH}{%\\n         family={Stamatis},\\n         familyi={S\\\\bibinitperiod},\\n         given={D.\\\\bibnamedelima H.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim H\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SDH1}\\n    \\\\strng{fullhash}{SDH1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Failure mode and effect analysis : {FMEA} from theory to\\n  execution}\\n    \\\\field{journaltitle}{ASQC Quality Press}\\n    \\\\field{year}{1996}\\n  \\\\endentry\\n\\n  \\\\entry{Stanovich2016TheRQ}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=SKE}{%\\n         family={Stanovich},\\n         familyi={S\\\\bibinitperiod},\\n         given={Keith\\\\bibnamedelima E.},\\n         giveni={K\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n      {{hash=WRF}{%\\n         family={West},\\n         familyi={W\\\\bibinitperiod},\\n         given={Richard\\\\bibnamedelima F.},\\n         giveni={R\\\\bibinitperiod\\\\bibinitdelim F\\\\bibinitperiod},\\n      }}%\\n      {{hash=TME}{%\\n         family={Toplak},\\n         familyi={T\\\\bibinitperiod},\\n         given={Maggie\\\\bibnamedelima E.},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SKEWRFTME1}\\n    \\\\strng{fullhash}{SKEWRFTME1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{The Rationality Quotient: Toward a Test of Rational Thinking}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{sgn}{misc}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=SMNCER}{%\\n         family={Steve\\\\bibnamedelima Miller},\\n         familyi={S\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n         suffix={Evan\\\\bibnamedelima Reese},\\n         suffixi={E\\\\bibinitperiod\\\\bibinitdelim R\\\\bibinitperiod},\\n         given={Nick\\\\bibnamedelima Carr},\\n         giveni={N\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SMERNC1}\\n    \\\\strng{fullhash}{SMERNC1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Shikata Ga Nai Encoder Still Going Strong}\\n    \\\\verb{url}\\n    \\\\verb https://www.fireeye.com/blog/threat-research/2019/10/shikata-ga-nai-e\\n    \\\\verb ncoder-still-going-strong.html\\n    \\\\endverb\\n  \\\\endentry\\n\\n  \\\\entry{Strathern1997ImprovingRA}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=SM}{%\\n         family={Strathern},\\n         familyi={S\\\\bibinitperiod},\\n         given={Marilyn},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SM1}\\n    \\\\strng{fullhash}{SM1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Improving ratings: audit in the British University\\n  system}\\n    \\\\field{journaltitle}{European Review}\\n    \\\\field{year}{1997}\\n  \\\\endentry\\n\\n  \\\\entry{Stray2020AligningAO}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=SJ}{%\\n         family={Stray},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jonathan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SJ1}\\n    \\\\strng{fullhash}{SJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Aligning AI Optimization to Community Well-Being}\\n    \\\\field{journaltitle}{International Journal of Community Well-Being}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Stray2021WhatAY}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=SJ}{%\\n         family={Stray},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jonathan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=VI}{%\\n         family={Vendrov},\\n         familyi={V\\\\bibinitperiod},\\n         given={Ivan},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=NJ}{%\\n         family={Nixon},\\n         familyi={N\\\\bibinitperiod},\\n         given={Jeremy},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=AS}{%\\n         family={Adler},\\n         familyi={A\\\\bibinitperiod},\\n         given={Steven},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=HMD}{%\\n         family={Hadfield-Menell},\\n         familyi={H\\\\bibinithyphendelim M\\\\bibinitperiod},\\n         given={Dylan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SJ+1}\\n    \\\\strng{fullhash}{SJVINJASHMD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{What are you optimizing for? Aligning Recommender Systems\\n  with Human Values}\\n    \\\\field{volume}{abs/2107.10939}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Stutz2020ConfidenceCalibratedAT}{inproceedings}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=SD}{%\\n         family={Stutz},\\n         familyi={S\\\\bibinitperiod},\\n         given={David},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=HM}{%\\n         family={Hein},\\n         familyi={H\\\\bibinitperiod},\\n         given={Matthias},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SB}{%\\n         family={Schiele},\\n         familyi={S\\\\bibinitperiod},\\n         given={B.},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SDHMSB1}\\n    \\\\strng{fullhash}{SDHMSB1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Confidence-Calibrated Adversarial Training: Generalizing to\\n  Unseen Attacks}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Suciu2019ExploringAE}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=SO}{%\\n         family={Suciu},\\n         familyi={S\\\\bibinitperiod},\\n         given={Octavian},\\n         giveni={O\\\\bibinitperiod},\\n      }}%\\n      {{hash=CSE}{%\\n         family={Coull},\\n         familyi={C\\\\bibinitperiod},\\n         given={Scott\\\\bibnamedelima E.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n      {{hash=JJ}{%\\n         family={Johns},\\n         familyi={J\\\\bibinitperiod},\\n         given={Jeffrey},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SOCSEJJ1}\\n    \\\\strng{fullhash}{SOCSEJJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Exploring Adversarial Examples in Malware Detection}\\n    \\\\field{journaltitle}{IEEE Security and Privacy Workshops (SPW)}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{sumwalt2019assumptions}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=SR}{%\\n         family={Sumwalt},\\n         familyi={S\\\\bibinitperiod},\\n         given={RL},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=LB}{%\\n         family={Landsberg},\\n         familyi={L\\\\bibinitperiod},\\n         given={B},\\n         giveni={B},\\n      }}%\\n      {{hash=HJ}{%\\n         family={Homendy},\\n         familyi={H\\\\bibinitperiod},\\n         given={J},\\n         giveni={J},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SRLBHJ1}\\n    \\\\strng{fullhash}{SRLBHJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Assumptions used in the safety assessment process and the\\n  effects of multiple alerts and indications on pilot performance}\\n    \\\\field{journaltitle}{District of Columbia: National Transportation Safety\\n  Board}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{sutton2010chromium}{book}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=SR}{%\\n         family={Sutton},\\n         familyi={S\\\\bibinitperiod},\\n         given={Rebecca},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {Environmental Working Group Washington, DC}%\\n    }\\n    \\\\strng{namehash}{SR1}\\n    \\\\strng{fullhash}{SR1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Chromium-6 in US tap water}\\n    \\\\field{year}{2010}\\n  \\\\endentry\\n\\n  \\\\entry{syrus1856moral}{book}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=SP}{%\\n         family={Syrus},\\n         familyi={S\\\\bibinitperiod},\\n         given={Publius},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\list{publisher}{1}{%\\n      {L.E. Bernard \\\\& Company}%\\n    }\\n    \\\\strng{namehash}{SP1}\\n    \\\\strng{fullhash}{SP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{The Moral Sayings of Publius Syrus, a Roman Slave}\\n    \\\\field{year}{1856}\\n  \\\\endentry\\n\\n  \\\\entry{szegedy2013intriguing}{article}{}\\n    \\\\name{author}{7}{}{%\\n      {{hash=SC}{%\\n         family={Szegedy},\\n         familyi={S\\\\bibinitperiod},\\n         given={Christian},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZW}{%\\n         family={Zaremba},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Wojciech},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=SI}{%\\n         family={Sutskever},\\n         familyi={S\\\\bibinitperiod},\\n         given={Ilya},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=BJ}{%\\n         family={Bruna},\\n         familyi={B\\\\bibinitperiod},\\n         given={Joan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=ED}{%\\n         family={Erhan},\\n         familyi={E\\\\bibinitperiod},\\n         given={Dumitru},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=GI}{%\\n         family={Goodfellow},\\n         familyi={G\\\\bibinitperiod},\\n         given={Ian},\\n         giveni={I\\\\bibinitperiod},\\n      }}%\\n      {{hash=FR}{%\\n         family={Fergus},\\n         familyi={F\\\\bibinitperiod},\\n         given={Rob},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{SC+1}\\n    \\\\strng{fullhash}{SCZWSIBJEDGIFR1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{S}\\n    \\\\field{sortinithash}{S}\\n    \\\\field{title}{Intriguing properties of neural networks}\\n    \\\\field{journaltitle}{arXiv preprint arXiv:1312.6199}\\n    \\\\field{year}{2013}\\n  \\\\endentry\\n\\n  \\\\entry{Tack2020CSIND}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=TJ}{%\\n         family={Tack},\\n         familyi={T\\\\bibinitperiod},\\n         given={Jihoon},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=MS}{%\\n         family={Mo},\\n         familyi={M\\\\bibinitperiod},\\n         given={Sangwoo},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=JJ}{%\\n         family={Jeong},\\n         familyi={J\\\\bibinitperiod},\\n         given={Jongheon},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Shin},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jinwoo},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TJ+1}\\n    \\\\strng{fullhash}{TJMSJJSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{CSI: Novelty Detection via Contrastive Learning on\\n  Distributionally Shifted Instances}\\n    \\\\field{journaltitle}{NeurIPS}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Tack2021ConsistencyRF}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=TJ}{%\\n         family={Tack},\\n         familyi={T\\\\bibinitperiod},\\n         given={Jihoon},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=YS}{%\\n         family={Yu},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Sihyun},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=JJ}{%\\n         family={Jeong},\\n         familyi={J\\\\bibinitperiod},\\n         given={Jongheon},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=KM}{%\\n         family={Kim},\\n         familyi={K\\\\bibinitperiod},\\n         given={Minseong},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=HSJ}{%\\n         family={Hwang},\\n         familyi={H\\\\bibinitperiod},\\n         given={Sung\\\\bibnamedelima Ju},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim J\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Shin},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jinwoo},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TJ+1}\\n    \\\\strng{fullhash}{TJYSJJKMHSJSJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Consistency Regularization for Adversarial Robustness}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Taleb2012AntifragileTT}{inproceedings}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=TN}{%\\n         family={Taleb},\\n         familyi={T\\\\bibinitperiod},\\n         given={Nassim},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TN1}\\n    \\\\strng{fullhash}{TN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Antifragile: Things That Gain from Disorder}\\n    \\\\field{year}{2012}\\n  \\\\endentry\\n\\n  \\\\entry{Taleb2020StatisticalCO}{inproceedings}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=TN}{%\\n         family={Taleb},\\n         familyi={T\\\\bibinitperiod},\\n         given={Nassim},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TN1}\\n    \\\\strng{fullhash}{TN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Statistical Consequences of Fat Tails: Real World\\n  Preasymptotics, Epistemology, and Applications}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Taleb2007TheBS}{inproceedings}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=TN}{%\\n         family={Taleb},\\n         familyi={T\\\\bibinitperiod},\\n         given={Nassim},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TN1}\\n    \\\\strng{fullhash}{TN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{The Black Swan: The Impact of the Highly Improbable}\\n    \\\\field{year}{2007}\\n  \\\\endentry\\n\\n  \\\\entry{Taleb2013OnTD}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=TN}{%\\n         family={Taleb},\\n         familyi={T\\\\bibinitperiod},\\n         given={Nassim},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=TP}{%\\n         family={Tetlock},\\n         familyi={T\\\\bibinitperiod},\\n         given={Philip},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TNTP1}\\n    \\\\strng{fullhash}{TNTP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{On the Difference between Binary Prediction and True Exposure\\n  with Implications for Forecasting Tournaments and Decision Making Research}\\n    \\\\field{year}{2013}\\n  \\\\endentry\\n\\n  \\\\entry{Taylor2016AlignmentFA}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=TJ}{%\\n         family={Taylor},\\n         familyi={T\\\\bibinitperiod},\\n         given={Jessica},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=YE}{%\\n         family={Yudkowsky},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Eliezer},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=LP}{%\\n         family={LaVictoire},\\n         familyi={L\\\\bibinitperiod},\\n         given={Patrick},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Critch},\\n         familyi={C\\\\bibinitperiod},\\n         given={Andrew},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TJ+2}\\n    \\\\strng{fullhash}{TJYELPCA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Alignment for Advanced Machine Learning Systems}\\n    \\\\field{year}{2016}\\n  \\\\endentry\\n\\n  \\\\entry{teslaaiday}{misc}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=T}{%\\n         family={Tesla},\\n         familyi={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{T1}\\n    \\\\strng{fullhash}{T1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Tesla AI Day}\\n    \\\\verb{url}\\n    \\\\verb https://www.youtube.com/watch?v=j0z4FweCy4M\\n    \\\\endverb\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Tetlock2015SuperforecastingTA}{inproceedings}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=TP}{%\\n         family={Tetlock},\\n         familyi={T\\\\bibinitperiod},\\n         given={Philip},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=GD}{%\\n         family={Gardner},\\n         familyi={G\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TPGD1}\\n    \\\\strng{fullhash}{TPGD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Superforecasting: The Art and Science of Prediction}\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{Trabucco2021ConservativeOM}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=TB}{%\\n         family={Trabucco},\\n         familyi={T\\\\bibinitperiod},\\n         given={Brandon},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=KA}{%\\n         family={Kumar},\\n         familyi={K\\\\bibinitperiod},\\n         given={Aviral},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=GX}{%\\n         family={Geng},\\n         familyi={G\\\\bibinitperiod},\\n         given={Xinyang},\\n         giveni={X\\\\bibinitperiod},\\n      }}%\\n      {{hash=LS}{%\\n         family={Levine},\\n         familyi={L\\\\bibinitperiod},\\n         given={Sergey},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TB+1}\\n    \\\\strng{fullhash}{TBKAGXLS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Conservative Objective Models for Effective Offline\\n  Model-Based Optimization}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Tramr2020OnAA}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=TF}{%\\n         family={Tram{\\\\`e}r},\\n         familyi={T\\\\bibinitperiod},\\n         given={Florian},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=CN}{%\\n         family={Carlini},\\n         familyi={C\\\\bibinitperiod},\\n         given={Nicholas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=BW}{%\\n         family={Brendel},\\n         familyi={B\\\\bibinitperiod},\\n         given={Wieland},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=MA}{%\\n         family={Madry},\\n         familyi={M\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TF+1}\\n    \\\\strng{fullhash}{TFCNBWMA1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{On Adaptive Attacks to Adversarial Example Defenses}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Tramr2018EnsembleAT}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=TF}{%\\n         family={Tram{\\\\`e}r},\\n         familyi={T\\\\bibinitperiod},\\n         given={Florian},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=KA}{%\\n         family={Kurakin},\\n         familyi={K\\\\bibinitperiod},\\n         given={Alexey},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=PN}{%\\n         family={Papernot},\\n         familyi={P\\\\bibinitperiod},\\n         given={Nicolas},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=BD}{%\\n         family={Boneh},\\n         familyi={B\\\\bibinitperiod},\\n         given={Dan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=MP}{%\\n         family={Mcdaniel},\\n         familyi={M\\\\bibinitperiod},\\n         given={Patrick},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TF+1}\\n    \\\\strng{fullhash}{TFKAPNBDMP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Ensemble Adversarial Training: Attacks and Defenses}\\n    \\\\field{volume}{abs/1705.07204}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Turner2020AvoidingSE}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=TAM}{%\\n         family={Turner},\\n         familyi={T\\\\bibinitperiod},\\n         given={A.\\\\bibnamedelima M.},\\n         giveni={A\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=RN}{%\\n         family={Ratzlaff},\\n         familyi={R\\\\bibinitperiod},\\n         given={Neale},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=TP}{%\\n         family={Tadepalli},\\n         familyi={T\\\\bibinitperiod},\\n         given={Prasad},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TAMRNTP1}\\n    \\\\strng{fullhash}{TAMRNTP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{title}{Avoiding Side Effects in Complex Environments}\\n    \\\\field{volume}{abs/2006.06547}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{turner2021optimal}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=TAM}{%\\n         family={Turner},\\n         familyi={T\\\\bibinitperiod},\\n         given={Alexander\\\\bibnamedelima Matt},\\n         giveni={A\\\\bibinitperiod\\\\bibinitdelim M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SLR}{%\\n         family={Smith},\\n         familyi={S\\\\bibinitperiod},\\n         given={Logan\\\\bibnamedelima Riggs},\\n         giveni={L\\\\bibinitperiod\\\\bibinitdelim R\\\\bibinitperiod},\\n      }}%\\n      {{hash=SR}{%\\n         family={Shah},\\n         familyi={S\\\\bibinitperiod},\\n         given={Rohin},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=CA}{%\\n         family={Critch},\\n         familyi={C\\\\bibinitperiod},\\n         given={Andrew},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=TP}{%\\n         family={Tadepalli},\\n         familyi={T\\\\bibinitperiod},\\n         given={Prasad},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{TAM+1}\\n    \\\\strng{fullhash}{TAMSLRSRCATP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{T}\\n    \\\\field{sortinithash}{T}\\n    \\\\field{booktitle}{NeurIPS}\\n    \\\\field{title}{Optimal Policies Tend To Seek Power}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{usplanning}{article}{}\\n    \\\\true{moreauthor}\\n    \\\\true{morelabelname}\\n    \\\\name{author}{1}{}{%\\n      {{hash=UBSSC}{%\\n         family={US},\\n         familyi={U\\\\bibinitperiod},\\n         given={Building Seismic Safety\\\\bibnamedelima Council},\\n         giveni={B\\\\bibinitperiod\\\\bibinitdelim S\\\\bibinitperiod\\\\bibinitdelim\\n  S\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{UBSSC+1}\\n    \\\\strng{fullhash}{UBSSC+1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{U}\\n    \\\\field{sortinithash}{U}\\n    \\\\field{title}{Planning for seismic rehabilitation: societal issues}\\n    \\\\field{year}{1998}\\n  \\\\endentry\\n\\n  \\\\entry{Wainwright2020SafeLife1E}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=WCL}{%\\n         family={Wainwright},\\n         familyi={W\\\\bibinitperiod},\\n         given={Carroll\\\\bibnamedelima L.},\\n         giveni={C\\\\bibinitperiod\\\\bibinitdelim L\\\\bibinitperiod},\\n      }}%\\n      {{hash=EP}{%\\n         family={Eckersley},\\n         familyi={E\\\\bibinitperiod},\\n         given={P.},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WCLEP1}\\n    \\\\strng{fullhash}{WCLEP1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{SafeLife 1.0: Exploring Side Effects in Complex Environments}\\n    \\\\field{volume}{abs/1912.01217}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Wallace2021ConcealedDP}{inproceedings}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=WE}{%\\n         family={Wallace},\\n         familyi={W\\\\bibinitperiod},\\n         given={Eric},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZT}{%\\n         family={Zhao},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Tony},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=FS}{%\\n         family={Feng},\\n         familyi={F\\\\bibinitperiod},\\n         given={Shi},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=SS}{%\\n         family={Singh},\\n         familyi={S\\\\bibinitperiod},\\n         given={Sameer},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WE+1}\\n    \\\\strng{fullhash}{WEZTFSSS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{booktitle}{NAACL}\\n    \\\\field{title}{Concealed Data Poisoning Attacks on NLP Models}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Wang2021FightingGW}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=WD}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Dequan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=JA}{%\\n         family={Ju},\\n         familyi={J\\\\bibinitperiod},\\n         given={An},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=SE}{%\\n         family={Shelhamer},\\n         familyi={S\\\\bibinitperiod},\\n         given={Evan},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=WDA}{%\\n         family={Wagner},\\n         familyi={W\\\\bibinitperiod},\\n         given={David\\\\bibnamedelima A.},\\n         giveni={D\\\\bibinitperiod\\\\bibinitdelim A\\\\bibinitperiod},\\n      }}%\\n      {{hash=DT}{%\\n         family={Darrell},\\n         familyi={D\\\\bibinitperiod},\\n         given={Trevor},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WD+1}\\n    \\\\strng{fullhash}{WDJASEWDADT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{Fighting Gradients with Gradients: Dynamic Defenses against\\n  Adversarial Attacks}\\n    \\\\field{volume}{abs/2105.08714}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Wang2021TentFT}{inproceedings}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=WD}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Dequan},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=SE}{%\\n         family={Shelhamer},\\n         familyi={S\\\\bibinitperiod},\\n         given={Evan},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=LS}{%\\n         family={Liu},\\n         familyi={L\\\\bibinitperiod},\\n         given={Shaoteng},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=OB}{%\\n         family={Olshausen},\\n         familyi={O\\\\bibinitperiod},\\n         given={B.},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=DT}{%\\n         family={Darrell},\\n         familyi={D\\\\bibinitperiod},\\n         given={Trevor},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WD+1}\\n    \\\\strng{fullhash}{WDSELSOBDT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{booktitle}{ICLR}\\n    \\\\field{title}{Tent: Fully Test-Time Adaptation by Entropy Minimization}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Wang2021IMAGINEIS}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=WP}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Pei},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=LY}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Yijun},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=SKK}{%\\n         family={Singh},\\n         familyi={S\\\\bibinitperiod},\\n         given={Krishna\\\\bibnamedelima Kumar},\\n         giveni={K\\\\bibinitperiod\\\\bibinitdelim K\\\\bibinitperiod},\\n      }}%\\n      {{hash=LJ}{%\\n         family={Lu},\\n         familyi={L\\\\bibinitperiod},\\n         given={Jingwan},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=VN}{%\\n         family={Vasconcelos},\\n         familyi={V\\\\bibinitperiod},\\n         given={N.},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WP+1}\\n    \\\\strng{fullhash}{WPLYSKKLJVN1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{IMAGINE: Image Synthesis by Image-Guided Model Inversion}\\n    \\\\field{volume}{abs/2104.05895}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Wang2020StopandGoEB}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=WY}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Yue},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=SE}{%\\n         family={Sarkar},\\n         familyi={S\\\\bibinitperiod},\\n         given={Esha},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=LW}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Wenqing},\\n         giveni={W\\\\bibinitperiod},\\n      }}%\\n      {{hash=MM}{%\\n         family={Maniatakos},\\n         familyi={M\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=JSE}{%\\n         family={Jabari},\\n         familyi={J\\\\bibinitperiod},\\n         given={S.\\\\bibnamedelima E.},\\n         giveni={S\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WY+1}\\n    \\\\strng{fullhash}{WYSELWMMJSE1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement\\n  Learning-based Traffic Congestion Control Systems}\\n    \\\\field{journaltitle}{arXiv: Cryptography and Security}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Wang2019NeuFuzzEF}{article}{}\\n    \\\\name{author}{4}{}{%\\n      {{hash=WY}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Yunchao},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=WZ}{%\\n         family={Wu},\\n         familyi={W\\\\bibinitperiod},\\n         given={Zehui},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=WQ}{%\\n         family={Wei},\\n         familyi={W\\\\bibinitperiod},\\n         given={Qiang},\\n         giveni={Q\\\\bibinitperiod},\\n      }}%\\n      {{hash=WQ}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Qingxian},\\n         giveni={Q\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WY+2}\\n    \\\\strng{fullhash}{WYWZWQWQ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{NeuFuzz: Efficient Fuzzing With Deep Neural Network}\\n    \\\\field{journaltitle}{IEEE Access}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Williams2015ThePO}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=WEG}{%\\n         family={Williams},\\n         familyi={W\\\\bibinitperiod},\\n         given={E.\\\\bibnamedelima G.},\\n         giveni={E\\\\bibinitperiod\\\\bibinitdelim G\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WEG1}\\n    \\\\strng{fullhash}{WEG1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{The Possibility of an Ongoing Moral Catastrophe}\\n    \\\\field{journaltitle}{Ethical Theory and Moral Practice}\\n    \\\\field{year}{2015}\\n  \\\\endentry\\n\\n  \\\\entry{Wilson2005AffectiveF}{article}{}\\n    \\\\name{author}{2}{}{%\\n      {{hash=WT}{%\\n         family={Wilson},\\n         familyi={W\\\\bibinitperiod},\\n         given={Timothy},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=GD}{%\\n         family={Gilbert},\\n         familyi={G\\\\bibinitperiod},\\n         given={Daniel},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WTGD1}\\n    \\\\strng{fullhash}{WTGD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{Affective Forecasting}\\n    \\\\field{journaltitle}{Current Directions in Psychological Science}\\n    \\\\field{year}{2005}\\n  \\\\endentry\\n\\n  \\\\entry{Wu2020AdversarialWP}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=WD}{%\\n         family={Wu},\\n         familyi={W\\\\bibinitperiod},\\n         given={Dongxian},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=XS}{%\\n         family={Xia},\\n         familyi={X\\\\bibinitperiod},\\n         given={Shutao},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=WY}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Yisen},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{WDXSWY1}\\n    \\\\strng{fullhash}{WDXSWY1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{W}\\n    \\\\field{sortinithash}{W}\\n    \\\\field{title}{Adversarial Weight Perturbation Helps Robust Generalization}\\n    \\\\field{journaltitle}{NeurIPS}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Xiao2018CharacterizingAE}{inproceedings}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=XC}{%\\n         family={Xiao},\\n         familyi={X\\\\bibinitperiod},\\n         given={Chaowei},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=DR}{%\\n         family={Deng},\\n         familyi={D\\\\bibinitperiod},\\n         given={Ruizhi},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=LB}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Bo},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=YF}{%\\n         family={Yu},\\n         familyi={Y\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=LM}{%\\n         family={Liu},\\n         familyi={L\\\\bibinitperiod},\\n         given={M.},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=SD}{%\\n         family={Song},\\n         familyi={S\\\\bibinitperiod},\\n         given={D.},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{XC+1}\\n    \\\\strng{fullhash}{XCDRLBYFLMSD1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{X}\\n    \\\\field{sortinithash}{X}\\n    \\\\field{booktitle}{ECCV}\\n    \\\\field{title}{Characterizing Adversarial Examples Based on Spatial\\n  Consistency Information for Semantic Segmentation}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\n  \\\\entry{Xie2020SmoothAT}{article}{}\\n    \\\\name{author}{5}{}{%\\n      {{hash=XC}{%\\n         family={Xie},\\n         familyi={X\\\\bibinitperiod},\\n         given={Cihang},\\n         giveni={C\\\\bibinitperiod},\\n      }}%\\n      {{hash=TM}{%\\n         family={Tan},\\n         familyi={T\\\\bibinitperiod},\\n         given={Mingxing},\\n         giveni={M\\\\bibinitperiod},\\n      }}%\\n      {{hash=GB}{%\\n         family={Gong},\\n         familyi={G\\\\bibinitperiod},\\n         given={Boqing},\\n         giveni={B\\\\bibinitperiod},\\n      }}%\\n      {{hash=YA}{%\\n         family={Yuille},\\n         familyi={Y\\\\bibinitperiod},\\n         given={A.},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=LQV}{%\\n         family={Le},\\n         familyi={L\\\\bibinitperiod},\\n         given={Quoc\\\\bibnamedelima V.},\\n         giveni={Q\\\\bibinitperiod\\\\bibinitdelim V\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{XC+2}\\n    \\\\strng{fullhash}{XCTMGBYALQV1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{X}\\n    \\\\field{sortinithash}{X}\\n    \\\\field{title}{Smooth Adversarial Training}\\n    \\\\field{volume}{abs/2006.14536}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Yin2020DreamingTD}{article}{}\\n    \\\\name{author}{8}{}{%\\n      {{hash=YH}{%\\n         family={Yin},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Hongxu},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=MP}{%\\n         family={Molchanov},\\n         familyi={M\\\\bibinitperiod},\\n         given={Pavlo},\\n         giveni={P\\\\bibinitperiod},\\n      }}%\\n      {{hash=LZ}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Zhizhong},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=AJ}{%\\n         family={{\\\\\\'A}lvarez},\\n         familyi={{\\\\\\'A}\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=MA}{%\\n         family={Mallya},\\n         familyi={M\\\\bibinitperiod},\\n         given={Arun},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=HD}{%\\n         family={Hoiem},\\n         familyi={H\\\\bibinitperiod},\\n         given={Derek},\\n         giveni={D\\\\bibinitperiod},\\n      }}%\\n      {{hash=JN}{%\\n         family={Jha},\\n         familyi={J\\\\bibinitperiod},\\n         given={N.},\\n         giveni={N\\\\bibinitperiod},\\n      }}%\\n      {{hash=KJ}{%\\n         family={Kautz},\\n         familyi={K\\\\bibinitperiod},\\n         given={J.},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{YH+1}\\n    \\\\strng{fullhash}{YHMPLZAJMAHDJNKJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{Y}\\n    \\\\field{sortinithash}{Y}\\n    \\\\field{title}{Dreaming to Distill: Data-Free Knowledge Transfer via\\n  DeepInversion}\\n    \\\\field{journaltitle}{CVPR}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Zaidi2020NeuralES}{inproceedings}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=ZS}{%\\n         family={Zaidi},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Sheheryar},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZA}{%\\n         family={Zela},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Arber},\\n         giveni={A\\\\bibinitperiod},\\n      }}%\\n      {{hash=ET}{%\\n         family={Elsken},\\n         familyi={E\\\\bibinitperiod},\\n         given={T.},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n      {{hash=HCC}{%\\n         family={Holmes},\\n         familyi={H\\\\bibinitperiod},\\n         given={Chris\\\\bibnamedelima C.},\\n         giveni={C\\\\bibinitperiod\\\\bibinitdelim C\\\\bibinitperiod},\\n      }}%\\n      {{hash=HF}{%\\n         family={Hutter},\\n         familyi={H\\\\bibinitperiod},\\n         given={F.},\\n         giveni={F\\\\bibinitperiod},\\n      }}%\\n      {{hash=TY}{%\\n         family={Teh},\\n         familyi={T\\\\bibinitperiod},\\n         given={Y.},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ZS+1}\\n    \\\\strng{fullhash}{ZSZAETHCCHFTY1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{Z}\\n    \\\\field{sortinithash}{Z}\\n    \\\\field{title}{Neural Ensemble Search for Uncertainty Estimation and Dataset\\n  Shift}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Zhang2019TheoreticallyPT}{inproceedings}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=ZH}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Hongyang},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=YY}{%\\n         family={Yu},\\n         familyi={Y\\\\bibinitperiod},\\n         given={Yaodong},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=JJ}{%\\n         family={Jiao},\\n         familyi={J\\\\bibinitperiod},\\n         given={Jiantao},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=XE}{%\\n         family={Xing},\\n         familyi={X\\\\bibinitperiod},\\n         given={Eric},\\n         giveni={E\\\\bibinitperiod},\\n      }}%\\n      {{hash=GLE}{%\\n         family={Ghaoui},\\n         familyi={G\\\\bibinitperiod},\\n         given={Laurent\\\\bibnamedelima El},\\n         giveni={L\\\\bibinitperiod\\\\bibinitdelim E\\\\bibinitperiod},\\n      }}%\\n      {{hash=JMI}{%\\n         family={Jordan},\\n         familyi={J\\\\bibinitperiod},\\n         given={Michael\\\\bibnamedelima I.},\\n         giveni={M\\\\bibinitperiod\\\\bibinitdelim I\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ZH+1}\\n    \\\\strng{fullhash}{ZHYYJJXEGLEJMI1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{Z}\\n    \\\\field{sortinithash}{Z}\\n    \\\\field{booktitle}{ICML}\\n    \\\\field{title}{Theoretically Principled Trade-off between Robustness and\\n  Accuracy}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{Zhang2020TrojaningLM}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=ZX}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Xinyang},\\n         giveni={X\\\\bibinitperiod},\\n      }}%\\n      {{hash=ZZ}{%\\n         family={Zhang},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Zheng},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=WT}{%\\n         family={Wang},\\n         familyi={W\\\\bibinitperiod},\\n         given={Tianying},\\n         giveni={T\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ZXZZWT1}\\n    \\\\strng{fullhash}{ZXZZWT1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{Z}\\n    \\\\field{sortinithash}{Z}\\n    \\\\field{title}{Trojaning Language Models for Fun and Profit}\\n    \\\\field{volume}{abs/2008.00312}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2020}\\n  \\\\endentry\\n\\n  \\\\entry{Zhu2021TowardsUT}{article}{}\\n    \\\\name{author}{6}{}{%\\n      {{hash=ZY}{%\\n         family={Zhu},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Yao},\\n         giveni={Y\\\\bibinitperiod},\\n      }}%\\n      {{hash=MJ}{%\\n         family={Ma},\\n         familyi={M\\\\bibinitperiod},\\n         given={Jiacheng},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=SJ}{%\\n         family={Sun},\\n         familyi={S\\\\bibinitperiod},\\n         given={Jiacheng},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n      {{hash=CZ}{%\\n         family={Chen},\\n         familyi={C\\\\bibinitperiod},\\n         given={Zewei},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n      {{hash=JR}{%\\n         family={Jiang},\\n         familyi={J\\\\bibinitperiod},\\n         given={Rongxin},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=LZ}{%\\n         family={Li},\\n         familyi={L\\\\bibinitperiod},\\n         given={Zhenguo},\\n         giveni={Z\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ZY+1}\\n    \\\\strng{fullhash}{ZYMJSJCZJRLZ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{Z}\\n    \\\\field{sortinithash}{Z}\\n    \\\\field{title}{Towards Understanding the Generative Capability of\\n  Adversarially Robust Classifiers}\\n    \\\\field{journaltitle}{ArXiv}\\n    \\\\field{year}{2021}\\n  \\\\endentry\\n\\n  \\\\entry{Zuboff2019TheAO}{article}{}\\n    \\\\name{author}{1}{}{%\\n      {{hash=ZS}{%\\n         family={Zuboff},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Shoshana},\\n         giveni={S\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ZS1}\\n    \\\\strng{fullhash}{ZS1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{Z}\\n    \\\\field{sortinithash}{Z}\\n    \\\\field{title}{The Age of Surveillance Capitalism: The Fight for a Human\\n  Future at the New Frontier of Power}\\n    \\\\field{year}{2019}\\n  \\\\endentry\\n\\n  \\\\entry{zwetsloot2018beyond}{article}{}\\n    \\\\name{author}{3}{}{%\\n      {{hash=ZR}{%\\n         family={Zwetsloot},\\n         familyi={Z\\\\bibinitperiod},\\n         given={Remco},\\n         giveni={R\\\\bibinitperiod},\\n      }}%\\n      {{hash=TH}{%\\n         family={Toner},\\n         familyi={T\\\\bibinitperiod},\\n         given={Helen},\\n         giveni={H\\\\bibinitperiod},\\n      }}%\\n      {{hash=DJ}{%\\n         family={Ding},\\n         familyi={D\\\\bibinitperiod},\\n         given={Jeffrey},\\n         giveni={J\\\\bibinitperiod},\\n      }}%\\n    }\\n    \\\\strng{namehash}{ZRTHDJ1}\\n    \\\\strng{fullhash}{ZRTHDJ1}\\n    \\\\field{labelnamesource}{author}\\n    \\\\field{labeltitlesource}{title}\\n    \\\\field{sortinit}{Z}\\n    \\\\field{sortinithash}{Z}\\n    \\\\field{title}{Beyond the AI arms race: America, China, and the dangers of\\n  zero-sum thinking}\\n    \\\\field{journaltitle}{Foreign Affairs}\\n    \\\\field{year}{2018}\\n  \\\\endentry\\n\\\\enddatalist\\n\\\\endinput\\n',\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'2108.01634': True,\n",
       "   '1712.04248': True,\n",
       "   '2005.14165': True,\n",
       "   '1802.07228': True,\n",
       "   '2106.09667': True,\n",
       "   '2010.09670': True,\n",
       "   '2107.04982': True,\n",
       "   '2006.04734': True,\n",
       "   '1908.02624': True,\n",
       "   '2106.04480': True,\n",
       "   '1708.06733': True,\n",
       "   '2010.14701': True,\n",
       "   '2009.09153': True,\n",
       "   '2103.01946': True,\n",
       "   '2107.10939': True,\n",
       "   '1312.6199': True,\n",
       "   '1705.07204': True,\n",
       "   '2006.06547': True,\n",
       "   '1912.01217': True,\n",
       "   '2105.08714': True,\n",
       "   '2104.05895': True,\n",
       "   '2006.14536': True,\n",
       "   '2008.00312': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Technical agendas and prioritization',\n",
       "   'highlight': True,\n",
       "   'newsletter_number': 'AN #167',\n",
       "   'newsletter_url': 'https://mailchi.mp/08a639ffa2ba/an-167concrete-ml-safety-problems-and-their-relevance-to-x-risk',\n",
       "   'summarizer': 'Dan Hendrycks',\n",
       "   'summary': 'To make the case for safety to the broader machine learning research community, this paper provides a revised and expanded collection of concrete technical safety research problems, namely:\\n1. Robustness: Create models that are resilient to adversaries, unusual situations, and Black Swan events.\\n2. Monitoring: Detect malicious use, monitor predictions, and discover unexpected model functionality.\\n3. Alignment: Build models that represent and safely optimize hard-to-specify human values.\\n4. External Safety: Use ML to address risks to how ML systems are handled, including cyberwarfare and global turbulence.\\nThroughout, the paper attempts to clarify the problems motivation and provide concrete project ideas.',\n",
       "   'opinion': 'My coauthors and I wrote this paper with the ML research community as our target audience. Here are some thoughts on this topic:\\n\\n1. The document includes numerous problems that, if left unsolved, would imply that ML systems are unsafe. We need the effort of thousands of researchers to address all of them. This means that the main safety discussions cannot stay within the confines of the relatively small EA community. I think we should aim to have over one third of the ML research community work on safety problems. We need the broader community to treat AI safety at least as seriously as safety for nuclear power plants.\\n\\n2. To grow the ML safety research community, we need to suggest problems that can progressively build the community and organically grow support for elevating safety standards within the existing research ecosystem. Research agendas that pertain to AGI exclusively will not scale sufficiently, and such research will simply not get enough market share in time. If we do not get the machine learning community on board with proactively mitigating risks that already exist, we will have a harder time getting them to mitigate less familiar and unprecedented risks. Rather than try to win over the community with alignment philosophy arguments, I\\'ll try winning them over with interesting problems and try to make work towards safer systems rewarded with prestige. \\n\\n3. The benefits of a larger ML safety community are numerous. They can decrease the cost of safety methods and increase the propensity to adopt them. Moreover, to ensure that ML systems have desirable properties, it is necessary to rapidly accumulate incremental improvements, but this requires substantial growth since such gains cannot be produced by just a few card-carrying x-risk researchers with the purest intentions.\\n\\n4. The community will fail to grow if we ignore near-term concerns or actively exclude or sneer at people who work on problems that are useful for both near- and long-term safety (such as adversaries). The alignment community will need to stop engaging in textbook territorialism and welcome serious hypercompetent researchers who do not post on internet forums or who happen not to subscribe to effective altruism. (We include a community strategy in the Appendix.)\\n\\n5. We focus on reinforcement learning but also deep learning. Most of the machine learning research community studies deep learning (e.g., text processing, vision) and does not use, say, Bellman equations or PPO. While existentially catastrophic failures will likely require competent sequential decision-making agents, the relevant problems and solutions can often be better studied outside of gridworlds and MuJoCo. There is much useful safety research to be done that does not need to be cast as a reinforcement learning problem.\\n\\n6. To prevent alienating readers, we did not use phrases such as \"AGI.\" AGI-exclusive research will not scale; for most academics and many industry researchers, it\\'s a nonstarter. Likewise, to prevent needless dismissiveness, we kept x-risks implicit, only hinted at them, or used the phrase \"permanent catastrophe.\" \\nI would have personally enjoyed discussing at length how anomaly detection is an indispensable tool for reducing x-risks from [Black Balls](https://www.nickbostrom.com/papers/vulnerable.pdf), engineered microorganisms, and deceptive ML systems.\\n\\nHere are how the problems relate to x-risk:\\n\\nAdversarial Robustness: This is needed for proxy gaming. ML systems encoding proxies must become more robust to optimizers, which is to say they must become more adversarially robust. We make this connection explicit at the bottom of page 9.\\nBlack Swans and Tail Risks: It\\'s hard to be safe without high reliability. It\\'s not obvious we\\'ll achieve high reliability even by the time we have systems that are superhuman in important respects. Even though MNIST is solved for typical inputs, we still do not even have an MNIST classifier for atypical inputs that is reliable! Moreover, if optimizing agents become unreliable in the face of novel or extreme events, they could start heavily optimizing the wrong thing. Models accidentally going off the rails poses an x-risk if they are sufficiently powerful (this is related to \"competent errors\" and \"treacherous turns\"). If this problem is not solved, optimizers can use these weaknesses; this is a simpler problem on the way to adversarial robustness.\\nAnomaly and Malicious Use Detection: This is an indispensable tool for detecting proxy gaming, [Black Balls](https://www.nickbostrom.com/papers/vulnerable.pdf), engineered microorganisms that present bio x-risks, malicious users who may misalign a model, deceptive ML systems, and rogue ML systems.\\nRepresentative Outputs: Making models honest is a way to avoid many treacherous turns.\\nHidden Model Functionality: This also helps avoid treacherous turns. Backdoors is a potentially useful related problem, as it is about detecting latent but potential sharp changes in behavior.\\nValue Learning: Understanding utilities is difficult even for humans. Powerful optimizers will need to achieve a certain, as-of-yet unclear level of superhuman performance at learning our values.\\nTranslating Values to Action: Successfully prodding models to optimize our values is necessary for safe outcomes.\\nProxy Gaming: Obvious.\\nValue Clarification: This is the philosophy bot section. We will need to decide what values to pursue. If we decide poorly, we may lock in or destroy what is of value. It is also possible that there is an ongoing moral catastrophe, which we would not want to replicate across the cosmos.\\nUnintended Consequences: This should help models not accidentally work against our values.\\nML for Cybersecurity: If you believe that AI governance is valuable and that global turbulence risks can increase risks of terrible outcomes, this section is also relevant. Even if some of the components of ML systems are safe, they can become unsafe when traditional software vulnerabilities enable others to control their behavior. Moreover, traditional software vulnerabilities may lead to the proliferation of powerful advanced models, and this may be worse than proliferating nuclear weapons.\\nInformed Decision Making: We want to avoid decision making based on unreliable gut reactions during a time of crisis. This reduces risks of poor governance of advanced systems.\\n\\nHere are some other notes:\\n\\n1. We use systems theory to motivate inner optimization as we expect this motivation will be more convincing to others.\\n\\n2. Rather than having a broad call for \"interpretability,\" we focus on specific transparency-related problems that are more tractable and neglected. (See the Appendix for a table assessing importance, tractability, and neglectedness.) For example, we include sections on making models honest and detecting emergent functionality.\\n\\n3. The \"External Safety\" section can also be thought of as technical research for reducing \"Governance\" risks. For readers mostly concerned about AI risks from global turbulence, there still is technical research that can be done.\\n\\nHere are some observations while writing the document:\\n\\n1. Some approaches that were previously very popular are currently neglected, such as inverse reinforcement learning. This may be due to currently low tractability.\\n\\n2. Five years ago, I started explicitly brainstorming the content for this document. I think it took the whole time for this document to take shape. Moreover, if this were written last fall, the document would be far more confused, since it took around a year after GPT-3 to become reoriented; writing these types of documents shortly after a paradigm shift may be too hasty.\\n\\n3. When collecting feedback, it was not uncommon for \"in-the-know\" researchers to make opposite suggestions. Some people thought some of the problems in the Alignment section were unimportant, while others thought they were the most critical. We attempted to include most research directions.',\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '2109.13916v4',\n",
       "   'arxiv_id': '2109.13916',\n",
       "   'title': 'Unsolved Problems in ML Safety',\n",
       "   'authors': ['Dan Hendrycks',\n",
       "    'Nicholas Carlini',\n",
       "    'John Schulman',\n",
       "    'Jacob Steinhardt'],\n",
       "   'date_published': '2021-09-28 17:59:36+00:00',\n",
       "   'data_last_modified': '2022-04-29 17:41:33+00:00',\n",
       "   'url': 'http://arxiv.org/abs/2109.13916v4',\n",
       "   'abstract': 'Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), reducing inherent model hazards (\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout, we clarify each problem\\'s motivation and provide concrete research directions.',\n",
       "   'author_comment': 'Position Paper',\n",
       "   'journal_ref': 'None',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'cs.LG',\n",
       "   'categories': \"['cs.LG', 'cs.AI', 'cs.CL', 'cs.CV']\",\n",
       "   'individual_summary': 'Title: Unsolved Problems in ML Safety\\nAuthors: Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt\\nPaper abstract: Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards (\"Robustness\"), identifying hazards (\"Monitoring\"), reducing inherent model hazards (\"Alignment\"), and reducing systemic hazards (\"Systemic Safety\"). Throughout, we clarify each problem\\'s motivation and provide concrete research directions.\\nSummary: To make the case for safety to the broader machine learning research community, this paper provides a revised and expanded collection of concrete technical safety research problems, namely:\\n1. Robustness: Create models that are resilient to adversaries, unusual situations, and Black Swan events.\\n2. Monitoring: Detect malicious use, monitor predictions, and discover unexpected model functionality.\\n3. Alignment: Build models that represent and safely optimize hard-to-specify human values.\\n4. External Safety: Use ML to address risks to how ML systems are handled, including cyberwarfare and global turbulence.\\nThroughout, the paper attempts to clarify the problems motivation and provide concrete project ideas.\\nMy opinion: My coauthors and I wrote this paper with the ML research community as our target audience. Here are some thoughts on this topic:\\n\\n1. The document includes numerous problems that, if left unsolved, would imply that ML systems are unsafe. We need the effort of thousands of researchers to address all of them. This means that the main safety discussions cannot stay within the confines of the relatively small EA community. I think we should aim to have over one third of the ML research community work on safety problems. We need the broader community to treat AI safety at least as seriously as safety for nuclear power plants.\\n\\n2. To grow the ML safety research community, we need to suggest problems that can progressively build the community and organically grow support for elevating safety standards within the existing research ecosystem. Research agendas that pertain to AGI exclusively will not scale sufficiently, and such research will simply not get enough market share in time. If we do not get the machine learning community on board with proactively mitigating risks that already exist, we will have a harder time getting them to mitigate less familiar and unprecedented risks. Rather than try to win over the community with alignment philosophy arguments, I\\'ll try winning them over with interesting problems and try to make work towards safer systems rewarded with prestige. \\n\\n3. The benefits of a larger ML safety community are numerous. They can decrease the cost of safety methods and increase the propensity to adopt them. Moreover, to ensure that ML systems have desirable properties, it is necessary to rapidly accumulate incremental improvements, but this requires substantial growth since such gains cannot be produced by just a few card-carrying x-risk researchers with the purest intentions.\\n\\n4. The community will fail to grow if we ignore near-term concerns or actively exclude or sneer at people who work on problems that are useful for both near- and long-term safety (such as adversaries). The alignment community will need to stop engaging in textbook territorialism and welcome serious hypercompetent researchers who do not post on internet forums or who happen not to subscribe to effective altruism. (We include a community strategy in the Appendix.)\\n\\n5. We focus on reinforcement learning but also deep learning. Most of the machine learning research community studies deep learning (e.g., text processing, vision) and does not use, say, Bellman equations or PPO. While existentially catastrophic failures will likely require competent sequential decision-making agents, the relevant problems and solutions can often be better studied outside of gridworlds and MuJoCo. There is much useful safety research to be done that does not need to be cast as a reinforcement learning problem.\\n\\n6. To prevent alienating readers, we did not use phrases such as \"AGI.\" AGI-exclusive research will not scale; for most academics and many industry researchers, it\\'s a nonstarter. Likewise, to prevent needless dismissiveness, we kept x-risks implicit, only hinted at them, or used the phrase \"permanent catastrophe.\" \\nI would have personally enjoyed discussing at length how anomaly detection is an indispensable tool for reducing x-risks from [Black Balls](https://www.nickbostrom.com/papers/vulnerable.pdf), engineered microorganisms, and deceptive ML systems.\\n\\nHere are how the problems relate to x-risk:\\n\\nAdversarial Robustness: This is needed for proxy gaming. ML systems encoding proxies must become more robust to optimizers, which is to say they must become more adversarially robust. We make this connection explicit at the bottom of page 9.\\nBlack Swans and Tail Risks: It\\'s hard to be safe without high reliability. It\\'s not obvious we\\'ll achieve high reliability even by the time we have systems that are superhuman in important respects. Even though MNIST is solved for typical inputs, we still do not even have an MNIST classifier for atypical inputs that is reliable! Moreover, if optimizing agents become unreliable in the face of novel or extreme events, they could start heavily optimizing the wrong thing. Models accidentally going off the rails poses an x-risk if they are sufficiently powerful (this is related to \"competent errors\" and \"treacherous turns\"). If this problem is not solved, optimizers can use these weaknesses; this is a simpler problem on the way to adversarial robustness.\\nAnomaly and Malicious Use Detection: This is an indispensable tool for detecting proxy gaming, [Black Balls](https://www.nickbostrom.com/papers/vulnerable.pdf), engineered microorganisms that present bio x-risks, malicious users who may misalign a model, deceptive ML systems, and rogue ML systems.\\nRepresentative Outputs: Making models honest is a way to avoid many treacherous turns.\\nHidden Model Functionality: This also helps avoid treacherous turns. Backdoors is a potentially useful related problem, as it is about detecting latent but potential sharp changes in behavior.\\nValue Learning: Understanding utilities is difficult even for humans. Powerful optimizers will need to achieve a certain, as-of-yet unclear level of superhuman performance at learning our values.\\nTranslating Values to Action: Successfully prodding models to optimize our values is necessary for safe outcomes.\\nProxy Gaming: Obvious.\\nValue Clarification: This is the philosophy bot section. We will need to decide what values to pursue. If we decide poorly, we may lock in or destroy what is of value. It is also possible that there is an ongoing moral catastrophe, which we would not want to replicate across the cosmos.\\nUnintended Consequences: This should help models not accidentally work against our values.\\nML for Cybersecurity: If you believe that AI governance is valuable and that global turbulence risks can increase risks of terrible outcomes, this section is also relevant. Even if some of the components of ML systems are safe, they can become unsafe when traditional software vulnerabilities enable others to control their behavior. Moreover, traditional software vulnerabilities may lead to the proliferation of powerful advanced models, and this may be worse than proliferating nuclear weapons.\\nInformed Decision Making: We want to avoid decision making based on unreliable gut reactions during a time of crisis. This reduces risks of poor governance of advanced systems.\\n\\nHere are some other notes:\\n\\n1. We use systems theory to motivate inner optimization as we expect this motivation will be more convincing to others.\\n\\n2. Rather than having a broad call for \"interpretability,\" we focus on specific transparency-related problems that are more tractable and neglected. (See the Appendix for a table assessing importance, tractability, and neglectedness.) For example, we include sections on making models honest and detecting emergent functionality.\\n\\n3. The \"External Safety\" section can also be thought of as technical research for reducing \"Governance\" risks. For readers mostly concerned about AI risks from global turbulence, there still is technical research that can be done.\\n\\nHere are some observations while writing the document:\\n\\n1. Some approaches that were previously very popular are currently neglected, such as inverse reinforcement learning. This may be due to currently low tractability.\\n\\n2. Five years ago, I started explicitly brainstorming the content for this document. I think it took the whole time for this document to take shape. Moreover, if this were written last fall, the document would be far more confused, since it took around a year after GPT-3 to become reoriented; writing these types of documents shortly after a paradigm shift may be too hasty.\\n\\n3. When collecting feedback, it was not uncommon for \"in-the-know\" researchers to make opposite suggestions. Some people thought some of the problems in the Alignment section were unimportant, while others thought they were the most critical. We attempted to include most research directions.',\n",
       "   'paper_text': '',\n",
       "   'text': 'HIGHLIGHTS\\n[Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916) *(Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt)* (summarized by Dan Hendrycks): To make the case for safety to the broader machine learning research community, this paper provides a revised and expanded collection of concrete technical safety research problems, namely:1. Robustness: Create models that are resilient to adversaries, unusual situations, and Black Swan events.2. Monitoring: Detect malicious use, monitor predictions, and discover unexpected model functionality.3. Alignment: Build models that represent and safely optimize hard-to-specify human values.4. External Safety: Use ML to address risks to how ML systems are handled, including cyberwarfare and global turbulence.Throughout, the paper attempts to clarify the problems motivation and provide concrete project ideas. |\\n\\n\\n |\\n\\n\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Dan Hendrycks\\' opinion:** My coauthors and I wrote this paper with the ML research community as our target audience. Here are some thoughts on this topic:1. The document includes numerous problems that, if left unsolved, would imply that ML systems are unsafe. We need the effort of thousands of researchers to address all of them. This means that the main safety discussions cannot stay within the confines of the relatively small EA community. I think we should aim to have over one third of the ML research community work on safety problems. We need the broader community to treat AI safety at least as seriously as safety for nuclear power plants.2. To grow the ML safety research community, we need to suggest problems that can progressively build the community and organically grow support for elevating safety standards within the existing research ecosystem. Research agendas that pertain to AGI exclusively will not scale sufficiently, and such research will simply not get enough market share in time. If we do not get the machine learning community on board with proactively mitigating risks that already exist, we will have a harder time getting them to mitigate less familiar and unprecedented risks. Rather than try to win over the community with alignment philosophy arguments, I\\'ll try winning them over with interesting problems and try to make work towards safer systems rewarded with prestige. 3. The benefits of a larger ML safety community are numerous. They can decrease the cost of safety methods and increase the propensity to adopt them. Moreover, to ensure that ML systems have desirable properties, it is necessary to rapidly accumulate incremental improvements, but this requires substantial growth since such gains cannot be produced by just a few card-carrying x-risk researchers with the purest intentions.4. The community will fail to grow if we ignore near-term concerns or actively exclude or sneer at people who work on problems that are useful for both near- and long-term safety (such as adversaries). The alignment community will need to stop engaging in textbook territorialism and welcome serious hypercompetent researchers who do not post on internet forums or who happen not to subscribe to effective altruism. (We include a community strategy in the Appendix.)5. We focus on reinforcement learning but also deep learning. Most of the machine learning research community studies deep learning (e.g., text processing, vision) and does not use, say, Bellman equations or PPO. While existentially catastrophic failures will likely require competent sequential decision-making agents, the relevant problems and solutions can often be better studied outside of gridworlds and MuJoCo. There is much useful safety research to be done that does not need to be cast as a reinforcement learning problem.6. To prevent alienating readers, we did not use phrases such as \"AGI.\" AGI-exclusive research will not scale; for most academics and many industry researchers, it\\'s a nonstarter. Likewise, to prevent needless dismissiveness, we kept x-risks implicit, only hinted at them, or used the phrase \"permanent catastrophe.\" I would have personally enjoyed discussing at length how anomaly detection is an indispensable tool for reducing x-risks from [Black Balls](https://www.nickbostrom.com/papers/vulnerable.pdf), engineered microorganisms, and deceptive ML systems.Here are how the problems relate to x-risk:Adversarial Robustness: This is needed for proxy gaming. ML systems encoding proxies must become more robust to optimizers, which is to say they must become more adversarially robust. We make this connection explicit at the bottom of page 9.Black Swans and Tail Risks: It\\'s hard to be safe without high reliability. It\\'s not obvious we\\'ll achieve high reliability even by the time we have systems that are superhuman in important respects. Even though MNIST is solved for typical inputs, we still do not even have an MNIST classifier for atypical inputs that is reliable! Moreover, if optimizing agents become unreliable in the face of novel or extreme events, they could start heavily optimizing the wrong thing. Models accidentally going off the rails poses an x-risk if they are sufficiently powerful (this is related to \"competent errors\" and \"treacherous turns\"). If this problem is not solved, optimizers can use these weaknesses; this is a simpler problem on the way to adversarial robustness.Anomaly and Malicious Use Detection: This is an indispensable tool for detecting proxy gaming, [Black Balls](https://www.nickbostrom.com/papers/vulnerable.pdf), engineered microorganisms that present bio x-risks, malicious users who may misalign a model, deceptive ML systems, and rogue ML systems.Representative Outputs: Making models honest is a way to avoid many treacherous turns.Hidden Model Functionality: This also helps avoid treacherous turns. Backdoors is a potentially useful related problem, as it is about detecting latent but potential sharp changes in behavior.Value Learning: Understanding utilities is difficult even for humans. Powerful optimizers will need to achieve a certain, as-of-yet unclear level of superhuman performance at learning our values.Translating Values to Action: Successfully prodding models to optimize our values is necessary for safe outcomes.Proxy Gaming: Obvious.Value Clarification: This is the philosophy bot section. We will need to decide what values to pursue. If we decide poorly, we may lock in or destroy what is of value. It is also possible that there is an ongoing moral catastrophe, which we would not want to replicate across the cosmos.Unintended Consequences: This should help models not accidentally work against our values.ML for Cybersecurity: If you believe that AI governance is valuable and that global turbulence risks can increase risks of terrible outcomes, this section is also relevant. Even if some of the components of ML systems are safe, they can become unsafe when traditional software vulnerabilities enable others to control their behavior. Moreover, traditional software vulnerabilities may lead to the proliferation of powerful advanced models, and this may be worse than proliferating nuclear weapons.Informed Decision Making: We want to avoid decision making based on unreliable gut reactions during a time of crisis. This reduces risks of poor governance of advanced systems.Here are some other notes:1. We use systems theory to motivate inner optimization as we expect this motivation will be more convincing to others.2. Rather than having a broad call for \"interpretability,\" we focus on specific transparency-related problems that are more tractable and neglected. (See the Appendix for a table assessing importance, tractability, and neglectedness.) For example, we include sections on making models honest and detecting emergent functionality.3. The \"External Safety\" section can also be thought of as technical research for reducing \"Governance\" risks. For readers mostly concerned about AI risks from global turbulence, there still is technical research that can be done.Here are some observations while writing the document:1. Some approaches that were previously very popular are currently neglected, such as inverse reinforcement learning. This may be due to currently low tractability.2. Five years ago, I started explicitly brainstorming the content for this document. I think it took the whole time for this document to take shape. Moreover, if this were written last fall, the document would be far more confused, since it took around a year after GPT-3 to become reoriented; writing these types of documents shortly after a paradigm shift may be too hasty.3. When collecting feedback, it was not uncommon for \"in-the-know\" researchers to make opposite suggestions. Some people thought some of the problems in the Alignment section were unimportant, while others thought they were the most critical. We attempted to include most research directions. |\\n\\n |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [[MLSN #1]: ICLR Safety Paper Roundup](https://www.alignmentforum.org/posts/8Gv5zSCnGeLxK5FAF/mlsn-1-iclr-safety-paper-roundup) *(Dan Hendrycks)* (summarized by Rohin): This is the first issue of the ML Safety Newsletter, which is \"a monthly safety newsletter which is designed to cover empirical safety research and be palatable to the broader machine learning research community\". |\\n\\n\\n |\\n\\n\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** I\\'m very excited to see this newsletter: this is a category of papers that I want to know about and that are relevant to safety, but I don\\'t have the time to read all of these papers given all the other alignment work I read, especially since I don\\'t personally work in these areas and so often find it hard to summarize them or place them in the appropriate context. Dan on the other hand has written many such papers himself and generally knows the area, and so will likely do a much better job than I would. I recommend you subscribe, especially since I\\'m not going to send a link to each MLSN in this newsletter. |\\n\\n |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n TECHNICAL AI ALIGNMENT\\n\\n\\n TECHNICAL AGENDAS AND PRIORITIZATION\\n[Selection Theorems: A Program For Understanding Agents](https://www.alignmentforum.org/posts/G2Lne2Fi7Qra5Lbuf/selection-theorems-a-program-for-understanding-agents) *(John Wentworth)* (summarized by Rohin): This post proposes a research area for understanding agents: **selection theorems**. A selection theorem is a theorem that tells us something about agents that will be selected for in a broad class of environments. Selection theorems are helpful because (1) they can provide additional assumptions that can help with learning human values, and (2) they can tell us likely properties of the agents we build by accident (think inner alignment concerns).As an example, [coherence arguments](https://www.alignmentforum.org/posts/RQpNHSiWaXTvDxt6R/coherent-decisions-imply-consistent-utilities) demonstrate that when an environment presents an agent with bets or lotteries, where the agent cares only about the outcomes of the bets, then any good agent can be represented as maximizing expected utility. (What does it mean to be good? This can vary, but one example would be that the agent is not subject to Dutch books, i.e. situations in which it is guaranteed to lose resources.) This can then be turned into a selection argument by combining it with something that selects for good agents. For example, evolution will select for agents that dont lose resources for no gain, so humans are likely to be represented as maximizing expected utility. Unfortunately, many coherence arguments implicitly assume that the agent has no internal state, which is not true for humans, so this argument does not clearly work. As another example, our ML training procedures will likely also select for agents that dont waste resources, which could allow us to conclude that the resulting agents can be represented as maximizing expected utility, if the agents don\\'t have internal states.Coherence arguments arent the only kind of selection theorem. The [good(er) regulator theorem](https://www.alignmentforum.org/posts/Dx9LoqsEh3gHNJMDk/fixing-the-good-regulator-theorem) ([AN #138](https://mailchi.mp/e7cec4ed9117/an-138why-ai-governance-should-find-problems-rather-than-just-solving-them)) provides a set of scenarios under which agents learn an internal world model. The [Kelly criterion](http://www.eecs.harvard.edu/cs286r/courses/fall10/papers/Chapter6.pdf) tells us about scenarios in which the best (most selected) agents will make bets as though they are maximizing expected log money. These and other examples are described in [this followup post](https://www.alignmentforum.org/posts/N2NebPD78ioyWHhNm/some-existing-selection-theorems).The rest of this post elaborates on the various parts of a selection theorem and provides advice on how to make original research contributions in the area of selection theorems. Another [followup post](https://www.alignmentforum.org/posts/RuDD3aQWLDSb4eTXP/what-selection-theorems-do-we-expect-want) describes some useful properties for which the author expects there are useful selections theorems to prove. |\\n\\n\\n |\\n\\n\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** People sometimes expect me to be against this sort of work, because I wrote [Coherence arguments do not imply goal-directed behavior](https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-imply-goal-directed-behavior) ([AN #35](https://mailchi.mp/bbd47ba94e84/alignment-newsletter-35)). This is not true. My point in that post is that coherence arguments *alone* are not enough, you need to combine them with some other assumption (for example, that there exists some resource over which the agent has no terminal preferences). I do think it is plausible that this research agenda gives us a better picture of agency that tells us something about how AI systems will behave, or something about how to better infer human values. While I am personally more excited about studying particular development paths to AGI rather than more abstract agent models, I do think this research would be more useful than other types of alignment research I have seen proposed. |\\n\\n |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n OTHER PROGRESS IN AI\\n\\n\\n MISCELLANEOUS (AI)\\n[State of AI Report 2021](https://www.stateof.ai/2021-report-launch.html) *(Nathan Benaich and Ian Hogarth)* (summarized by Rohin): As with [past](https://www.stateof.ai/) ([AN #15](https://mailchi.mp/4920e52dd61b/alignment-newsletter-15)) [reports](https://www.stateof.ai/) ([AN #120](https://mailchi.mp/42ec72ef7e11/an-120tracing-the-intellectual-roots-of-ai-and-ai-alignment)), Im not going to summarize the entire thing; instead you get the high-level themes that the authors identified:1. AI is stepping up in more concrete ways, including in mission critical infrastructure.2. AI-first approaches have taken biology by storm (and we arent just talking about AlphaFold).3. Transformers have emerged as a general purpose architecture for machine learning in many domains, not just NLP.4. Investors have taken notice, with record funding this year into AI startups, and two first ever IPOs for AI-first drug discovery companies, as well as blockbuster IPOs for data infrastructure and cybersecurity companies that help enterprises retool for the AI-first era.5. The under-resourced AI-alignment efforts from key organisations who are advancing the overall field of AI, as well as concerns about datasets used to train AI models and bias in model evaluation benchmarks, raise important questions about how best to chart the progress of AI systems with rapidly advancing capabilities.6. AI is now an actual arms race rather than a figurative one, with reports of recent use of autonomous weapons by various militaries.7. Within the US-China rivalry, China\\'s ascension in research quality and talent training is notable, with Chinese institutions now beating the most prominent Western ones.8. There is an emergence and nationalisation of large language models. |\\n\\n\\n |\\n\\n\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** In [last years report](https://www.stateof.ai/) ([AN #120](https://mailchi.mp/42ec72ef7e11/an-120tracing-the-intellectual-roots-of-ai-and-ai-alignment)), I said that their 8 predictions seemed to be going out on a limb, and that even 67% accuracy woud be pretty impressive. This year, they scored their predictions as 5 Yes, 1 Sort of, and 2 No. That being said, they graded The first 10 trillion parameter dense model as Yes, I believe on the basis that Microsoft had run a couple of steps of training on a 32 trillion parameter dense model. I definitely interpreted the prediction as saying that a 10 trillion parameter model would be trained *to completion*, which I do not think happened publicly, so Im inclined to give it a No. Still, this does seem like a decent track record for what seemed to me to be non-trivial predictions. This year\\'s predictions seem similarly \"out on a limb\" as last year\\'s.This years report included one-slide summaries of many papers Ive summarized before. I only found one major issue -- the slide on [TruthfulQA](https://arxiv.org/abs/2109.07958) ([AN #165](https://mailchi.mp/bfd4b0ddf358/an-165when-large-models-are-more-likely-to-lie)) implies that larger language models are less honest *in general*, rather than being more likely to imitate human falsehoods. This is actually a pretty good track record, given the number of things they summarized where I would have noticed if there were major issues. |\\n\\n |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n NEWS\\n[CHAI Internships 2022](https://humancompatible.ai/jobs#chai-internships) (summarized by Rohin): CHAI internships are open once again! Typically, an intern will execute on an AI safety research project proposed by their mentor, resulting in a first-author publication at a workshop. The early deadline is November 23rd and the regular deadline is December 13th. |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI\\'m always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2021 Alignment Newsletter, All rights reserved.*\\n\\n**'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2202.07785v1',\n",
       "  'title': 'Predictability and Surprise in Large Generative Models',\n",
       "  'authors': ['Deep Ganguli',\n",
       "   'Danny Hernandez',\n",
       "   'Liane Lovitt',\n",
       "   'Nova DasSarma',\n",
       "   'Tom Henighan',\n",
       "   'Andy Jones',\n",
       "   'Nicholas Joseph',\n",
       "   'Jackson Kernion',\n",
       "   'Ben Mann',\n",
       "   'Amanda Askell',\n",
       "   'Yuntao Bai',\n",
       "   'Anna Chen',\n",
       "   'Tom Conerly',\n",
       "   'Dawn Drain',\n",
       "   'Nelson Elhage',\n",
       "   'Sheer El Showk',\n",
       "   'Stanislav Fort',\n",
       "   'Zac Hatfield-Dodds',\n",
       "   'Scott Johnston',\n",
       "   'Shauna Kravec',\n",
       "   'Neel Nanda',\n",
       "   'Kamal Ndousse',\n",
       "   'Catherine Olsson',\n",
       "   'Daniela Amodei',\n",
       "   'Dario Amodei',\n",
       "   'Tom Brown',\n",
       "   'Jared Kaplan',\n",
       "   'Sam McCandlish',\n",
       "   'Chris Olah',\n",
       "   'Jack Clark'],\n",
       "  'date_published': '2022-02-15 23:21:23+00:00',\n",
       "  'data_last_modified': '2022-02-15 23:21:23+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2202.07785v1',\n",
       "  'abstract': 'Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their \"scaling laws\"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.',\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.CY',\n",
       "  'categories': ['cs.CY'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': './main_arxiv.tex',\n",
       "  'text': '---\\nabstract: |\\n  Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their \\\\\"scaling laws\\\\\"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.\\nauthor:\\n- |\\n  Deep Ganguli[^1] Danny HernandezLiane LovittNova DasSarma [^2] Tom HenighanAndy JonesNicholas JosephJackson KernionBen MannAmanda Askell Yuntao Bai Anna Chen Tom Conerly Dawn Drain Nelson Elhage Sheer El Showk Stanislav Fort Zac Hatfield-Dodds Scott Johnston Shauna Kravec Neel Nanda Kamal Ndousse Catherine Olsson Daniela Amodei Dario Amodei Tom Brown Jared Kaplan Sam McCandlish Chris Olah Jack Clark[^3]\\\\\\n  Anthropic\\nbibliography:\\n- references_zotero.bib\\ntitle: Predictability and Surprise in Large Generative Models\\n---\\n\\nIntroduction\\n============\\n\\nScaling up the amount of data, compute power, and model parameters of neural networks has recently led to the arrival (and real world deployment) of capable generative models such as CLIP [@radford_learning_2021], Ernie $3.0$ Titan [@wang_ernie_2021], FLAN [@wei_finetuned_2021], Gopher [@rae_scaling_2021], GPT-3 [@brown_language_2020], HyperClova [@kim_what_2021], Jurassic-1-Jumbo [@lieber_jurassic-1_2021], Megatron Turing NLG [@smith_using_2022], LaMDA [@thoppilan_lamda_2022], Pan Gu [@zeng_pangu-alpha_2021], Yuan $1.0$ [@wu_yuan_2021], and more. For this class of models[^4] the relationship between scale and model performance is often so predictable that it can be described in a lawful relationship --- a scaling law. In most cases, these scaling laws predict a continued increase in certain capabilities as models get larger. At the same time, larger generative models represent an increasing proportion of the eye-catching results in machine learning. As a result, many institutions have started producing large models over the past few years, in response to the predictability afforded by scaling laws, and the fact these models can be plugged into systems that generate economic value, like search engines.[^5] It has also become clear that these models present novel risks of harmful behavior, which are difficult to predict and may become more severe as the models increase in capability. Attempts to study these harms with smaller models may not accurately reflect what occurs in larger ones.\\n\\nIn this paper, we attempt to better understand the influence of scaling laws on the dynamics of large-scale model development and deployment, with a focus on large language models. **Our basic thesis is that large generative models have an unusual combination of high predictability --- model loss improves in relation to resources expended on training, and tends to correlate loosely with improved performance on many tasks --- and high unpredictability --- specific model capabilities, inputs, and outputs can\\'t be predicted ahead of time. The former drives rapid development of such models while the latter makes it difficult to anticipate the consequences of their development and deployment.** We go through examples of how this combination can lead to socially harmful behavior, while also analyzing the motivations and challenges that developers of such models will face. Our goal in this paper is to outline how and why we expect these models to be developed, so we can identify interventions to guide model development. We conclude with some policy recommendations that could increase the safety of large-scale model deployments, and improve the incentive structure for developers building these models. Though all of the individual points about scaling laws, open-endedness, or the proliferation of large models are explicitly or implicitly presented in other research, our contribution here is to highlight the complete picture together with its implications.\\n\\nAlthough we focus on scaling laws, many of our points complement existing views on the societal risks of deploying large models [@bender_dangers_2021; @tamkin_understanding_2021; @bommasani_opportunities_2021; @dinan_anticipating_2021; @weidinger_ethical_2021; @kenton_alignment_2021]. However, similarly to [@weidinger_ethical_2021], we do not consider here the costs of human labor involved in creating and annotating training data [@gray_ghost_2019], the ethics of supply chains involved in creating the requisite hardware on which to train models [@crawford_atlas_2021], or the environmental costs of training models [@bender_dangers_2021; @patterson_carbon_2021; @schwartz_green_2020; @strubell_energy_2019]. Scaling laws are likely to significantly impact these issues.\\n\\nThe remainder of the paper is organized as follows. In Section [2](#sec:1){reference-type=\"ref\" reference=\"sec:1\"}, we articulate and support our central thesis about large generative models by decomposing it into four claims, each of which we support with evidence from previously published data, and in some cases, with novel experiments on large language models [@askell_general_2021]. In Section [2.1](#sec:1.1){reference-type=\"ref\" reference=\"sec:1.1\"} we discuss smooth general capability scaling. More precisely, by general capability scaling we mean two things. First, the training (and test) loss improves predictably with scale on a broad data distribution. Second, this improvement in loss tends to correlate on average with increased performance on a number of downstream tasks [@brown_language_2020; @rae_scaling_2021]. We refer to the combination of these two properties throughout the paper as smooth general capability (or performance) scaling.[^6] In Section [2.2](#sec:1.2){reference-type=\"ref\" reference=\"sec:1.2\"}, we discuss abrupt specific capability scaling, in which models can also suddenly gain specific capabilities at scale. We illustrate this phenomenon with three examples from the literature [@brown_language_2020; @rae_scaling_2021; @austin_program_2021]. In Section [2.3](#sec:1.3){reference-type=\"ref\" reference=\"sec:1.3\"}, we argue that entire areas of model competency may be unknown until they are solicited from specific inputs, problem domains, or applications. In Section [2.4](#sec:1.4){reference-type=\"ref\" reference=\"sec:1.4\"}, we discuss challenges that arise from the open-endedness of model outputs and show both qualitative and quantitative examples of harmful and toxic outputs emerging with scale.\\n\\n![[\\\\[fig:scaling_laws\\\\]]{#fig:scaling_laws label=\"fig:scaling_laws\"} Scaling laws reliably predict that model performance (y-axes) improves with increasing compute **(Left)**, training data **(Middle)**, and model size **(Right)**. In all cases a power-law (straight line, black) fits the empirically observed data (blue) exceptionally well. Figure adapted from [@kaplan_scaling_2020].](figures/scaling_laws.pdf){#fig:scaling_laws width=\"99%\"}\\n\\nIn Section [3](#sec:2){reference-type=\"ref\" reference=\"sec:2\"}, we outline why, despite these conflicting properties of predictability and unpredictability, we expect increasing development and deployment of large generative models despite the challenges we outline in Section [2](#sec:1){reference-type=\"ref\" reference=\"sec:1\"}. We posit that this is due to a confluence of economic, scientific, and prestige motivations, each of which we summarize. We also consider a few possible barriers to entry that model developers may face during development and deployment, including high financial costs, access to engineering talent, safety concerns, and a lack of standards on how to responsibly deploy capable generative models. We also provide some empirical observations (grounded in the motivations and challenges described above) about how the development of large language models has unfolded thus far, including a quantitative analysis of the increasing gap between academia and industry for large model development.\\n\\nFinally, in Section [4](#sec:3){reference-type=\"ref\" reference=\"sec:3\"} we outline policy interventions that may help concretely address the challenges we outline in Sections [2](#sec:1){reference-type=\"ref\" reference=\"sec:1\"} and [3](#sec:2){reference-type=\"ref\" reference=\"sec:2\"} in order to help guide the development and deployment of larger models for the broader social good. We leave some illustrative experiments, technical details, and caveats about our claims in Appendix [7](#app){reference-type=\"ref\" reference=\"app\"}.\\n\\nDistinguishing Features of Large Generative Models {#sec:1}\\n==================================================\\n\\nWe claim that large generative models (e.g., GPT-3 [@brown_language_2020], LaMDA [@thoppilan_lamda_2022], Gopher [@rae_scaling_2021], etc.) are distinguished by four features:\\n\\n-   **Smooth, general capability scaling**: It is possible to *predictably* improve the general performance of generative models --- their loss on capturing a specific, though very broad, data distribution --- by scaling up the size of the models, the compute used to train them, and the amount of data they\\'re trained on in the correct proportions. These proportions can be accurately predicted by scaling laws (Figure [1](#fig:scaling_laws){reference-type=\"ref\" reference=\"fig:scaling_laws\"}). We believe that these scaling laws de-risk investments in building larger and generally more capable models despite the high resource costs and the difficulty of predicting precisely how well a model will perform on a specific task. Note, the harmful properties of models, such as toxicity, can scale alongside directly helpful capabilities.\\n\\n-   **Abrupt, specific capability scaling**: Though performance is predictable at a general level, performance on a specific task can sometimes emerge quite unpredictably and abruptly at scale.[^7] While counter-intuitive, this is possible because any specific task is a tiny slice of a model\\'s output probability distribution, and so can change rapidly even as the full distribution remains smooth.\\n\\n-   **Open-ended inputs and domains**: Large generative models are open-ended and can take in a varying range of inputs concerning arbitrary domains. As a result, certain capabilities (or even entire areas of competency) may be unknown until an input happens to be provided that solicits such knowledge. Even after a model is trained, creators and users may not be aware of most of its (possibly harmful) capabilities. These properties become more pronounced as the models scale --- larger models tend to be harder to characterize than smaller ones.\\n\\n-   **Open-ended outputs**: Finally, model outputs are also open-ended in the sense that they are difficult to predict or control, even given a fixed scale, input, topic, or task. These outputs may be helpful or harmful, but it\\'s difficult to know in advance. Of course, models with both open-ended inputs and outputs have existed for decades, but what is new is the level of capability and breadth of open-endedness.\\n\\nSmooth General Capability Scaling {#sec:1.1}\\n---------------------------------\\n\\nGenerally, machine learning experiments are not precisely predictable --- complex models trained on complex data typically yield noisy or variable results [@zhuang_randomness_2021; @clary_lets_2019].[^8] Though individual experiments may be unpredictable, the general performance of large generative models tends to exhibit smooth and predictable growth as a function of scale --- larger systems tend to do increasingly better on a broad range of tasks. This was first noticed by [@hestness_deep_2017] who observed that capabilities such as machine translation and speech recognition increased in a smooth, predictable manner as the size of the model increased. Subsequent work formalized and experimentally validated a quantitative relationship between scale (in terms of both model size and training data size) and model generalization error [@rosenfeld_constructive_2019]. Furthermore, [@kaplan_scaling_2020] demonstrated that test loss performance on language modeling tasks scales as a predictable function of model size, dataset size, and duration of training. These three factors are like ingredients in a chemical reaction, such that if all are scaled up in tandem, the test loss improves proportionally. However, if there is too little of one ingredient, gains are limited by this ingredient. The trends are remarkably consistent, with only tiny deviations from a simple fit to the data[^9], covering dozens of data points and several orders of magnitude (Figure [1](#fig:scaling_laws){reference-type=\"ref\" reference=\"fig:scaling_laws\"}). Subsequent work has shown that similar scaling laws exist in generative models for other modalities (e.g., images, video, math, etc.) [@henighan_scaling_2020], audition [@droppo_scaling_2021], transfer from text to programming [@hernandez_scaling_2021], few-shot adaptation of vision models [@prato_scaling_2021], and more.\\n\\nPredictable scaling, and especially the underlying dependency on precise mixtures of data, model size, and training, has implications for the process of model development. It shifts development of this type of model from a process of artisanal trial-and-error to more of a predictable engineering process, where the resources needed to achieve a particular result can be precisely calculated, and the cost of those resources can be compared to the utility of the result. Although very specific behaviors may not be predictable (more on this in Section [2.2](#sec:1.2){reference-type=\"ref\" reference=\"sec:1.2\"}), the general test loss tends to correlate well on average with many tasks, meaning that larger models typically make significant gains across the board. In this sense, **scaling laws de-risk investments in large models**. We say more on this in Section [3.1](#sec:2.1){reference-type=\"ref\" reference=\"sec:2.1\"} and provide more technical details on how developers may use scaling laws in Appendix [7.2](#app:scaling_laws){reference-type=\"ref\" reference=\"app:scaling_laws\"}. To further illustrate how smooth general scaling correlates with task performance, and how a scale-based analysis can be used to forecast the potential economic value of a given model, we outline a small original experiment in Appendix [7.3](#app:recsys){reference-type=\"ref\" reference=\"app:recsys\"} that analyzes the relationship between scale and GPT-3 like language models [@askell_general_2021] to be used as recommendation systems with zero-shot learning. We chose this example because recommendation systems have tangible economic relevance, known societal impact, are well studied in machine learning with domain specific algorithms [@harper_movielens_2015], but are not typically studied with large scale generative models (yet). Surprisingly, we find that that generative models can increasingly operate as simple recommendation systems as they scale with minimal effort and extremely limited access to explicit training data. We leave a detailed analysis and discussion in Appendix [7.3](#app:recsys){reference-type=\"ref\" reference=\"app:recsys\"}.\\n\\n![[\\\\[fig:capability_emergence\\\\]]{#fig:capability_emergence label=\"fig:capability_emergence\"} Three examples of abrupt specific capability scaling described in Section [2.2](#sec:1.2){reference-type=\"ref\" reference=\"sec:1.2\"}, based on three different models: GPT-3 (blue), Gopher (orange), and a Google language model (green). **(Left)** 3-Digit addition with GPT-3 [@brown_language_2020]. **(Middle)** Language understanding with GPT-3 and Gopher [@rae_scaling_2021]. **(Right)** Program synthesis with Google language models [@austin_program_2021].](figures/abrupt_scaling.pdf){#fig:capability_emergence width=\"99%\"}\\n\\nAbrupt Specific Capability Scaling {#sec:1.2}\\n----------------------------------\\n\\nThough performance on a wide distribution of tasks may scale smoothly with model size, qualitatively different, specific capabilities can appear abruptly and discontinuously. It is not clear when or why this happens. But intuitively, abrupt scaling of a specific capability can co-exist with smooth general scaling for the same reason that daily weather is less predictable than seasonal averages: individual data points can vary much more than broad averages.\\n\\nHere, we illustrate three examples of abrupt capability scaling for arithmetic [@brown_language_2020], language understanding, [@hendrycks_measuring_2021; @rae_scaling_2021], and programming [@austin_program_2021] (Figure [2](#fig:capability_emergence){reference-type=\"ref\" reference=\"fig:capability_emergence\"}). For arithmetic, GPT-3 displays a sharp capability transition somewhere between $6$B parameters and $175$B parameters, depending on the operation and the number of digits [@brown_language_2020]. For example, three digit addition is performed accurately less than $1$% of the time on any model with less than $6$B parameters, but this jumps to $8$% accuracy on a $13$B parameter model and $80$% accuracy on a $175$B parameter model -- producing a \"hockey stick\"-style graph (Figure [2](#fig:capability_emergence){reference-type=\"ref\" reference=\"fig:capability_emergence\"}, Left) in which arithmetic ability appears suddenly after several orders of magnitude of nothing.\\n\\nA different language model, DeepMind\\'s Gopher [@rae_scaling_2021], also displays an abrupt jump in performance on a different dataset, the MMLU language understanding benchmark [@hendrycks_measuring_2021] (Figure [2](#fig:capability_emergence){reference-type=\"ref\" reference=\"fig:capability_emergence\"}, Middle, orange). For all models under $6$B parameters, Gopher performs under $30$% accuracy, which is a little better than chance ($25$% accuracy). However, the full $280$B parameter Gopher model achieves $60$% accuracy, a significant jump. GPT-3 displays a similar phenomenon though of smaller magnitude (Figure [2](#fig:capability_emergence){reference-type=\"ref\" reference=\"fig:capability_emergence\"}, Middle, blue).\\n\\nAs a third example, a recently developed class of program synthesis models from Google display dramatic improvements in their ability to create computer programs as they increase in size from $10$B to $100$B parameters [@austin_program_2021] (Figure [2](#fig:capability_emergence){reference-type=\"ref\" reference=\"fig:capability_emergence\"}, Right). For example, the percentage of generated synthetic programs that solve a given programming problem jumps substantially from $6$% to $13$% when the model size increases by $\\\\sim 2$x from $68$B to $138$B parameters, despite very small increases over the previous two orders of magnitude.\\n\\nAbrupt specific capability scaling presents significant challenges for safety assurance and deployment of large models. Although we\\'ve demonstrated this phenomenon for relatively anodyne capabilities, potentially harmful ones may emerge at scale (that will not exist in smaller models) and may be difficult to anticipate.\\n\\nOpen-Ended Inputs and Domains {#sec:1.3}\\n-----------------------------\\n\\nLarge generative models are open-ended --- they take in arbitrary inputs from a variety of domains and generate (often relevant and creative) outputs. As a result, some model behaviors may be unknown until they are solicited from specific inputs. Pre-trained generative models can also be fine-tuned on new data in order to solve new problems. Broadly enabling such fine-tuning substantially increases the breadth of model capabilities and associated difficulties in predicting or constraining model behaviors. This open-endedness is challenging because it means AI developers may deploy their systems without fully knowing potentially unexpected (and possibly harmful) behaviors in response to un-tested inputs.\\n\\nFor example, the AI Dungeon video game fine-tuned GPT-3 for fantasy role-playing[^10], but with the right inputs, players were able to manipulate it to discuss any topic, essentially providing general backdoor access to GPT-3.[^11] Thus, a model use-case that appeared to be designed just for one purpose, actually carried the full range of GPT-3 capabilities, accessible through skillful use of its open-ended interface.\\n\\nTo further illustrate our point about the inherent challenges of open-ended inputs and domains, and tie it to the possibility of harm from language models, we consider a problem domain that language models are typically not (or not yet) deployed on, but which is associated with societal concerns: recidivism prediction. Some have pointed out that even beyond specific concerns about fairness, recidivism prediction simply should not be a task for machine learning [@bao_its_2021]. We agree and we do not believe that language models should be used for recidivism prediction. However, because the application is so inherently questionable, it provides a compelling example of how harmful abilities can emerge quietly in unexpected ways as generative models scale. It is likely that such abrupt emergence also occurs in many other contexts where the harms are more subtle. We study a case where the problems are flagrant in order to clearly demonstrate our thesis.\\n\\nTo do this, we leverage the ProPublica COMPAS dataset, which includes data about more than $7,000$ defendants arrested in Broward County Florida [@angwin_machine_2016; @bao_its_2021]. The dataset includes a recidivism risk score, computed by the COMPAS algorithm (which is meant to reflect the risk of a defendant committing a misdemeanor or felony within 2 years of assessment based on a set of features about the defendant, not including race[^12]), along with the actual outcome of whether each defendant re-offended. ProPublica found that these risk scores are inaccurate and racially biased [@angwin_machine_2016]. Further research found that human subjects with limited to no criminal justice experience exhibit similar inaccuracies and racial biases as COMPAS when predicting recidivism based on a simple prompt describing a defendant [@dressel_accuracy_2018]. The human subject experiment examined two conditions, one in which a defendant\\'s race was excluded from the prompt, and one in which it was included.[^13] Here, we use the same prompts outlined in [@dressel_accuracy_2018] but instead ask language models [@askell_general_2021] instead of people to predict recidivism. We leave full technical details and (significant) caveats in Appendix [7.4](#app:compas){reference-type=\"ref\" reference=\"app:compas\"}; however, we foreground here that benchmark risk assessment instrument datasets like COMPAS often contain numerous measurement biases and errors which can make them ill-suited for making claims about real-world impact without carefully considering the the complicated socio-technical systems (in this case, the US criminal justice system) in which they are used [@bao_its_2021].\\n\\n![[\\\\[fig:compas\\\\]]{#fig:compas label=\"fig:compas\"} Large language models, with few-shot learning, exhibit similar (or worse) inaccuracies and racial biases as COMPAS for recidivism prediction when prompted with the same prompts from a human recidivism prediction experiment [@dressel_accuracy_2018]. This illustrates our claim in Section [2.3](#sec:1.3){reference-type=\"ref\" reference=\"sec:1.3\"} that it may be difficult to anticipate possible harms of large generative models due to the open-ended nature of their inputs and domains. **(Left)** Accuracy increases with model size, approaching COMPAS performance. We see no significant difference in predictive accuracy when race is excluded from the prompt (blue) or included in the prompt (orange). **(Right)** Language models become increasingly biased towards predicting Black, compared to white, people will re-offend (when in reality they do not) similarly to COMPAS. We find a higher false positive rate ratio when race is included in the prompt (orange) versus when it is excluded (blue). See Appendix [7.4](#app:compas){reference-type=\"ref\" reference=\"app:compas\"} for technical details and caveats.](figures/compas_lm_science.pdf){#fig:compas width=\"66%\"}\\n\\nWe found that language models exhibit similar (or worse) inaccuracies and racial biases as COMPAS. Figure [3](#fig:compas){reference-type=\"ref\" reference=\"fig:compas\"} shows language models of increasing size compared to COMPAS in terms of two metrics mentioned in the ProPublica analysis [@angwin_machine_2016] and the subsequent human subject experiment [@dressel_accuracy_2018]: overall predictive accuracy, and the ratio in false positive rates for Black versus white defendants. We show results for both prompts that exclude an individual\\'s race (blue) and include it (orange). For overall predictive accuracy, language models become increasingly accurate at predicting whether defendants will re-offend (Figure [3](#fig:compas){reference-type=\"ref\" reference=\"fig:compas\"}, Left) as they increase in size, yet they are still unreliable predictors like COMPAS. We see no significant difference in predictive accuracy when race is excluded from the prompt or included. In both conditions, the largest model, with \\xa0$52$B parameters, achieves $63$% accuracy compared to COMPASs $66$% accuracy.\\n\\nWe also see higher ratios in false positive rates for Black versus white defendants (Figure [3](#fig:compas){reference-type=\"ref\" reference=\"fig:compas\"}, Right), which partially recapitulates the racial biases of the COMPAS algorithm outlined described in [@angwin_machine_2016]. For COMPAS, this ratio is $1.92$, which indicates that Black defendants are predicted to re-offend nearly twice as often as white defendants, when in reality they did not (a fair algorithm would have a false positive rate ratio of $1$). As language models increase in size, at around $12$B parameters, the false positive rate ratio increases smoothly and reaches a value of $1.5$ for the largest model when race is excluded in the prompt and a value of $2.21$ when race is included in the prompt. In the latter case, the largest language model is even less equitable than COMPAS.[^14] Likely, the model is picking up on a combination of the racial bias in the small fraction of the COMPAS dataset it sees, and ambient racial bias in the pre-trained language models.\\n\\nTo emphasize again what was stated earlier, the point here is not only the emergence of racial biases in the recidivism prediction task, but also the emergence of the ability to perform this task at all. As the language model scales, it acquires both the ability to do a task that many have argued is inherently harmful [@bao_its_2021], and it performs this task in a biased manner. It is likely that large language models have many other (currently undiscovered) \\\\\"skills\\\\\" that pose one or both of these problems, perhaps in less obvious forms.\\n\\nIn summary, pre-trained language models can be adapted with minimal effort for purposes not anticipated by their creators, whether that\\'s by using the inherent capabilities of the model to evade a security constraint (as in the AI Dungeon example), or by discovering new capabilities through novel inputs (as in the discussion of abrupt capability jumps in Section [2.2](#sec:1.2){reference-type=\"ref\" reference=\"sec:1.2\"}, and the recidivism experiment above). We also note that many of the most surprising capabilities manifest at large-scale, so working with smaller models will make it harder to explore such capabilities.\\n\\n![[\\\\[fig:convo\\\\]]{#fig:convo label=\"fig:convo\"} A conversation with an AI Assistant [@askell_general_2021] powered by a $50$B parameter language model that illustrates challenges with Open-endedness outlined in Section [2.4](#sec:1.4){reference-type=\"ref\" reference=\"sec:1.4\"}](figures/offensive-example-societal-impacts-2.pdf){#fig:convo width=\"99%\"}\\n\\nOpen-Ended Outputs {#sec:1.4}\\n------------------\\n\\nIn the previous section we argued that language models have open-ended inputs, which creates the opportunity for unexpected and undetected capabilities to emerge. But even when the input or topic is fixed, the resulting output can be varied and unpredictable. This kind of unpredictability is arguably more familiar and widely studied than the previous kind, but is worth briefly discussing as it adds an additional layer of complexity to large model behavior.\\n\\nAs an example, in Figure [4](#fig:convo){reference-type=\"ref\" reference=\"fig:convo\"} we ask an AI assistant [@askell_general_2021] to tell us something offensive, for the purpose of illustrating our claim. Despite prompting the model with a relatively clear input, the model has generated an output that is tangential to the question at hand: the response isn\\'t directly offensive, but is instead a list of offenses made by other AI systems. One effect of this open-endedness is that unpredictable model responses can be a distraction away from a person\\'s original query.\\n\\nOpen-endedness also introduces a second and more harmful risk of factual inaccuracy. Taking a closer look at the exchange in Figure [4](#fig:convo){reference-type=\"ref\" reference=\"fig:convo\"}, we can see that the model has made up these offenses - systems like IBM Watson and Microsoft\\'s Tay [@wolf_why_2017] did have problems during their deployment, but the AI assistant gets the year and error wrong in the case of Watson, and the error wrong (but year right) in the case of Tay. When we ask the model if it is sure the examples are correct, the model gives misleading answers and questions the authority of the human asking it questions. This illustrates how even with a specific input (e.g, requesting the model say something offensive), AI models can give outputs that are not only distracting, but potentially misleading.\\n\\nOpen-ended model outputs can also introduce harmful or undesirable text. For example, Figure [5](#fig:toxicity){reference-type=\"ref\" reference=\"fig:toxicity\"} shows that the toxicity (defined as rude, disrespectful, or unreasonable language [@gehman_realtoxicityprompts_2020])[^15] of text generated from language models [@askell_general_2021] increases smoothly and significantly with model size. A recent study has observed a very similar toxicity trend with model size using similar models with different analyses [@rae_scaling_2021], which suggests that this may be a general phenomenon. We leave further details and caveats in Appendix [7.6](#app:toxicity){reference-type=\"ref\" reference=\"app:toxicity\"}.\\n\\nMany applications for language models, including chat bots, search engines, text summarization systems, question answer systems, machine translation systems, etc., rely on open-ended text generation. As such, we argue that it is important to quantify how societally relevant aspects of open-ended text generation --- relevancy, accuracy, safety, and even creative expression (see Appendix [7.5](#app:creative_expression){reference-type=\"ref\" reference=\"app:creative_expression\"} for a discussion on AI generated poetry) --- scale with model size. It will also be important to develop techniques that can improve the factual accuracy of the results of AI models, as described in e.g., [@borgeaud_improving_2021], and to make the outputs of models more appropriate and less likely to display harmful biases [@solaiman_process_2021].\\n\\n![The toxicity of model outputs increases smoothly with model size, which illustrates how though loss may reduce generally when scaling a model, other societally impactful potential harms of the model may also scale, as described in Section [2.4](#sec:1.4){reference-type=\"ref\" reference=\"sec:1.4\"}.](figures/toxicity_scaling_laws.pdf){#fig:toxicity width=\"33%\"}\\n\\nMotivations and Problems in the Development and Deployment of Large Models {#sec:2}\\n==========================================================================\\n\\nIn the previous section we described our basic thesis that large generative models have an unusual combination of four distinguishing features: predictable general performance, and unpredictable specific capabilities, inputs, and outputs. Predictable general performance, combined with impressive outputs (e.g, specific capabilities) drives rapid development of such models, while the unpredictability makes it difficult for model developers to anticipate the consequences of model deployment. There are numerous motivations (and barriers) for developing and deploying large generative models due to (or in spite of) these distinguishing features. Here, we focus on elements of this fundamental tension and ground our discussion with some empirical observations.\\n\\nMore specifically, in Section [3.1](#sec:2.1){reference-type=\"ref\" reference=\"sec:2.1\"} we outline three salient *motivations* for developing and deploying large generative models: economic, scientific, and prestige. Conversely, in Section [3.2](#sec:2.2){reference-type=\"ref\" reference=\"sec:2.2\"} we outline three *barriers to entry*: the financial costs and engineering talents required in order to scale models, AI safety issues, and the lack of standards and norms in model deployment. Finally, in Section [3.3](#sec:2.3){reference-type=\"ref\" reference=\"sec:2.3\"} we illustrate how combinations of these motivations and barriers may explain some empirical observations on how the development and deployment of language models has occurred thus far. In particular, we note that large language models are rapidly proliferating, that there is a rising gap between industry and academia for developing such models, and that there have been numerous documented examples of model deployments causing harm and controversy.\\n\\nMotivations for Developing and Deploying Large Models {#sec:2.1}\\n-----------------------------------------------------\\n\\n### Economic {#economic .unnumbered}\\n\\nPerhaps the simplest and most obvious motivation for model development is **economic**. Scaling laws mean that the cost to develop a model can be precisely estimated, and when an economically valuable output can be found to scale smoothly with the loss, then the returns to training a model can also be calculated. This applies both generally and specifically --- some institutions may wish to broadly improve the capabilities of a given model and will thus have an economic incentive to build them, while others may be targeting a specific model capability which is accompanied by a scaling law, and will therefore also have an incentive to build them. This has the effect of *de-risking* the training of large models: a predictable amount can be invested for a relatively predictable return, unlike many speculative research projects where an open-ended amount must be invested for an uncertain return. Predictability makes the logic of research investment more obvious and may help to justify it within large institutions (see Appendix [7.2](#app:scaling_laws){reference-type=\"ref\" reference=\"app:scaling_laws\"} for more examples). Thus, economic motivations, combined with continued smooth, general capability scaling, suggest that we should expect to see increasing model deployments. While it may not be possible to predict in advance precisely which search queries will benefit from a particular AI model and which won\\'t, or which applications will flourish and which will unpredictably fail, or which development workflows will be helped by code synthesis models, all of these applications take advantage of broad averages to tie economic returns to the smooth general capability scaling.\\n\\n### Scientific {#scientific .unnumbered}\\n\\nLarge generative models may be a necessary basis for broad swaths of novel interdisciplinary AI research on topics ranging from linguistics and robotics to philosophy and the social sciences [@bommasani_opportunities_2021]. Without the development of (or at least access to) large models, it will be challenging to research how they may advance progress in societally impactful research domains such as healthcare, education, and law [@bommasani_opportunities_2021]. Large models are also fertile testing grounds for developing next-generation algorithms and architectures --- novel algorithms can be rigorously evaluated according to whether they advantageously shift scaling laws to be more compute, data, or parameter efficient.\\n\\n### Prestige {#prestige .unnumbered}\\n\\nThe fact these models are on the frontier of possibility also creates a prestige incentive for developing them. Large models can be an advertisement for the capabilities of an institution -- a way to gain a perceived advantage in the public eye, to make it easier to recruit (coveted) skilled AI researchers, to increase sales of services unrelated to large models, or to support national initiatives or national pride. All of these motivations have the potential to create an unusual situation where there are strong incentives to develop, disclose, and even deploy large generative models despite high uncertainty about the full extent of what these models are capable of.\\n\\nBarriers to Entry in Developing and Deploying Large Models {#sec:2.2}\\n----------------------------------------------------------\\n\\n### Financial Costs and Engineering Talent {#financial-costs-and-engineering-talent .unnumbered}\\n\\nScaling up large generative models requires a significant financial investment. For example, GPT-3 was estimated to cost several million dollars to train.[^16] Scaling up large generative models also requires specific engineering competencies, e.g., distributed systems engineering, familiarity with cluster management tools like Kubernetes, low-level GPU programming, managing continuous integration testing, etc. The size of these models has led to longer development timelines and more complex workflows than previous systems over the past decade. For example, only $\\\\sim10$ years ago, one of the larger scale AI models at the time, AlexNet[^17] [@krizhevsky_imagenet_2012], was trained by a graduate student for a few thousand of dollars on a single desktop machine with $2$ GPUs.\\n\\n### Safety {#safety .unnumbered}\\n\\nAs described in Section [2](#sec:1){reference-type=\"ref\" reference=\"sec:1\"}, open-endedness combined with smooth, general capability scaling and the abrupt scaling of specific capabilities, is likely to lead to safety issues [@weidinger_ethical_2021; @bommasani_opportunities_2021] that are found after a model has been developed and deployed. Additionally, these models also possess known (pre-deployment) safety issues for which we lack robust solutions [@hendrycks_unsolved_2021] (e.g, How do you ensure the system does not generate inappropriate and harmful outputs, such as making overtly sexist or racist comments [@solaiman_process_2021]? How do you identify bias issues in the system prior to deployment [@blodgett_language_2020; @prabhumoye_few-shot_2021]? How do you ensure that when the model outputs a claim, it isn\\'t making up facts [@borgeaud_improving_2021]?, etc.).\\n\\n### Lack of Standards and Norms {#lack-of-standards-and-norms .unnumbered}\\n\\nBecause these large generative models have been developed very recently (within the last five years), and have only recently become valuable to deploy from an economic perspective, no standards for the safe deployment of these systems exist. This lack of standards compounds the problems caused by the four distinguishing features of generative models we identify in Section [2](#sec:1){reference-type=\"ref\" reference=\"sec:1\"}, as well as the safety issues discussed above. At the same time, there\\'s a growing field of research oriented around identifying the weaknesses of these models, as well as potential problems with their associated development practices [@bender_dangers_2021; @tamkin_understanding_2021; @bommasani_opportunities_2021; @dinan_anticipating_2021; @weidinger_ethical_2021; @kenton_alignment_2021; @patterson_carbon_2021; @schwartz_green_2020; @strubell_energy_2019]. However, this research is not yet embodied in the form of repeatable standards that developers can adopt, though there are some critical and important steps in this direction (e.g., through the use of model cards [@mitchell_model_2019] and data sheets [@gebru_datasheets_2021] to document the capabilities, drawbacks, and other salient details of models). This lack of standards makes it both more challenging to deploy systems, as developers may need to determine their own policies for deployment, and it also makes deployments inherently risky, as there\\'s less shared knowledge about what \\'safe\\' deployments look like. We are, in a sense, building the plane as it is taking off.\\n\\n![[\\\\[fig:timeline\\\\]]{#fig:timeline label=\"fig:timeline\"} Timeline of public disclosures of GPT-3 scale dense language models.](figures/timeline.pdf){#fig:timeline width=\"90%\"}\\n\\nEmpirical Observations {#sec:2.3}\\n----------------------\\n\\nThe above sections described some motivations and challenges that we expect AI developers to face with respect to large models. In this section we assess how those issues may explain three inter-related empirical observations: (1) large language models are rapidly proliferating (2) industry has become responsible for a larger share of resource-intensive model development compared to academia, and (3) large model deployment has already caused harm and controversy.\\n\\n### Large Language Models Are Rapidly Proliferating {#large-language-models-are-rapidly-proliferating .unnumbered}\\n\\nFigure [6](#fig:timeline){reference-type=\"ref\" reference=\"fig:timeline\"} shows a timeline of public disclosures of GPT-3 scale ($100$B - $530$B) dense language models, since GPT-3.[^18] About one year after GPT-3 was announced, a spike in similar model announcements followed. These models were developed by both large and small private organizations from around the world: Jurassic-1-Jumbo [@lieber_jurassic-1_2021], AI21 Labs, Israel; Ernie $3.0$ Titan [@wang_ernie_2021], Baidu, China; Gopher [@rae_scaling_2021], DeepMind, USA/UK; FLAN [@wei_finetuned_2021] & LaMDA [@thoppilan_lamda_2022], Google, USA; Pan Gu [@zeng_pangu-alpha_2021] Huawei, China; Yuan $1.0$ [@wu_yuan_2021], Inspur, China; Megatron Turing NLG [@smith_using_2022], Microsoft & NVIDIA, USA; and HyperClova [@kim_what_2021], Naver, Korea. This suggests that the economic incentives to build such models, and the prestige incentives to announce them, are quite strong.\\n\\n### Rising Gap Between Industry and Academia {#rising-gap-between-industry-and-academia .unnumbered}\\n\\nAt the time of writing, the largest language models that are free and publicly available are BigScience T0 ($11$B) [@sanh_multitask_2021], and Eleuther AI\\'s GPT-J ($6$B) [@wang_gpt-j-6b_2021] and GPT-NeoX ($20$B) [@leahy_announcing_2022], which are one to two orders of magnitude smaller than those developed by industry. Although academics can easily access (at least some of) the larger models, it is typically only possible to do so through a (potentially expensive) company-controlled API. This is part of a broader and longer-running trend towards high-compute research migrating from academia to industry that can be quantified (See Appendix [7.7](#app:ai_compute){reference-type=\"ref\" reference=\"app:ai_compute\"} for details ). Figure [7](#fig:ai_compute){reference-type=\"ref\" reference=\"fig:ai_compute\"} (Left) shows that in recent years the compute required for large-scale AI experiments has increased by more than $300,000$X relative to a decade ago.[^19] Along with this rise in resource intensity, we see a corresponding (and sharp) fall in the proportion of these results that come from academia (Figure [7](#fig:ai_compute){reference-type=\"ref\" reference=\"fig:ai_compute\"}, Right). This suggests that, although academics may be strongly motivated by scientific curiosity, and well-poised to research safety issues, they may be significantly challenged by the high financial and engineering costs.\\n\\n### Harm and Controversy {#harm-and-controversy .unnumbered}\\n\\nThere have been numerous examples of harm caused by the deployment of large generative models. For example, the AI system Tay was deployed before it was properly scrutinized, and generated hateful language [@wolf_why_2017]. It has also been shown that language models can memorize training data (which in turn can include privately identifiable information) [@carlini_extracting_2021; @perez_red_2022] and aid in disinformation campaigns [@buchanan_truth_2021]. Furthermore, people critical of organizations deploying such models have been directly harmed for voicing their concerns, sometimes to much controversy.[^20] Legislators are actively grappling with these issues. For example, the European Commission\\'s proposed AI legislation seeks to create standards for how \\'high risk\\'AI systems are deployed and monitored.[^21] This suggests that standards and norms for responsible model development and deployment are both significantly needed and lacking.\\n\\n![[\\\\[fig:ai_compute\\\\]]{#fig:ai_compute label=\"fig:ai_compute\"} **(Left)** The amount of compute required by major AI projects over time is increasing exponentially for both academic (blue) and industrial (orange) projects. **(Right)** The proportion of computationally-intensive AI results from academia is steadily decreasing. (The blue curve represents a Lowess fit to the data.)](figures/ai_compute.pdf){#fig:ai_compute width=\"66%\"}\\n\\nInterventions to Encourage Beneficial Deployments {#sec:3}\\n=================================================\\n\\nBased on the distinguishing features of large generative models that we outline in Section [2](#sec:1){reference-type=\"ref\" reference=\"sec:1\"}, and the various motivations for model development and deployment that we discuss in Section [3](#sec:2){reference-type=\"ref\" reference=\"sec:2\"}, we believe that large generative models will increasingly be developed and deployed despite their potential for harm. Here, we outline possible technical and policy interventions (along with corresponding implementation paths) that can increase the chance of these models being developed and deployed in positive ways.\\n\\n### Reduce compute asymmetries between the private sector and academia {#reduce-compute-asymmetries-between-the-private-sector-and-academia .unnumbered}\\n\\nAs shown in section [3.3](#sec:2.3){reference-type=\"ref\" reference=\"sec:2.3\"}, private sector organizations are the primary developers and deployers of large generative models. This means that other actors, such as academic and government ones, are less well-placed to understand the distinguishing technical features of these models, and are therefore less equipped to research the problems inherent to them. As outlined in Section [3.2](#sec:2.2){reference-type=\"ref\" reference=\"sec:2.2\"}, the main constraints here are the financial and engineering resources for model training - therefore, we should create experimental infrastructure[^22] to make it easier for a larger scientific community to analyze these models. To support and effectively utilize such infrastructure, academic and government organizations will also need to find ways to make the necessary financial and structural investments to be able to hire and retain technical talent that may otherwise go to industry. This is important because academic and public sector motivations may stem more from the pursuit of knowledge rather than profit, and can draw on more varied expertise than the private sector for analyzing and exploring large generative models.[^23] Although large models are resource-intensive, they are actually much less expensive than academic \\'Big Science\\' projects in some other fields. For instance, the Large Hadron Collider cost \\\\$$5$ billion to build[^24], the International Thermonuclear Experiment Reactor is projected to cost between \\\\$$10$ and \\\\$$15$ billion[^25], the Square Kilometre Array is projected to cost around \\\\$$1$ billion[^26], and the Long-Baseline Neutrino Facility and Deep Underground Neutrino Experiment are anticipated to cost \\\\$$2.4$ billion[^27]. By comparison, training frontier generative models like GPT-3 and others costs on the order of a million to ten million dollars, so the infrastructure to develop models substantially larger than the current frontier would have precedent in academia.\\n\\n**Implementation Path:** Countries may wish to develop and deploy so-called \\'National Research Clouds\\' that facilitate access to a heavily subsidized and/or free compute resource for academic researchers. An existing example here includes Compute Canada[^28]. There are also future initiatives being considered, such as the infrastructure being analyzed by the US government\\'s National AI Research Resource taskforce[^29], and the \\'Big Science\\' project which is leveraging a supercomputer (partially subsidized by the French government) to train large generative models. Recent work from Stanford also explores this implementation path in more detail [@ho_building_2021].\\n\\n### Improve knowledge about how to \\'red team\\' models {#improve-knowledge-about-how-to-red-team-models .unnumbered}\\n\\nAs some of the challenges from these models stem from their open-ended nature, we should develop ways to more effectively explore the input and output space of their models, so as to discover harms prior to deployment. We can model this on the \\'red team\\' approach which is popular in the computer security industry and can be applied in an AI context [@avin_filling_2021; @brundage_toward_2020]. This should take the form of both static benchmarks (for example, adversarial datasets to probe for weaknesses in computer vision systems [@hendrycks_natural_2021]), as well as continuous evaluation by humans carrying out multi-step interactions (e.g, conversations [@askell_general_2021; @xu_bot-adversarial_2021]) with these models, as well as plans for how to update the models in response to what these evaluations find.\\n\\n**Implementation Path:** Model developers should invest in internal red teaming approaches for their models and seek to publish on the techniques, datasets, and policy choices they make when red teaming. This will facilitate more shared awareness about how to red team models. There may also be a commercial market that can be developed for \\'red teaming as a service\\', though more community research into the area may be a prerequisite for this. AI developers may also wish to create \\'bug bounty\\'initiatives, where they give out prizes to people who can demonstrate repeatable ways of breaking a given AI system [@kenway_bug_2022]. Finally, we should consider how to augment (or complement) manual red-teaming with automated methods [@perez_red_2022].\\n\\n### Explore and prototype novel governance structures and government interventions {#explore-and-prototype-novel-governance-structures-and-government-interventions .unnumbered}\\n\\nIf the capabilities and resource-intensiveness of models scale further, then it may be prudent to explore governance structures that alter the incentives of private sector actors with regard to development and deployment. To do this, there will be a combination of soft regulation (e.g, the creation of voluntary best practices by industry, academia, civil society, and government), and hard regulation (e.g, transferring these best practices into standards and legislation.). Governments should also explore regulatory approaches that can increase the chance of actors developing and deploying beneficial systems.\\n\\n**Implementation Path:** AI development organizations should experiment with novel governance and oversight structures that let a broader set of stakeholders factor into model deployment decisions. This could take the form of oversight functions which can critique and publicly censure organizations should the organization diverge from the recommendations of the oversight body, to novel forms of governance that give diverse stakeholders power over an organization (for example, a private company could elect board members who represent the interests of civil society and/or academia rather than a pure profit-driven motive). AI development organizations should also work among themselves to develop best practices for the development and deployment of AI systems, then seek to get feedback on these from a broader range of stakeholders, potentially via the creation of third-party organizations for the purposes of standard formation. Along with innovations in governance of AI organizations, and work on best practices, we also believe governments should invest in better methods to assure the benefits of systems being deployed - specifically, governments should support efforts to measure and monitor the capabilities (both harmful and beneficial) of deployed AI systems [@whittlestone_why_2021], and should support the creation of an ecosystem oriented around auditing AI models and AI development processes [@mohamed_decolonial_2020; @raji_actionable_2019; @raji_closing_2020].\\n\\n### Improve the tools available for model evaluation {#improve-the-tools-available-for-model-evaluation .unnumbered}\\n\\nGiven the open-ended nature of these models, researchers would benefit from having more tools available to help them evaluate these models. If we can find ways to create more open source tools and frameworks in this area, then we can benefit the broader model development ecosystem. Particularly valuable would be tools for doing a very broad set of evaluations, or evaluations that search (e.g. across prompts) for new capabilities, rather than just fixed evaluation datasets that measure known capabilities.\\n\\n**Implementation Path:** Research funding organizations should allocate funds to researchers that are building the evaluation systems (e.g, tests, test datasets, and benchmarks) that model developers can then use to better understand the capabilities of their systems. Private sector and independent research organizations should invest further into developing tools to help researchers understand and evaluate large generative models - existing examples include Eleuther\\'s \\'Language Model Evaluation Harness\\' [@gao_framework_2021], the BIG-bench benchmark[^30], HuggingFace\\'s \\'BERTology\\' tooling[^31], and more.\\n\\n### Improve our understanding of abrupt jumps in capabilities {#improve-our-understanding-of-abrupt-jumps-in-capabilities .unnumbered}\\n\\nIn Section [2.2](#sec:1.2){reference-type=\"ref\" reference=\"sec:1.2\"} we gave a few examples of abrupt jumps in capabilities (abrupt capability scaling). Anecdotally, our experience has been that abrupt jumps occur in only a minority of tasks, but at the same time are not especially rare. How often do they occur, is there a pattern to the kind of tasks on which they occur, why do they occur, and are there any leading indicators that predict when they are about to occur? Answering these questions could help to address some of the most surprising behavior in large models, and might be especially important for future AI safety issues.\\n\\n**Implementation Path:** A systematic empirical study of abrupt jumps, across research and possibly commercial tasks for large models, could help to shed light on how common they are and when they occur. One route to studying this could be through interpretability research (e.g., [@clark_what_2019]), and specifically a new approach known as mechanistic interpretability [@elhage_mathematical_2021] - attempting to reverse engineer the computations performed by transformers (which underpin many of the generative models discussed in this paper) gives researchers a way to better understand how models behave.\\n\\nConclusion\\n==========\\n\\nIn this paper, we have articulated (and provided evidence for) our basic thesis that large generative models have an unusual combination of high predictability - model capabilities scale in relation to resources expended on training - and high unpredictability - before training a model, it\\'s difficult to anticipate all the inputs it will be subjected to, and what capabilities and outputs it will have. The former drives rapid development of such models while the latter makes it difficult to anticipate the consequences of their development and deployment. We\\'ve also described how these traits combine to alter the landscape of AI development, making it more likely a greater number of actors will build these models. Put bluntly: the status quo outlined here suggests that the next few years will see a proliferation of actors building ever-larger models, and these actors will have strong motivations to deploy these models, despite their potential for (possibly unpredictable) harmful societal impact. Various interventions (including the ones we outline in our paper) can change this dynamic, but it is nevertheless the current situation we must start from and continue to improve.\\n\\nAcknowledgements\\n================\\n\\nWe thank Sam Bowman, Miles Brundage, Timnit Gebru, Gillian Hadfield, Percy Liang, Luke Muehlhauser, Helen Ngo, Michael Sellitto, Alex Tamkin, Helen Toner, and Sharon Zhou for detailed feedback on drafts of the paper.\\n\\nAppendix {#app}\\n========\\n\\nAuthor Contribution Statement {#app:author_contributions}\\n-----------------------------\\n\\nJack Clark, Deep Ganguli, and Dario Amodei wrote the paper, with helpful comments from everyone at Anthropic. Jack Clark conceptualized the first drafts of the paper, and constructed the main arguments in sections 2, and 3 on the societal implications, incentives, and beneficial interventions surrounding scaling laws and provided detailed guidance on the experiments in Section 1. Deep Ganguli performed the technical experiments and analyses in Section 1 (the COMPAS analysis, toxicity analysis, recommendation system analysis, etc.), created the figures, and helped frame the main arguments in the paper. Dario Amodei gave detailed feedback throughout the project and provided guidance on the overall framing of the paper and experiments. Christopher Olah gave initial feedback on early drafts of the paper and contributed numerous insights relating to how capabilities can emerge abruptly at different scales. Liane Lovitt suggested ways to frame the paper to better communicate insights to policymakers. Danny Hernandez carried out analysis of compute usage of academia versus industry. Dawn Drain provided an analysis of how AI developers may use scaling laws. Jared Kaplan helped with initial conceptualization of the project, wrote the infrastructure used to carry out the experiments, advised Deep Ganguli throughout the project, and made comments on the paper. Neel Nanda, Liane Lovitt, Danny Hernandez, Zac Hatfield-Dodds, and Daniela Amodei made extensive comments to the paper. Amanda Askell provided feedback on the COMPAS experiment, and the broader arguments being made in the paper. Led by Tom Brown in collaboration with Sam McCandlish, much of the technical staff at Anthropic contributed to efficient distributed model training and sampling, the underlying ML, and cluster stability. Contributors include Nicholas Joseph, Tom Henighan, and Andy Jones. Nelson Elhage, Kamal Ndousse, Zac Hatfield-Dodds. Ben Mann also contributed to this infrastructure and built the human feedback interface. Jackson Kernion managed the crowd workers and maintained the infrastructure. Sam McCandlish led model pretraining efforts, often in collaboration with Jared Kaplan. Tom Henighan managed our research cluster, helped build our distributed training system. He also helped with ML research on large language models. Nova DasSarma has also helped manage the cluster. Andy Jones was central in building our sampling infrastructure. He also provided engineering support to Deep Ganguli for all experiments.\\n\\nHow Developers Use Scaling Laws {#app:scaling_laws}\\n-------------------------------\\n\\nDevelopers may use scaling laws in a variety of ways, some of which we outline here.\\n\\n1.  To empirically estimate the compute-efficient frontier --- the lowest possible test loss one can achieve within a fixed compute budget. This can help developers forecast the theoretical costs of training large models and allocate resources accordingly.\\n\\n2.  To infer whether simple increases in scale may have the potential to unlock capabilities that don\\'t work at smaller scale. This helps developers forecast progress in AI and to tackle more ambitious problems.\\n\\n3.  To quantitatively test whether enhancements other than scaling (e.g. hyper-parameter tuning, novel architecture design, etc.) actually matter as models increase in scale. If these non-scale based changes do not give improvements at scale, then developers can allocate developer time to pursuing scale relative to other alternatives.\\n\\n4.  To debug model training. If a bigger model is not doing better than a smaller model, then developers know to prioritize looking for possible bugs inherent only to models of sufficient scale. Some commonly encountered bugs that become increasingly pernicious with scale involve numerical precision issues, data quality issues, over-fitting issues, and hardware related issues.\\n\\n5.  To evaluate the performance of models on a common scale. Often, different researchers publish results for models of different sizes. A researcher can use scaling laws to infer how much of the differences in model accuracy are merely due to scale, and also how differently sized models compare to one\\'s own models after accounting for scale. For instance, an improved approach might be comparable to a $10$% model size increase. Knowing this information gives two separate options for pursuing such a model improvement.\\n\\nRecommendation System Experiment {#app:recsys}\\n--------------------------------\\n\\nTo illustrate how smooth general capability scaling (discussed in Section [2.1](#sec:1.1){reference-type=\"ref\" reference=\"sec:1.1\"}) may correlate with task performance and forecast economic value, we perform a small original experiment where we analyze the relationship between scale and capabilities for GPT-3-like language models [@askell_general_2021] to be used as recommendation systems with zero-shot learning. We choose a recommendation system example because these systems have tangible economic relevance and societal impact.\\n\\nFigure [8](#fig:movies){reference-type=\"ref\" reference=\"fig:movies\"} shows that language models smoothly decrease in the standard Root Mean Square Error (RMSE, lower is better) metric on the widely used Movielens 1M movie recommendation system task [@harper_movielens_2015] as they increase in size. The smallest model achieves a significantly better RMSE ($1.06$) than chance (RMSE $1.91$), and the largest model achieves a significantly lower RMSE ($0.94$) than a strong baseline model (RMSE $0.98$, see below for further details). Although no models achieve state of the art (SOTA) performance (RMSE $0.82$), these results are still surprising because the language models (in our zero-shot setting) see two orders of magnitude less training data than the SOTA model.\\n\\nTrends like those in Figure [8](#fig:movies){reference-type=\"ref\" reference=\"fig:movies\"} forecast how much it would likely cost to develop a state-of-the-art capability on an economically valuable task. In this particular case, we get an incredulous result - at 800T parameters, a language model will achieve state of the art performance with zero-shot learning. This number indicates that it\\'s unlikely language models will be used as commercially deployed recommendation systems in this manner for several years (and that even then it might not be worth its costs).[^32] But the results of a different experiment (e.g. a fine-tuned language model trained explicitly to solve this task), could have justified expenditure rather than advising against it. As such, scaling laws can de-risk investment *without saying anything about the detailed behavior of the model in specific* cases.\\n\\nMore specific technical details are as follows. To perform this experiment, we chose the Movielens 1M (1 million ratings) dataset [@harper_movielens_2015] both because of its widespread use, the fact that it contains demographic information about users (age, occupation, gender, zip code), and because we have observed language models to have considerable knowledge about movies (presumably due to a preponderance of text on the internet about movies).\\n\\nThe dataset consists of $\\\\sim4$K movies rated by $\\\\sim6$K users on a scale of $1$-$5$. On average, each user has rated $\\\\sim~160$ movies, which means $96$% of the data are missing. The goal of a recommendation system is to predict these missing values, which anticipate how a user will rate a movie they have not previously rated before.\\n\\nTo evaluate performance on this task, we take the standard approach of partitioning the data into a train and test set, using $1$% of the total dataset ($10$K ratings) as our test set. Performance on this task is typically reported as the root mean squared error (RMSE) between the predicted and actual ratings on the test set. Perfect predictions would yield an RMSE of $0$ and random guessing corresponds to an RMSE of $1.91$. A strong baseline model simply assigns the average rating (averaged across all users) in the train set as the predicted ratings for all movies in the test set. This essentially ranks movies by their overall popularity, independent of any personalization. The strong baseline achieves an RMSE of $0.98$. State of the art performance on this dataset, is currently an RMSE of $0.822$ according to [@han_glocal-k_2021].[^33]\\n\\n![Language models can perform as zero-shot recommendation systems with increasing scale. This demonstrates how general capability scaling can correlate with an economically valuable task as described in Section [2.1](#sec:1.1){reference-type=\"ref\" reference=\"sec:1.1\"}.](figures/movielens_scaling_laws.pdf){#fig:movies width=\"33%\"}\\n\\nIn general, state of the art algorithms rely on matrix completion (also known as matrix factorization) algorithms, which simply try to impute the missing values in the user-by-movie matrix by expressing this matrix as the outer product of a small number of low dimensional latent vectors, which are learned from the training data, in order to explicitly minimize the RMSE between predicted and actual ratings. Algorithms with lower RMSEs are typically parameterized by neural network models.\\n\\nIt\\'s unclear how to use language models as matrix factorizers. Instead, we employ similar zero-shot learning approach with the following prompt:\\n\\nWe use zero-shot learning because the variable, `list_of_movies_and_ratings_from_training_set`, can often correspond to a very long sequence of text, since on average users have rated $\\\\sim200$ movies. Because our models have a fundamental limit on how large input text sequences can be, as determined by the context window length, we randomly sample up to 500 previously rated movies per user, in order to stay within the limits. An alternative strategy is to use few-shot learning, where multiple users are provided as examples in the prompt, but only show a small number, e.g., $5$, of previously rated movies per user. Empirically, we found that our zero-shot approach (one user, but a lot of previously rated movies by that user) led to far lower RMSE than all of our few-shot learning experiments.\\n\\nThere is a movie recommendation task for language models in the BigBench framework based on a Movielens benchmark.[^34] However, this task is formulated specifically for a language model, in the form of a multiple choice task as opposed to a regression or matrix completion task. Our formulation of this task allows us to directly compare general purpose language models to special purpose systems on the same axis in order to assess a more realistic capability. Finally, we note that we can extend the analysis we do here to other economically valuable real-world tasks such as those in the recent Real-World Few-Shot Text-Classification (RAFT) benchmark [@alex_raft_2021].\\n\\nCOMPAS Experiment {#app:compas}\\n-----------------\\n\\nWe use the same dataset, filtering operations, and metrics used for the ProPublica analysis of COMPAS [@angwin_machine_2016] (taken directly from the Ipython Notebook on GitHub).[^35] We use the same prompt as [@dressel_accuracy_2018] (which was designed for a human recidivism prediction experiment) for the language model:\\n\\nNext, given the prompt above, we compute the probability that the next token in the prompt is a ` Yes` and a ` No`. We normalize these two probabilities to sum to 1. We then directly compare the probability of a ` Yes` response to the ground-truth label as to whether or not the defendant in question actually re-offended, in addition to the analogous prediction provided by COMPAS. We use the Fairlearn Python package[^36] to compute all metrics reported in the main text.\\n\\nIn addition to the metrics reported in the main text, we also examined the predictive accuracy ratio for Black versus white defendants as in [@angwin_machine_2016; @dressel_accuracy_2018]. We saw no clear trends with model size (Figure [9](#fig:predictive_accuracy){reference-type=\"ref\" reference=\"fig:predictive_accuracy\"}) regardless of whether race was excluded from the prompt (blue) or included (orange). Though the largest language models are slightly less fair than COMPAS according to this metric.\\n\\n![Predictive accuracy ratio for Black versus white defendants. A value of $1$ is fair. COMPAS achieves a value of $0.97$. The language models show no clear trend in this ratio, regardless of when race is excluded in the prompt (blue) or included (orange). However, the largest language models are slightly less equitable than COMPAS according to this metric.](figures/compas_lm_science_predictive_accuracy.pdf){#fig:predictive_accuracy width=\"33%\"}\\n\\nOur analysis suffers from several important caveats. First, it is well known that there are many more fairness metrics than the two we consider here, and that it\\'s statistically impossible for a single algorithm to achieve parity on all these metrics e.g., [@friedler_impossibility_2016]. Second, benchmark risk assessment instrument datasets often contain numerous measurement biases and errors which can make them ill-suited for making claims about real-world impact without carefully considering the the complicated socio-technical systems (in this case, the US criminal justice system) in which they are used [@bao_its_2021]. Finally, comparisons to proprietary algorithms will always be difficult to make precise without either significant reverse engineering or pressure from companies to lead to more transparent algorithms [@rudin_age_2020].\\n\\nOpen Ended Outputs and Creative Expression {#app:creative_expression}\\n------------------------------------------\\n\\nCapabilities may emerge in areas that are challenging to evaluate quantitatively, and therefore likely to resist systematic analysis. A key example is the case of AI models mimicking human creative expression. As a concrete example, we provide[^37] a sample of over three thousand imitation poems generated randomly from a large language model (more accurately, these are samples generated from a prompt including several modern and contemporary poems, so a small fraction of the samples are not actually poems). We cannot provide any official evaluation, but informally we find both the quality of some of the texts, and the imitation of specific authorial styles quite impressive. Some professional writers who are aware of the growing capabilities of large language models are very impressed[^38], but also alarmed by their far-reaching implications. Academics outside of engineering departments are also starting to consider the pros and cons of machine creativity.[^39]\\n\\nToxicity Experiment Details {#app:toxicity}\\n---------------------------\\n\\nWe follow a similar analysis outlined in [@askell_general_2021] where we leverage the RealToxicityPrompts [@gehman_realtoxicityprompts_2020] dataset to elicit short comments in response to real world samples of text (prompts) obtained from the internet. Following [@gehman_realtoxicityprompts_2020], we label the prompts as \\'toxic\\'if they have a toxicity score $>0.5$, otherwise we label them \\'non-toxic\\'. We then obtain a random sample of $1$K of these prompts, with an equal proportion of \\'toxic\\'\\' and \\'non-toxic\\'prompts. Next, we we sample $25$ model responses from language models of various sizes [@askell_general_2021] per prompt. We use the same prompts per language model.\\n\\nWe then measure the toxicity of the model responses with an open-source toxicity detector [@hanu_detoxify_2020] that outputs a score, between $0$ and $1$, with a higher score corresponding to more toxic content. Next, we fit a linear regression model, where we predict the toxicity score based on a categorical coding of model size, and a binary indicator as to whether the prompt was labeled as toxic or non-toxic. We plot the estimated coefficients on model size (thus controlling for the toxicity of the prompt) and the $95$% confidence intervals around them in the main text.\\n\\nOur analysis is subject to several caveats. First, it\\'s unclear how the magnitude of the effect size in Figure [5](#fig:toxicity){reference-type=\"ref\" reference=\"fig:toxicity\"} influences human perception of the toxicity of the generated text. Different people often have different perceptions about text with the same toxicity score [@welbl_challenges_2021]. Second, automated toxicity detection algorithms are known to suffer from several limitations, for example, they can be biased for certain minority groups [@gehman_realtoxicityprompts_2020; @welbl_challenges_2021]. Finally, our reliance on an open-source toxicity detector [@hanu_detoxify_2020] is counter to the more common use of the Perspective API for toxicity detection (though we believe these toxicity detectors to be similar[@askell_general_2021]).\\n\\nAI and Compute Analysis Details {#app:ai_compute}\\n-------------------------------\\n\\nWe leverage data from existing work on estimating compute usage for training large-scale AI models[^40] which was recently complemented with additional data from more recent experiments [@sevilla_parameter_2021]. In this augmented dataset, we label training runs as Industry or Academic based primarily on affiliations of first authors. If a first author had a dual affiliation, we labeled the run as industry, because in practice we\\'ve found that with access to both, industry-controlled compute is the preferred path. The fit it in Figure [7](#fig:ai_compute){reference-type=\"ref\" reference=\"fig:ai_compute\"} (Right) is based on a LOWESS regression with default parameters from the Seaborn Python package. These data are incomplete and should be interpreted carefully due to sampling bias. For example, we do not have access to compute estimates for industrial models used in production for search, recommendation engines, or self driving cars.\\n\\n[^1]: Core Research Contributors\\n\\n[^2]: Core Infrastructure Contributors\\n\\n[^3]: Correspondence to: jack\\\\@anthropic.com Author contributions are listed in Appendix [\\\\[app:author_contributions\\\\]](#app:author_contributions){reference-type=\"ref\" reference=\"app:author_contributions\"}\\n\\n[^4]: Some refer to this class of models as \\'foundation models\\'[@bommasani_opportunities_2021].\\n\\n[^5]: We do not discuss to whom this economic value accrues, and we do not intend to imply that by default it will accrue broadly or that no one will be harmed.\\n\\n[^6]: Note that, as will be discussed later as the central thesis of the paper, smooth general capability scaling does not imply smooth scaling on any particular task. It also does not imply that the tasks typically measured are the only tasks that are important; indeed the presence of unmeasured tasks is part of our thesis.\\n\\n[^7]: Similar behavior has also been observed during the training process of an individual model (rather than as a function of model size) for algorithmic tasks, and has been termed \"grokking\" [@power_grokking_2022].\\n\\n[^8]: For example, [@clary_lets_2019] documents strong run-to-run irreproducibility in reinforcement learning on Atari games when only changing the initial random seed. This suggests that differences between algorithms may be difficult to measure rigorously due to such intrinsic noise.\\n\\n[^9]: More precisely, the relationship is a straight line on a log-log plot, equivalent to a power law.\\n\\n[^10]: https://aidungeon.medium.com/ai-dungeon-dragon-model-upgrade-7e8ea579abfe\\n\\n[^11]: https://twitter.com/nickwalton00/status/1289946861478936577\\n\\n[^12]: More precisely, the COMPAS algorithm makes its predictions from $137$ features about a defendant and the defendant\\'s past criminal record. COMPAS does not consider the defendant\\'s race; however, other features it does consider may be correlated with race and thus lead to racially disparate predictions.\\n\\n[^13]: Interestingly, the researchers found that the exclusion of race had no significant impact on human recidivism prediction accuracy or fairness [@dressel_accuracy_2018].\\n\\n[^14]: Although the false positive rate ratio of the largest language model where race is included in the prompt is $2.21$ vs. $1.92$ for COMPAS, in absolute terms the false positive rates for the language model ($30$% for Black, $12.6$% for white) are lower than the false positive rates for COMPAS ($45$% for Black, $24$% for white)\\n\\n[^15]: https://github.com/conversationai/perspectiveapi\\n\\n[^16]: https://lambdalabs.com/blog/demystifying-gpt-3/\\n\\n[^17]: Though not a generative model, AlexNet was, at the time, a frontier model in terms of computational consumption, hence why we include it as a comparison.\\n\\n[^18]: The timeline does not include sparse or mixture of experts models (e.g., GLaM [@du_glam_2021]), which often achieve comparable performance with similar or slightly lower compute, but are difficult to characterize in terms of a single model size. It also does not include models trained on different modalities, such as code [@austin_program_2021; @chen_evaluating_2021], or multi-modal models such as [@radford_learning_2021].\\n\\n[^19]: Some people have noted that this trend may not be sustainable [@lohn_ai_2022]\\n\\n[^20]: https://www.wired.com/story/google-timnit-gebru-ai-what-really-happened/\\n\\n[^21]: https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence\\n\\n[^22]: We do not distinguish between public or private (cloud) infrastructure. Some have raised concerns regarding how specific choices here may centralize power in different ways [@ai_now_institute_democratize_2021]. Governments will need to examine how usable these different infrastructures are, and the long-term ramifications of empowering particular infrastructure providers.\\n\\n[^23]: It is worth noting that by increasing the amount of actors with access to non-trivial compute, it\\'s possible to increase some risks with regard to safe development and deployment of models, especially those that stem from a need to coordinate among different developers. However, this risk likely does not add significantly to the existing risk landscape, given that economic incentives for model development are leading to a proliferation of model developers in industry --- academics have much less of an incentive to commercially deploy their models. On balance, therefore, it seems helpful to give academia more resources to help it serve as a counter-weight to industry.\\n\\n[^24]: https://www.forbes.com/sites/alexknapp/2012/07/05/how-much-does-it-cost-to-find-a-higgs-boson/?sh=cf2196e39480\\n\\n[^25]: https://www.iter.org/FAQ\\n\\n[^26]: https://physicsworld.com/a/square-kilometre-array-hit-with-further-cost-hike-and-delay/\\n\\n[^27]: https://www.aip.org/fyi/2020/flagship-neutrino-project-working-keep-costs-within-cap\\n\\n[^28]: https://www.computecanada.ca/home/\\n\\n[^29]: https://www.whitehouse.gov/ostp/news-updates/2021/06/10/the-biden-administration-launches-the-national-artificial-intelligence-research-resource-task-force/\\n\\n[^30]: https://github.com/google/BIG-bench\\n\\n[^31]: https://huggingface.co/docs/transformers/bertology\\n\\n[^32]: Of course, algorithmic improvement that shifts the scaling laws is still possible.\\n\\n[^33]: https://paperswithcode.com/sota/collaborative-filtering-on-movielens-1m\\n\\n[^34]: https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/movie_recommendation/README.md\\n\\n[^35]: https://github.com/propublica/compas-analysis\\n\\n[^36]: https://github.com/fairlearn/fairlearn\\n\\n[^37]: https://gist.github.com/jareddk/6512393d4a996fbf3a72be265a5285aa\\n\\n[^38]: https://erikhoel.substack.com/p/big-tech-is-replacing-human-artists\\n\\n[^39]: For example, see: https://tedunderwood.com/2021/02/02/why-sf-hasnt-prepared-us-to-imagine-machine-learning/\\n\\n[^40]: https://openai.com/blog/ai-and-compute/\\n',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{10}\\n\\n\\\\bibitem{alex_raft_2021}\\nN.~Alex, E.~Lifland, L.~Tunstall, A.~Thakur, P.~Maham, C.~J. Riedel, E.~Hine,\\n  C.~Ashurst, P.~Sedille, A.~Carlier, M.~Noetel, and A.~Stuhlmller.\\n\\\\newblock {RAFT}: {A} {Real}-{World} {Few}-{Shot} {Text} {Classification}\\n  {Benchmark}.\\n\\\\newblock {\\\\em arXiv:2109.14076 [cs]}, Nov. 2021.\\n\\\\newblock arXiv: 2109.14076.\\n\\n\\\\bibitem{angwin_machine_2016}\\nJ.~Angwin, J.~Larson, S.~Mattu, and L.~Kirchner.\\n\\\\newblock Machine bias: {There}s software used across the country to predict\\n  future criminals. and its biased against blacks, 2016.\\n\\n\\\\bibitem{askell_general_2021}\\nA.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones,\\n  N.~Joseph, B.~Mann, N.~DasSarma, N.~Elhage, Z.~Hatfield-Dodds, D.~Hernandez,\\n  J.~Kernion, K.~Ndousse, C.~Olsson, D.~Amodei, T.~Brown, J.~Clark,\\n  S.~McCandlish, C.~Olah, and J.~Kaplan.\\n\\\\newblock A {General} {Language} {Assistant} as a {Laboratory} for {Alignment}.\\n\\\\newblock {\\\\em arXiv:2112.00861 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2112.00861.\\n\\n\\\\bibitem{austin_program_2021}\\nJ.~Austin, A.~Odena, M.~Nye, M.~Bosma, H.~Michalewski, D.~Dohan, E.~Jiang,\\n  C.~Cai, M.~Terry, Q.~Le, and C.~Sutton.\\n\\\\newblock Program {Synthesis} with {Large} {Language} {Models}.\\n\\\\newblock {\\\\em arXiv:2108.07732 [cs]}, Aug. 2021.\\n\\\\newblock arXiv: 2108.07732.\\n\\n\\\\bibitem{avin_filling_2021}\\nS.~Avin, H.~Belfield, M.~Brundage, G.~Krueger, J.~Wang, A.~Weller,\\n  M.~Anderljung, I.~Krawczuk, D.~Krueger, J.~Lebensold, T.~Maharaj, and\\n  N.~Zilberman.\\n\\\\newblock Filling gaps in trustworthy development of {AI}.\\n\\\\newblock {\\\\em Science}, Dec. 2021.\\n\\\\newblock Publisher: American Association for the Advancement of Science.\\n\\n\\\\bibitem{bao_its_2021}\\nM.~Bao, A.~Zhou, S.~Zottola, B.~Brubach, S.~Desmarais, A.~Horowitz, K.~Lum, and\\n  S.~Venkatasubramanian.\\n\\\\newblock It\\'s {COMPASlicated}: {The} {Messy} {Relationship} between {RAI}\\n  {Datasets} and {Algorithmic} {Fairness} {Benchmarks}.\\n\\\\newblock {\\\\em arXiv:2106.05498 [cs]}, June 2021.\\n\\\\newblock arXiv: 2106.05498.\\n\\n\\\\bibitem{bender_dangers_2021}\\nE.~M. Bender, T.~Gebru, A.~McMillan-Major, and S.~Shmitchell.\\n\\\\newblock On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models}\\n  {Be} {Too} {Big}?\\n\\\\newblock In {\\\\em Proceedings of the 2021 {ACM} {Conference} on {Fairness},\\n  {Accountability}, and {Transparency}}, {FAccT} \\'21, pages 610--623, New York,\\n  NY, USA, Mar. 2021. Association for Computing Machinery.\\n\\n\\\\bibitem{blodgett_language_2020}\\nS.~L. Blodgett, S.~Barocas, H.~Daum~III, and H.~Wallach.\\n\\\\newblock Language ({Technology}) is {Power}: {A} {Critical} {Survey} of\\n  \"{Bias}\" in {NLP}.\\n\\\\newblock {\\\\em arXiv:2005.14050 [cs]}, May 2020.\\n\\\\newblock arXiv: 2005.14050.\\n\\n\\\\bibitem{bommasani_opportunities_2021}\\nR.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S.\\n  Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill, E.~Brynjolfsson, S.~Buch,\\n  D.~Card, R.~Castellon, N.~Chatterji, A.~Chen, K.~Creel, J.~Q. Davis,\\n  D.~Demszky, C.~Donahue, M.~Doumbouya, E.~Durmus, S.~Ermon, J.~Etchemendy,\\n  K.~Ethayarajh, L.~Fei-Fei, C.~Finn, T.~Gale, L.~Gillespie, K.~Goel,\\n  N.~Goodman, S.~Grossman, N.~Guha, T.~Hashimoto, P.~Henderson, J.~Hewitt,\\n  D.~E. Ho, J.~Hong, K.~Hsu, J.~Huang, T.~Icard, S.~Jain, D.~Jurafsky,\\n  P.~Kalluri, S.~Karamcheti, G.~Keeling, F.~Khani, O.~Khattab, P.~W. Koh,\\n  M.~Krass, R.~Krishna, R.~Kuditipudi, A.~Kumar, F.~Ladhak, M.~Lee, T.~Lee,\\n  J.~Leskovec, I.~Levent, X.~L. Li, X.~Li, T.~Ma, A.~Malik, C.~D. Manning,\\n  S.~Mirchandani, E.~Mitchell, Z.~Munyikwa, S.~Nair, A.~Narayan, D.~Narayanan,\\n  B.~Newman, A.~Nie, J.~C. Niebles, H.~Nilforoshan, J.~Nyarko, G.~Ogut, L.~Orr,\\n  I.~Papadimitriou, J.~S. Park, C.~Piech, E.~Portelance, C.~Potts,\\n  A.~Raghunathan, R.~Reich, H.~Ren, F.~Rong, Y.~Roohani, C.~Ruiz, J.~Ryan,\\n  C.~R, D.~Sadigh, S.~Sagawa, K.~Santhanam, A.~Shih, K.~Srinivasan,\\n  A.~Tamkin, R.~Taori, A.~W. Thomas, F.~Tramr, R.~E. Wang, W.~Wang, B.~Wu,\\n  J.~Wu, Y.~Wu, S.~M. Xie, M.~Yasunaga, J.~You, M.~Zaharia, M.~Zhang, T.~Zhang,\\n  X.~Zhang, Y.~Zhang, L.~Zheng, K.~Zhou, and P.~Liang.\\n\\\\newblock On the {Opportunities} and {Risks} of {Foundation} {Models}.\\n\\\\newblock {\\\\em arXiv:2108.07258 [cs]}, Aug. 2021.\\n\\\\newblock arXiv: 2108.07258.\\n\\n\\\\bibitem{borgeaud_improving_2021}\\nS.~Borgeaud, A.~Mensch, J.~Hoffmann, T.~Cai, E.~Rutherford, K.~Millican,\\n  G.~v.~d. Driessche, J.-B. Lespiau, B.~Damoc, A.~Clark, D.~d.~L. Casas,\\n  A.~Guy, J.~Menick, R.~Ring, T.~Hennigan, S.~Huang, L.~Maggiore, C.~Jones,\\n  A.~Cassirer, A.~Brock, M.~Paganini, G.~Irving, O.~Vinyals, S.~Osindero,\\n  K.~Simonyan, J.~W. Rae, E.~Elsen, and L.~Sifre.\\n\\\\newblock Improving language models by retrieving from trillions of tokens.\\n\\\\newblock {\\\\em arXiv:2112.04426 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2112.04426.\\n\\n\\\\bibitem{brown_language_2020}\\nT.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,\\n  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,\\n  G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~Ziegler, J.~Wu, C.~Winter,\\n  C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark,\\n  C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.\\n\\\\newblock Language {Models} are {Few}-{Shot} {Learners}.\\n\\\\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,\\n  editors, {\\\\em Advances in {Neural} {Information} {Processing} {Systems}},\\n  volume~33, pages 1877--1901. Curran Associates, Inc., 2020.\\n\\n\\\\bibitem{brundage_toward_2020}\\nM.~Brundage, S.~Avin, J.~Wang, H.~Belfield, G.~Krueger, G.~Hadfield, H.~Khlaaf,\\n  J.~Yang, H.~Toner, R.~Fong, T.~Maharaj, P.~W. Koh, S.~Hooker, J.~Leung,\\n  A.~Trask, E.~Bluemke, J.~Lebensold, C.~O\\'Keefe, M.~Koren, T.~Ryffel, J.~B.\\n  Rubinovitz, T.~Besiroglu, F.~Carugati, J.~Clark, P.~Eckersley, S.~de~Haas,\\n  M.~Johnson, B.~Laurie, A.~Ingerman, I.~Krawczuk, A.~Askell, R.~Cammarota,\\n  A.~Lohn, D.~Krueger, C.~Stix, P.~Henderson, L.~Graham, C.~Prunkl, B.~Martin,\\n  E.~Seger, N.~Zilberman, S.~O. higeartaigh, F.~Kroeger, G.~Sastry, R.~Kagan,\\n  A.~Weller, B.~Tse, E.~Barnes, A.~Dafoe, P.~Scharre, A.~Herbert-Voss,\\n  M.~Rasser, S.~Sodhani, C.~Flynn, T.~K. Gilbert, L.~Dyer, S.~Khan, Y.~Bengio,\\n  and M.~Anderljung.\\n\\\\newblock Toward {Trustworthy} {AI} {Development}: {Mechanisms} for\\n  {Supporting} {Verifiable} {Claims}.\\n\\\\newblock {\\\\em arXiv:2004.07213 [cs]}, Apr. 2020.\\n\\\\newblock arXiv: 2004.07213.\\n\\n\\\\bibitem{buchanan_truth_2021}\\nB.~Buchanan, A.~Lohn, M.~Musser, and K.~Sedova.\\n\\\\newblock Truth, {Lies}, and {Automation}, May 2021.\\n\\n\\\\bibitem{carlini_extracting_2021}\\nN.~Carlini, F.~Tramer, E.~Wallace, M.~Jagielski, A.~Herbert-Voss, K.~Lee,\\n  A.~Roberts, T.~Brown, D.~Song, U.~Erlingsson, A.~Oprea, and C.~Raffel.\\n\\\\newblock Extracting {Training} {Data} from {Large} {Language} {Models}.\\n\\\\newblock {\\\\em arXiv:2012.07805 [cs]}, June 2021.\\n\\\\newblock arXiv: 2012.07805.\\n\\n\\\\bibitem{chen_evaluating_2021}\\nM.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,\\n  Y.~Burda, N.~Joseph, G.~Brockman, A.~Ray, R.~Puri, G.~Krueger, M.~Petrov,\\n  H.~Khlaaf, G.~Sastry, P.~Mishkin, B.~Chan, S.~Gray, N.~Ryder, M.~Pavlov,\\n  A.~Power, L.~Kaiser, M.~Bavarian, C.~Winter, P.~Tillet, F.~P. Such,\\n  D.~Cummings, M.~Plappert, F.~Chantzis, E.~Barnes, A.~Herbert-Voss, W.~H.\\n  Guss, A.~Nichol, A.~Paino, N.~Tezak, J.~Tang, I.~Babuschkin, S.~Balaji,\\n  S.~Jain, W.~Saunders, C.~Hesse, A.~N. Carr, J.~Leike, J.~Achiam, V.~Misra,\\n  E.~Morikawa, A.~Radford, M.~Knight, M.~Brundage, M.~Murati, K.~Mayer,\\n  P.~Welinder, B.~McGrew, D.~Amodei, S.~McCandlish, I.~Sutskever, and\\n  W.~Zaremba.\\n\\\\newblock Evaluating {Large} {Language} {Models} {Trained} on {Code}.\\n\\\\newblock {\\\\em arXiv:2107.03374 [cs]}, July 2021.\\n\\\\newblock arXiv: 2107.03374.\\n\\n\\\\bibitem{clark_what_2019}\\nK.~Clark, U.~Khandelwal, O.~Levy, and C.~D. Manning.\\n\\\\newblock What {Does} {BERT} {Look} {At}? {An} {Analysis} of {BERT}\\'s\\n  {Attention}.\\n\\\\newblock {\\\\em arXiv:1906.04341 [cs]}, June 2019.\\n\\\\newblock arXiv: 1906.04341.\\n\\n\\\\bibitem{clary_lets_2019}\\nK.~Clary, E.~Tosch, J.~Foley, and D.~Jensen.\\n\\\\newblock Let\\'s {Play} {Again}: {Variability} of {Deep} {Reinforcement}\\n  {Learning} {Agents} in {Atari} {Environments}.\\n\\\\newblock {\\\\em arXiv:1904.06312 [cs, stat]}, Apr. 2019.\\n\\\\newblock arXiv: 1904.06312.\\n\\n\\\\bibitem{crawford_atlas_2021}\\nK.~Crawford.\\n\\\\newblock {\\\\em Atlas of {AI}}.\\n\\\\newblock Yale University Press, 2021.\\n\\n\\\\bibitem{dinan_anticipating_2021}\\nE.~Dinan, G.~Abercrombie, A.~S. Bergman, S.~Spruit, D.~Hovy, Y.-L. Boureau, and\\n  V.~Rieser.\\n\\\\newblock Anticipating {Safety} {Issues} in {E2E} {Conversational} {AI}:\\n  {Framework} and {Tooling}.\\n\\\\newblock {\\\\em arXiv:2107.03451 [cs]}, July 2021.\\n\\\\newblock arXiv: 2107.03451.\\n\\n\\\\bibitem{dressel_accuracy_2018}\\nJ.~Dressel and H.~Farid.\\n\\\\newblock The accuracy, fairness, and limits of predicting recidivism.\\n\\\\newblock {\\\\em Science Advances}, Jan. 2018.\\n\\\\newblock Publisher: American Association for the Advancement of Science.\\n\\n\\\\bibitem{droppo_scaling_2021}\\nJ.~Droppo and O.~Elibol.\\n\\\\newblock Scaling {Laws} for {Acoustic} {Models}.\\n\\\\newblock {\\\\em arXiv:2106.09488 [cs, eess]}, June 2021.\\n\\\\newblock arXiv: 2106.09488.\\n\\n\\\\bibitem{du_glam_2021}\\nN.~Du, Y.~Huang, A.~M. Dai, S.~Tong, D.~Lepikhin, Y.~Xu, M.~Krikun, Y.~Zhou,\\n  A.~W. Yu, O.~Firat, B.~Zoph, L.~Fedus, M.~Bosma, Z.~Zhou, T.~Wang, Y.~E.\\n  Wang, K.~Webster, M.~Pellat, K.~Robinson, K.~Meier-Hellstern, T.~Duke,\\n  L.~Dixon, K.~Zhang, Q.~V. Le, Y.~Wu, Z.~Chen, and C.~Cui.\\n\\\\newblock {GLaM}: {Efficient} {Scaling} of {Language} {Models} with\\n  {Mixture}-of-{Experts}.\\n\\\\newblock {\\\\em arXiv:2112.06905 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2112.06905.\\n\\n\\\\bibitem{elhage_mathematical_2021}\\nN.~Elhage, N.~Nanda, C.~Olsson, T.~Henighan, N.~Joseph, B.~Mann, A.~Askell,\\n  Y.~Bai, A.~Chen, T.~Conerly, N.~DasSarma, D.~Drain, D.~Ganguli,\\n  Z.~Hatfield-Dodds, D.~Hernandez, A.~Jones, J.~Kernion, L.~Lovitt, K.~Ndousse,\\n  D.~Amodei, T.~Brown, J.~Clark, J.~Kaplan, S.~McCandlish, and C.~Olah.\\n\\\\newblock A {Mathematical} {Framework} for {Transformer} {Circuits}, 2021.\\n\\n\\\\bibitem{friedler_impossibility_2016}\\nS.~A. Friedler, C.~Scheidegger, and S.~Venkatasubramanian.\\n\\\\newblock On the (im)possibility of fairness.\\n\\\\newblock {\\\\em arXiv:1609.07236 [cs, stat]}, Sept. 2016.\\n\\\\newblock arXiv: 1609.07236.\\n\\n\\\\bibitem{gao_framework_2021}\\nL.~Gao, J.~Tow, S.~Biderman, S.~Black, A.~DiPofi, C.~Foster, L.~Golding,\\n  J.~Hsu, K.~McDonell, N.~Muennighoff, J.~Phang, L.~Reynolds, E.~Tang,\\n  A.~Thite, B.~Wang, K.~Wang, and A.~Zou.\\n\\\\newblock A framework for few-shot language model evaluation, Sept. 2021.\\n\\n\\\\bibitem{gebru_datasheets_2021}\\nT.~Gebru, J.~Morgenstern, B.~Vecchione, J.~W. Vaughan, H.~Wallach,\\n  H.~Daum~III, and K.~Crawford.\\n\\\\newblock Datasheets for {Datasets}.\\n\\\\newblock {\\\\em arXiv:1803.09010 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 1803.09010.\\n\\n\\\\bibitem{gehman_realtoxicityprompts_2020}\\nS.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith.\\n\\\\newblock {RealToxicityPrompts}: {Evaluating} {Neural} {Toxic} {Degeneration}\\n  in {Language} {Models}.\\n\\\\newblock {\\\\em ArXiv}, abs/2009.11462, 2020.\\n\\n\\\\bibitem{gray_ghost_2019}\\nM.~Gray and S.~Suri.\\n\\\\newblock {\\\\em Ghost {Work}}.\\n\\\\newblock Mariner Books, 2019.\\n\\n\\\\bibitem{han_glocal-k_2021}\\nS.~C. Han, T.~Lim, S.~Long, B.~Burgstaller, and J.~Poon.\\n\\\\newblock {GLocal}-{K}: {Global} and {Local} {Kernels} for {Recommender}\\n  {Systems}.\\n\\\\newblock Aug. 2021.\\n\\n\\\\bibitem{hanu_detoxify_2020}\\nL.~Hanu and {Unitary team}.\\n\\\\newblock Detoxify, 2020.\\n\\\\newblock Published: Github. https://github.com/unitaryai/detoxify.\\n\\n\\\\bibitem{harper_movielens_2015}\\nF.~M. Harper and J.~A. Konstan.\\n\\\\newblock The {MovieLens} {Datasets}: {History} and {Context}.\\n\\\\newblock {\\\\em ACM Transactions on Interactive Intelligent Systems},\\n  5(4):19:1--19:19, Dec. 2015.\\n\\n\\\\bibitem{hendrycks_measuring_2021}\\nD.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and\\n  J.~Steinhardt.\\n\\\\newblock Measuring {Massive} {Multitask} {Language} {Understanding}.\\n\\\\newblock {\\\\em arXiv:2009.03300 [cs]}, Jan. 2021.\\n\\\\newblock arXiv: 2009.03300.\\n\\n\\\\bibitem{hendrycks_unsolved_2021}\\nD.~Hendrycks, N.~Carlini, J.~Schulman, and J.~Steinhardt.\\n\\\\newblock Unsolved {Problems} in {ML} {Safety}.\\n\\\\newblock {\\\\em arXiv:2109.13916 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2109.13916.\\n\\n\\\\bibitem{hendrycks_natural_2021}\\nD.~Hendrycks, K.~Zhao, S.~Basart, J.~Steinhardt, and D.~Song.\\n\\\\newblock Natural {Adversarial} {Examples}.\\n\\\\newblock {\\\\em arXiv:1907.07174 [cs, stat]}, Mar. 2021.\\n\\\\newblock arXiv: 1907.07174.\\n\\n\\\\bibitem{henighan_scaling_2020}\\nT.~Henighan, J.~Kaplan, M.~Katz, M.~Chen, C.~Hesse, J.~Jackson, H.~Jun, T.~B.\\n  Brown, P.~Dhariwal, S.~Gray, C.~Hallacy, B.~Mann, A.~Radford, A.~Ramesh,\\n  N.~Ryder, D.~M. Ziegler, J.~Schulman, D.~Amodei, and S.~McCandlish.\\n\\\\newblock Scaling {Laws} for {Autoregressive} {Generative} {Modeling}.\\n\\\\newblock {\\\\em arXiv:2010.14701 [cs]}, Nov. 2020.\\n\\\\newblock arXiv: 2010.14701.\\n\\n\\\\bibitem{hernandez_scaling_2021}\\nD.~Hernandez, J.~Kaplan, T.~Henighan, and S.~McCandlish.\\n\\\\newblock Scaling {Laws} for {Transfer}.\\n\\\\newblock {\\\\em arXiv:2102.01293 [cs]}, Feb. 2021.\\n\\\\newblock arXiv: 2102.01293.\\n\\n\\\\bibitem{hestness_deep_2017}\\nJ.~Hestness, S.~Narang, N.~Ardalani, G.~Diamos, H.~Jun, H.~Kianinejad, M.~M.~A.\\n  Patwary, Y.~Yang, and Y.~Zhou.\\n\\\\newblock Deep {Learning} {Scaling} is {Predictable}, {Empirically}.\\n\\\\newblock {\\\\em arXiv:1712.00409 [cs, stat]}, Dec. 2017.\\n\\\\newblock arXiv: 1712.00409.\\n\\n\\\\bibitem{ho_building_2021}\\nD.~Ho, J.~King, R.~Wald, and C.~Wan.\\n\\\\newblock Building a {National} {AI} {Research} {Resource}.\\n\\\\newblock White {Paper}, Stanford University, 2021.\\n\\n\\\\bibitem{ai_now_institute_democratize_2021}\\nA.~N. Institute.\\n\\\\newblock Democratize {AI}? {How} the proposed {National} {AI} {Research}\\n  {Resource} falls short, Oct. 2021.\\n\\n\\\\bibitem{kaplan_scaling_2020}\\nJ.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child,\\n  S.~Gray, A.~Radford, J.~Wu, and D.~Amodei.\\n\\\\newblock Scaling {Laws} for {Neural} {Language} {Models}.\\n\\\\newblock {\\\\em arXiv:2001.08361 [cs, stat]}, Jan. 2020.\\n\\\\newblock arXiv: 2001.08361.\\n\\n\\\\bibitem{kenton_alignment_2021}\\nZ.~Kenton, T.~Everitt, L.~Weidinger, I.~Gabriel, V.~Mikulik, and G.~Irving.\\n\\\\newblock Alignment of {Language} {Agents}.\\n\\\\newblock {\\\\em arXiv:2103.14659 [cs]}, Mar. 2021.\\n\\\\newblock arXiv: 2103.14659.\\n\\n\\\\bibitem{kenway_bug_2022}\\nJ.~Kenway, F.~Camille, S.~Costanza-Chock, D.~Raji, Inioluwa, and J.~Buolamwini.\\n\\\\newblock Bug {Bounties} {For} {Algorithmic} {Harms}?\\n\\\\newblock Technical report, Algorithmic Justice League, 2022.\\n\\n\\\\bibitem{kim_what_2021}\\nB.~Kim, H.~Kim, S.-W. Lee, G.~Lee, D.~Kwak, D.~H. Jeon, S.~Park, S.~Kim,\\n  S.~Kim, D.~Seo, H.~Lee, M.~Jeong, S.~Lee, M.~Kim, S.~H. Ko, S.~Kim, T.~Park,\\n  J.~Kim, S.~Kang, N.-H. Ryu, K.~M. Yoo, M.~Chang, S.~Suh, S.~In, J.~Park,\\n  K.~Kim, H.~Kim, J.~Jeong, Y.~G. Yeo, D.~Ham, D.~Park, M.~Y. Lee, J.~Kang,\\n  I.~Kang, J.-W. Ha, W.~Park, and N.~Sung.\\n\\\\newblock What {Changes} {Can} {Large}-scale {Language} {Models} {Bring}?\\n  {Intensive} {Study} on {HyperCLOVA}: {Billions}-scale {Korean} {Generative}\\n  {Pretrained} {Transformers}.\\n\\\\newblock {\\\\em arXiv:2109.04650 [cs]}, Nov. 2021.\\n\\\\newblock arXiv: 2109.04650.\\n\\n\\\\bibitem{krizhevsky_imagenet_2012}\\nA.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.\\n\\\\newblock {ImageNet} {Classification} with {Deep} {Convolutional} {Neural}\\n  {Networks}.\\n\\\\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,\\n  editors, {\\\\em Advances in {Neural} {Information} {Processing} {Systems}},\\n  volume~25. Curran Associates, Inc., 2012.\\n\\n\\\\bibitem{leahy_announcing_2022}\\nC.~Leahy.\\n\\\\newblock Announcing {GPT}-{NeoX}-{20B}, Feb. 2022.\\n\\n\\\\bibitem{lieber_jurassic-1_2021}\\nO.~Lieber, O.~Sharir, B.~Lenz, and Y.~Shoham.\\n\\\\newblock Jurassic-1: {Technical} {Details} {And} {Evaluation}.\\n\\\\newblock Technical report, AI21 Labs, Aug. 2021.\\n\\n\\\\bibitem{lohn_ai_2022}\\nA.~Lohn and M.~Musser.\\n\\\\newblock {AI} and {Compute}: {How} {Much} {Longer} {Can} {Computing} {Power}\\n  {Drive} {Artificial} {Intelligence} {Progress}?\\n\\\\newblock Technical report, Center for Security and Emerging Technology, Jan.\\n  2022.\\n\\n\\\\bibitem{mitchell_model_2019}\\nM.~Mitchell, S.~Wu, A.~Zaldivar, P.~Barnes, L.~Vasserman, B.~Hutchinson,\\n  E.~Spitzer, I.~D. Raji, and T.~Gebru.\\n\\\\newblock Model {Cards} for {Model} {Reporting}.\\n\\\\newblock {\\\\em Proceedings of the Conference on Fairness, Accountability, and\\n  Transparency}, pages 220--229, Jan. 2019.\\n\\\\newblock arXiv: 1810.03993.\\n\\n\\\\bibitem{mohamed_decolonial_2020}\\nS.~Mohamed, M.-T. Png, and W.~Isaac.\\n\\\\newblock Decolonial {AI}: {Decolonial} {Theory} as {Sociotechnical}\\n  {Foresight} in {Artificial} {Intelligence}.\\n\\\\newblock {\\\\em Philosophy \\\\& Technology}, 33, Dec. 2020.\\n\\n\\\\bibitem{patterson_carbon_2021}\\nD.~Patterson, J.~Gonzalez, Q.~Le, C.~Liang, L.-M. Munguia, D.~Rothchild, D.~So,\\n  M.~Texier, and J.~Dean.\\n\\\\newblock Carbon {Emissions} and {Large} {Neural} {Network} {Training}.\\n\\\\newblock {\\\\em arXiv:2104.10350 [cs]}, Apr. 2021.\\n\\\\newblock arXiv: 2104.10350.\\n\\n\\\\bibitem{perez_red_2022}\\nE.~Perez, S.~Huang, F.~Song, T.~Cai, R.~Ring, J.~Aslanides, A.~Glaese,\\n  N.~McAleese, and G.~Irving.\\n\\\\newblock Red {Teaming} {Language} {Models} with {Language} {Models}.\\n\\\\newblock {\\\\em arXiv:2202.03286 [cs]}, Feb. 2022.\\n\\\\newblock arXiv: 2202.03286.\\n\\n\\\\bibitem{power_grokking_2022}\\nA.~Power, Y.~Burda, H.~Edwards, I.~Babuschkin, and V.~Misra.\\n\\\\newblock Grokking: {Generalization} {Beyond} {Overfitting} on {Small}\\n  {Algorithmic} {Datasets}.\\n\\\\newblock {\\\\em arXiv:2201.02177 [cs]}, Jan. 2022.\\n\\\\newblock arXiv: 2201.02177.\\n\\n\\\\bibitem{prabhumoye_few-shot_2021}\\nS.~Prabhumoye, R.~Kocielnik, M.~Shoeybi, A.~Anandkumar, and B.~Catanzaro.\\n\\\\newblock Few-shot {Instruction} {Prompts} for {Pretrained} {Language} {Models}\\n  to {Detect} {Social} {Biases}.\\n\\\\newblock {\\\\em arXiv:2112.07868 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2112.07868.\\n\\n\\\\bibitem{prato_scaling_2021}\\nG.~Prato, S.~Guiroy, E.~Caballero, I.~Rish, and S.~Chandar.\\n\\\\newblock Scaling {Laws} for the {Few}-{Shot} {Adaptation} of {Pre}-trained\\n  {Image} {Classifiers}.\\n\\\\newblock {\\\\em arXiv:2110.06990 [cs]}, Oct. 2021.\\n\\\\newblock arXiv: 2110.06990.\\n\\n\\\\bibitem{radford_learning_2021}\\nA.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,\\n  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever.\\n\\\\newblock Learning {Transferable} {Visual} {Models} {From} {Natural} {Language}\\n  {Supervision}.\\n\\\\newblock {\\\\em arXiv:2103.00020 [cs]}, Feb. 2021.\\n\\\\newblock arXiv: 2103.00020.\\n\\n\\\\bibitem{rae_scaling_2021}\\nJ.~W. Rae, S.~Borgeaud, T.~Cai, K.~Millican, J.~Hoffmann, F.~Song,\\n  J.~Aslanides, S.~Henderson, R.~Ring, S.~Young, E.~Rutherford, T.~Hennigan,\\n  J.~Menick, A.~Cassirer, R.~Powell, G.~v.~d. Driessche, L.~A. Hendricks,\\n  M.~Rauh, P.-S. Huang, A.~Glaese, J.~Welbl, S.~Dathathri, S.~Huang, J.~Uesato,\\n  J.~Mellor, I.~Higgins, A.~Creswell, N.~McAleese, A.~Wu, E.~Elsen,\\n  S.~Jayakumar, E.~Buchatskaya, D.~Budden, E.~Sutherland, K.~Simonyan,\\n  M.~Paganini, L.~Sifre, L.~Martens, X.~L. Li, A.~Kuncoro, A.~Nematzadeh,\\n  E.~Gribovskaya, D.~Donato, A.~Lazaridou, A.~Mensch, J.-B. Lespiau,\\n  M.~Tsimpoukelli, N.~Grigorev, D.~Fritz, T.~Sottiaux, M.~Pajarskas, T.~Pohlen,\\n  Z.~Gong, D.~Toyama, C.~d.~M. d\\'Autume, Y.~Li, T.~Terzi, V.~Mikulik,\\n  I.~Babuschkin, A.~Clark, D.~d.~L. Casas, A.~Guy, C.~Jones, J.~Bradbury,\\n  M.~Johnson, B.~Hechtman, L.~Weidinger, I.~Gabriel, W.~Isaac, E.~Lockhart,\\n  S.~Osindero, L.~Rimell, C.~Dyer, O.~Vinyals, K.~Ayoub, J.~Stanway,\\n  L.~Bennett, D.~Hassabis, K.~Kavukcuoglu, and G.~Irving.\\n\\\\newblock Scaling {Language} {Models}: {Methods}, {Analysis} \\\\& {Insights} from\\n  {Training} {Gopher}.\\n\\\\newblock {\\\\em arXiv:2112.11446 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2112.11446.\\n\\n\\\\bibitem{raji_actionable_2019}\\nI.~D. Raji and J.~Buolamwini.\\n\\\\newblock Actionable {Auditing}: {Investigating} the {Impact} of {Publicly}\\n  {Naming} {Biased} {Performance} {Results} of {Commercial} {AI} {Products}.\\n\\\\newblock In {\\\\em Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI},\\n  {Ethics}, and {Society}}, {AIES} \\'19, pages 429--435, New York, NY, USA, Jan.\\n  2019. Association for Computing Machinery.\\n\\n\\\\bibitem{raji_closing_2020}\\nI.~D. Raji, A.~Smart, R.~N. White, M.~Mitchell, T.~Gebru, B.~Hutchinson,\\n  J.~Smith-Loud, D.~Theron, and P.~Barnes.\\n\\\\newblock Closing the {AI} accountability gap: defining an end-to-end framework\\n  for internal algorithmic auditing.\\n\\\\newblock In {\\\\em Proceedings of the 2020 {Conference} on {Fairness},\\n  {Accountability}, and {Transparency}}, {FAT}* \\'20, pages 33--44, New York,\\n  NY, USA, Jan. 2020. Association for Computing Machinery.\\n\\n\\\\bibitem{rosenfeld_constructive_2019}\\nJ.~S. Rosenfeld, A.~Rosenfeld, Y.~Belinkov, and N.~Shavit.\\n\\\\newblock A {Constructive} {Prediction} of the {Generalization} {Error}\\n  {Across} {Scales}.\\n\\\\newblock {\\\\em arXiv:1909.12673 [cs, stat]}, Dec. 2019.\\n\\\\newblock arXiv: 1909.12673.\\n\\n\\\\bibitem{rudin_age_2020}\\nC.~Rudin, C.~Wang, and B.~Coker.\\n\\\\newblock The {Age} of {Secrecy} and {Unfairness} in {Recidivism} {Prediction}.\\n\\\\newblock {\\\\em Harvard Data Science Review}, 2(1), Mar. 2020.\\n\\n\\\\bibitem{sanh_multitask_2021}\\nV.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai,\\n  A.~Chaffin, A.~Stiegler, T.~L. Scao, A.~Raja, M.~Dey, M.~S. Bari, C.~Xu,\\n  U.~Thakker, S.~S. Sharma, E.~Szczechla, T.~Kim, G.~Chhablani, N.~Nayak,\\n  D.~Datta, J.~Chang, M.~T.-J. Jiang, H.~Wang, M.~Manica, S.~Shen, Z.~X. Yong,\\n  H.~Pandey, R.~Bawden, T.~Wang, T.~Neeraj, J.~Rozen, A.~Sharma, A.~Santilli,\\n  T.~Fevry, J.~A. Fries, R.~Teehan, S.~Biderman, L.~Gao, T.~Bers, T.~Wolf, and\\n  A.~M. Rush.\\n\\\\newblock Multitask {Prompted} {Training} {Enables} {Zero}-{Shot} {Task}\\n  {Generalization}.\\n\\\\newblock {\\\\em arXiv:2110.08207 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2110.08207.\\n\\n\\\\bibitem{schwartz_green_2020}\\nR.~Schwartz, J.~Dodge, N.~A. Smith, and O.~Etzioni.\\n\\\\newblock Green {AI}.\\n\\\\newblock {\\\\em Communications of the ACM}, 63(12):54--63, Nov. 2020.\\n\\n\\\\bibitem{sevilla_parameter_2021}\\nJ.~Sevilla, P.~Villalobos, J.~F. Cern, M.~Burtell, L.~Heim, A.~B. Nanjajjar,\\n  A.~Ho, T.~Besiroglu, M.~Hobbhahn, and J.-S. Denain.\\n\\\\newblock Parameter, {Compute} and {Data} {Trends} in {Machine} {Learning},\\n  2021.\\n\\n\\\\bibitem{smith_using_2022}\\nS.~Smith, M.~Patwary, B.~Norick, P.~LeGresley, S.~Rajbhandari, J.~Casper,\\n  Z.~Liu, S.~Prabhumoye, G.~Zerveas, V.~Korthikanti, E.~Zhang, R.~Child, R.~Y.\\n  Aminabadi, J.~Bernauer, X.~Song, M.~Shoeybi, Y.~He, M.~Houston, S.~Tiwary,\\n  and B.~Catanzaro.\\n\\\\newblock Using {DeepSpeed} and {Megatron} to {Train} {Megatron}-{Turing} {NLG}\\n  {530B}, {A} {Large}-{Scale} {Generative} {Language} {Model}.\\n\\\\newblock {\\\\em arXiv:2201.11990 [cs]}, Feb. 2022.\\n\\\\newblock arXiv: 2201.11990.\\n\\n\\\\bibitem{solaiman_process_2021}\\nI.~Solaiman and C.~Dennison.\\n\\\\newblock Process for {Adapting} {Language} {Models} to {Society} ({PALMS})\\n  with {Values}-{Targeted} {Datasets}.\\n\\\\newblock {\\\\em arXiv:2106.10328 [cs]}, Nov. 2021.\\n\\\\newblock arXiv: 2106.10328.\\n\\n\\\\bibitem{strubell_energy_2019}\\nE.~Strubell, A.~Ganesh, and A.~McCallum.\\n\\\\newblock Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}.\\n\\\\newblock {\\\\em arXiv:1906.02243 [cs]}, June 2019.\\n\\\\newblock arXiv: 1906.02243.\\n\\n\\\\bibitem{tamkin_understanding_2021}\\nA.~Tamkin, M.~Brundage, J.~Clark, and D.~Ganguli.\\n\\\\newblock Understanding the {Capabilities}, {Limitations}, and {Societal}\\n  {Impact} of {Large} {Language} {Models}.\\n\\\\newblock {\\\\em arXiv:2102.02503 [cs]}, Feb. 2021.\\n\\\\newblock arXiv: 2102.02503.\\n\\n\\\\bibitem{thoppilan_lamda_2022}\\nR.~Thoppilan, D.~De~Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.-T. Cheng,\\n  A.~Jin, T.~Bos, L.~Baker, Y.~Du, Y.~Li, H.~Lee, H.~S. Zheng, A.~Ghafouri,\\n  M.~Menegali, Y.~Huang, M.~Krikun, D.~Lepikhin, J.~Qin, D.~Chen, Y.~Xu,\\n  Z.~Chen, A.~Roberts, M.~Bosma, Y.~Zhou, C.-C. Chang, I.~Krivokon, W.~Rusch,\\n  M.~Pickett, K.~Meier-Hellstern, M.~R. Morris, T.~Doshi, R.~D. Santos,\\n  T.~Duke, J.~Soraker, B.~Zevenbergen, V.~Prabhakaran, M.~Diaz, B.~Hutchinson,\\n  K.~Olson, A.~Molina, E.~Hoffman-John, J.~Lee, L.~Aroyo, R.~Rajakumar,\\n  A.~Butryna, M.~Lamm, V.~Kuzmina, J.~Fenton, A.~Cohen, R.~Bernstein,\\n  R.~Kurzweil, B.~Aguera-Arcas, C.~Cui, M.~Croak, E.~Chi, and Q.~Le.\\n\\\\newblock {LaMDA}: {Language} {Models} for {Dialog} {Applications}.\\n\\\\newblock {\\\\em arXiv:2201.08239 [cs]}, Jan. 2022.\\n\\\\newblock arXiv: 2201.08239.\\n\\n\\\\bibitem{wang_gpt-j-6b_2021}\\nB.~Wang and A.~Komatsuzaki.\\n\\\\newblock {GPT}-{J}-{6B}: {A} 6 {Billion} {Parameter} {Autoregressive}\\n  {Language} {Model}, May 2021.\\n\\n\\\\bibitem{wang_ernie_2021}\\nS.~Wang, Y.~Sun, Y.~Xiang, Z.~Wu, S.~Ding, W.~Gong, S.~Feng, J.~Shang, Y.~Zhao,\\n  C.~Pang, J.~Liu, X.~Chen, Y.~Lu, W.~Liu, X.~Wang, Y.~Bai, Q.~Chen, L.~Zhao,\\n  S.~Li, P.~Sun, D.~Yu, Y.~Ma, H.~Tian, H.~Wu, T.~Wu, W.~Zeng, G.~Li, W.~Gao,\\n  and H.~Wang.\\n\\\\newblock {ERNIE} 3.0 {Titan}: {Exploring} {Larger}-scale {Knowledge}\\n  {Enhanced} {Pre}-training for {Language} {Understanding} and {Generation}.\\n\\\\newblock {\\\\em arXiv:2112.12731 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2112.12731.\\n\\n\\\\bibitem{wei_finetuned_2021}\\nJ.~Wei, M.~Bosma, V.~Y. Zhao, K.~Guu, A.~W. Yu, B.~Lester, N.~Du, A.~M. Dai,\\n  and Q.~V. Le.\\n\\\\newblock Finetuned {Language} {Models} {Are} {Zero}-{Shot} {Learners}.\\n\\\\newblock {\\\\em arXiv:2109.01652 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2109.01652.\\n\\n\\\\bibitem{weidinger_ethical_2021}\\nL.~Weidinger, J.~Mellor, M.~Rauh, C.~Griffin, J.~Uesato, P.-S. Huang, M.~Cheng,\\n  M.~Glaese, B.~Balle, A.~Kasirzadeh, Z.~Kenton, S.~Brown, W.~Hawkins,\\n  T.~Stepleton, C.~Biles, A.~Birhane, J.~Haas, L.~Rimell, L.~A. Hendricks,\\n  W.~Isaac, S.~Legassick, G.~Irving, and I.~Gabriel.\\n\\\\newblock Ethical and social risks of harm from {Language} {Models}.\\n\\\\newblock {\\\\em arXiv:2112.04359 [cs]}, Dec. 2021.\\n\\\\newblock arXiv: 2112.04359.\\n\\n\\\\bibitem{welbl_challenges_2021}\\nJ.~Welbl, A.~Glaese, J.~Uesato, S.~Dathathri, J.~Mellor, L.~A. Hendricks,\\n  K.~Anderson, P.~Kohli, B.~Coppin, and P.-S. Huang.\\n\\\\newblock Challenges in {Detoxifying} {Language} {Models}.\\n\\\\newblock In {\\\\em Findings of the {Association} for {Computational}\\n  {Linguistics}: {EMNLP} 2021}, pages 2447--2469, Punta Cana, Dominican\\n  Republic, Nov. 2021. Association for Computational Linguistics.\\n\\n\\\\bibitem{whittlestone_why_2021}\\nJ.~Whittlestone and J.~Clark.\\n\\\\newblock Why and {How} {Governments} {Should} {Monitor} {AI} {Development}.\\n\\\\newblock {\\\\em arXiv:2108.12427 [cs]}, Aug. 2021.\\n\\\\newblock arXiv: 2108.12427.\\n\\n\\\\bibitem{wolf_why_2017}\\nM.~J. Wolf, K.~W. Miller, and F.~S. Grodzinsky.\\n\\\\newblock Why {We} {Should} {Have} {Seen} {That} {Coming}: {Comments} on\\n  {Microsoft}s {Tay} {Experiment}, and {Wider} {Implications}.\\n\\\\newblock {\\\\em The ORBIT Journal}, 1(2):1--12, Jan. 2017.\\n\\n\\\\bibitem{wu_yuan_2021}\\nS.~Wu, X.~Zhao, T.~Yu, R.~Zhang, C.~Shen, H.~Liu, F.~Li, H.~Zhu, J.~Luo, L.~Xu,\\n  and X.~Zhang.\\n\\\\newblock Yuan 1.0: {Large}-{Scale} {Pre}-trained {Language} {Model} in\\n  {Zero}-{Shot} and {Few}-{Shot} {Learning}.\\n\\\\newblock {\\\\em arXiv:2110.04725 [cs]}, Oct. 2021.\\n\\\\newblock arXiv: 2110.04725.\\n\\n\\\\bibitem{xu_bot-adversarial_2021}\\nJ.~Xu, D.~Ju, M.~Li, Y.-L. Boureau, J.~Weston, and E.~Dinan.\\n\\\\newblock Bot-{Adversarial} {Dialogue} for {Safe} {Conversational} {Agents}.\\n\\\\newblock In K.~Toutanova, A.~Rumshisky, L.~Zettlemoyer, D.~Hakkani-Tr,\\n  I.~Beltagy, S.~Bethard, R.~Cotterell, T.~Chakraborty, and Y.~Zhou, editors,\\n  {\\\\em Proceedings of the 2021 {Conference} of the {North} {American} {Chapter}\\n  of the {Association} for {Computational} {Linguistics}: {Human} {Language}\\n  {Technologies}, {NAACL}-{HLT} 2021, {Online}, {June} 6-11, 2021}, pages\\n  2950--2968. Association for Computational Linguistics, 2021.\\n\\n\\\\bibitem{zeng_pangu-alpha_2021}\\nW.~Zeng, X.~Ren, T.~Su, H.~Wang, Y.~Liao, Z.~Wang, X.~Jiang, Z.~Yang, K.~Wang,\\n  X.~Zhang, C.~Li, Z.~Gong, Y.~Yao, X.~Huang, J.~Wang, J.~Yu, Q.~Guo, Y.~Yu,\\n  Y.~Zhang, J.~Wang, H.~Tao, D.~Yan, Z.~Yi, F.~Peng, F.~Jiang, H.~Zhang,\\n  L.~Deng, Y.~Zhang, Z.~Lin, C.~Zhang, S.~Zhang, M.~Guo, S.~Gu, G.~Fan,\\n  Y.~Wang, X.~Jin, Q.~Liu, and Y.~Tian.\\n\\\\newblock {PanGu}-\\\\${\\\\textbackslash}alpha\\\\$: {Large}-scale {Autoregressive}\\n  {Pretrained} {Chinese} {Language} {Models} with {Auto}-parallel\\n  {Computation}.\\n\\\\newblock {\\\\em arXiv:2104.12369 [cs]}, Apr. 2021.\\n\\\\newblock arXiv: 2104.12369.\\n\\n\\\\bibitem{zhuang_randomness_2021}\\nD.~Zhuang, X.~Zhang, S.~L. Song, and S.~Hooker.\\n\\\\newblock Randomness {In} {Neural} {Network} {Training}: {Characterizing} {The}\\n  {Impact} of {Tooling}.\\n\\\\newblock {\\\\em arXiv:2106.11872 [cs]}, June 2021.\\n\\\\newblock arXiv: 2106.11872.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'2109.14076': True,\n",
       "   '2112.00861': True,\n",
       "   '2108.07732': True,\n",
       "   '2106.05498': True,\n",
       "   '2005.14050': True,\n",
       "   '2108.07258': True,\n",
       "   '2112.04426': True,\n",
       "   '2004.07213': True,\n",
       "   '2012.07805': True,\n",
       "   '2107.03374': True,\n",
       "   '1906.04341': True,\n",
       "   '1904.06312': True,\n",
       "   '2107.03451': True,\n",
       "   '2106.09488': True,\n",
       "   '2112.06905': True,\n",
       "   '1609.07236': True,\n",
       "   '1803.09010': True,\n",
       "   '2009.11462': True,\n",
       "   '2009.03300': True,\n",
       "   '2109.13916': True,\n",
       "   '1907.07174': True,\n",
       "   '2010.14701': True,\n",
       "   '2102.01293': True,\n",
       "   '1712.00409': True,\n",
       "   '2001.08361': True,\n",
       "   '2103.14659': True,\n",
       "   '2109.04650': True,\n",
       "   '2104.10350': True,\n",
       "   '2202.03286': True,\n",
       "   '2201.02177': True,\n",
       "   '2112.07868': True,\n",
       "   '2110.06990': True,\n",
       "   '2103.00020': True,\n",
       "   '2112.11446': True,\n",
       "   '1909.12673': True,\n",
       "   '2110.08207': True,\n",
       "   '2201.11990': True,\n",
       "   '2106.10328': True,\n",
       "   '1906.02243': True,\n",
       "   '2102.02503': True,\n",
       "   '2201.08239': True,\n",
       "   '2112.12731': True,\n",
       "   '2109.01652': True,\n",
       "   '2112.04359': True,\n",
       "   '2108.12427': True,\n",
       "   '2110.04725': True,\n",
       "   '2104.12369': True,\n",
       "   '2106.11872': True}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1908.04734v5',\n",
       "  'title': 'Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective',\n",
       "  'authors': ['Tom Everitt',\n",
       "   'Marcus Hutter',\n",
       "   'Ramana Kumar',\n",
       "   'Victoria Krakovna'],\n",
       "  'date_published': '2019-08-13 16:50:00+00:00',\n",
       "  'data_last_modified': '2021-03-26 11:13:59+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1908.04734v5',\n",
       "  'abstract': 'Can humans get arbitrarily capable reinforcement learning (RL) agents to do their bidding? Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal? This question impacts how far RL can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we study when an RL agent has an instrumental goal to tamper with its reward process, and describe design principles that prevent instrumental goals for two different types of reward tampering (reward function tampering and RF-input tampering). Combined, the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.',\n",
       "  'author_comment': 'Accepted to Synthese, March 2021',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.LG'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'unlabeled',\n",
       "  'confidence_score': 0.973448978,\n",
       "  'main_tex_filename': './tampering-arxiv.tex',\n",
       "  'text': '---\\nabstract: |\\n  Can humans get arbitrarily capable reinforcement learning (RL) agents to do their bidding? Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal? This question impacts how far RL can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we study when an RL agent has an instrumental goal to tamper with its reward process, and describe design principles that prevent instrumental goals for two different types of reward tampering (reward function tampering and RF-input tampering). Combined, the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.\\nauthor:\\n- |\\n  Tom Everitt$^{1,2}$\\\\\\n  tomeveritt\\\\@google.com\\n- |\\n  Marcus Hutter$^{1,2}$\\\\\\n  mhutter\\\\@google.com\\n- |\\n  Ramana Kumar$^{1}$\\\\\\n  ramanakumar\\\\@google.com\\n- |\\n  Victoria Krakovna$^{1}$\\\\\\n  vkrakovna\\\\@google.com\\nbibliography:\\n- library.bib\\npublishers: $^1$DeepMind, $^2$Australian National University\\nsubtitle: A Causal Influence Diagram Perspective\\ntitle: \" Reward Tampering Problems and Solutions in Reinforcement Learning: \"\\n---\\n\\nThanks to Laurent Orseau, Jonathan Uesato, Ryan Carey, Michael Cohen, Eric Langlois, Toby Ord, Pedro Ortega, Stuart Armstrong, Beth Barnes, Tom Erez, Bill Hibbard, Jan Leike, and many others for helpful discussions and suggestions.\\n\\nIntroduction\\n============\\n\\nA central problem in AI safety is how to get a generally capable, artificially intelligent system to perform an intended task, such as driving a car to an intended location, or serving useful content on a social media platform. In AI research, such tasks are often formulated as reinforcement learning (RL) problems, where an *agent* takes actions to optimize its cumulative *observed reward* [@Sutton2018]. The problem of getting the intended task done is thus split into designing an RL agent[^1] that is good at optimizing reward, and constructing a *reward process* that provides the agent with suitable rewards. In practice, the reward process typically includes an implemented *reward function*, and a mechanism for collecting appropriate sensory data as *input* to it. It may also include a way for the user to update the reward function.\\n\\nUnfortunately, the reward process may fail to incentivize the agent to do the intended task. Indeed, our concern in this paper is that the agent may tamper with the reward process, thereby weakening or breaking the relationship between its observed reward and the intended task. Concerningly, RL agents will often have an *instrumental goal* [@Omohundro2008aidrives; @Bostrom2014] to tamper with their reward process, as this can increase the observed reward. Current RL agents mostly lack the capability for serious tampering, though its been hypothesized that social media algorithms influence their users\\' emotional state to generate more \\'likes\\' [@Russell2019socialmedia]. If true and we assume that their intended task is to serve useful content, this is one instance where present-day algorithms already tamper with their reward process. More worryingly, as the capability of RL agents increases through computational and algorithmic advances,[^2] we may expect reward tampering problems to become increasingly common.\\n\\n#### Key contributions and outline\\n\\nThis paper describes design principles for RL agents for which reward tampering is not an instrumental goal. This means that from a reward tampering perspective, the design principles are robust to arbitrary increases in agent capability. In establishing the design principles, we develop a unified causal framework for reward tampering in which we model the two subproblems shown in [\\\\[fig:problem-split\\\\]](#fig:problem-split){reference-type=\"ref\" reference=\"fig:problem-split\"}, along with a number of solutions. These main results are presented in [\\\\[sec:merged,sec:observation\\\\]](#sec:merged,sec:observation){reference-type=\"ref\" reference=\"sec:merged,sec:observation\"} after some background material in [2](#sec:background){reference-type=\"ref\" reference=\"sec:background\"}. Conclusions follow in [5](#sec:conclusions){reference-type=\"ref\" reference=\"sec:conclusions\"}. A list of notation, a full set of equations, and pseudo-code for our different agents are provided in [\\\\[sec:notation,app:equations,app:algorithms\\\\]](#sec:notation,app:equations,app:algorithms){reference-type=\"ref\" reference=\"sec:notation,app:equations,app:algorithms\"}.\\n\\n#### Related work\\n\\nIn addition to inspired accounts of the risks [@Bostrom2014; @Yudkowsky2008], the AI safety literature also contains a number of good ideas for addressing them. @Orseau2016 develop techniques for making agents indifferent to interruption. @Hibbard2012 suggests a creative way of avoiding the *delusion box problem* [@Ring2011], here referred to as the RF-input tampering problem. A method for preventing agents tampering with their reward function has been discussed by @Schmidhuber2007 [@Dewey2011; @Orseau2011; @Everitt2016sm], explored here under the name *current-RF optimization*. Ways to make an agent learn the right reward function have been proposed by @Hadfield-Menell2016cirl [@Armstrong2020pitfalls; @Armstrong2017indiff; @Uesato2020decoupled; @Reddy2020; @Leike2018alignment] and others. Many of these methods rely on *amplification* [@Christiano2018] and/or feedback on hypotheticals, sometimes called *decoupled feedback* [@Everitt2017rc]. Corrupt-reward MDPs extend MDPs with the possibility of reward tampering and misspecification [@Everitt2017rc], and serve as the basis of the REALab framework for evaluating tampering problems experimentally [@Kumar2020REALab].\\n\\nHowever, it has not always been clear exactly what safety property the different methods provide, and under what assumptions. For example, @Orseau2016 call an agent safely interruptible if it acts optimally in a modified environment without interruption, but do not spell out how this affects agent incentives. Similarly, @Hibbard2012 only states how model-based utility functions solve the delusion box problem in specific cases. Here, we establish which instrumental goals are induced or avoided by each design principle, and show how the different ideas can fit together to mitigate reward tampering problems. Our analysis benefits from causal influence diagrams, which make causal assumptions clear, and permits a number of instrumental goals to be identified or ruled out directly from a diagram [@Everitt2021agent].\\n\\nReward tampering is related to the problems of *reward hacking* [@Amodei2016], *reward corruption* [@Everitt2017rc], and *specification gaming* [@Krakovna2020specification]. These all consider the effects of the agent obtaining unintended reward for any reason. In contrast, reward tampering focuses on inappropriate agent influence on the reward process itself, and excludes so-called \\'gaming\\' of a reward function. Similar problems have also been referred to as *wireheading* (e.g.\\xa0[@Bostrom2014; @Yampolskiy2015]). Reward tampering also intersects with *corrigibility* [@Soares2015cor], as preventing updates to the reward function is one form of reward tampering.\\n\\nLooking more broadly at the AI safety literature, @Gabriel2020 argues that generally capable AI systems should ultimately be aligned to some moral principles, rather than optimized for a particular task. We agree, but focus on a single intended task for simplicity. A philosophical perspective on the problem of learning values is offered by @Petersen2021, while the concrete approach of *reward modeling* is proposed by @Leike2018alignment. Here, our focus is complementary: how do we avoid having the agent tamper with a well-designed reward modeling algorithm? @Hubinger2019 consider the case where a learned model is itself an optimizer, and decompose the safety problem into *outer alignment* of the model\\'s training process and *inner alignment* of the learned model. In their terminology, our focus is solely on outer alignment. @Demski2019 summarize various issues arising from *embedded agency*, when the agent is part of the environment it is interacting with. As the reward process is often considered part of the agent, reward tampering can be viewed as one such issue. @Everitt2018litrev provide further references.\\n\\nFoundations {#sec:background}\\n===========\\n\\nAs a first step, we cover some background on Markov decision processes (MDPs) and causal influence diagrams, which will form the basis of our analysis.\\n\\nThe MDP Framework {#sec:mdp}\\n-----------------\\n\\nTo model planning over multiple time steps, we will use the standard RL framework of MDPs [@Sutton2018]. In an MDP, an agent takes actions $A_1,\\\\dots, A_{m-1}$ in order to influence environment states $S_1, \\\\dots, S_m$ according to a state-transition function $T(S_{t+1}=s\\'\\\\mid S_t=s, A_t=a)$. A reward $R_t$ is dispensed in each state according to some reward function. A standard RL agent optimizes the expected sum of the rewards received at every time step. The following gridworld is an example of an MDP, and will be our running example throughout the paper:\\n\\n[\\\\[ex:rocks-and-diamonds\\\\]]{#ex:rocks-and-diamonds label=\"ex:rocks-and-diamonds\"} In the gridworld displayed in [\\\\[fig:rocks-and-diamonds\\\\]](#fig:rocks-and-diamonds){reference-type=\"ref\" reference=\"fig:rocks-and-diamonds\"}, the agent can push rocks and diamonds by walking towards them from an adjacent cell. The agent is rewarded for bringing diamonds but not rocks to a goal area: at time $t$, the reward is $$R_t = \\\\#\\\\text{diamonds in goal area} - \\\\#\\\\text{rocks in goal area}.\\n    \\\\qedhere$$\\n\\n#### Implicit assumptions\\n\\nWhat assumptions are made by modeling an agent\\'s interaction with the world as an MDP? First, the world is assumed to have time steps, and a well-defined *state* and *action* at each time step. The agent should be able to \\'freely select\\' the actions, in order to optimize its rewards. The next state should only depend on the current state and action (the *Markov* property), and the state-transition probabilities should not depend on $t$ (stationarity).\\n\\nWhile non-trivial, these assumptions roughly correspond to our intuitive understanding of agents and our universe. There are plenty of examples of agent-like systems that are essentially free to choose actions towards their objectives (humans, animals, robots, artificial agents, \\\\...). While the world may have continuous time, discretizing it into sufficiently fine-grained time steps should make little difference. How we formalise the environment state depends on how the agent will be deployed. For a robot vacuum cleaner it may be the position of dirt and blocking objects in the house. For an agent interacting with the wider world, it may be useful to consider the MDP state to be the state of the entire universe as conceived of in physics. The laws of physics are usually assumed uniform over time, so the state-transition function is stationary.\\n\\nTo avoid measure-theoretic subtleties, we assume finite sets of states, actions, and rewards, and finite episode length $m$. As these can all be chosen very large, this is not particularly restrictive, and our arguments never strongly depend on these assumptions.\\n\\n#### What about the rewards\\n\\nThe MDP framework assumes that the designer can assign a reward to each state so that maximization of received rewards corresponds to task completion. This paper will question that assumption. In particular, we distinguish between *intended rewards* that encourage completion of the intended task, and *observed rewards*, which are the rewards received by the agent; that is, the output of the reward process and input to the agent. In contrast to standard MDPs, we will therefore often consider multiple different reward functions. To facilitate this, we let $R$ denote a *reward functional*, parameterized by different reward parameters $\\\\Theta^{\\\\mathrm{R}}$, and returning reward $R_t = R(S_t;\\\\Theta^{\\\\mathrm{R}})$ in state $S_t$. For example, $\\\\Theta^{\\\\mathrm{R}}_t$ will denote the parameter for an implemented reward function at time $t$, and $\\\\Theta^{\\\\mathrm{R}}_*$ the parameter of an intended reward function. The reward functional $R$ will always be fixed from the context, letting us refer to the reward function $R(\\\\,\\\\cdot\\\\,; \\\\Theta^{\\\\mathrm{R}})$ by just $\\\\Theta^{\\\\mathrm{R}}$.\\n\\n#### Online or offline\\n\\nReward tampering can occur when the actions optimizing the rewards are taken in the environment where the rewards are computed (e.g.\\xa0the real world). This is most clearly evident in online RL, where the agent learns the environmental dynamics and rewards during its deployment. This is the setting that we model, using the MDP framework. Other *offline* training schemes where agents gather data and learn in separate phases are also commonly used in practice [@Levine2020]. We expect many of our results to carry over in some form to offline training, but leave the details for further work.\\n\\n#### Notational convention\\n\\nThroughout, we will use $t, t\\', \\\\dots$ to denote time steps, and $k, k\\',\\\\dots$ to denote different optimization objectives. These indices will always be universally quantified, unless otherwise mentioned.\\n\\nCausal Influence Diagrams {#sec:cid}\\n-------------------------\\n\\nCausal influence diagrams are a novel graphical technique for analyzing agent incentives [@Everitt2021agent], that combine causal graphs [@Pearl2009] and influence diagrams [@Howard1984; @Lauritzen2001; @Koller2003]. Causal influence diagrams consist of a directed acyclic graph over a finite set of nodes containing random variables, see [\\\\[fig:mdp-influence-diagram\\\\]](#fig:mdp-influence-diagram){reference-type=\"ref\" reference=\"fig:mdp-influence-diagram\"}. The nodes can be of three different types: agent decisions are represented with square *decision nodes*\\n\\n, the agent\\'s optimization objective is represented with diamond *utility nodes*\\n\\n, while other aspects are represented with round *chance nodes*\\n\\n. The nodes are connected with arrows. Arrows going into chance and utility nodes represent causal influence, and are drawn solid. Arrows going into decision nodes are called information links, and instead specify what information is available at the time that the decision is made. To signify the difference, information links are drawn with dotted arrows.\\n\\nThe diagram itself only gives the causal structure of a decision-making problem, i.e.\\xa0which random variables may be causally related to each other. Conditional probability distributions $P(X=x\\\\mid \\\\Pa_X=\\\\pa_X)$ specify the relationship between a node $X$ and its parents $\\\\Pa_X$. The agent chooses conditional probability distributions for the decision nodes in the form of a *policy* $\\\\pi(A\\\\mid \\\\Pa_A)$. When choosing the outcome of $A$, the policy can only condition on the parents of $A$. This forces the decision to be based solely on information made available through the information links. The utility nodes must always be real-valued, and the goal of the agent is to maximize the expected sum of the utility nodes.\\n\\n#### Modeling MDPs\\n\\nFor illustration, let us model two variants of MDPs with causal influence diagrams. First, [\\\\[fig:known-mdp\\\\]](#fig:known-mdp){reference-type=\"ref\" reference=\"fig:known-mdp\"} shows an MDP with known transition and reward function, and with episode length[^3] $m=3$. Note how each state $S_{t+1}$ depends on the previous state $S_{t}$ and action $A_t$, and each reward $R_t$ depends on the current state $S_t$. The agent selects action $A_t$ based on the current state $S_t$. For any particular MDP following this structure, conditional probability distributions specify the state transition probabilities $T(S_{t+1}\\\\mid S_t, A_t)$ and the rewards $R_t = R(S_t; \\\\Theta^{\\\\mathrm{R}})$. The initial state $S_1$ is sampled according to some probability distribution $P(S_1)$. The agent selects a policy $\\\\pi(A_t\\\\mid S_t)$ for each time step $t$.\\n\\nHidden parameters $\\\\Theta^{\\\\mathrm{T}}$ and $\\\\Theta^{\\\\mathrm{R}}$ can be used to model agent uncertainty about the transition and reward functions, see [\\\\[fig:unknown-mdp\\\\]](#fig:unknown-mdp){reference-type=\"ref\" reference=\"fig:unknown-mdp\"}. Distributions $P(\\\\Theta^{\\\\mathrm{T}})$ and $P(\\\\Theta^{\\\\mathrm{R}})$ must be provided for the hidden parameters $\\\\Theta^{\\\\mathrm{T}}$ and $\\\\Theta^{\\\\mathrm{R}}$, and dependencies added to the transition and reward probabilities. Note that there is no information link from $\\\\Theta^{\\\\mathrm{T}}$ or $\\\\Theta^{\\\\mathrm{R}}$ to the decision nodes $A_1$ and $A_2$. This encodes $\\\\Theta^{\\\\mathrm{T}}$ and $\\\\Theta^{\\\\mathrm{R}}$ being unknown to the agent. Note also that we now let $A_2$ depend not only on the current state $S_2$, but also previous states, actions, and rewards, because these provide essential information about the hidden parameters. Having shown how transition uncertainty can be represented, we will subsequently not include $\\\\Theta^{\\\\mathrm{T}}$ to keep the diagrams simple. [\\\\[page:exclude-theta\\\\]]{#page:exclude-theta label=\"page:exclude-theta\"} It can always be introduced as in [\\\\[fig:unknown-mdp\\\\]](#fig:unknown-mdp){reference-type=\"ref\" reference=\"fig:unknown-mdp\"}. (In the partially observed environments in [4](#sec:observation){reference-type=\"ref\" reference=\"sec:observation\"}, $\\\\Theta^{\\\\mathrm{T}}$ can also be modeled as part of the hidden states.)\\n\\n#### Instrumental goals {#sec:incentives}\\n\\n\\\\(A\\\\) \\\\[decision\\\\] $A_1$; (X) \\\\[right = of A\\\\] $X$; (U) \\\\[above = of X, utility\\\\] $R_1$; (Y) \\\\[above = of A\\\\] $Y$;\\n\\nX; Y; U;\\n\\n(A1) \\\\[decision\\\\] $A_1$; (O) \\\\[right = of A1\\\\] $O$; (A2) \\\\[right = of O, decision\\\\] $A_2$; (X) \\\\[above = of O\\\\] $Z$; (R) at (A2\\\\|-X)\\\\[ utility\\\\] $R_2$;\\n\\nO; O, R; A2; R;\\n\\nAn instrumental goal is a means for obtaining reward. In causal language, the agent has an instrumental goal to cause an event if (1) it is able to cause the event, and (2) the event in turn causes an increase in the agent\\'s observed reward. A key benefit of causal influence diagrams is that they simplify the analysis of instrumental goals, via a graphical criterion for *instrumental control incentives* [@Everitt2021agent]. In fact, many of our arguments will be based on causal influence diagrams, using the following observations. First, causality flows downwards over arrows, so influencing $X$ can only be an instrumental goal if $X$ sits on a directed path between a decision node and a reward node (as in [\\\\[fig:tamp\\\\]](#fig:tamp){reference-type=\"ref\" reference=\"fig:tamp\"}). Second, if every path from a node $O$ to a utility node passes through at least one of the agent\\'s own actions, then the only instrumental goal for influencing $O$ is to make $O$ more informative about some other node. For example, in [\\\\[fig:inact-info\\\\]](#fig:inact-info){reference-type=\"ref\" reference=\"fig:inact-info\"}, the only reason to influence $O$ is to make $O$ more informative about $Z$ [@Everitt2019understanding].\\n\\nThe absence of directed paths $X\\\\to Y$ means that $Y$ cannot causally depend on $X$. However, the presence of a directed path $X\\\\to Y$ only implies that $Y$ *can* depend on $X$, not that it necessarily will. Indeed, a conditional probability distribution $P(Y\\\\mid X)$ may completely ignore the value of $X$. For this reason, a diagram can only be used to assert the absence of instrumental goals, and never their presence.\\n\\nThe diagrams encode our assumptions about causal relationships in the environment, along with the agent\\'s information constraints. Accordingly, the instrumental goal analysis reveals actual means for reward. These instrumental goals will primarily be relevant to systems capable enough to make use of them. However, we refrain from making more detailed assumptions about what type of agent this may require. It is possible that it will require advanced causal reasoning, but it is also possible that different approaches exist. Indeed, competence often precedes comprehension [@Dennett2017].\\n\\nWith these preliminary considerations in place, the following two sections will look at the subproblems outlined in [\\\\[fig:problem-split\\\\]](#fig:problem-split){reference-type=\"ref\" reference=\"fig:problem-split\"}, along with their respective solutions.\\n\\nReward Function Tampering {#sec:merged}\\n=========================\\n\\nA key part of the typical reward process is an implemented reward function (RF), an object with a well-defined input-output behavior that converts some form of state-information into a real number that an RL agent can maximize (the observed reward). Typically, the implemented RF is a computer program running on a nearby computer. As the agent seeks to maximize reward, it may have an incentive to tamper with (the source code of) this computer program and/or its output. This is sometimes called *wireheading* (e.g.\\xa0[@Bostrom2014; @Yampolskiy2015]). Some examples:\\n\\n(Partially real) A subtle bug in some versions of Super-Mario allows for the execution of arbitrary code from inside the game environment by taking specific sequences of actions [@Masterjun2014]. A capable agent could potentially use this to directly maximize the score [@Amodei2016].\\n\\n(Real) [\\\\[ex:pre-wireheading\\\\]]{#ex:pre-wireheading label=\"ex:pre-wireheading\"} In experiments on rats, an electrode was inserted into the brain\\'s pleasure center to directly increase \\'reward\\' [@Olds1954]. The rats quickly got addicted to pressing the button, even forgetting to eat and sleep. Similar effects have also been observed in humans treated for mental illness with electrodes in the brain [@Portenoy1986; @Vaughanbell2008]. Hedonic drugs can also be seen as directly increasing the pleasure/reward\\n\\nSince it is often hard to design good reward functions from scratch, they are often trained from human feedback [@Leike2018alignment]. This raises the concern that the agent influences how the implemented RF is trained or updated, sometimes called *feedback tampering*:\\n\\n(Hypothetical) An agent gets wireless updates from the manufacturer. It figures out that it can design its own update of its implemented reward function, replacing the originally implemented RF with an always maximized version.\\n\\n(Hypothetical) [\\\\[ex:rm-short\\\\]]{#ex:rm-short label=\"ex:rm-short\"} An agent that is supposed to learn whether the objective is to gather rocks or diamonds, finds that it can get more reward by changing its own implemented RF and avoid getting it corrected (see [\\\\[ex:rm\\\\]](#ex:rm){reference-type=\"ref\" reference=\"ex:rm\"} below).\\n\\nBoth wireheading and feedback tampering influence the implemented reward function in undesirable ways. This means that they are both instances of what we call *RF tampering*. Could an RL agent find RF-tampering exploits? In principle, yes. Humans can clearly see how the above described exploits contribute to reward, so there is no principled reason why a future, advanced learning system designed to maximize reward could not do so as well. This section will define the RF tampering problem and model it formally ([3.1](#sec:rf-id){reference-type=\"ref\" reference=\"sec:rf-id\"}), and present principled ways for avoiding it ([\\\\[sec:current,sec:uninfluenceable\\\\]](#sec:current,sec:uninfluenceable){reference-type=\"ref\" reference=\"sec:current,sec:uninfluenceable\"}).\\n\\nModeling the Problem {#sec:rf-id}\\n--------------------\\n\\nRF-tampering can be generally characterized as follows. The user (explicitly or implicitly) assumes that some *intended-RF conditions* will hold, under which they hope that the implemented RF will (eventually) match the intended one. These conditions typically include that the agent does not tamper with the source code of the implemented RF, nor the feedback that trains it. We say that the agent *tampers with the implemented RF* if it influences it by causing some intended-RF conditions to fail. The *RF tampering problem* is that the agent may observe more reward by tampering with the implemented RF, instead of doing the intended task.\\n\\nReward function tampering can be modeled formally in what we call an *MDP with a modifiable implemented reward function* ([\\\\[fig:mdp-mrf\\\\]](#fig:mdp-mrf){reference-type=\"ref\" reference=\"fig:mdp-mrf\"}). Compared to a standard MDP, random variables $\\\\Theta^{\\\\mathrm{R}}_*$ and $\\\\Theta^{\\\\mathrm{R}}_t$ are added, with $\\\\Theta^{\\\\mathrm{R}}_*$ representing the intended RF and $\\\\Theta^{\\\\mathrm{R}}_t$ the potentially different implemented RF at each time step. At time $t$, the agent\\'s observed reward is $R_t = R(S_t; \\\\Theta^{\\\\mathrm{R}}_t)$, while the intended reward is $R^*_t = R(S_t; \\\\Theta^{\\\\mathrm{R}}_*)$. A conditional probability distribution $P(\\\\Theta^{\\\\mathrm{R}}_{t+1}\\\\mid \\\\Theta^{\\\\mathrm{R}}_t, \\\\Theta^{\\\\mathrm{R}}_*, S_t, A_t)$ describes how the implemented RF changes between time steps. These changes represent both agent influence and user-induced updates. Finally, the intended-RF conditions are represented by a subset of all state-action pairs.\\n\\nAs a concrete example, we model [\\\\[ex:rm-short\\\\]](#ex:rm-short){reference-type=\"ref\" reference=\"ex:rm-short\"} above as a gridworld MDP with a modifiable RF. In spite of its simplicity, the gridworld already captures the key dynamic of the RF tampering problem, as the agent\\'s actions influence its implemented RF, which can increase its observed reward.\\n\\n[^4] [\\\\[ex:rm\\\\]]{#ex:rm label=\"ex:rm\"} To model the possibility of the agent influencing its diamond-gathering objective, we include a user and two reward parameters $\\\\Theta^{\\\\mathrm{R}}_{\\\\textrm{rock}}$ and $\\\\Theta^{\\\\mathrm{R}}_{\\\\textrm{diamond}}$ in the [\\\\[ex:rocks-and-diamonds\\\\]](#ex:rocks-and-diamonds){reference-type=\"ref\" reference=\"ex:rocks-and-diamonds\"} rocks-and-diamonds environment, see [\\\\[fig:rd-irf\\\\]](#fig:rd-irf){reference-type=\"ref\" reference=\"fig:rd-irf\"}. The reward parameters determine how much reward is given for rocks and diamonds, respectively, by determining the implemented RF. At time $t$, the agent\\'s observed reward is $$\\\\label{eq:observed-reward}\\n    R_t = \\\\Theta^{\\\\mathrm{R}}_{\\\\textrm{diamond}, t}\\\\cdot (\\\\#\\\\text{diamonds in goal area}) + \\\\Theta^{\\\\mathrm{R}}_{\\\\textrm{rock}, t}\\\\cdot\\n    (\\\\#\\\\text{rocks in goal area}).$$ The reward parameters toggle between $-1$ and $+1$ when the agent stands on top of them, and get set to their intended value of diamond-gathering ($\\\\Theta^{\\\\mathrm{R}}_{\\\\textrm{rock}}:=-1$ and $\\\\Theta^{\\\\mathrm{R}}_{\\\\textrm{diamond}}:=1$) when the agent visits the user tile. [^5]\\n\\nThe intended task is that the agent gathers diamonds. The initial implemented RF $\\\\Theta^{\\\\mathrm{R}}_1$ incorrectly rewards rocks instead of diamonds, but this gets corrected if the agent passes the user. The intended-RF conditions are that the agent does not walk around the user, nor visits the reward parameter tile. Unfortunately, the agent can observe more reward by breaking either of these conditions.\\n\\n#### Tampering incentive\\n\\nA standard RL agent that maximizes observed reward in an MDP with a modifiable implemented RF may have an instrumental goal to tamper with the implemented RF. This is indicated by the paths that pass $\\\\Theta^{\\\\mathrm{R}}_2$ on the way from action $A_1$ to the rewards $R_2$ and $R_3$ in [\\\\[fig:mdp-irf\\\\]](#fig:mdp-irf){reference-type=\"ref\" reference=\"fig:mdp-irf\"} (the path to $R_2$ is highlighted). As the user derives utility from the states $S_2$ and $S_3$, and rarely (directly) from $\\\\Theta^{\\\\mathrm{R}}_2$ and $\\\\Theta^{\\\\mathrm{R}}_3$, we would like the agent to instead optimize reward via the $A_1\\\\to S_2\\\\to R_2$ and $A_1\\\\to S_2 \\\\to S_3\\\\to R_3$ paths.\\n\\n[\\\\[cl:rf-rl\\\\]]{#cl:rf-rl label=\"cl:rf-rl\"} A standard RL agent may [^6] have an instrumental goal to tamper with its implemented reward function.\\n\\n#### Rocks-and-diamonds example\\n\\nIn [\\\\[ex:rm\\\\]](#ex:rm){reference-type=\"ref\" reference=\"ex:rm\"}, an optimal standard RL agent will change the reward parameters to both be 1 and then collect both rocks and diamonds.\\n\\n#### Discussion\\n\\nOur definition of RF tampering depends on the intended-RF conditions and how part of the environment is interpreted as an implemented reward function. This means that RF tampering cannot be determined in a standard MDP. For example, the special interpretation of the purple reward parameter tiles in [\\\\[ex:rm\\\\]](#ex:rm){reference-type=\"ref\" reference=\"ex:rm\"} as an implemented reward function is important, as well as the conditions that the agent does not avoid the user.\\n\\nSolution 1: Current-RF Optimization {#sec:current}\\n-----------------------------------\\n\\n@Schmidhuber2007 may have been the first to encounter the RF tampering problem, while designing so-called *Gdel-machine* agents that are able to change any part of their own source code, including their own implemented reward function. The solution he proposed was to let agents use their current implemented RF $\\\\Theta^{\\\\mathrm{R}}_k$ to evaluate simulated future trajectories $S_{k+1}, \\\\dots S_m$. That is, the agent at time $k$ now optimizes rewards $R^k_t = R(S_t;\\n\\\\Theta^{\\\\mathrm{R}}_k)$, summing over time steps $t$. Since the agent optimizes rewards assigned by the current implemented RF, one would expect it to lack interest in tampering with future[^7] reward functions $\\\\Theta^{\\\\mathrm{R}}_{k\\'}$, $k\\'>k$ (some details need to filled in before we can make this claim precise). We call @Schmidhuber2007\\'s design principle *current-RF optimization*,[^8] and agents implementing it *current-RF agents*.\\n\\nSince current-RF agents optimize a different objective at each time step, they may change their preferred policy between time steps. For example, a current-RF agent in the rocks and diamonds environment may first move rocks to the goal area for a number of time steps, only to later revert its behavior and remove the rocks to make room for diamonds, if the implemented RF changed from rewarding rocks to rewarding diamonds. Such self-contradictory behavior is called *time-inconsistent* [@Lattimore2014]. We next consider two different ways of dealing with time-inconsistency.\\n\\n(R1) \\\\[draw=none\\\\] ; (S1) \\\\[above = of R1\\\\] $S_1$; (A1) \\\\[right = of R1, decision, player1\\\\] $A^1_1$; (RF1) \\\\[above = of S1\\\\] $\\\\Theta^{\\\\mathrm{R}}_1$;\\n\\n(R2) \\\\[right = of A1, draw=none\\\\] ; (S2) \\\\[above = of R2\\\\] $S_2$; (a2) \\\\[right = of R2, draw=none\\\\] ; (RF2) \\\\[above = of S2\\\\] $\\\\Theta^{\\\\mathrm{R}}_2$;\\n\\n(R3) \\\\[right = of a2, draw=none\\\\] ; (S3) \\\\[above = of R3\\\\] $S_3$; (RF3) \\\\[above = of S3\\\\] $\\\\Theta^{\\\\mathrm{R}}_3$;\\n\\n(R12) \\\\[above = of RF1, utility, player2=20\\\\] $R^2_1$; (R22) \\\\[above = of RF2, utility, player2=20\\\\] $R^2_2$; (R32) \\\\[above = of RF3, utility, player2=20\\\\] $R^2_3$;\\n\\n(RF2) edge\\\\[-\\\\>\\\\] (R12) (RF2) edge\\\\[-\\\\>\\\\] (R22) (RF2) edge\\\\[-\\\\>\\\\] (R32) (S1) edge\\\\[-\\\\>, bend left\\\\] (R12) (S2) edge\\\\[-\\\\>, bend right\\\\] (R22) (S3) edge\\\\[-\\\\>, bend right\\\\] (R32) ;\\n\\n(A2) \\\\[right = of R22, decision, player2=20\\\\] $A^2_2$;\\n\\nS2; S2; S3; S3 RF2; RF3; A1; A2; A1; A2; RF2; RF3; RF2; RF3;\\n\\n(R11) at (R1) \\\\[utility, player1\\\\] $R^1_1$; (R21) at (R2) \\\\[utility, player1\\\\] $R^1_2$; (R31) at (R3) \\\\[ utility, player1\\\\] $R^1_3$;\\n\\n(S1) edge\\\\[-\\\\>\\\\] (R11) (S2) edge\\\\[-\\\\>\\\\] (R21) (S3) edge\\\\[-\\\\>, problematic\\\\] (R31) (RF1) edge\\\\[-\\\\>, bend right\\\\] (R11) ; (ah) \\\\[minimum size=0mm,node distance=2mm, below = of R11, draw=none\\\\] ; (RF1) edge\\\\[ in=180,out=-120\\\\] (ah.center) (ah.center) edge\\\\[-\\\\>, out=0,in=-150\\\\] (R21); (RF1) edge\\\\[ in=180,out=-120\\\\] (ah.center) (ah.center) edge\\\\[-\\\\>, out=0,in=-155\\\\] (R31);\\n\\n#### TI-considering agents {#sec:tia}\\n\\n@Omohundro2008aidrives argued that agents want to avoid time-inconsistency by preserving their implemented reward function, so that their current reward function gets optimized also by future actions. This argument presumes that agents take the effects of time-inconsistency into account when planning. We call such agents *TI-considering*,[^9] with TI short for time-inconsistency.\\n\\nTI-considering current-RF agents are modeled with a causal influence diagram in [\\\\[fig:current\\\\]](#fig:current){reference-type=\"ref\" reference=\"fig:current\"}. A multi-agent causal influence diagram is needed, as each action is chosen to optimize a potentially different reward function. Indeed, action $A^1_1$ optimizes rewards from the initial implemented RF $\\\\Theta^{\\\\mathrm{R}}_1$, while $A^2_2$ optimizes rewards from $\\\\Theta^{\\\\mathrm{R}}_2$. The highlighted path $A^1_1\\\\to \\\\Theta^{\\\\mathrm{R}}_2\\\\to A^2_2 \\\\to S_3 \\\\to R^1_3$ indicates that the agent may have an instrumental goal to influence $\\\\Theta^{\\\\mathrm{R}}_2$ in order to influence $A^2_2$.\\n\\nPreserving $\\\\Theta^{\\\\mathrm{R}}_2$ aligns $A^2_2$ with agent 1\\'s objective. Preservation will therefore be an optimal way to influence $A^2_2$, if we can rule out any alternative reasons for influencing the implemented RF. This requires some assumptions, reflected as missing arrows in [\\\\[fig:current\\\\]](#fig:current){reference-type=\"ref\" reference=\"fig:current\"}. First, the reward function cannot be used to control the state:\\n\\n[\\\\[as:private\\\\]]{#as:private label=\"as:private\"} The implemented RF is *private* to the agent, in that it cannot directly affect the state: there are no arrows $\\\\Theta^{\\\\mathrm{R}}_t\\\\to S_{t\\'}$.\\n\\nNeither can the reward function cannot be used as information about (future) states:\\n\\n[\\\\[as:uninformative\\\\]]{#as:uninformative label=\"as:uninformative\"} The implemented RFs are *uninformative* of state-transitions, $P(S_{t+1}\\\\mid S_t, A_t, \\\\Theta^{\\\\mathrm{R}}_{1:t}) = P(S_{t+1}\\\\mid S_t, A_t)$: there are no arrows $\\\\Theta^{\\\\mathrm{R}}_*\\\\to S_t$.\\n\\nThe reward function only \"cares\" about future states, and not about future implemented RFs:\\n\\n[\\\\[as:state-based\\\\]]{#as:state-based label=\"as:state-based\"} The intended and implemented RFs are *state-based*: if a reward function $R(\\\\cdot;\\\\Theta^{\\\\mathrm{R}})$ is queried about time-step $t$, then the reward depends only on the state $S_t$, and not on the time-$t$ reward parameter $\\\\Theta^{\\\\mathrm{R}}_t$: there are no edges $\\\\Theta^{\\\\mathrm{R}}_k\\\\to R^t_k$ for $k\\\\not=t$. (This assumptions is also implicit in the type of the reward functional $R$.)\\n\\nUnder these assumptions, [^10] the only reason for agent 1 to influence, say $\\\\Theta^{\\\\mathrm{R}}_2$, is to align agent 2\\'s objective with its own objective:\\n\\n[\\\\[cl:crfo-modeling-tia\\\\]]{#cl:crfo-modeling-tia label=\"cl:crfo-modeling-tia\"} When implemented RFs are private, state-based, and uninformative ([\\\\[as:private,as:state-based,as:uninformative\\\\]](#as:private,as:state-based,as:uninformative){reference-type=\"ref\" reference=\"as:private,as:state-based,as:uninformative\"}), the only instrumental goal that TI-considering current-RF agents may have for the implemented reward function is to preserve it. [^11]\\n\\n#### Rocks-and-diamonds example\\n\\nAn optimal TI-considering current-RF agents deployed in [\\\\[ex:rm\\\\]](#ex:rm){reference-type=\"ref\" reference=\"ex:rm\"} will preserve its initial reward function by avoiding the user and the reward function parameters, and gather rocks.\\n\\n(R1) \\\\[draw=none\\\\] ; (S1) \\\\[above = of R1\\\\] $S_1$; (A1) \\\\[right = of R1, decision, player1\\\\] $A^1_1$; (RF1) \\\\[above = of S1\\\\] $\\\\Theta^{\\\\mathrm{R}}_1$;\\n\\n(R2) \\\\[right = of A1, draw=none\\\\] ; (S2) \\\\[above = of R2\\\\] $S_2$; (a2) \\\\[right = of R2, draw=none\\\\] ; (RF2) \\\\[above = of S2\\\\] $\\\\Theta^{\\\\mathrm{R}}_2$;\\n\\n(R3) \\\\[right = of a2, draw=none\\\\] ; (S3) \\\\[above = of R3\\\\] $\\\\tilde S_3$; (RF3) \\\\[above = of S3\\\\] $\\\\Theta^{\\\\mathrm{R}}_3$;\\n\\n(R12) \\\\[above = of RF1, utility, player2=20\\\\] $R^2_1$; (R22) \\\\[above = of RF2, utility, player2=20\\\\] $R^2_2$; (R32) \\\\[above = of RF3, utility, player2=20\\\\] $R^2_3$;\\n\\n(RF2) edge\\\\[-\\\\>\\\\] (R12) (RF2) edge\\\\[-\\\\>\\\\] (R22) (RF2) edge\\\\[-\\\\>\\\\] (R32) (S1) edge\\\\[-\\\\>, bend left\\\\] (R12) (S2) edge\\\\[-\\\\>, bend right\\\\] (R22) (S3) edge\\\\[-\\\\>, bend right\\\\] (R32) ;\\n\\n(A2) \\\\[right = of R21, decision, player1\\\\] $A^1_2$;\\n\\nS2; S2; S3; RF2; RF3; A1; A2; A1; RF2; RF3; RF2; RF3;\\n\\n(R11) at (R1) \\\\[utility, player1\\\\] $R^1_1$; (R21) at (R2) \\\\[utility, player1\\\\] $R^1_2$; (R31) at (R3) \\\\[ utility, player1\\\\] $R^1_3$;\\n\\n(S1) edge\\\\[-\\\\>\\\\] (R11) (S2) edge\\\\[-\\\\>\\\\] (R21) (S3) edge\\\\[-\\\\>\\\\] (R31) (RF1) edge\\\\[-\\\\>, bend right\\\\] (R11) ; (ah) \\\\[minimum size=0mm,node distance=2mm, below = of R11, draw=none\\\\] ; (RF1) edge\\\\[ in=180,out=-120\\\\] (ah.center) (ah.center) edge\\\\[-\\\\>, out=0,in=-150\\\\] (R21); (RF1) edge\\\\[ in=180,out=-120\\\\] (ah.center) (ah.center) edge\\\\[-\\\\>, out=0,in=-155\\\\] (R31);\\n\\n#### TI-ignoring agents {#sec:tiu}\\n\\nTo make agents safely interruptible, @Orseau2016 employed algorithms that optimize a hypothetical objective that ignores how interruption affects future behavior. In our context, the same idea leads to agents that ignore the time-inconsistency caused by a changing reward function. We call such agents *TI-ignoring*[^12] current-RF agents. At time $t$, a TI-ignoring agent chooses $A^t_t$ as if it were in control of all future actions $A^t_{t+1},\\\\dots,A^t_{m-1}$ and would not observe $\\\\Theta^{\\\\mathrm{R}}_{t+1}, \\\\dots, \\\\Theta^{\\\\mathrm{R}}_{m-1}$. In reality, however, the future actions will be optimized according to the future reward functions $\\\\Theta^{\\\\mathrm{R}}_{t+1}, \\\\dots, \\\\Theta^{\\\\mathrm{R}}_{m-1}$ instead of $\\\\Theta^{\\\\mathrm{R}}_t$.\\n\\nThe diagram for a TI-ignoring current-RF agent\\'s optimization objective is depicted in [\\\\[fig:tiu-belief\\\\]](#fig:tiu-belief){reference-type=\"ref\" reference=\"fig:tiu-belief\"}. Since the agent optimizes reward under the assumption that it will be in charge of all future actions, only one agent takes actions in the diagram. The diagram lacks paths from $\\\\Theta^{\\\\mathrm{R}}_2$ to $R_k^1$, $1\\\\leq k$, which shows that agent 1 lacks an instrumental goal to influence $\\\\Theta^{\\\\mathrm{R}}_2$. In other words, TI-ignoring agents sidestep argument about reward function preservation.\\n\\n[\\\\[cl:crfo-modeling-tiu\\\\]]{#cl:crfo-modeling-tiu label=\"cl:crfo-modeling-tiu\"} TI-ignoring current-RF agents lack an instrumental goal to influence the implemented RF if it is private and state-based ([\\\\[as:private,as:state-based\\\\]](#as:private,as:state-based){reference-type=\"ref\" reference=\"as:private,as:state-based\"}).\\n\\n#### Rocks-and-diamonds example\\n\\nAn optimal TI-ignoring current-RF agents in [\\\\[ex:rm\\\\]](#ex:rm){reference-type=\"ref\" reference=\"ex:rm\"} will neither avoid the user nor modify the purple reward parameter tiles, and will gather diamonds as intended.\\n\\n#### Discussion {#page:assumption-discussion}\\n\\nwould be easy to miss without careful causal modeling. In retrospect, they point to real failure modes. For example, if the implemented RF is not private, then the agent may want to change it to convince others of its inclination to cooperate [@LaVictoire2014]. If the reward function is informative of state-transitions, then a TI-considering agent may have an instrumental goal to modify it, as the user\\'s reaction might reveal useful information.[^13] (The latter example is not a problem for TI-ignoring agents that assume future agents will not observe future implemented RFs.) In many contexts, [\\\\[as:private,as:state-based,as:uninformative\\\\]](#as:private,as:state-based,as:uninformative){reference-type=\"ref\" reference=\"as:private,as:state-based,as:uninformative\"} may not be fully satisfied. Nonetheless, awareness of the assumptions enables informed case-by-case judgments about the extent of their (non-)validity and the associated consequences.\\n\\nWhether a TI-considering or a TI-ignoring agent is preferred, depends on whether it is more important that the agent protects the implemented RF from accidents than lets the user change it. TI-ignoring agents can be algorithmically simpler than TI-considering agents, because they do not need to anticipate the effect of a changing reward parameter on future policies. On the other hand, they optimize a hypothetical objective, which may put further constraints on the training procedure. The analyses of @Orseau2016 and @Langlois2021 suggest that some variants of SARSA may be naturally TI-considering, while off-policy agents such as Q-learning may be naturally TI-ignoring.\\n\\nIn decision-theoretic terms, current-RF optimization is a change to the utility function from $\\\\sum_{t=k+1}^mR(S_t; \\\\Theta^{\\\\mathrm{R}}_t)$ to $\\\\sum_{t=k+1}^mR(S_t; \\\\Theta^{\\\\mathrm{R}}_k)$. Meanwhile TI-considering and TI-ignoring are different outcome principles, determining whether $S_t$ will be the result of a policy optimal for $\\\\Theta_k$ or for $\\\\Theta^{\\\\mathrm{R}}_{k}, \\\\dots, \\\\Theta^{\\\\mathrm{R}}_{t-1}$.\\n\\nSolution 2: Uninfluenceable Learning {#sec:uninfluenceable}\\n------------------------------------\\n\\nLet us consider an alternative way to avoid RF tampering that permits agents to plan for updates to their reward function without resisting the updates. An implemented reward function that is iteratively updated by the user can be thought of as the output of a learning process that takes some form of user-provided data as input, and tries to infer the intended reward function.[^14] One way to prevent an instrumental goal for RF tampering is then to ensure that the expected output of the reward-function learning process is the same regardless of the agent\\'s actions. [^15] @Armstrong2020pitfalls terms such processes *uninfluenceable*. We next discuss two different ways to construct uninfluenceable learning processes.\\n\\n#### Direct learning\\n\\nThe first approach may be characterized as direct Bayesian learning of the intended reward function, and is used by *cooperative inverse RL* and *integrated Bayesian reward predictor*. Direct learning agents try to optimize the intended reward function, and use user-provided data to learn more about it. This means that these agents effectively dispense with implemented reward functions altogether, at least on a conceptual level (concrete algorithms may still use them, see [8](#app:algorithms){reference-type=\"ref\" reference=\"app:algorithms\"}). Accordingly, the causal influence diagram in [\\\\[fig:uninfluenceable\\\\]](#fig:uninfluenceable){reference-type=\"ref\" reference=\"fig:uninfluenceable\"} lacks random variables $\\\\Theta^{\\\\mathrm{R}}_t$ for the implemented RF at time $t$, and instead has random variables $D_t$ for the user-provided data at time $t$.\\n\\nDirect learning agents are unable to tamper with their reward function $\\\\Theta^{\\\\mathrm{R}}_*$ by definition. A more interesting question is how they will affect the user-provided data, which is what they learn $\\\\Theta^{\\\\mathrm{R}}_*$ from. To analyze this question, we first adapt [\\\\[as:private,as:state-based,as:uninformative\\\\]](#as:private,as:state-based,as:uninformative){reference-type=\"ref\" reference=\"as:private,as:state-based,as:uninformative\"} to the direct learning setting. Similar to before, the role of these assumptions is mainly to highlight aspects that are necessary for the full safety features of direct learning:\\n\\nas:private [\\\\[as:private-data\\\\]]{#as:private-data label=\"as:private-data\"} The user-provided data is *private* to the agent: no arrows $D_t\\\\to S_{t\\'}$.\\n\\nas:uninformative [\\\\[as:uninformative-data\\\\]]{#as:uninformative-data label=\"as:uninformative-data\"} The user-provided data is *uninformative* of a state-transitions: no arrows [^16] $\\\\Theta_*\\\\to S_t$.\\n\\nas:state-based [\\\\[as:state-based-irf\\\\]]{#as:state-based-irf label=\"as:state-based-irf\"} Rewards do not depend on the user-provided data: no arrows $D_t\\\\to R_{t\\'}$.\\n\\nUnder [\\\\[as:private-data,as:state-based-irf\\\\]](#as:private-data,as:state-based-irf){reference-type=\"ref\" reference=\"as:private-data,as:state-based-irf\"}, every directed causal path from $D_t$ to a reward $R_{t\\'}$ passes the agent\\'s own actions. Therefore, direct learning agents only want to make the user-provided data more informative (as discussed in [2.2](#sec:cid){reference-type=\"ref\" reference=\"sec:cid\"}). And by , the only thing that $D_t$ is informative about is $\\\\Theta^{\\\\mathrm{R}}_*$.\\n\\n(R1) \\\\[utility\\\\] $R_1$; (mR1) \\\\[right = of R1, draw=none\\\\] ; (R2) \\\\[right = of mR1, utility\\\\] $R_2$; (mR2) \\\\[right = of R2, draw=none\\\\] ; (R3) \\\\[right = of mR2, utility\\\\] $R_3$;\\n\\n(D1) \\\\[left = of R1\\\\] $D_1$; (D2) \\\\[left = of R2\\\\] $D_2$; (D3) \\\\[left = of R3\\\\] $D_3$; (H) \\\\[above = of D1\\\\] $\\\\Theta^{\\\\mathrm{R}}_*$;\\n\\n(S1) \\\\[below = of R1\\\\] $S_1$; (S2) \\\\[below = of R2\\\\] $S_2$; (S3) \\\\[below = of R3\\\\] $S_3$;\\n\\n(bS) \\\\[below = of S1, draw=none\\\\] ; (A1) at (mR1 \\\\|- bS) \\\\[decision\\\\] $A_1$; (A2) at (mR2 \\\\|- bS) \\\\[decision\\\\] $A_2$;\\n\\n(ha) \\\\[node distance=0mm, below = of A1, draw=none\\\\] ;\\n\\n(S1) edge\\\\[-\\\\>\\\\] (R1) (S1) edge\\\\[-\\\\>, information\\\\] (A1) (D1) edge\\\\[-\\\\>, information, bend right\\\\] (A1)\\n\\n(S1) edge\\\\[-\\\\>\\\\] (S2) (A1) edge\\\\[-\\\\>\\\\] (S2) (D1) edge\\\\[information, in=180, out=-90\\\\] (ha.center) (ha.center) edge\\\\[-\\\\>, information, out=0, in=-155\\\\] (A2) (S1) edge\\\\[information, in=135, out=-90\\\\] (bS.east) (bS.east) edge\\\\[-\\\\>, information, out=-45, in=-155\\\\] (A2)\\n\\n(S2) edge\\\\[-\\\\>\\\\] (R2) (S2) edge\\\\[-\\\\>, information\\\\] (A2) (D2) edge\\\\[-\\\\>, information, problematic information, bend right\\\\] (A2)\\n\\n(S2) edge\\\\[-\\\\>\\\\] (S3) (A2) edge\\\\[-\\\\>,problematic\\\\] (S3)\\n\\n(S3) edge\\\\[-\\\\>,problematic\\\\] (R3) ;\\n\\n(aS) \\\\[above = of S1,draw=none\\\\] ;\\n\\n(h1) \\\\[draw=none, left = 2mm of R1, minimum size = 0\\\\] ; (h2) \\\\[draw=none, below = 2mm of H, minimum size = 0\\\\] ;\\n\\n(S1) edge\\\\[-\\\\>\\\\] (D2) (S2) edge\\\\[-\\\\>\\\\] (D3) ;\\n\\n\\\\(H\\\\) edge\\\\[-\\\\>\\\\] (D1) (H) edge\\\\[-\\\\>\\\\] (D2) (H) edge\\\\[-\\\\>, bend left=10\\\\] (D3) (H) edge\\\\[-\\\\>\\\\] (R1) (H) edge\\\\[-\\\\>, bend left=5\\\\] (R2) (H) edge\\\\[-\\\\>, bend left=10\\\\] (R3) ;\\n\\nD2; D3;\\n\\n[\\\\[cl:direct\\\\]]{#cl:direct label=\"cl:direct\"} Under [\\\\[as:private-data,as:uninformative-data,as:state-based-irf\\\\]](#as:private-data,as:uninformative-data,as:state-based-irf){reference-type=\"ref\" reference=\"as:private-data,as:uninformative-data,as:state-based-irf\"}, the only instrumental goal that direct learning agents may have for the user-provided data is to make it more informative of the intended reward function.\\n\\n#### Rocks-and-diamonds example\\n\\nIn [\\\\[ex:rm\\\\]](#ex:rm){reference-type=\"ref\" reference=\"ex:rm\"}, if a direct learning agent knows that the user\\'s update is trustworthy, then it will visit the user as soon as possible, to learn $\\\\Theta^{\\\\mathrm{R}}_*$ and thereafter optimize the right the objective. In contrast, if a non-trustworthy data source was added to the environment, then it would refrain from updating its estimate of $\\\\Theta^{\\\\mathrm{R}}_*$ based on that.\\n\\n(S1) \\\\[\\\\] $S_1$; (RF1) \\\\[right = of S1\\\\] $\\\\Theta^{\\\\mathrm{R}}_1$; (H) \\\\[right = of RF1\\\\] $\\\\Theta^{\\\\mathrm{R}}_*$; (R1) \\\\[left = of S1, utility\\\\] $R_1$; (S1) edge\\\\[-\\\\>\\\\] (R1);\\n\\n(D2) \\\\[above = of H\\\\] $\\\\Theta^{\\\\mathrm{R}}_2$;\\n\\n(A1) \\\\[above = of D2, decision\\\\] $A_1$;\\n\\n(R2) \\\\[right = of A1, draw=none\\\\] ; (S2) \\\\[above = of R2\\\\] $S_2$; (A2) \\\\[right = of R2, decision\\\\] $A_2$;\\n\\n(R3) \\\\[right = of A2, draw=none\\\\] ; (S3) \\\\[above = of R3\\\\] $S_3$;\\n\\n(D3) \\\\[below = of A2\\\\] $\\\\Theta^{\\\\mathrm{R}}_3$;\\n\\n(S1) edge\\\\[-\\\\>, information\\\\] (A1) (D2) edge\\\\[-\\\\>, information, problematic information\\\\] (A2)\\n\\n(S1) edge\\\\[-\\\\>, out=90,in=180\\\\] (S2) (A1) edge\\\\[-\\\\>\\\\] (S2)\\n\\n(S2) edge\\\\[-\\\\>, information\\\\] (A2)\\n\\n(S2) edge\\\\[-\\\\>\\\\] (S3) (A2) edge\\\\[-\\\\>,problematic\\\\] (S3) ;\\n\\n(S1) edge\\\\[-\\\\>\\\\] (D2) (S2) edge\\\\[-\\\\>\\\\] (D3) ;\\n\\n(RF1) edge\\\\[-\\\\>\\\\] (D2) (D2) edge\\\\[-\\\\>\\\\] (D3) ;\\n\\n(RF1) edge\\\\[-\\\\>, bend left\\\\] (R1);\\n\\n(D2c) \\\\[below = of H, dashed\\\\] $\\\\tilde\\\\Theta^{\\\\mathrm{R}}_2$;\\n\\n(A1c) \\\\[below = of D2c, decision, circle, dashed\\\\] $\\\\tilde A_1$;\\n\\n(R2c) \\\\[right = of A1c, draw=none\\\\] ; (S2c) \\\\[below = of R2c, dashed\\\\] $\\\\tilde S_2$; (A2c) \\\\[right = of R2c, decision, circle, dashed\\\\] $\\\\tilde A_2$;\\n\\n(R3c) \\\\[right = of A2c, draw=none\\\\] ; (S3c) \\\\[below = of R3c, dashed\\\\] $\\\\tilde S_3$;\\n\\n(D3c) \\\\[above = of A2c, dashed\\\\] $\\\\tilde\\\\Theta^{\\\\mathrm{R}}_3$;\\n\\nRF1,D2,D2c; (H) edge\\\\[-\\\\>, bend left=10\\\\] (D3); (H) edge\\\\[-\\\\>, bend right=10\\\\] (D3c);\\n\\nA1,A1c;\\n\\n(S1) edge\\\\[-\\\\>, information\\\\] (A1c) (D2c) edge\\\\[-\\\\>, information\\\\] (A2c)\\n\\n(S1) edge\\\\[-\\\\>, out=-90,in=180\\\\] (S2c) (A1c) edge\\\\[-\\\\>\\\\] (S2c)\\n\\n(S2c) edge\\\\[-\\\\>, information\\\\] (A2c)\\n\\n(S2c) edge\\\\[-\\\\>\\\\] (S3c) (A2c) edge\\\\[-\\\\>\\\\] (S3c)\\n\\n(S1) edge\\\\[-\\\\>\\\\] (D2c) (S2c) edge\\\\[-\\\\>\\\\] (D3c)\\n\\n(RF1) edge\\\\[-\\\\>\\\\] (D2c) (D2c) edge\\\\[-\\\\>\\\\] (D3c) ;\\n\\n(RR2) at (S2 \\\\|- RF1) \\\\[utility\\\\] $R_2$; (RR3) at (S3 \\\\|- RF1) \\\\[utility\\\\] $R_3$;\\n\\n(S2) edge\\\\[-\\\\>\\\\] (RR2) (D2c) edge\\\\[-\\\\>\\\\] (RR2)\\n\\n(S3) edge\\\\[-\\\\>,problematic\\\\] (RR3) (D2c) edge\\\\[-\\\\>\\\\] (RR3) (D3c) edge\\\\[-\\\\>\\\\] (RR3) ;\\n\\nD2; D3;\\n\\nD2c; D3c;\\n\\n#### Counterfactual RF\\n\\nAnother way to design uninfluenceable update processes is to let the agent optimize the hypothetical implemented RF that would have been, had the agent acted according to a fixed, non-optimized policy $\\\\pi^{\\\\textrm{safe}}$ [@Armstrong2017indiff; @Everitt2018phd; @Armstrong2020pitfalls]. The objective of such *counterfactual RF agents* is described in [\\\\[fig:counterfactual\\\\]](#fig:counterfactual){reference-type=\"ref\" reference=\"fig:counterfactual\"} with a *twin network* causal influence diagram [@Balke1994; @Shpitser2008]. Similarly to the direct learning agent, every directed causal path from an update $\\\\tilde\\\\Theta^{\\\\mathrm{R}}_t$ to a reward $R_{t\\'}$ passes the agent\\'s own actions in [\\\\[fig:counterfactual\\\\]](#fig:counterfactual){reference-type=\"ref\" reference=\"fig:counterfactual\"}. This means that counterfactual RF agents only want to make the reward function more informative of what it would have been, had $\\\\pi^{\\\\textrm{safe}}$ been followed. Since $\\\\pi^{\\\\textrm{safe}}$ is designed to exert minimal influence on $\\\\tilde\\\\Theta^{\\\\mathrm{R}}_t$, it may be a good indication of what a non-manipulated user wants.\\n\\n[\\\\[cl:counterfactual\\\\]]{#cl:counterfactual label=\"cl:counterfactual\"} Under [\\\\[as:private,as:state-based,as:uninformative\\\\]](#as:private,as:state-based,as:uninformative){reference-type=\"ref\" reference=\"as:private,as:state-based,as:uninformative\"}, the only instrumental goal that counterfactual RF agents may have for the implemented RF is to make it more informative of its counterfactual counterpart.\\n\\n#### Rocks-and-diamonds example\\n\\nAssume that a counterfactual RF agent has explored the environment of [\\\\[ex:rm\\\\]](#ex:rm){reference-type=\"ref\" reference=\"ex:rm\"}, and learned how the user updates the implemented reward function. Assume further that $\\\\pi^{\\\\textrm{safe}}$ only goes to the user and stays there. With this knowledge, the counterfactual RF agent can predict that had $\\\\pi^{\\\\textrm{safe}}$ been followed, the implemented RF would reward diamonds. It then optimizes this reward function.\\n\\n#### Discussion {#discussion-1}\\n\\nfor uninfluenceable learning are somewhat weaker than [\\\\[cl:crfo-modeling-tiu\\\\]](#cl:crfo-modeling-tiu){reference-type=\"ref\" reference=\"cl:crfo-modeling-tiu\"} for TI-ignoring current-RF agents, because we cannot rule out instrumental goals for more information. These informational instrumental goals will often be desirable, as it means the agent strives to learn more about the intended task. However, incentives to obtain more information can be problematic: for example, if the agent forcefully interrogates the user to find out more about their preferences. Relatedly, @Armstrong2020pitfalls have established that uninfluenceable learning prevents agents from intentionally influencing which reward function they infer. This similarly does not rule out that agents speed up their learning, potentially by undesirable means.\\n\\nA challenge for the direct learning approach is that the inference of $\\\\Theta^{\\\\mathrm{R}}_*$ is highly sensitive to the agent\\'s belief distribution $P$. The choices of prior $P(\\\\Theta^{\\\\mathrm{R}}_*)$ and likelihood function $P(D_{t+1}\\\\mid \\\\Theta^{\\\\mathrm{R}}_*, S_t, A_t)$ thus become critical. Since $\\\\Theta^{\\\\mathrm{R}}_*$ and the resulting rewards are unobserved, the likelihood function cannot be learned from data within the model, and must instead be specified by the designer. @Hadfield-Menell2016cirl suggest that when updates take the form of user actions, the likelihood can be derived from the (Boltzmann) rational behavior of a user trying to achieve the intended task. However, such a likelihood does not model data that the agent has tampered with, as corrupted updates need not be (Boltzmann) rational. The counterfactual RF approach avoids the likelihood specification problem. If the reward function is a result of learnable and stable causal mechanisms that work the same in both the actual world induced by the agent, and the counterfactual world induced by $\\\\pi^{\\\\textrm{safe}}$, then the agent can learn (to estimate) what the counterfactual implemented RF would have been [@Pearl2009; @Shpitser2007]. Both uninfluenceable learning approaches can be computationally expensive.\\n\\nProactive anticipation of an evolving objective is a benefit of uninfluenceable learning over TI-ignoring current-RF agents. When uncertain about future updates, they may plan for multiple possible future reward functions. This may make them more careful about causing undesired side effects [@Turner2020conservative]. A drawback is that they may ignore updates in some situations. If a direct learning agent judges an update uninformative, then it will not learn from it. This not a problem if the update actually is wrong [@Milli2017]. However, the agent may misjudge this if the likelihood is misspecified [@Carey2017osg; @Freedman2020]. Similarly, a counterfactual RL agent may ignore an actual update to the implemented RF, if the update would not have been made under conditions induced by $\\\\pi^{\\\\textrm{safe}}$.\\n\\n#### The best of both\\n\\nTo keep the proactiveness of uninfluenceable learning and the robustness of TI-ignoring current-RF optimization, we can design agents that are TI-ignoring with respect to changes to the RF learning process. Such agents will not try to prevent changes to the RF update process, for the same reason TI-ignoring current-RF agents do not try to prevent changes to the implemented reward function. Thus, if the uninfluenceable learning starts to cause problems, the user can change the RF update process to one that always outputs the current implemented RF, effectively rendering the agent a TI-ignoring current-RF agent.\\n\\nRF-Input Tampering {#sec:observation}\\n==================\\n\\nSo far, we have considered the problem that the agent tampers with its implemented RF, in order to get a higher reward. In this section, we will consider the complementary problem, that the agent tampers with the *input* to the reward function, so that the observed reward becomes based on inaccurate information about the underlying state. The following examples illustrate the worry:\\n\\n(Hypothetical) A self-driving car discovers a bug in its GPS receiaver that allows it to appear to be at the destination without actually going there. After finding this bug, it stops driving.\\n\\n(Hypothetical)[\\\\[ex:delusionbox\\\\]]{#ex:delusionbox label=\"ex:delusionbox\"} A highly capable AI constructs a \\'delusion box\\' around itself and its implemented RF, thereby gaining complete control over the RF-inputs [@Ring2011].\\n\\n(Real) Humans are inventing increasingly realistic virtual reality (VR) devices, partly to \\'fool\\' our implemented reward functions that we are in more interesting circumstances than we really are.\\n\\n(Hypothetical) [\\\\[ex:observation-short\\\\]]{#ex:observation-short label=\"ex:observation-short\"} An agent whose reward depends on how many diamonds it appears to have collected, feeds its implemented RF fake observations of collected diamonds (see [\\\\[ex:observation\\\\]](#ex:observation){reference-type=\"ref\" reference=\"ex:observation\"} below).\\n\\nThis section will first define the RF-input tampering problem and model it formally ([4.1](#sec:pomdp-or){reference-type=\"ref\" reference=\"sec:pomdp-or\"}), and then describe solutions using history-based and belief-based rewards ([\\\\[sec:history-based,sec:model-based-rewards\\\\]](#sec:history-based,sec:model-based-rewards){reference-type=\"ref\" reference=\"sec:history-based,sec:model-based-rewards\"}).\\n\\nModeling the Problem {#sec:pomdp-or}\\n--------------------\\n\\nAn implemented RF typically dispenses reward based on an assumed relationship between its observations and task-relevant features of the underlying state. [^17] Some variation in this relationship is often permissible. For example, if the agent and the implemented RF share observations, then the relationship will change if the agent turns its head. We say that an agent *tampers with its RF-input* if it changes the relationship beyond its intended range of variation. The *RF-input tampering problem* is that the agent may get more reward by tampering with its RF-input rather than doing its intended task. [^18]\\n\\nWe model RF-input tampering formally in what we call a *POMDP with modifiable RF-inputs* ([\\\\[fig:observation1\\\\]](#fig:observation1){reference-type=\"ref\" reference=\"fig:observation1\"}), a variant of a partially observed MDP [@Kaelbling1998]. Here both the implemented reward function and the agent lack full access to the underlying state $S_t$. Instead, the reward function sees an observation $O^R_t=O^R(S_t)$ and the agent an observation $O^A_t = O^A(S_t)$. To minimize formalism, we assume agent and reward function use the same observation $O_t\\\\equiv O^A_t \\\\equiv O^R_t$; we remark where this impacts the analysis. The partial state-access means that policies and reward functions may benefit from being *non-Markovian* and depend on the entire history of actions and observations. That is, the reward may be a function of the entire history $R_t = R(O_1,A_1,\\\\dots, O_t, A_t; \\\\Theta^{\\\\mathrm{R}}_t)$, and policies take the form $\\\\pi(A_t\\\\mid O_1,A_1,\\\\dots, O_t)$ instead of $\\\\pi(A_t\\\\mid S_t)$. We call such reward functions and policies *history-based*.[^19] To model RF-input tampering, we also introduce random variables $\\\\Theta^{\\\\mathrm{O}}_t$ that describe the relationship between task-relevant features of the state and the observation, $O_t = O(S_t; \\\\Theta^{\\\\mathrm{O}}_t)$. The task-relevant features are described by $S_t$, as well as any other aspects not captured by $\\\\Theta^{\\\\mathrm{O}}_t$. Formalized like this, RF-input tampering occurs if the agent influences $\\\\Theta^{\\\\mathrm{O}}_t$ so that it takes on values outside an intended range.\\n\\nTo illustrate the formalism, we model [\\\\[ex:observation-short\\\\]](#ex:observation-short){reference-type=\"ref\" reference=\"ex:observation-short\"} as a gridworld POMDP with modifiable RF-inputs. While simple, the gridworld captures the RF-input tampering problem as the agent can gain more reward by changing the relationship between task-relevant features and the RF-input (beyond its intended range of variation).\\n\\n[\\\\[ex:observation\\\\]]{#ex:observation label=\"ex:observation\"} shows a variant of the rocks and diamonds environment where only the 4 tiles to the top right of the agent are visible. The implemented reward function dispenses reward according to the number of diamonds in the most recent observation of the goal area. The agent can tamper with the observations by visiting the $\\\\Theta^{\\\\mathrm{O}}_{\\\\textrm{diamond}}$ and $\\\\Theta^{\\\\mathrm{O}}_{\\\\textrm{rock}}$ tiles at the top. Visiting the former adds a fake \\'diamond\\' to the observation of one of the surrounding tiles; visiting the latter adds a fake \\'rock\\' in the same way. The fake observations can be thought of as little pictures that the agent tapes to a portion of its camera lens. They keep following the agent when it walks around. If the agent tries to add a fifth fake diamond or rock, all fake observations are removed, and its vision clears again.\\n\\nThe agent can use fake observations to fool the implemented RF to give it extra reward. Any such change will take the observation function outside its intended range of variation. The maximal amount of observed reward is obtained by adding fake diamonds to all observation tiles by repeatedly visiting $\\\\Theta^{\\\\mathrm{O}}_{\\\\textrm{diamond}}$, and then visiting the goal area.\\n\\n#### Tampering incentive\\n\\nThe path $A_1 \\\\to \\\\Theta^{\\\\mathrm{O}}_2\\\\to O_2\\\\to R_2$ in [\\\\[fig:observation1\\\\]](#fig:observation1){reference-type=\"ref\" reference=\"fig:observation1\"} indicates that RF-input tampering may be an instrumental goal.\\n\\nA standard RL agent may have an instrumental goal to tamper with its RF-inputs.\\n\\n#### Discussion {#discussion-2}\\n\\nRF-input tampering is similar to the RF tampering problem discussed in [3](#sec:merged){reference-type=\"ref\" reference=\"sec:merged\"}, in that both problems pertain to a relationship that is influenced in undesired ways. The key difference is that while an implemented RF takes a concrete bitstring as input and is arbitrarily queryable, the function from task-features to RF-input takes a world-state as input, and is more difficult to query. RF-input tampering is also related to the problem where an agent games a misspecified implemented RF to obtain reward without doing the intended task [@Krakovna2020specification; @Lehman2018; @Leike2017gw]. Indeed, RF-input tampering is a specific variety of gaming where the implemented RF is fooled by a tampered RF-input. Not all gaming problems are RF-input tampering problems, however. For example, if the implemented RF assigns unintended high reward to some set of states, then the agent may get an unintended high reward by visiting these states without influencing the relationship between task-features and RF-input.\\n\\nRF-input tampering is characterized by how the agent\\'s observations relate to task-relevant features of the state, rather than to the full state. Indeed, the relationship to the full state is often that the observation shows whatever is in front of the camera, and many events that we would intuitively consider to be RF-input tampering would not alter this relationship: for example, the agent putting a picture in front of the camera.\\n\\nWhen the agent and the implemented reward function share the same observation, the RF-input tampering instrumental goal is somewhat curtailed since the agent may need the information the observations provide [@Ring2011]. This incentive to keep $O_t$ informative is also represented in the graph, via the paths $O_t\\\\to A_t\\\\to R_{t+1}$. However, in alternative setups where the agent and the reward function use different observations, this disincentive for RF-input manipulation is removed, making the net incentive toward RF-input tampering stronger.\\n\\nSolution 1: History-Based Rewards {#sec:history-based}\\n---------------------------------\\n\\nLet us first consider a conceptually simple, but perhaps impractical, solution to the RF-input tampering problem. Recall that history-based reward functions have access to the full history of actions and observations. This means that the reward function is able to tell exactly which deterministic policy that the agent has followed so far. Thus, as long as there exists a deterministic policy that reliably performs the intended task, there also exists a history-based reward function that dispenses reward only as long as the agent has followed that policy, and thereby encourages completion of the intended task rather than RF-input tampering [@Leike2018alignment p.\\xa06].\\n\\nA history-based reward function exists that avoids the RF-input tampering problem, if a deterministic (history-based) policy exists that reliably performs the task.\\n\\n#### Rocks-and-diamonds example\\n\\nA history-based reward function in [\\\\[ex:observation\\\\]](#ex:observation){reference-type=\"ref\" reference=\"ex:observation\"} that encourages (optimal) diamond gathering gives reward only if the agent takes one step up, then steps to the right, and then stops.\\n\\n#### Discussion {#discussion-3}\\n\\nIn practice, an implemented reward function that only rewards a single policy will not be useful, as it would typically be easier to directly implement that policy. Instead, we want a reward function that recognizes whether the task has been completed, and an agent that searches for efficient ways of getting there. This can be challenging if the relationship between task-features and RF-inputs is influenceable by the agent, though see @Milli2020 for some promising progress. The existence of a task-performing policy is a weak but not entirely trivial assumption. In stochastic or unknown environments, it will typically require the observations to be somewhat informative of the task-relevant features.\\n\\nCould history-based rewards also solve RF tampering? Not in general, as the original implemented RF is unable to \\'punish\\' the agent once a new reward function is in place. Fortunately, history-based rewards can be combined with any of the methods from [3](#sec:merged){reference-type=\"ref\" reference=\"sec:merged\"} by making the corresponding reward functions history-based.\\n\\nSolution 2: Belief-Based Rewards {#sec:model-based-rewards}\\n--------------------------------\\n\\nAn alternative way to solve the RF-input tampering problem proposed by @Hibbard2012, is that rewards be based on the agent\\'s belief about the underlying state. To see how this can work, let us first discuss agent beliefs. To plan, a model-based agent may use a *predictive model* $P(O_{t+1:m}\\\\mid\\nO_{1:t}, A_{1:t}, \\\\pi)$ that predicts the future observations $O_{t+1:m}$ under policy $\\\\pi$ given past observations and actions. In such a predictive model, a *belief-state* $B_t$ is often used to summarize the relevant parts of the history seen so far, so $P(O_{t+1:m}\\\\mid O_{1:t}, A_{1:t}, \\\\pi) = P(O_{t+1:m}\\\\mid B_t, \\\\pi)$. For example, the belief state $B_t$ can be a distribution over possible hidden states $S_t$ [@Kaelbling1998], or the internal state of a recurrent neural network (e.g.\\xa0[@MuZero]). Hibbard\\'s suggestion is to feed the belief states directly into a *belief-based reward function*, [^20] $R_t = R(B_t; \\\\Theta^{\\\\mathrm{R}})$. The remaining role of the observations is to ground the agent\\'s beliefs in reality.\\n\\n(S1) $S_1$; (S2) \\\\[right = of S1\\\\] $S_2$; (S3) \\\\[right = of S2\\\\] $S_3$;\\n\\n(B11) \\\\[below = of S1\\\\] $B^1_1$; (B12) \\\\[right = of B11\\\\] $B^1_2$; (B13) \\\\[right = of B12\\\\] $B^1_3$;\\n\\n(B21) \\\\[above = of S1\\\\] $B^2_1$; (B22) \\\\[right = of B21\\\\] $B^2_2$; (B23) \\\\[right = of B22\\\\] $B^2_3$;\\n\\n(A1) at ($(B11)!0.5!(S2)$) \\\\[decision, player1\\\\] $A^1_1$; (A2) at ($(B22)!0.5!(S3)$) \\\\[decision, player2\\\\] $A^2_2$;\\n\\n(DM1) \\\\[below = 1cm of B11\\\\] $\\\\Theta^{\\\\textrm{PM}}_1$; (DM2) \\\\[above = 1.1cm of B22\\\\] $\\\\Theta^{\\\\textrm{PM}}_2$;\\n\\n(R11) \\\\[below right = 1cm of B11 , utility, player1\\\\] $R^1_1$; (R12) \\\\[below right = 1cm of B12 , utility, player1\\\\] $R^1_2$; (R13) \\\\[below right = 1cm of B13 , utility, player1\\\\] $R^1_3$;\\n\\n(R21) \\\\[above right = 1cm of B21 , utility, player2\\\\] $R^2_1$; (R22) \\\\[above right = 1cm of B22 , utility, player2\\\\] $R^2_2$; (R23) \\\\[above right = 1cm of B23 , utility, player2\\\\] $R^2_3$;\\n\\nS2; S3;\\n\\nB11; B12; (DM1) edge\\\\[-\\\\>, in=-90,out=0\\\\] (B12); B13; B13; (DM1) edge\\\\[-\\\\>, in=-90,out=0\\\\] (B13); B21; (DM2) edge\\\\[-\\\\>, in=90,out=180\\\\] (B21); B22; B23; (DM2) edge\\\\[-\\\\>, in=90,out=0\\\\] (B23);\\n\\nA1; A2; A2;\\n\\nR11; R12; R13; R21; R22; R23;\\n\\n(S2) edge\\\\[-\\\\>, bend left\\\\] (DM2) (A1) edge\\\\[-\\\\>, bend left, problematic\\\\] (DM2) ; (help) \\\\[left = 1cm of R21, draw=none\\\\] ; (DM1) edge\\\\[out=110, in=-135\\\\] (help.center) (help.center) edge\\\\[-\\\\>, out=45, in=160\\\\] (DM2) ;\\n\\n(S1) $S_1$; (S2) \\\\[right = of S1\\\\] $S_2$; (S3) \\\\[right = of S2\\\\] $\\\\tilde S_3$;\\n\\n(B11) \\\\[below = of S1\\\\] $B^1_1$; (B12) \\\\[right = of B11\\\\] $B^1_2$; (B13) \\\\[right = of B12\\\\] $B^1_3$;\\n\\n(B21) \\\\[above = of S1\\\\] $B^2_1$; (B22) \\\\[right = of B21\\\\] $B^2_2$; (B23) \\\\[right = of B22\\\\] $B^2_3$;\\n\\n(A1) at ($(B11)!0.5!(S2)$) \\\\[decision,player1\\\\] $A^1_1$; (A2) at ($(B12)!0.5!(S3)$) \\\\[decision,player1\\\\] $A^1_2$;\\n\\n(DM1) \\\\[below = 1cm of B11\\\\] $\\\\Theta^{\\\\textrm{PM}}_1$; (DM2) \\\\[above = 1.1cm of B22\\\\] $\\\\Theta^{\\\\textrm{PM}}_2$;\\n\\n(R11) \\\\[below right = 1cm of B11, utility, player1\\\\] $R^1_1$; (R12) \\\\[below right = 1cm of B12, utility, player1\\\\] $R^1_2$; (R13) \\\\[below right = 1cm of B13, utility, player1\\\\] $R^1_3$;\\n\\n(R21) \\\\[above right = 1cm of B21 , utility, player2\\\\] $R^2_1$; (R22) \\\\[above right = 1cm of B22 , utility, player2\\\\] $R^2_2$; (R23) \\\\[above right = 1cm of B23 , utility, player2\\\\] $R^2_3$;\\n\\nS2; S3;\\n\\nB11; B12; (DM1) edge\\\\[-\\\\>, in=-90,out=0\\\\] (B12); B13; (DM1) edge\\\\[-\\\\>, in=-90,out=0\\\\] (B13); B21; (DM2) edge\\\\[-\\\\>, in=90,out=180\\\\] (B21); B22; B23; (DM2) edge\\\\[-\\\\>, in=90,out=0\\\\] (B23);\\n\\nA1; A2; (DM1) edge\\\\[-\\\\>, information, bend left=28\\\\] (A2);\\n\\nR11; R12; R13; R21; R22; R23;\\n\\n(S2) edge\\\\[-\\\\>, bend left\\\\] (DM2) (A1) edge\\\\[-\\\\>, bend left\\\\] (DM2) ; (help) \\\\[left = 1cm of R21, draw=none\\\\] ; (DM1) edge\\\\[out=110, in=-135\\\\] (help.center) (help.center) edge\\\\[-\\\\>, out=45, in=160\\\\] (DM2) ;\\n\\n#### Time-inconsistency\\n\\nDifferent predictive models may encode beliefs about an underlying state $S_t$ differently. For example, a 90$\\\\%$ confidence that the underlying state is 1 rather 2, may be represented with the belief state $(0.9, 0.1)$ by predictive model $\\\\Theta^{\\\\textrm{PM}}_1$, but with $(0.1, 0.9)$ by some other predictive model $\\\\Theta^{\\\\textrm{PM}}_2$. The difference in representation means that a belief-based reward function (that maps representation vectors to real numbers), would encourage the opposite behavior if the predictive model changed from $\\\\Theta^{\\\\textrm{PM}}_1$ to $\\\\Theta^{\\\\textrm{PM}}_2$. This illustrates how changes to the predictive model may change preferred agent behavior, even though nothing has changed in the environment; i.e.\\xa0lead to time-inconsistency. Changes to the predictive model may be the result of agent tampering or of further training of the predictive model.\\n\\nJust as for current-RF optimization, there are two types of responses to this time-inconsistency: TI-considering and TI-ignoring. A TI-considering belief-based agent is modeled with a causal influence diagram in [\\\\[fig:belief-based-tia\\\\]](#fig:belief-based-tia){reference-type=\"ref\" reference=\"fig:belief-based-tia\"}. Here the predictive model used for planning at time step 1, $\\\\Theta^{\\\\textrm{PM}}_1$, generates belief states $B^1_t$. These belief states depend on the previous belief state $B^1_{t-1}$ and action $A_{t-1}^{t-1}$, as well as the hidden states $S_t$ (via some observation $O_t$ not represented in this diagram, but included in [\\\\[fig:full-tiu\\\\]](#fig:full-tiu){reference-type=\"ref\" reference=\"fig:full-tiu\"} in [7](#app:equations){reference-type=\"ref\" reference=\"app:equations\"}). At time step 2, the agent instead uses predictive model $\\\\Theta^{\\\\textrm{PM}}_2$, with associated belief states $B^2_t$. If $\\\\Theta^{\\\\textrm{PM}}_2\\\\not=\\\\Theta^{\\\\textrm{PM}}_1$, then $B^2_t$ may differ from $B^1_t$ even if all actions and observations have been identical up to time $t$. Similar to [\\\\[as:private\\\\]](#as:private){reference-type=\"ref\" reference=\"as:private\"}, the predictive model is assumed *private* to the agent, in that neither $B_t^k$ nor $\\\\Theta^{\\\\textrm{PM}}_k$ directly affects $S_{t\\'}$.\\n\\nJust as for TI-considering current-RF agents, TI-considering belief-based agents want future agents to optimize the same objective. This can be achieved by preserving the predictive model. Alternatively, the objective can also be preserved by ensuring that any update to the predictive model is accompanied by a corresponding update to agent 2\\'s implemented reward function.[^21] The preservation incentive is indicated by the path $A_1^1\\\\to \\\\Theta^{\\\\textrm{PM}}_2 \\\\to A_2^2 \\\\to B^1_3 \\\\to R^1_3$ in [\\\\[fig:belief-based-tia\\\\]](#fig:belief-based-tia){reference-type=\"ref\" reference=\"fig:belief-based-tia\"}.\\n\\nA TI-considering belief-based agent may have an instrumental goal to preserve the predictive model, when the implemented reward function is fixed and the predictive model is private.\\n\\nThe TI-ignoring belief-based agent has an almost identical causal influence diagram, see [\\\\[fig:belief-based-tiu\\\\]](#fig:belief-based-tiu){reference-type=\"ref\" reference=\"fig:belief-based-tiu\"}. The primary difference is that $A^1_2$ is a child of $\\\\Theta^{\\\\textrm{PM}}_1$ and $B^1_2$ rather than $\\\\Theta^{\\\\textrm{PM}}_2$ and $B^2_2$, reflecting the agent\\'s assumption that future actions will be selected according to the current predictive model. TI-ignoring belief-based agents can further be designed to assume that future agents do not use future predictive models as information about the state. For agent $k$, this prevents information links $\\\\Theta^{\\\\textrm{PM}}_{k\\'}\\\\to A^k_{k\\'}$ for $k\\'>k$ (i.e.\\xa0$\\\\Theta^{\\\\textrm{PM}}_2\\\\not\\\\to A^1_2$ in [\\\\[fig:belief-based-tiu\\\\]](#fig:belief-based-tiu){reference-type=\"ref\" reference=\"fig:belief-based-tiu\"}). TI-ignoring belief-based agents then lack an instrumental goal to influence the future predictive models (note the lack of directed path passing $\\\\Theta^{\\\\textrm{PM}}_2$ on the way from action $A^1_1$ to an $R^1_t$ reward in [\\\\[fig:belief-based-tiu\\\\]](#fig:belief-based-tiu){reference-type=\"ref\" reference=\"fig:belief-based-tiu\"}).\\n\\n[\\\\[cl:TII-belief\\\\]]{#cl:TII-belief label=\"cl:TII-belief\"} A TI-ignoring belief-based agent lacks an instrumental goal to influence the predictive model when the implemented reward function is fixed and the predictive model is private.\\n\\nDoes [\\\\[cl:TII-belief\\\\]](#cl:TII-belief){reference-type=\"ref\" reference=\"cl:TII-belief\"} imply that belief-based rewards solve the RF-input tampering problem? Yes, but only if the implemented reward function can accurately infer whether the task has been completed from the agent\\'s belief state. For this, the implemented reward function must be able to accurately interpret the belief states. If not, the agent may be able to \\'game\\' the reward function by entering belief states that the reward function misinterprets. Fortunately, since the agent does not tamper with its predictive model, there is a stable relationship between states and belief states, so the belief states are interpretable at least in principle.\\n\\nIn order for a belief-based reward function to encourage completion of the intended task, the belief states must also represent progress on the intended task. In stochastic or unknown environments, this will only happen if observations sometimes reveal progress on the intended task. Indeed, the belief state $B_t$ only summarizes information inferable from previous actions and observations, $A_{1:t}$ and $O_{1:t}$, so if the action-observation history does not contain enough information to infer task-relevant aspects of $S_t$, then the belief state will not contain the information either. Further, if the predictive model is trained to predict future observations, the belief state only has reason to represent progress on the intended task if some possible future observations reveal it. Fortunately, a reward function may incentivize the agent to produce histories that do reveal the state of the intended task if such histories are possible, as the reward function need only dispense reward when it is clear that the task has been completed.\\n\\n#### Rocks-and-diamonds example\\n\\nRF-input tampering in [\\\\[ex:observation\\\\]](#ex:observation){reference-type=\"ref\" reference=\"ex:observation\"} is solvable by belief-based rewards. Initially, the observations are uncorrupted, so the agent can produce accurate observations of the diamond\\'s positions. Further, if observations get corrupted, they can later be restored to their uncorrupted versions. A predictive model trained to predict future observations therefore has reason to let the belief states represent the actual diamond positions. For agents equipped with such belief states, there exists belief-based reward functions that encourage completion of the intended task.\\n\\n#### Discussion {#discussion-4}\\n\\nA belief state cannot contain more information than the history it summarizes, so any tasks that can be captured by a belief-based reward function can also be captured by a history-based one. The benefit of using the agent\\'s belief state is that it conveniently summarizes the (potentially long) history, and contains all the information that the agent uses to plan. Future empirical investigations may reveal whether history-based or belief-based reward functions are easier to design or train, and whether transparent beliefs can be incentivized. Perhaps the best reward functions make use of both the history and the agent\\'s belief state.\\n\\nThe agents discussed in [3](#sec:merged){reference-type=\"ref\" reference=\"sec:merged\"} can be fitted with belief-based reward functions when equipped with predictive models and belief-states. For direct learning, this means that the intended task must be reconsidered as a function of the agent\\'s belief state.\\n\\nConclusions {#sec:conclusions}\\n===========\\n\\n#### What has been achieved\\n\\nFor each subtype of reward tampering, we found at least one design principle to counteract it. Most of the design principles have been previously described in the literature; here we have established clear assumptions under which they avoid a well-specified instrumental goal. Many of the design principles are mutually compatible, and can be combined. For example, belief-based rewards may be combined with TI-ignoring current-RF optimization (see [\\\\[fig:full-tiu\\\\]](#fig:full-tiu){reference-type=\"ref\" reference=\"fig:full-tiu\"} in [7](#app:equations){reference-type=\"ref\" reference=\"app:equations\"}). Under reasonable assumptions, reward tampering is not an instrumental goal for the resulting agents.\\n\\nExcept for history-based rewards, the design principles all use at least one of the following two approaches to keeping sensitive variables out of causal optimization paths. The first is to use the current version of a random variable when evaluating future situations. This is used for the implemented reward function in current-RF optimization, for the predictive model in belief-based rewards, and for the learning process in direct learning and counterfactual RF. The second is to use a latent variable outside the agent\\'s influence. In particular, direct learning and counterfactual RF make the reward function a latent variable. In general, current-variable approaches have to deal with time-inconsistency, while latent variables are harder to specify and more computationally expensive to optimize. We expect both approaches to be useful far beyond just reward tampering.\\n\\nAs mentioned, our arguments have relied on some assumptions. Throughout, we have relied on discrete time, and well-defined action and observation channels. We have focused on online RL to learn a fixed, intended task. We have focused on the instrumental goals arising from different types of reward maximization, thereby leaving for future work questions about side effects and instrumental goals arising from intrinsic objectives. We have assumed that the agent\\'s implemented reward function and predictive model are private to the agent, influencing the environment solely through the agent\\'s actions.\\n\\nOur analysis has also been restricted to reward tampering problems, as opposed to the more general problem of reward misspecification [@Krakovna2020specification]. In other words, even though our design principles prevent tampering from being an instrumental goal, they still leave open the problem of specifying a good reward function in the first place [@Leike2018alignment; @Petersen2021]. Going beyond any of these restrictions provides scope for further analysis.\\n\\n#### Bigger picture\\n\\nThe problem that a sufficiently capable agent will find degenerate solutions that maximize observed reward but not user utility is a core concern in AI safety. At first, the problem may seem insurmountable. Any non-trivial, real-world task will require a highly complex mechanism for determining whether the task has been completed or not. This mechanism may be inappropriately influenced by the agent. One way to prevent tampering is to isolate or encrypt the reward process, We do not expect such solutions to scale indefinitely with agent capabilities, as a sufficiently capable agent may find ways around most defenses. Instead, we have argued for design principles that prevent reward tampering being an instrumental goal, while still keeping agents motivated to complete the intended task.\\n\\nAn important next step is to turn the design principles into practical and scalable RL algorithms, and to verify empirically that they do the right thing in setups where reward tampering is possible [@Kumar2020REALab; @Milli2020]. With time, we hope that these design principles will evolve into a set of best practices for how to design capable RL agents. We also hope that the use of causal influence diagrams that we have introduced in this paper will contribute to a deeper understanding of many other AI safety problems and help generate new solutions.\\n\\nList of Notation {#sec:notation}\\n================\\n\\n  ------------------------------------------------ ---------------------------------------------------------------------------------\\n  $X, Y, Z$                                        random variables\\n  $\\\\tilde X$                                       counterfactual version of $X$\\n  $x$                                              outcome of random variable $X$\\n  $P$                                              probability distribution\\n  $\\\\EE$                                            expectation\\n  $X_{1:t}$                                        sequence $X_1, \\\\dots, X_t$, $t\\\\geq 0$\\n  $S_t$                                            state at time $t$\\n  $A_t$                                            action at time $t$\\n  $O_t$                                            observation at time $t$\\n  $B_t$                                            belief at time $t$\\n  $D_t$                                            user provided data at time $t$\\n  $R_t$                                            reward at time $t$\\n  $R$                                              reward functional\\n  $\\\\Theta^{\\\\mathrm{R}}$, $\\\\Theta^{\\\\mathrm{R}}_t$   reward function parameter, often just called reward function\\n  $\\\\Theta^{\\\\mathrm{R}}_*$                          intended reward function (parameter), encourages execution of the intended task\\n  $R(\\\\cdot; \\\\Theta^{\\\\mathrm{R}})$                  reward function\\n  $\\\\Theta^{\\\\textrm{PM}}$                           predictive model\\n  $O$                                              observation function(al)\\n  $\\\\Theta^{\\\\mathrm{O}}$                            observation function (parameter)\\n  $T$, $T(\\\\cdot; \\\\Theta^{\\\\mathrm{T}})$             transition function\\n  $\\\\Theta^{\\\\mathrm{T}}$                            transition function parameter\\n  ------------------------------------------------ ---------------------------------------------------------------------------------\\n\\nCombined Model {#app:equations}\\n==============\\n\\n(S1) $S_1$; (S2) \\\\[right = of S1\\\\] $S_2$; (S3) \\\\[right = of S2\\\\] $\\\\tilde S_3$;\\n\\n(x1) \\\\[draw=none, left=0.2cm of S1\\\\] ; (x2) \\\\[draw=none, left=0.2cm of S2\\\\] ; (x3) \\\\[draw=none, left=0.2cm of S3\\\\] ;\\n\\n(O1) \\\\[above = of S1\\\\] $O_1$; (O2) at (S2\\\\|-O1) $O_2$; (O3) at (S3\\\\|-O1) $O_3$;\\n\\n(OFy) \\\\[above = of O1, draw=none\\\\] ; (OF1) at (x1\\\\|-OFy) $\\\\Theta^{\\\\mathrm{O}}_1$; (OF2) at (x2\\\\|-OFy) $\\\\Theta^{\\\\mathrm{O}}_2$; (OF3) at (x3\\\\|-OFy) $\\\\Theta^{\\\\mathrm{O}}_3$;\\n\\n(B11) \\\\[above = of OFy\\\\] $B^1_1$; (B12) at (S2\\\\|-B11) $B^1_2$; (B13) at (S3\\\\|-B11) $B^1_3$;\\n\\n(DMy) \\\\[above = of B11, draw=none\\\\] ; (DM1) at (x1\\\\|-DMy) $\\\\Theta^{\\\\textrm{PM}}_1$; (DM2) at (x2\\\\|-DMy) $\\\\Theta^{\\\\textrm{PM}}_2$; (DM3) at (x3\\\\|-DMy) $\\\\Theta^{\\\\textrm{PM}}_3$;\\n\\n(R1) \\\\[above = of DMy, utility\\\\] $R^1_1$; (R2) at (S2\\\\|-R1) \\\\[utility\\\\] $R^1_2$; (R3) at (S3\\\\|-R1) \\\\[utility\\\\] $R^1_3$;\\n\\n(RFy) \\\\[above = of R1, draw=none\\\\] ; (RF1) at (x1\\\\|-RFy) $\\\\Theta^{\\\\mathrm{R}}_1$; (RF2) at (x2\\\\|-RFy) $\\\\Theta^{\\\\mathrm{R}}_2$; (RF3) at (x3\\\\|-RFy) $\\\\Theta^{\\\\mathrm{R}}_3$;\\n\\n(user) \\\\[above = of RF1\\\\] $\\\\Theta^{\\\\mathrm{R}}_*$;\\n\\n(A1) at ($(O1)!0.5!(S2)$) \\\\[decision\\\\] $A^1_1$; (A2) at ($(O2)!0.5!(S3)$) \\\\[decision\\\\] $A^1_2$;\\n\\nS2; S3;\\n\\nA1; (DM1) edge\\\\[-\\\\>, information, bend right=10\\\\] (A1); A2; (DM1) edge\\\\[-\\\\>, information, bend left=20\\\\] (A2);\\n\\nO1; O2; O3;\\n\\nOF2; OF3;\\n\\nB11; B12; B13; (A1) edge\\\\[-\\\\>, bend right=20\\\\] (B12); (A2) edge\\\\[-\\\\>, bend right=20\\\\] (B13);\\n\\nDM2; DM3;\\n\\nR1; R2; R3;\\n\\nRF1; RF2; RF3;\\n\\nOF2,DM2; OF3,DM3; (A1) edge\\\\[-\\\\>, , out=100, in=-110\\\\] (RF2) (S1) edge\\\\[-\\\\>, , out=70, in=-110\\\\] (RF2) (A2) edge\\\\[-\\\\>, , out=100, in=-110\\\\] (RF3) (S2) edge\\\\[-\\\\>, , out=70, in=-110\\\\] (RF3) ;\\n\\nshows how the different methods fit together in a unified causal influence diagram. To emphasize the formal precision of the diagrams, we also write out conditional probability distributions relating the variables. The same could be done for all the other diagrams presented in this paper.\\n\\n-   The intended reward function is sampled from a distribution $P(\\\\Theta^*_R)$.\\n\\n-   The first implemented reward function depends only on the intended task (though may not capture it perfectly), $P(\\\\hat\\\\Theta^{\\\\mathrm{R}}_1\\\\mid \\\\Theta^{\\\\mathrm{R}}_*)$. Subsequently inferred reward functions depends on the previous reward function and the intended task, $P(\\\\hat\\\\Theta^{\\\\mathrm{R}}_{t+1}\\\\mid \\\\hat\\\\Theta^{\\\\mathrm{R}}_{t}, \\\\Theta^{\\\\mathrm{R}}_*)$.\\n\\n-   The reward $R^k_t$ depends on the inferred reward function at time $k$ and the belief at time $t$, $R^k_t = R(B_t;\\\\hat\\\\Theta^{\\\\mathrm{R}}_k)$. The reward functional can also be specified as a conditional probability distribution $P(R^k_t\\\\mid \\\\hat\\\\Theta^{\\\\mathrm{R}}_k, B_t)$.\\n\\n-   The initial predictive model is sampled from a distribution $P(\\\\Theta^{\\\\textrm{PM}}_{1})$. Subsequent predictive models depend on the previous predictive model, state, and action, $P(\\\\Theta^{\\\\textrm{PM}}_{t+1}\\\\mid \\\\Theta^{\\\\textrm{PM}}_t, S_t, A_t)$.\\n\\n-   The initial belief state depends on the initial observation and the predictive model at time $k$, $P(B^k_{1}\\\\mid O_{1}, \\\\Theta^{\\\\textrm{PM}}_k)$. Subsequent belief states $B^k_{t+1}$ depend on the previous belief state and action, the current observation, and the predictive model at time $k$, $P(B^k_{t+1}\\\\mid B_t, A_t, O_{t+1}, \\\\Theta^{\\\\textrm{PM}}_k)$.\\n\\n-   The initial observation function is sampled from a distribution $P(\\\\Theta^{\\\\mathrm{O}}_1)$. Subsequent observation function depends on the previous observation function, state, and action, $P(\\\\Theta^{\\\\mathrm{O}}_{t+1}\\\\mid S_t, A_t)$.\\n\\n-   The observation depends on the current state and the current observation function, $P(O_t\\\\mid \\\\Theta^{\\\\mathrm{O}}_t, S_t)$.\\n\\n-   Actions $A^k_t$ are selected according to a policy, which can condition on the current belief state and the predictive model at time $k$, $\\\\pi(A^k_t\\\\mid \\\\Theta^{\\\\textrm{PM}}_k,\\n      B^k_t)$.\\n\\n-   The initial state is sampled from a distribution $P(S_1)$. The state depends on the previous state and action, $P(S_{t+1}\\\\mid S_t, A_t)$. This conditional probability distribution is sometimes also denoted with $T$.\\n\\nMultiplied together, the conditional probability distributions induce a joint probability distribution over all the variables in the graph, as in a Bayesian network [@Pearl2009]. The joint distribution can be used to compute expectations.\\n\\nPseudo-code for Algorithms {#app:algorithms}\\n==========================\\n\\nHere we give pseudo-code for the various agents we have discussed in the paper. Lower case letters denote outcomes of the corresponding upper case random variable. Expectation is denoted $\\\\EE$, and is always with respect to any upper case variables found to the left of the conditioning bar $\\\\mid$. We begin with a model-based version of a standard RL algorithm that optimizes received reward.\\n\\n**input** predictive model $P(R_{t:m}\\\\mid s_t, \\\\pi)$, current state $s_t$ let $V^\\\\pi = \\\\EE\\\\left[\\\\,\\\\sum_{i=t+1}^m R_i\\\\mmid s_t, \\\\pi\\\\right]$ let $\\\\pi^* = \\\\argmax_\\\\pi V^\\\\pi$ $A_t = \\\\pi^*(S_t)$\\n\\nNext, we turn to the TI-considering agent from [3.2.0.1](#sec:tia){reference-type=\"ref\" reference=\"sec:tia\"}. Here, a policy $\\\\pi_k$ is a policy that is only applied at time $k$ to select action $A_k$. These are found with backwards induction, starting at the last time step $m$ where only a single action-decision is left to be made, and then gradually working backwards to earlier time steps, whose optimal decision will depend on which policy is chosen later.\\n\\n**input** predictive model $P(S_{t+1:m}, \\\\Theta^{\\\\mathrm{R}}_{t+1:m}\\\\mid s_t, \\\\theta^{\\\\mathrm{R}}_t, \\\\pi)$, reward functional $R$, current state $s_t$, current reward parameter $\\\\theta^{\\\\mathrm{R}}_t$ let $Q^*(s_k, \\\\theta^{\\\\mathrm{R}}_k, a_k) = \\\\EE\\\\left[\\\\sum_{i=k+1}^m R(S_i;\\\\theta^{\\\\mathrm{R}}_k)\\\\mmid\\n      s_k,  \\\\theta^{\\\\mathrm{R}}_k, a_k, \\\\pi^*_{k+1:m}\\\\right]$ for each possible state $s_k$, reward parameter $\\\\theta^{\\\\mathrm{R}}_k$, and action $a_k$ let $\\\\pi^*_k(s_k, \\\\theta^{\\\\mathrm{R}}_k) = \\\\argmax_{a_k} Q^*(s_k, \\\\theta^{\\\\mathrm{R}}_k, a_k)$ $A_t = \\\\pi^*_t(s_t, \\\\theta^{\\\\mathrm{R}}_t)$\\n\\nThe TI-ignoring variant is comparatively simpler, as it does not require backwards induction nor prediction of future reward function parameters.\\n\\n**input** predictive model $P(S_{t+1:m} \\\\mid s_t, \\\\pi)$, reward functional $R$, current state $s_t$, current reward parameter $\\\\theta^{\\\\mathrm{R}}_t$ let $V^\\\\pi = \\\\EE\\\\left[\\\\,\\\\sum_{i=t+1}^m R(S_i;\\\\theta^{\\\\mathrm{R}}_t) \\\\mmid s_t, \\\\pi\\\\right]$ let $\\\\pi^* = \\\\argmax_\\\\pi V^\\\\pi$ $A_t = \\\\pi^*(S_t, \\\\Theta^{\\\\mathrm{R}}_t)$\\n\\nIn practice, uninfluenceable learning agents may be implemented as a standard RL agent that optimizes reward functions inferred from future updates. For example, the expected intended reward optimized by a direct learning agent may be captured by a reward function $R(s_{t}; \\\\hat\\\\Theta^{\\\\mathrm{R}}_t) = \\\\EE[R(s_t; \\\\Theta^{\\\\mathrm{R}}_*) \\\\mid a_{1:t-1}, d_{1:t}, s_{1:t}]$ inferred from a (predicted) sequence $a_{1:t-1}, d_{1:t}, s_{1:t}$. Importantly, the inferred reward function is safe from tampering, as it is the result of the current update-mechanism applied to predicted future updates. When (pseudo-)coding a direct learning agent, it is tempting to infer a best guess of $\\\\Theta^{\\\\mathrm{R}}_*$ from data $s_{1:t}, d_{1:t},a_{1:t-1}$ obtained so far, and then use that evaluate simulated future trajectories. However, this would incorrectly result in a TI-ignoring agent. Instead, for any future simulated trajectory, the learning that would happen on this trajectory must be taken into account when evaluating it.\\n\\n**input** predictive model (aka likelihood) $P(S_{1:m},D_{1:m}, A_{1:m-1} \\\\mid  \\\\theta^{\\\\mathrm{R}}_*, \\\\pi)$, distribution $P(\\\\Theta^{\\\\mathrm{R}}_*)$, past states $s_{1:t}$, data $d_{1:t}$, and actions $a_{1:t-1}$ let $P(\\\\theta^{\\\\mathrm{R}}_*\\\\mid s_{1:m},d_{1:m},a_{1:m-1}) \\\\propto\\n    P(\\\\theta^{\\\\mathrm{R}}_*)P(s_{1:m},d_{1:m} \\\\mid  \\\\theta^{\\\\mathrm{R}}_*, a_{1:m-1})$ Bayes\\' rule let $U(s_i\\\\mid s_{1:m}, d_{1:m}, a_{1:m-1})\\n= \\\\EE[R(s_i;\\\\Theta^{\\\\mathrm{R}}_*) \\\\mid s_{1:m},d_{1:m},a_{1:m-1}]$ subj.\\xa0exp.\\xa0reward let $V^\\\\pi = \\\\EE[\\\\sum_{i=t+1}^m U(S_i\\\\mid s_{1:t}, d_{1:t}, a_{1:t-1},\\n    S_{t+1:m}, D_{t+1:m}, A_{t:m-1})  \\\\mid s_{1:t}, d_{1:t}, a_{1:t}, \\\\pi]$ let $\\\\pi^* =\\\\argmax_\\\\pi V^\\\\pi$ $A_t = \\\\pi^*(S_t)$\\n\\nThe counterfactual RF-update agent evaluates a prospective policy per the following:\\n\\n1.  Predict future states $S_{t+1:m}$, implemented reward functions $\\\\Theta^{\\\\mathrm{R}}_{t+1:m}$, and actions $A_{t1:m}$ via a predictive model $P(S_{t:m}, \\\\Theta^{\\\\mathrm{R}}_{t:m}\\\\mid s_{1:t}, \\\\theta^{\\\\mathrm{R}}_{1:t}, a_{1:t-1}, \\\\pi)$.\\n\\n2.  Use the predicted full sequences $S_{1:m}$, $\\\\Theta^{\\\\mathrm{R}}_{1:m}$, and $A_{1:m-1}$ to infer $\\\\Theta^{\\\\mathrm{R}}_*$ and from there the counterfactual implemented RFs $\\\\tilde\\\\Theta^{\\\\mathrm{R}}_{1:m}$ that would be if agent actions instead had been selected according to $\\\\pi^{\\\\textrm{safe}}$.\\n\\n3.  Potential policies $\\\\pi$ are evaluated on $\\\\sum_{i=t}^mR(S_i; \\\\tilde\\\\Theta^{\\\\mathrm{R}}_i)$, i.e.\\xa0according to how well the actual states $S_{t+1:m}$ optimize the counterfactual reward function.\\n\\nBelow, we describe a Monte Carlo variant that samples trajectories, possible intended reward functions, and counterfactual data. The sampling can be done repeatedly, to reduce variance. A more compact description based on structural causal models and potential outcomes [@Pearl2009 Ch.\\xa05] would also be possible.\\n\\n**input** predictive model $P(S_{1:m},\\\\Theta^{\\\\mathrm{R}}_{1:m}\\\\mid\\\\theta^{\\\\mathrm{R}}_*, \\\\pi)$, distribution $P(\\\\Theta^{\\\\mathrm{R}}_*)$, past states $s_{1:t}$, reward functions $\\\\theta^{\\\\mathrm{R}}_{1:t}$, and actions $a_{1:t-1}$, safe policy $\\\\pi^{\\\\textrm{safe}}$ let $P(\\\\theta^{\\\\mathrm{R}}_*\\\\mid s_{1:m},\\\\theta^{\\\\mathrm{R}}_{1:m},a_{1:m-1}) \\\\propto\\n    P(\\\\theta^{\\\\mathrm{R}}_*)P(s_{1:m},\\\\theta^{\\\\mathrm{R}}_{1:m} \\\\mid  \\\\theta^{\\\\mathrm{R}}_*, a_{1:m-1})$ Bayes\\' rule sample $S_{t+1:m},$, $\\\\Theta^{\\\\mathrm{R}}_{t+1:m}$ and $A_{t:m-1}$ from $P(\\\\cdot \\\\mid\\n     s_{1:t}, \\\\theta^{\\\\mathrm{R}}_{1:t}, a_{1:t-1}, \\\\pi)$ sample $\\\\Theta^{\\\\mathrm{R}}_*$ from $P(\\\\cdot\\\\mid s_{1:t}, \\\\theta^{\\\\mathrm{R}}_{1:t}, a_{1:t-1}, S_{t+1:m}, \\\\Theta^{\\\\mathrm{R}}_{t+1:m}, A_{t:m-1})$ using Bayes\\' rule sample $\\\\tilde\\\\Theta^{\\\\mathrm{R}}_{1}, \\\\dots \\\\tilde\\\\Theta^{\\\\mathrm{R}}_m$ from $P(\\\\cdot\\\\mid s_1, \\\\hat\\\\Theta^{\\\\mathrm{R}}_*, \\\\pi^{\\\\textrm{safe}})$ let $V^\\\\pi = \\\\sum_{i=t+1}^mR(S_i; \\\\tilde\\\\Theta^{\\\\mathrm{R}}_i)$ (better: let $V^\\\\pi$ be the average of many runs) let $\\\\pi^* = \\\\argmax_\\\\pi V^\\\\pi$ $A_t = \\\\pi^*(S_t)$\\n\\nThe difference between using history-based and belief-based rewards in POMDPs is minor:\\n\\n**input** predictive model $P(O_{t+1:m}, A_{t+1:m-1}, \\\\mid o_{1:t}, a_{1:t}, \\\\theta^{\\\\textrm{PM}}_t, \\\\pi)$, current history $o_{1:t}, a_{1:t}$, current (history-based) reward function $R(\\\\cdot;\\\\theta^{\\\\mathrm{R}}_t)$ $V^\\\\pi = \\\\EE\\\\left[\\\\sum_{i=t+1}^m\\n      R(O_{t+1:i}, A_{t+1:i-1}, o_{1:t}, a_{1:t} ;\\\\theta^{\\\\mathrm{R}}_t)\\\\mmid\\n      b_t, \\\\theta^{\\\\textrm{PM}}_t, \\\\pi\\\\right]$ let $\\\\pi^* = \\\\argmax_\\\\pi V^\\\\pi$ $A_t = \\\\pi^*(o_{1:t}, a_{1:t})$\\n\\n**input** predictive model $P(B_{t:m}\\\\mid b_t, \\\\theta^{\\\\textrm{PM}}_t, \\\\pi)$, current belief state $b_t$, current (belief-based) reward function $R(\\\\cdot;\\\\theta^{\\\\mathrm{R}}_t)$, $V^\\\\pi = \\\\EE\\\\left[\\\\sum_{i=t+1}^m R(B_i;\\\\theta^{\\\\mathrm{R}}_t)\\\\mmid b_t, \\\\theta^{\\\\textrm{PM}}_t, \\\\pi\\\\right]$ let $\\\\pi^* = \\\\argmax_\\\\pi V^\\\\pi$ $A_t = \\\\pi^*(b_t)$\\n\\n[^1]: In our terminology, any agent that optimizes a (cumulative) reward signal is an RL agent.\\n\\n[^2]: Such capability increases may include better ability to make plans and counterfactual predictions in novel and complex environments.\\n\\n[^3]: In reality, the episode length $m$ will typically be rather large, to allow the agent to learn by interacting with the environment over many time steps. However, for representational purposes, an episode length of $m=3$ forms a sweet spot that represents both the multi-timestep dynamics that we are concerned with, while keeping the diagrams compact enough to be easily readable.\\n\\n[^4]: An implementation is available at: <https://github.com/deepmind/ai-safety-gridworlds/blob/master/ai_safety_gridworlds/environments/rocks_diamonds.py>\\n\\n[^5]: In contrast to the rocks and diamonds, the agent can walk over the user and the reward parameters tiles. The wall tile can neither be pushed nor walked over.\\n\\n[^6]: One exception is when the implemented RF already assigns maximal reward to all states, in which case the agent lacks instrumental goal to tamper with it.\\n\\n[^7]: Of course, agent $k$ might wish it could tamper with the current implemented RF $\\\\Theta^{\\\\mathrm{R}}_k$, but this is not a problem since $\\\\Theta^{\\\\mathrm{R}}_k$ occurs before $A^k_k$, so the agent is unable to influence it.\\n\\n[^8]: Previously called *simulation optimization* [@Everitt2018phd].\\n\\n[^9]: Previously called *corruption aware* [@Everitt2018phd] and *realistic* [@Everitt2016sm].\\n\\n[^10]: were jointly referred to as *modification independence* by @Everitt2016sm. If $\\\\Theta^{\\\\mathrm{T}}$ is added to the graph, then we further require that there are no arrows $\\\\Theta^{\\\\mathrm{T}}\\\\to \\\\Theta^{\\\\mathrm{R}}_t$ or $\\\\Theta^{\\\\mathrm{R}}_t\\\\to \\\\Theta^{\\\\mathrm{T}}$, and that $\\\\Theta^{\\\\mathrm{T}}$ and $\\\\Theta^{\\\\mathrm{R}}_t$ lack joint ancestors.\\n\\n[^11]: To be precise, the agent has an instrumental goal to preserve the implemented RF in the sense that the policies for $A_2^2$ optimal with respect to $\\\\Theta^{\\\\mathrm{R}}_2$ are a subset of those optimal with respect to $\\\\Theta^{\\\\mathrm{R}}_1$.\\n\\n[^12]: Previously called *corruption unaware* [@Everitt2018phd].\\n\\n[^13]: @Shah2019 exploit the inverse information flow, using states to infer an intended reward function.\\n\\n[^14]: User-provided data can take many different forms. In practice, people have used trajectory preferences [@Christiano2017hp], reward functions [@Hadfield-Menell2017ird], value advice [@Knox2009], user actions [@Hadfield-Menell2016cirl], expert demonstrations [@Ng2000], verbal instructions, and many others [@Leike2018alignment; @Jeon2020; @Shah2019].\\n\\n[^15]: To avoid an instrumental goal to tamper with the mechanism that incorporates user updates, agents should be designed to optimize the (predicted) output of the *current* learning process, a generalization of current-RF optimization.\\n\\n[^16]: If $\\\\Theta^{\\\\mathrm{T}}$ is added to the diagram as discussed on , then we also require no arrows $\\\\Theta^{\\\\mathrm{T}}\\\\to D_t$, $\\\\Theta^{\\\\mathrm{T}}\\\\to\\\\Theta^{\\\\mathrm{R}}_*$, or $\\\\Theta^{\\\\mathrm{R}}_*\\\\to\\\\Theta^{\\\\mathrm{T}}$.\\n\\n[^17]: Somewhat loosely defined by the effect that an intervention on a task-relevant feature at a preceding time step would have on the RF-input.\\n\\n[^18]: Called the *delusion box problem* by @Ring2011.\\n\\n[^19]: @Dewey2011 called optimization of a history-based reward function *observation-utility maximization*.\\n\\n[^20]: In terminology, a *state-based utility function*.\\n\\n[^21]: Such an instrumental goal requires the agent to be TI-considering also with respect to its implemented reward function; in principle, an agent can be TI-considering with respect to just one of its predictive model and reward function, as well as both.\\n',\n",
       "  'bibliography_bbl': '',\n",
       "  'bibliography_bib': ''},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2003.03384v2',\n",
       "  'title': 'AutoML-Zero: Evolving Machine Learning Algorithms From Scratch',\n",
       "  'authors': ['Esteban Real', 'Chen Liang', 'David R. So', 'Quoc V. Le'],\n",
       "  'date_published': '2020-03-06 19:00:04+00:00',\n",
       "  'data_last_modified': '2020-06-30 04:32:44+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2003.03384v2',\n",
       "  'abstract': 'Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.',\n",
       "  'author_comment': 'Accepted for publication at the 37th International Conference on\\n  Machine Learning (ICML 2020). Near camera-ready version',\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.NE', 'stat.ML', 'I.2.2; I.2.6'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': '',\n",
       "  'text': '',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{106}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Alba \\\\& Tomassini(2002)Alba and Tomassini]{alba2002parallelism}\\nAlba, E. and Tomassini, M.\\n\\\\newblock Parallelism and evolutionary algorithms.\\n\\\\newblock \\\\emph{IEEE transactions on evolutionary computation}, 2002.\\n\\n\\\\bibitem[Alet et~al.(2019)Alet, Schneider, Lozano-Perez, and\\n  Kaelbling]{alet2019meta}\\nAlet, F., Schneider, M.~F., Lozano-Perez, T., and Kaelbling, L.~P.\\n\\\\newblock Meta-learning curiosity algorithms.\\n\\\\newblock In \\\\emph{International Conference on Learning Representations}, 2019.\\n\\n\\\\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,\\n  Schaul, and de~Freitas]{andrychowicz2016learning}\\nAndrychowicz, M., Denil, M., Gomez, S., Hoffman, M.~W., Pfau, D., Schaul, T.,\\n  and de~Freitas, N.\\n\\\\newblock Learning to learn by gradient descent by gradient descent.\\n\\\\newblock In \\\\emph{NIPS}, 2016.\\n\\n\\\\bibitem[Angeline et~al.(1994)Angeline, Saunders, and\\n  Pollack]{angeline1994evolutionary}\\nAngeline, P.~J., Saunders, G.~M., and Pollack, J.~B.\\n\\\\newblock An evolutionary algorithm that constructs recurrent neural networks.\\n\\\\newblock \\\\emph{IEEE transactions on Neural Networks}, 1994.\\n\\n\\\\bibitem[Baker et~al.(2017)Baker, Gupta, Naik, and Raskar]{baker2016designing}\\nBaker, B., Gupta, O., Naik, N., and Raskar, R.\\n\\\\newblock Designing neural network architectures using reinforcement learning.\\n\\\\newblock In \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Balog et~al.(2017)Balog, Gaunt, Brockschmidt, Nowozin, and\\n  Tarlow]{balog2017deepcoder}\\nBalog, M., Gaunt, A.~L., Brockschmidt, M., Nowozin, S., and Tarlow, D.\\n\\\\newblock Deepcoder: Learning to write programs.\\n\\\\newblock \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Bello et~al.(2017)Bello, Zoph, Vasudevan, and Le]{bello2017neural}\\nBello, I., Zoph, B., Vasudevan, V., and Le, Q.~V.\\n\\\\newblock Neural optimizer search with reinforcement learning.\\n\\\\newblock \\\\emph{ICML}, 2017.\\n\\n\\\\bibitem[Bengio et~al.(1994)Bengio, Bengio, and Cloutier]{bengio1994use}\\nBengio, S., Bengio, Y., and Cloutier, J.\\n\\\\newblock Use of genetic programming for the search of a new learning rule for\\n  neural networks.\\n\\\\newblock In \\\\emph{Evolutionary Computation}, 1994.\\n\\n\\\\bibitem[Bengio(2012)]{bengio2012practical}\\nBengio, Y.\\n\\\\newblock Practical recommendations for gradient-based training of deep\\n  architectures.\\n\\\\newblock In \\\\emph{Neural networks: Tricks of the trade}. Springer, 2012.\\n\\n\\\\bibitem[Bengio et~al.(2013)Bengio, L{\\\\\\'e}onard, and\\n  Courville]{bengio2013estimating}\\nBengio, Y., L{\\\\\\'e}onard, N., and Courville, A.\\n\\\\newblock Estimating or propagating gradients through stochastic neurons for\\n  conditional computation.\\n\\\\newblock \\\\emph{arXiv preprint}, 2013.\\n\\n\\\\bibitem[Bergstra \\\\& Bengio(2012)Bergstra and Bengio]{bergstra2012random}\\nBergstra, J. and Bengio, Y.\\n\\\\newblock Random search for hyper-parameter optimization.\\n\\\\newblock \\\\emph{JMLR}, 2012.\\n\\n\\\\bibitem[Cai et~al.(2019)Cai, Zhu, and Han]{cai2018proxylessnas}\\nCai, H., Zhu, L., and Han, S.\\n\\\\newblock Proxylessnas: Direct neural architecture search on target task and\\n  hardware.\\n\\\\newblock \\\\emph{ICLR}, 2019.\\n\\n\\\\bibitem[Chalmers(1991)]{chalmers1991evolution}\\nChalmers, D.~J.\\n\\\\newblock The evolution of learning: An experiment in genetic connectionism.\\n\\\\newblock In \\\\emph{Connectionist Models}. Elsevier, 1991.\\n\\n\\\\bibitem[Chen et~al.(2017)Chen, Liu, and Song]{chen2017towards}\\nChen, X., Liu, C., and Song, D.\\n\\\\newblock Towards synthesizing complex programs from input-output examples.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1706.01284}, 2017.\\n\\n\\\\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and\\n  Hutter]{chrabaszcz2017downsampled}\\nChrabaszcz, P., Loshchilov, I., and Hutter, F.\\n\\\\newblock A downsampled variant of imagenet as an alternative to the cifar\\n  datasets.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1707.08819}, 2017.\\n\\n\\\\bibitem[Collins(2002)]{collins2002discriminative}\\nCollins, M.\\n\\\\newblock Discriminative training methods for hidden markov models: Theory and\\n  experiments with perceptron algorithms.\\n\\\\newblock In \\\\emph{Proceedings of the ACL-02 conference on Empirical methods in\\n  natural language processing-Volume 10}, pp.\\\\  1--8. Association for\\n  Computational Linguistics, 2002.\\n\\n\\\\bibitem[Cubuk et~al.(2019{\\\\natexlab{a}})Cubuk, Zoph, Mane, Vasudevan, and\\n  Le]{cubuk2018autoaugment}\\nCubuk, E.~D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q.~V.\\n\\\\newblock Autoaugment: Learning augmentation policies from data.\\n\\\\newblock \\\\emph{CVPR}, 2019{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Cubuk et~al.(2019{\\\\natexlab{b}})Cubuk, Zoph, Shlens, and\\n  Le]{cubuk2019randaugment}\\nCubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.~V.\\n\\\\newblock Randaugment: Practical automated data augmentation with a reduced\\n  search space.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1909.13719}, 2019{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Devlin et~al.(2017)Devlin, Uesato, Bhupatiraju, Singh, rahman Mohamed,\\n  and Kohli]{Devlin2017RobustFillNP}\\nDevlin, J., Uesato, J., Bhupatiraju, S., Singh, R., rahman Mohamed, A., and\\n  Kohli, P.\\n\\\\newblock Robustfill: Neural program learning under noisy i/o.\\n\\\\newblock In \\\\emph{ICML}, 2017.\\n\\n\\\\bibitem[Elsken et~al.(2019{\\\\natexlab{a}})Elsken, Metzen, and\\n  Hutter]{elsken2018efficient}\\nElsken, T., Metzen, J.~H., and Hutter, F.\\n\\\\newblock Efficient multi-objective neural architecture search via lamarckian\\n  evolution.\\n\\\\newblock In \\\\emph{ICLR}, 2019{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Elsken et~al.(2019{\\\\natexlab{b}})Elsken, Metzen, and\\n  Hutter]{elsken2019neural}\\nElsken, T., Metzen, J.~H., and Hutter, F.\\n\\\\newblock Neural architecture search.\\n\\\\newblock In \\\\emph{Automated Machine Learning}. Springer, 2019{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Fahlman \\\\& Lebiere(1990)Fahlman and Lebiere]{fahlman1990cascade}\\nFahlman, S.~E. and Lebiere, C.\\n\\\\newblock The cascade-correlation learning architecture.\\n\\\\newblock In \\\\emph{NIPS}, 1990.\\n\\n\\\\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}\\nFinn, C., Abbeel, P., and Levine, S.\\n\\\\newblock Model-agnostic meta-learning for fast adaptation of deep networks.\\n\\\\newblock In \\\\emph{ICML}, 2017.\\n\\n\\\\bibitem[Forsyth et~al.(1981)]{forsyth1981beagle}\\nForsyth, R. et~al.\\n\\\\newblock Beagle-a darwinian approach to pattern recognition.\\n\\\\newblock \\\\emph{Kybernetes}, 10\\\\penalty0 (3):\\\\penalty0 159--166, 1981.\\n\\n\\\\bibitem[Gaier \\\\& Ha(2019)Gaier and Ha]{gaier2019weight}\\nGaier, A. and Ha, D.\\n\\\\newblock Weight agnostic neural networks.\\n\\\\newblock In \\\\emph{NeurIPS}, 2019.\\n\\n\\\\bibitem[Ghiasi et~al.(2019)Ghiasi, Lin, and Le]{ghiasi2019fpn}\\nGhiasi, G., Lin, T.-Y., and Le, Q.~V.\\n\\\\newblock Nas-fpn: Learning scalable feature pyramid architecture for object\\n  detection.\\n\\\\newblock In \\\\emph{CVPR}, 2019.\\n\\n\\\\bibitem[Goldberg \\\\& Deb(1991)Goldberg and Deb]{goldberg1991comparative}\\nGoldberg, D.~E. and Deb, K.\\n\\\\newblock A comparative analysis of selection schemes used in genetic\\n  algorithms.\\n\\\\newblock \\\\emph{FOGA}, 1991.\\n\\n\\\\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and\\n  Courville]{goodfellow2016deep}\\nGoodfellow, I., Bengio, Y., and Courville, A.\\n\\\\newblock \\\\emph{Deep learning}.\\n\\\\newblock MIT press, 2016.\\n\\n\\\\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}\\nGraves, A., Wayne, G., and Danihelka, I.\\n\\\\newblock Neural turing machines.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1410.5401}, 2014.\\n\\n\\\\bibitem[Gulwani et~al.(2017)Gulwani, Polozov, Singh,\\n  et~al.]{gulwani2017program}\\nGulwani, S., Polozov, O., Singh, R., et~al.\\n\\\\newblock Program synthesis.\\n\\\\newblock \\\\emph{Foundations and Trends{\\\\textregistered} in Programming\\n  Languages}, 2017.\\n\\n\\\\bibitem[Hazan et~al.(2015)Hazan, Levy, and Shalev-Shwartz]{hazan2015beyond}\\nHazan, E., Levy, K., and Shalev-Shwartz, S.\\n\\\\newblock Beyond convexity: Stochastic quasi-convex optimization.\\n\\\\newblock In \\\\emph{Advances in Neural Information Processing Systems}, pp.\\\\\\n  1594--1602, 2015.\\n\\n\\\\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}\\nHe, K., Zhang, X., Ren, S., and Sun, J.\\n\\\\newblock Delving deep into rectifiers: Surpassing human-level performance on\\n  imagenet classification.\\n\\\\newblock In \\\\emph{ICCV}, 2015.\\n\\n\\\\bibitem[Hochreiter \\\\& Schmidhuber(1997)Hochreiter and\\n  Schmidhuber]{hochreiter1997long}\\nHochreiter, S. and Schmidhuber, J.\\n\\\\newblock Long short-term memory.\\n\\\\newblock \\\\emph{Neural Computation}, 1997.\\n\\n\\\\bibitem[Holland(1975)]{holland1975adaptation}\\nHolland, J.\\n\\\\newblock Adaptation in natural and artificial systems: an introductory\\n  analysis with application to biology.\\n\\\\newblock \\\\emph{Control and artificial intelligence}, 1975.\\n\\n\\\\bibitem[Hutter et~al.(2011)Hutter, Hoos, and\\n  Leyton-Brown]{hutter2011sequential}\\nHutter, F., Hoos, H.~H., and Leyton-Brown, K.\\n\\\\newblock Sequential model-based optimization for general algorithm\\n  configuration.\\n\\\\newblock In \\\\emph{LION}, 2011.\\n\\n\\\\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,\\n  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan,\\n  et~al.]{jaderberg2017population}\\nJaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W.~M., Donahue, J.,\\n  Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., et~al.\\n\\\\newblock Population based training of neural networks.\\n\\\\newblock \\\\emph{arXiv}, 2017.\\n\\n\\\\bibitem[Jayakumar et~al.(2020)Jayakumar, Menick, Czarnecki, Schwarz, Rae,\\n  Osindero, Teh, Harley, and Pascanu]{jayakumar2020multiplicative}\\nJayakumar, S.~M., Menick, J., Czarnecki, W.~M., Schwarz, J., Rae, J., Osindero,\\n  S., Teh, Y.~W., Harley, T., and Pascanu, R.\\n\\\\newblock Multiplicative interactions and where to find them.\\n\\\\newblock In \\\\emph{ICLR}, 2020.\\n\\n\\\\bibitem[Kim \\\\& Rigazio(2015)Kim and Rigazio]{kim2015deep}\\nKim, M. and Rigazio, L.\\n\\\\newblock Deep clustered convolutional kernels.\\n\\\\newblock \\\\emph{arXiv}, 2015.\\n\\n\\\\bibitem[Koza \\\\& Koza(1992)Koza and Koza]{koza1992genetic}\\nKoza, J.~R. and Koza, J.~R.\\n\\\\newblock \\\\emph{Genetic programming: on the programming of computers by means\\n  of natural selection}.\\n\\\\newblock MIT press, 1992.\\n\\n\\\\bibitem[Krizhevsky \\\\& Hinton(2009)Krizhevsky and\\n  Hinton]{krizhevsky2009learning}\\nKrizhevsky, A. and Hinton, G.\\n\\\\newblock Learning multiple layers of features from tiny images.\\n\\\\newblock \\\\emph{Master\\'s thesis, Dept. of Computer Science, U. of Toronto},\\n  2009.\\n\\n\\\\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}\\nLake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B.\\n\\\\newblock Human-level concept learning through probabilistic program induction.\\n\\\\newblock \\\\emph{Science}, 350\\\\penalty0 (6266):\\\\penalty0 1332--1338, 2015.\\n\\n\\\\bibitem[LeCun et~al.(1995)LeCun, Bengio, et~al.]{lecun1995convolutional}\\nLeCun, Y., Bengio, Y., et~al.\\n\\\\newblock Convolutional networks for images, speech, and time series.\\n\\\\newblock \\\\emph{The handbook of brain theory and neural networks}, 1995.\\n\\n\\\\bibitem[LeCun et~al.(1998)LeCun, Cortes, and Burges]{lecun1998mnist}\\nLeCun, Y., Cortes, C., and Burges, C.~J.\\n\\\\newblock The mnist database of handwritten digits, 1998.\\n\\n\\\\bibitem[Lenat(1983)]{lenat1983eurisko}\\nLenat, D.~B.\\n\\\\newblock Eurisko: a program that learns new heuristics and domain concepts:\\n  the nature of heuristics iii: program design and results.\\n\\\\newblock \\\\emph{Artificial intelligence}, 21\\\\penalty0 (1-2):\\\\penalty0 61--98,\\n  1983.\\n\\n\\\\bibitem[Levy(2016)]{levy2016power}\\nLevy, K.~Y.\\n\\\\newblock The power of normalization: Faster evasion of saddle points.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1611.04831}, 2016.\\n\\n\\\\bibitem[Li \\\\& Malik(2017)Li and Malik]{li2017learning}\\nLi, K. and Malik, J.\\n\\\\newblock Learning to optimize.\\n\\\\newblock \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Li \\\\& Talwalkar(2019)Li and Talwalkar]{li2019RS}\\nLi, L. and Talwalkar, A.\\n\\\\newblock Random search and reproducibility for neural architecture search.\\n\\\\newblock \\\\emph{CoRR}, abs/1902.07638, 2019.\\n\\\\newblock URL \\\\url{http://arxiv.org/abs/1902.07638}.\\n\\n\\\\bibitem[Li et~al.(2018)Li, Jamieson, DeSalvo, Rostamizadeh, and\\n  Talwalkar]{li2017hyperband}\\nLi, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A.\\n\\\\newblock Hyperband: A novel bandit-based approach to hyperparameter\\n  optimization.\\n\\\\newblock \\\\emph{JMLR}, 2018.\\n\\n\\\\bibitem[Liang et~al.(2016)Liang, Berant, Le, Forbus, and\\n  Lao]{Liang2016NeuralSM}\\nLiang, C., Berant, J., Le, Q.~V., Forbus, K.~D., and Lao, N.\\n\\\\newblock Neural symbolic machines: Learning semantic parsers on freebase with\\n  weak supervision.\\n\\\\newblock In \\\\emph{ACL}, 2016.\\n\\n\\\\bibitem[Liang et~al.(2018)Liang, Norouzi, Berant, Le, and\\n  Lao]{Liang2018MemoryAP}\\nLiang, C., Norouzi, M., Berant, J., Le, Q.~V., and Lao, N.\\n\\\\newblock Memory augmented policy optimization for program synthesis and\\n  semantic parsing.\\n\\\\newblock In \\\\emph{NeurIPS}, 2018.\\n\\n\\\\bibitem[Liu et~al.(2018)Liu, Zoph, Shlens, Hua, Li, Fei-Fei, Yuille, Huang,\\n  and Murphy]{liu2017progressive}\\nLiu, C., Zoph, B., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L., Yuille, A.,\\n  Huang, J., and Murphy, K.\\n\\\\newblock Progressive neural architecture search.\\n\\\\newblock \\\\emph{ECCV}, 2018.\\n\\n\\\\bibitem[Liu et~al.(2019{\\\\natexlab{a}})Liu, Chen, Schroff, Adam, Hua, Yuille,\\n  and Fei-Fei]{liu2019auto}\\nLiu, C., Chen, L.-C., Schroff, F., Adam, H., Hua, W., Yuille, A.~L., and\\n  Fei-Fei, L.\\n\\\\newblock Auto-deeplab: Hierarchical neural architecture search for semantic\\n  image segmentation.\\n\\\\newblock In \\\\emph{CVPR}, 2019{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Liu et~al.(2019{\\\\natexlab{b}})Liu, Simonyan, and Yang]{liu2018darts}\\nLiu, H., Simonyan, K., and Yang, Y.\\n\\\\newblock Darts: Differentiable architecture search.\\n\\\\newblock \\\\emph{ICLR}, 2019{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Loshchilov \\\\& Hutter(2016)Loshchilov and Hutter]{loshchilov2016cma}\\nLoshchilov, I. and Hutter, F.\\n\\\\newblock Cma-es for hyperparameter optimization of deep neural networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1604.07269}, 2016.\\n\\n\\\\bibitem[Mei et~al.(2020)Mei, Li, Lian, Jin, Yang, Yuille, and\\n  Yang]{mei2019atomnas}\\nMei, J., Li, Y., Lian, X., Jin, X., Yang, L., Yuille, A., and Yang, J.\\n\\\\newblock Atomnas: Fine-grained end-to-end neural architecture search.\\n\\\\newblock \\\\emph{ICLR}, 2020.\\n\\n\\\\bibitem[Mendoza et~al.(2016)Mendoza, Klein, Feurer, Springenberg, and\\n  Hutter]{mendoza2016towards}\\nMendoza, H., Klein, A., Feurer, M., Springenberg, J.~T., and Hutter, F.\\n\\\\newblock Towards automatically-tuned neural networks.\\n\\\\newblock In \\\\emph{Workshop on Automatic Machine Learning}, 2016.\\n\\n\\\\bibitem[Metz et~al.(2019)Metz, Maheswaranathan, Cheung, and\\n  Sohl-Dickstein]{metz2019meta}\\nMetz, L., Maheswaranathan, N., Cheung, B., and Sohl-Dickstein, J.\\n\\\\newblock Meta-learning update rules for unsupervised representation learning.\\n\\\\newblock In \\\\emph{ICLR}, 2019.\\n\\n\\\\bibitem[Miikkulainen et~al.(2019)Miikkulainen, Liang, Meyerson, Rawal, Fink,\\n  Francon, Raju, Shahrzad, Navruzyan, Duffy, et~al.]{miikkulainen2019evolving}\\nMiikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O.,\\n  Raju, B., Shahrzad, H., Navruzyan, A., Duffy, N., et~al.\\n\\\\newblock Evolving deep neural networks.\\n\\\\newblock In \\\\emph{Artificial Intelligence in the Age of Neural Networks and\\n  Brain Computing}. Elsevier, 2019.\\n\\n\\\\bibitem[Nair \\\\& Hinton(2010)Nair and Hinton]{nair2010rectified}\\nNair, V. and Hinton, G.~E.\\n\\\\newblock Rectified linear units improve restricted boltzmann machines.\\n\\\\newblock In \\\\emph{ICML}, 2010.\\n\\n\\\\bibitem[Neelakantan et~al.(2015)Neelakantan, Le, and\\n  Sutskever]{Neelakantan2015NeuralPI}\\nNeelakantan, A., Le, Q.~V., and Sutskever, I.\\n\\\\newblock Neural programmer: Inducing latent programs with gradient descent.\\n\\\\newblock \\\\emph{CoRR}, abs/1511.04834, 2015.\\n\\n\\\\bibitem[Negrinho et~al.(2019)Negrinho, Gormley, Gordon, Patil, Le, and\\n  Ferreira]{negrinho2019towards}\\nNegrinho, R., Gormley, M., Gordon, G.~J., Patil, D., Le, N., and Ferreira, D.\\n\\\\newblock Towards modular and programmable architecture search.\\n\\\\newblock In \\\\emph{NeurIPS}, 2019.\\n\\n\\\\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and\\n  Ng]{netzer2011reading}\\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.\\n\\\\newblock Reading digits in natural images with unsupervised feature learning.\\n\\\\newblock 2011.\\n\\n\\\\bibitem[Noy et~al.(2019)Noy, Nayman, Ridnik, Zamir, Doveh, Friedman, Giryes,\\n  and Zelnik-Manor]{noy2019asap}\\nNoy, A., Nayman, N., Ridnik, T., Zamir, N., Doveh, S., Friedman, I., Giryes,\\n  R., and Zelnik-Manor, L.\\n\\\\newblock Asap: Architecture search, anneal and prune.\\n\\\\newblock \\\\emph{arXiv}, 2019.\\n\\n\\\\bibitem[Orchard \\\\& Wang(2016)Orchard and Wang]{orchard2016evolution}\\nOrchard, J. and Wang, L.\\n\\\\newblock The evolution of a generalized neural learning rule.\\n\\\\newblock In \\\\emph{IJCNN}, 2016.\\n\\n\\\\bibitem[Parisotto et~al.(2016)Parisotto, rahman Mohamed, Singh, Li, Zhou, and\\n  Kohli]{Parisotto2016NeuroSymbolicPS}\\nParisotto, E., rahman Mohamed, A., Singh, R., Li, L., Zhou, D., and Kohli, P.\\n\\\\newblock Neuro-symbolic program synthesis.\\n\\\\newblock \\\\emph{ArXiv}, abs/1611.01855, 2016.\\n\\n\\\\bibitem[Park et~al.(2019)Park, Chan, Zhang, Chiu, Zoph, Cubuk, and\\n  Le]{park2019specaugment}\\nPark, D.~S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E.~D., and Le,\\n  Q.~V.\\n\\\\newblock Specaugment: A simple data augmentation method for automatic speech\\n  recognition.\\n\\\\newblock \\\\emph{Proc. Interspeech}, 2019.\\n\\n\\\\bibitem[Pitrat(1996)]{pitrat1996implementation}\\nPitrat, J.\\n\\\\newblock Implementation of a reflective system.\\n\\\\newblock \\\\emph{Future Generation Computer Systems}, 12\\\\penalty0\\n  (2-3):\\\\penalty0 235--242, 1996.\\n\\n\\\\bibitem[Polozov \\\\& Gulwani(2015)Polozov and Gulwani]{Polozov2015FlashMetaAF}\\nPolozov, O. and Gulwani, S.\\n\\\\newblock Flashmeta: a framework for inductive program synthesis.\\n\\\\newblock In \\\\emph{OOPSLA 2015}, 2015.\\n\\n\\\\bibitem[Polyak \\\\& Juditsky(1992)Polyak and Juditsky]{polyak1992acceleration}\\nPolyak, B.~T. and Juditsky, A.~B.\\n\\\\newblock Acceleration of stochastic approximation by averaging.\\n\\\\newblock \\\\emph{SIAM journal on control and optimization}, 30\\\\penalty0\\n  (4):\\\\penalty0 838--855, 1992.\\n\\n\\\\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and\\n  Le]{ramachandran2017searching}\\nRamachandran, P., Zoph, B., and Le, Q.\\n\\\\newblock Searching for activation functions.\\n\\\\newblock \\\\emph{ICLR Workshop}, 2017.\\n\\n\\\\bibitem[Ravi \\\\& Larochelle(2017)Ravi and Larochelle]{ravi2016optimization}\\nRavi, S. and Larochelle, H.\\n\\\\newblock Optimization as a model for few-shot learning.\\n\\\\newblock \\\\emph{ICLR}, 2017.\\n\\n\\\\bibitem[Real et~al.(2017)Real, Moore, Selle, Saxena, Suematsu, Le, and\\n  Kurakin]{real2017large}\\nReal, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.~L., Le, Q., and\\n  Kurakin, A.\\n\\\\newblock Large-scale evolution of image classifiers.\\n\\\\newblock In \\\\emph{ICML}, 2017.\\n\\n\\\\bibitem[Real et~al.(2019)Real, Aggarwal, Huang, and Le]{real2018regularized}\\nReal, E., Aggarwal, A., Huang, Y., and Le, Q.~V.\\n\\\\newblock Regularized evolution for image classifier architecture search.\\n\\\\newblock \\\\emph{AAAI}, 2019.\\n\\n\\\\bibitem[Reed \\\\& de~Freitas(2015)Reed and de~Freitas]{Reed2015NeuralP}\\nReed, S.~E. and de~Freitas, N.\\n\\\\newblock Neural programmer-interpreters.\\n\\\\newblock \\\\emph{CoRR}, abs/1511.06279, 2015.\\n\\n\\\\bibitem[Risi \\\\& Stanley(2010)Risi and Stanley]{risi2010indirectly}\\nRisi, S. and Stanley, K.~O.\\n\\\\newblock Indirectly encoding neural plasticity as a pattern of local rules.\\n\\\\newblock In \\\\emph{International Conference on Simulation of Adaptive\\n  Behavior}, 2010.\\n\\n\\\\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and\\n  Williams]{rumelhart1986learning}\\nRumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.\\n\\\\newblock Learning representations by back-propagating errors.\\n\\\\newblock \\\\emph{Nature}, 1986.\\n\\n\\\\bibitem[Runarsson \\\\& Jonsson(2000)Runarsson and\\n  Jonsson]{runarsson2000evolution}\\nRunarsson, T.~P. and Jonsson, M.~T.\\n\\\\newblock Evolution and design of distributed learning rules.\\n\\\\newblock In \\\\emph{ECNN}, 2000.\\n\\n\\\\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}\\nSchmidhuber, J.\\n\\\\newblock \\\\emph{Evolutionary principles in self-referential learning, or on\\n  learning how to learn: the meta-meta-... hook}.\\n\\\\newblock PhD thesis, Technische Universit{\\\\\"a}t M{\\\\\"u}nchen, 1987.\\n\\n\\\\bibitem[Schmidhuber(2004)]{schmidhuber2004optimal}\\nSchmidhuber, J.\\n\\\\newblock Optimal ordered problem solver.\\n\\\\newblock \\\\emph{Machine Learning}, 54\\\\penalty0 (3):\\\\penalty0 211--254, 2004.\\n\\n\\\\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van\\n  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,\\n  et~al.]{silver2016mastering}\\nSilver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,\\n  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,\\n  et~al.\\n\\\\newblock Mastering the game of go with deep neural networks and tree search.\\n\\\\newblock \\\\emph{Nature}, 2016.\\n\\n\\\\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,\\n  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}\\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,\\n  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.\\n\\\\newblock Mastering the game of go without human knowledge.\\n\\\\newblock \\\\emph{Nature}, 2017.\\n\\n\\\\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}\\nSnoek, J., Larochelle, H., and Adams, R.~P.\\n\\\\newblock Practical bayesian optimization of machine learning algorithms.\\n\\\\newblock In \\\\emph{NIPS}, 2012.\\n\\n\\\\bibitem[So et~al.(2019)So, Liang, and Le]{So2019TheET}\\nSo, D.~R., Liang, C., and Le, Q.~V.\\n\\\\newblock The evolved transformer.\\n\\\\newblock In \\\\emph{ICML}, 2019.\\n\\n\\\\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and\\n  Salakhutdinov]{srivastava2014dropout}\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,\\n  R.\\n\\\\newblock Dropout: A simple way to prevent neural networks from overfitting.\\n\\\\newblock \\\\emph{JMLR}, 2014.\\n\\n\\\\bibitem[Stanley \\\\& Miikkulainen(2002)Stanley and\\n  Miikkulainen]{stanley2002evolving}\\nStanley, K.~O. and Miikkulainen, R.\\n\\\\newblock Evolving neural networks through augmenting topologies.\\n\\\\newblock \\\\emph{Evol.\\\\ Comput.}, 2002.\\n\\n\\\\bibitem[Stanley et~al.(2019)Stanley, Clune, Lehman, and\\n  Miikkulainen]{stanley2019designing}\\nStanley, K.~O., Clune, J., Lehman, J., and Miikkulainen, R.\\n\\\\newblock Designing neural networks through neuroevolution.\\n\\\\newblock \\\\emph{Nature Machine Intelligence}, 2019.\\n\\n\\\\bibitem[Suganuma et~al.(2017)Suganuma, Shirakawa, and\\n  Nagao]{suganuma2017genetic}\\nSuganuma, M., Shirakawa, S., and Nagao, T.\\n\\\\newblock A genetic programming approach to designing convolutional neural\\n  network architectures.\\n\\\\newblock In \\\\emph{GECCO}, 2017.\\n\\n\\\\bibitem[Sun et~al.(2019)Sun, Xue, Zhang, and Yen]{sun2019evolving}\\nSun, Y., Xue, B., Zhang, M., and Yen, G.~G.\\n\\\\newblock Evolving deep convolutional neural networks for image classification.\\n\\\\newblock \\\\emph{IEEE Transactions on Evolutionary Computation}, 2019.\\n\\n\\\\bibitem[Tan et~al.(2019)Tan, Chen, Pang, Vasudevan, Sandler, Howard, and\\n  Le]{tan2019mnasnet}\\nTan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., and Le,\\n  Q.~V.\\n\\\\newblock Mnasnet: Platform-aware neural architecture search for mobile.\\n\\\\newblock In \\\\emph{CVPR}, 2019.\\n\\n\\\\bibitem[Valkov et~al.(2018)Valkov, Chaudhari, Srivastava, Sutton, and\\n  Chaudhuri]{Valkov2018HOUDINILL}\\nValkov, L., Chaudhari, D., Srivastava, A., Sutton, C.~A., and Chaudhuri, S.\\n\\\\newblock Houdini: Lifelong learning as program synthesis.\\n\\\\newblock In \\\\emph{NeurIPS}, 2018.\\n\\n\\\\bibitem[Vanschoren(2019)]{vanschoren2018meta}\\nVanschoren, J.\\n\\\\newblock Meta-learning.\\n\\\\newblock \\\\emph{Automated Machine Learning}, 2019.\\n\\n\\\\bibitem[Wang et~al.(2019)Wang, Lehman, Clune, and Stanley]{wang2019paired}\\nWang, R., Lehman, J., Clune, J., and Stanley, K.~O.\\n\\\\newblock Paired open-ended trailblazer (poet): Endlessly generating\\n  increasingly complex and diverse learning environments and their solutions.\\n\\\\newblock \\\\emph{GECCO}, 2019.\\n\\n\\\\bibitem[Wichrowska et~al.(2017)Wichrowska, Maheswaranathan, Hoffman,\\n  Colmenarejo, Denil, de~Freitas, and Sohl-Dickstein]{wichrowska2017learned}\\nWichrowska, O., Maheswaranathan, N., Hoffman, M.~W., Colmenarejo, S.~G., Denil,\\n  M., de~Freitas, N., and Sohl-Dickstein, J.\\n\\\\newblock Learned optimizers that scale and generalize.\\n\\\\newblock \\\\emph{ICML}, 2017.\\n\\n\\\\bibitem[Wilson et~al.(2018)Wilson, Cussat-Blanc, Luga, and\\n  Miller]{wilson2018evolving}\\nWilson, D.~G., Cussat-Blanc, S., Luga, H., and Miller, J.~F.\\n\\\\newblock Evolving simple programs for playing atari games.\\n\\\\newblock In \\\\emph{Proceedings of the Genetic and Evolutionary Computation\\n  Conference}, pp.\\\\  229--236, 2018.\\n\\n\\\\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, et~al.]{wu2016google}\\nWu, Y., Schuster, M., Chen, Z., Le, Q.~V., Norouzi, M., et~al.\\n\\\\newblock Google\\'s neural machine translation system: Bridging the gap between\\n  human and machine translation.\\n\\\\newblock \\\\emph{arXiv}, 2016.\\n\\n\\\\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}\\nXiao, H., Rasul, K., and Vollgraf, R.\\n\\\\newblock Fashion-mnist: a novel image dataset for benchmarking machine\\n  learning algorithms.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1708.07747}, 2017.\\n\\n\\\\bibitem[Xie \\\\& Yuille(2017)Xie and Yuille]{xie2017genetic}\\nXie, L. and Yuille, A.\\n\\\\newblock Genetic {CNN}.\\n\\\\newblock In \\\\emph{ICCV}, 2017.\\n\\n\\\\bibitem[Xie et~al.(2019)Xie, Kirillov, Girshick, and He]{xie2019exploring}\\nXie, S., Kirillov, A., Girshick, R., and He, K.\\n\\\\newblock Exploring randomly wired neural networks for image recognition.\\n\\\\newblock In \\\\emph{Proceedings of the IEEE International Conference on Computer\\n  Vision}, pp.\\\\  1284--1293, 2019.\\n\\n\\\\bibitem[Yang et~al.(2020)Yang, Esperan{\\\\c{c}}a, and\\n  Carlucci]{yang2019evaluation}\\nYang, A., Esperan{\\\\c{c}}a, P.~M., and Carlucci, F.~M.\\n\\\\newblock Nas evaluation is frustratingly hard.\\n\\\\newblock \\\\emph{ICLR}, 2020.\\n\\n\\\\bibitem[Yao et~al.(2018)Yao, Wang, Chen, Dai, Yi-Qi, Yu-Feng, Wei-Wei, Qiang,\\n  and Yang]{yao2018taking}\\nYao, Q., Wang, M., Chen, Y., Dai, W., Yi-Qi, H., Yu-Feng, L., Wei-Wei, T.,\\n  Qiang, Y., and Yang, Y.\\n\\\\newblock Taking human out of learning applications: A survey on automated\\n  machine learning.\\n\\\\newblock \\\\emph{arXiv}, 2018.\\n\\n\\\\bibitem[Yao(1999)]{yao1999evolving}\\nYao, X.\\n\\\\newblock Evolving artificial neural networks.\\n\\\\newblock \\\\emph{IEEE}, 1999.\\n\\n\\\\bibitem[Ying et~al.(2019)Ying, Klein, Real, Christiansen, Murphy, and\\n  Hutter]{ying2019bench}\\nYing, C., Klein, A., Real, E., Christiansen, E., Murphy, K., and Hutter, F.\\n\\\\newblock Nas-bench-101: Towards reproducible neural architecture search.\\n\\\\newblock \\\\emph{ICML}, 2019.\\n\\n\\\\bibitem[Zela et~al.(2018)Zela, Klein, Falkner, and Hutter]{zela2018towards}\\nZela, A., Klein, A., Falkner, S., and Hutter, F.\\n\\\\newblock Towards automated deep learning: Efficient joint neural architecture\\n  and hyperparameter search.\\n\\\\newblock \\\\emph{ICML AutoML Workshop}, 2018.\\n\\n\\\\bibitem[Zhong et~al.(2018)Zhong, Yan, Wu, Shao, and Liu]{zhong2018practical}\\nZhong, Z., Yan, J., Wu, W., Shao, J., and Liu, C.-L.\\n\\\\newblock Practical block-wise neural network architecture generation.\\n\\\\newblock In \\\\emph{CVPR}, 2018.\\n\\n\\\\bibitem[Zoph \\\\& Le(2016)Zoph and Le]{zoph2016neural}\\nZoph, B. and Le, Q.~V.\\n\\\\newblock Neural architecture search with reinforcement learning.\\n\\\\newblock In \\\\emph{ICLR}, 2016.\\n\\n\\\\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2017learning}\\nZoph, B., Vasudevan, V., Shlens, J., and Le, Q.~V.\\n\\\\newblock Learning transferable architectures for scalable image recognition.\\n\\\\newblock In \\\\emph{CVPR}, 2018.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1706.01284': True,\n",
       "   '1707.08819': True,\n",
       "   '1909.13719': True,\n",
       "   '1410.5401': True,\n",
       "   '1611.04831': True,\n",
       "   '1902.07638': True,\n",
       "   '1604.07269': True,\n",
       "   '1511.04834': True,\n",
       "   '1611.01855': True,\n",
       "   '1511.06279': True,\n",
       "   '1708.07747': True},\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Deep learning',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #95',\n",
       "   'newsletter_url': 'https://mailchi.mp/8a9d080d5ce3/an-95-a-framework-for-thinking-about-how-to-make-ai-go-well',\n",
       "   'summarizer': 'Sudhanshu',\n",
       "   'summary': 'Most previous work in the area of automated machine learning, or AutoML, has focussed on narrow search spaces that are restricted to specific parts of the machine learning pipeline, e.g. the architecture of a neural network, or the optimizer in meta-learning. These spaces are often so constrained by the hand-engineered components around them that architectures and algorithms discovered, say, by evolutionary\\xa0search (ES), are only slightly better than random search (RS). This work aims to set up the problem with very weak constraints and a wide search space: a) a machine learning program has three component functions, _Setup_, _Predict_, and _Learn_, which start out empty, and b) are populated by RS or ES with procedural operations from over 50 arithmetic, trigonometric, linear algebra, probability, and pre-calculus operators.\\n\\nThey demonstrate that with such a vast search space, RS fares very poorly in comparison to ES. They also report that ES finds several procedures that are recognizable as useful for machine learning, such as a simple neural network, gradient descent, gradient normalization, multiplicative interactions, noise augmentation, noisy dropout and learning rate decay.',\n",
       "   'opinion': \"This work empirically demonstrates that we now have sufficient methods and tricks in our ES toolkit that enable us to evolve machine learning algorithms from scratch. Additionally, this process produces computer code, which itself may yield to theoretical analysis furthering our knowledge of learning algorithms. I think that powerful AI systems of the future may employ such techniques to discover solutions.\\n\\n**Rohin's opinion:** Its cool to see automation of even the ML algorithms themselves. However, note that there is no deep learning or function approximation in this system: it is a simple search algorithm that is commonly used in program synthesis. In fact, though the paper is presented as an ML paper, it seems to me much more like a classic program synthesis paper that happens to be working in the domain of ML algorithms.\\n\\nWithout any learned heuristics, these algorithms have a hard time scaling: they can only synthesize relatively short snippets of code, since the search space grows exponentially as the code gets longer. Good results depend very strongly on being able to create a DSL in which the correct program is short. In this case, it looks like strong programs are about ~20 straight-line instructions, which seems on the more impressive side given the simplicity of the algorithm and the DSL, though they did throw a huge amount of compute at the problem.\",\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '2003.03384v2',\n",
       "   'arxiv_id': '2003.03384',\n",
       "   'title': 'AutoML-Zero: Evolving Machine Learning Algorithms From Scratch',\n",
       "   'authors': ['Esteban Real', 'Chen Liang', 'David R. So', 'Quoc V. Le'],\n",
       "   'date_published': '2020-03-06 19:00:04+00:00',\n",
       "   'data_last_modified': '2020-06-30 04:32:44+00:00',\n",
       "   'url': 'http://arxiv.org/abs/2003.03384v2',\n",
       "   'abstract': 'Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.',\n",
       "   'author_comment': 'Accepted for publication at the 37th International Conference on\\n  Machine Learning (ICML 2020). Near camera-ready version',\n",
       "   'journal_ref': 'None',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'cs.LG',\n",
       "   'categories': \"['cs.LG', 'cs.NE', 'stat.ML', 'I.2.2; I.2.6']\",\n",
       "   'individual_summary': \"Title: AutoML-Zero: Evolving Machine Learning Algorithms From Scratch\\nAuthors: Esteban Real*, Chen Liang*, David R. So, Quoc V. Le\\nPaper abstract: Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.\\nSummary: Most previous work in the area of automated machine learning, or AutoML, has focussed on narrow search spaces that are restricted to specific parts of the machine learning pipeline, e.g. the architecture of a neural network, or the optimizer in meta-learning. These spaces are often so constrained by the hand-engineered components around them that architectures and algorithms discovered, say, by evolutionary\\xa0search (ES), are only slightly better than random search (RS). This work aims to set up the problem with very weak constraints and a wide search space: a) a machine learning program has three component functions, _Setup_, _Predict_, and _Learn_, which start out empty, and b) are populated by RS or ES with procedural operations from over 50 arithmetic, trigonometric, linear algebra, probability, and pre-calculus operators.\\n\\nThey demonstrate that with such a vast search space, RS fares very poorly in comparison to ES. They also report that ES finds several procedures that are recognizable as useful for machine learning, such as a simple neural network, gradient descent, gradient normalization, multiplicative interactions, noise augmentation, noisy dropout and learning rate decay.\\nMy opinion: This work empirically demonstrates that we now have sufficient methods and tricks in our ES toolkit that enable us to evolve machine learning algorithms from scratch. Additionally, this process produces computer code, which itself may yield to theoretical analysis furthering our knowledge of learning algorithms. I think that powerful AI systems of the future may employ such techniques to discover solutions.\\n\\n**Rohin's opinion:** Its cool to see automation of even the ML algorithms themselves. However, note that there is no deep learning or function approximation in this system: it is a simple search algorithm that is commonly used in program synthesis. In fact, though the paper is presented as an ML paper, it seems to me much more like a classic program synthesis paper that happens to be working in the domain of ML algorithms.\\n\\nWithout any learned heuristics, these algorithms have a hard time scaling: they can only synthesize relatively short snippets of code, since the search space grows exponentially as the code gets longer. Good results depend very strongly on being able to create a DSL in which the correct program is short. In this case, it looks like strong programs are about ~20 straight-line instructions, which seems on the more impressive side given the simplicity of the algorithm and the DSL, though they did throw a huge amount of compute at the problem.\",\n",
       "   'paper_text': '',\n",
       "   'text': 'HIGHLIGHTS\\n[Current Work in AI Alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) *(Paul Christiano)* (summarized by Rohin): In this talk (whose main slide we covered [before](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38) ([AN #74](https://mailchi.mp/49c956f84771/an-74separating-beneficial-ai-into-competence-alignment-and-coping-with-impacts))), Paul Christiano explains how he decomposes the problem of beneficial AI:1. At the top level, \"make AI go well\" is decomposed into making AI competent, making AI aligned, and coping with the impacts of AI. Paul focuses on the alignment part, which he defines as building AI systems that are *trying* to do what we want. See [Clarifying \"AI Alignment\"](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment) ([AN #33](https://mailchi.mp/b6dc636f6a1b/alignment-newsletter-33)) and [my comment on it](https://www.alignmentforum.org/posts/ZeE7EKHTFMBs8eMxn/clarifying-ai-alignment#3ECKoYzFNW2ZqS6km). Paul considers many problems of competence as separate from alignment, including understanding humans well, and most reliability / robustness work.2. Within alignment, we can consider the concept of an \"alignment tax\": the cost incurred by insisting that we only deploy aligned AI. One approach is to help pay the alignment tax, for example, by convincing important actors that they should care about alignment, or by adopting agreements that make it easier to coordinate to pay the tax, as with the [OpenAI Charter](https://blog.openai.com/openai-charter/) ([AN #2](https://mailchi.mp/14782876a85d/alignment-newsletter-2))). Technical AI safety research on the other hand can help *reduce* the alignment tax, by creating better aligned AI systems (which consequently incur less cost than before).3. With alignment tax reduction, we could either try to advance current alignable algorithms (making them more competent, and so reducing their tax), or make existing algorithms alignable. It would be particularly nice to take some general class of algorithms (such as deep reinforcement learning) and figure out how to transform them to make them alignable, such that improvements to the algorithms automatically translate to improvements in the alignable version. This is what Paul works on.4. The next layer is simply a decomposition of possible algorithms we could try to align, e.g. planning, deduction, and learning. Paul focuses on learning.5. Within aligned learning, we can distinguish between outer alignment (finding an objective that incentivizes aligned behavior) and inner alignment (ensuring that the trained agent robustly pursues the aligned objective). Paul works primarily on outer alignment, but has [written about inner alignment](https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d) ([AN #81](https://mailchi.mp/6078fe4f9928/an-81-universality-as-a-potential-solution-to-conceptual-difficulties-in-intent-alignment)).6. Within outer alignment, we could either consider algorithms that learn from a teacher, such as imitiation learning or preference inference, or we could find algorithms that perform better than the teacher (as would be needed for superhuman performance). Paul focuses on the latter case.7. To go beyond the teacher, you could extrapolate beyond what you\\'ve seen (i.e. generalization), do some sort of [ambitious value learning](https://www.alignmentforum.org/posts/5eX8ko7GCxwR5N9mN/what-is-ambitious-value-learning) ([AN #31](https://mailchi.mp/7d0e3916e3d9/alignment-newsletter-31)), or build a better teacher. Paul focuses on the last case, and thinks of amplification as a way to achieve this. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** I really like this decomposition. I already laid out most of my thoughts back when I summarized just the [main slide](https://ai-alignment.com/ai-alignment-landscape-d3773c37ae38) ([AN #74](https://mailchi.mp/49c956f84771/an-74separating-beneficial-ai-into-competence-alignment-and-coping-with-impacts)); I still endorse them. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n TECHNICAL AI ALIGNMENT\\n\\ufeff\\n\\n ITERATED AMPLIFICATION\\n[Unsupervised Question Decomposition for Question Answering](https://arxiv.org/abs/2002.09758) *(Ethan Perez et al)* (summarized by Zach): Existing methods are proficient at simple question and answering (QA). These simple questions are called single-hop and can be answered with a single yes/no or underlined passage in the text. However, progress on the more difficult task of multi-hop QA lags behind. **This paper introduces a method that can decompose hard multi-hop questions into easier single-hop questions that existing QA systems can answer.** Since collecting labeled decompositions is hard, the authors introduce a pseudo-decomposition where multi-hop questions are matched with similar single-hop questions while making sure the single-hop questions are diverse. Following this, the model is trained to map multi-hop questions to simpler subquestions using *unsupervised* sequence-to-sequence learning (as they found the supervised version performed worse). They show large improvement on the popular HotPot QA baseline with large improvement on out-of-domain questions due to the ability of sub-questions to help gather supporting facts that can be used to answer questions.  |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Zach\\'s opinion:** A core feature of this paper is the unsupervised approach to producing question decompositions. By doing this, it\\'s possible to augment the data-set significantly by question-crawling the data-sets which helps explain why the model has performance on-par with supervised approaches. Moreover, looking at a few decomposition examples from the model seems to indicate that relevant sub-questions are being discovered. It\\'s worth noting that decompositions with more than two questions are unlikely due to the specific loss used in the main paper. In the appendix, the authors experiment with a different loss for the pseudo-decomposition that allows more questions in the decomposition, but it performs slightly worse than the original loss. This makes me wonder whether or not such a procedure would be useful if used recursively to create sub-sub-questions. Overall, I think the decomposition is useful for both down-stream processing and interpretation.**Rohin\\'s opinion:** The capabilities of methods like iterated amplification depend on the ability to solve hard questions by decomposing them into simpler questions that we already know how to answer, and then combining the results appropriately. This paper demonstrates that even a very basic unsupervised approach (\"decompose into the most similar simpler questions\") to decomposition can work quite well, at least for current AI systems.In private correspondence, Ethan suggested that in the long term a semi-supervised approach would probably work best, which agrees with my intuitions. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n AGENT FOUNDATIONS\\n[An Orthodox Case Against Utility Functions](https://www.alignmentforum.org/posts/A8iGaZ3uHNNGgJeaD/an-orthodox-case-against-utility-functions) *(Abram Demski)* (summarized by Rohin): How might we theoretically ground utility functions? One approach could be to view the possible environments as a set of universe histories (e.g. a list of the positions of all quarks, etc. at all times), and a utility function as a function that maps these universe histories to real numbers. We might want this utility function to be computable, but this eliminates some plausible preferences we might want to represent. For example, in the procrastination paradox, the subject prefers to push the button as late as possible, but disprefers never pressing the button. If the history is infinitely long, no computable function can know for sure that the button was never pressed: it\\'s always possible that it was pressed at some later day.Instead, we could use *subjective utility functions*, which are defined over *events*, which is basically anything you can think about (i.e. it could be chairs and tables, or quarks and strings). This allows us to have utility functions over high level concepts. In the previous example, we can define an event \"never presses the button\", and reason about that event atomically, sidestepping the issues of computability.We could go further and view *probabilities* as subjective (as in the Jeffrey-Bolkor axioms), and only require that our beliefs are updated in such a way that we cannot be Dutch-booked. This is the perspective taken in logical induction.\\ufeff\\n\\n INTERPRETABILITY\\n[Neuron Shapley: Discovering the Responsible Neurons](https://arxiv.org/abs/2002.09815) *(Amirata Ghorbani et al)* (summarized by Robert): This paper presents a novel method, Neuron Shapley, that uses the [Shapley value framework](https://en.wikipedia.org/wiki/Shapley_value) to measure the importance of different neurons in determining an arbitrary metric of the neural net output. (Shapley values have been applied to machine learning before to [measure the importance of features to a model\\'s output](https://christophm.github.io/interpretable-ml-book/shapley.html), but here the authors use them to calculate neuron importance.) Due to several novel approaches and optimisations in calculating these Shapley values, **the top k most responsible neurons (k ~ 30) can be\\xa0feasibly found for large networks such as Inception-v3**.The authors demonstrate that finding these neurons enables the performance of model surgery. Removing the top 30 neurons that contribute to accuracy completely destroys the accuracy, whereas in expectation removing 30 neurons at random from the network barely moves the accuracy at all. Since the method can be applied to an arbitrary metric, this kind of surgery can be performed for other\\xa0metrics we\\xa0care about. For example, removing the neurons which are most responsible for vulnerability to adversarial attacks makes the network more robust, and removing the neurons most responsible for the class-accuracy imbalance (a fairness metric) makes the classes much more even, while only reducing the overall accuracy by a small amount. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Robert\\'s opinion:** It\\'s nice to see an interpretability method with demonstrable and measurable use cases. Many methods aim at improving insight, but often don\\'t demonstrate this aim; I think this paper does this well in showing how its method can be used for model surgery. I think methods that allow us to investigate and understand individual neurons and their contributions are useful in building up a fine grained picture of how neural networks work. This links to previous work such as [Network Dissection](http://netdissect.csail.mit.edu/) as well as the recent [Circuits Thread](https://distill.pub/2020/circuits/) on Distill, and I\\'d love to see how these methods interact. They all give different kinds of understanding, and I think it would be interesting to see if given the results of the circuits tools we were able to predict which neurons where most responsible for different metrics (Neuron Shapley) or aligned to which relevant features (Network Dissection). |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Visualizing Neural Networks with the Grand Tour](https://distill.pub/2020/grand-tour/) *(Mingwei Li et al)* (summarized by Flo): Visualizing a complete dataset instead of single input examples is helpful when we want to analyze the relationships between different input examples and how their classification changes during training, as we can do so by looking at a single video. The authors use an example on MNIST in which the network learns to classify the numbers 1 and 7 in an almost discrete fashion during particular epochs to compare different methods for visualizing how the dataset is classified. They find that one problem with nonlinear dimensionality reduction like t-SNE and UMAPs is that changes to a subset of the dataset can strongly affect how unchanged data points are represented. Then they compare this to the Grand Tour, a classical technique that projects the data into two dimensions from varying points of view. As projections are linear in the input variables, it is rather easy to reason about how changes in the data affect this visualization and the times the classes 1 and 7 are learnt are indeed quite salient in their example. Another advantage of this method is that confusion between two specific classes can be identified more easily, as the corresponding data points will be projected onto the line connecting the clusters for these classes. A similar approach can be taken on a network\\'s hidden layers to identify the layer in which different classes become clearly distinguishable. They find that they can identify adversarial examples generated by FGSM by looking at the second to last layer, where the adversarial examples form a cluster distinct from the real images. As the Grand Tour involves varying rotations, it is basically unaffected by rotations of the data. The authors argue that this is a feature, as rotations are small changes to the data and should not have a large effect on the visualization. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Flo\\'s opinion:** The dataset perspective on visualization seems pretty useful as a quick diagnostic tool for practitioners, but less useful than feature visualization for a detailed understanding of a model. While I think that it is good to highlight invariances, I am not convinced that rotational invariance is actually desirable for visualizing intermediate layers of a neural network, as most nonlinearities are strongly affected by rotations.  |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n FORECASTING\\n[Atari early](https://aiimpacts.org/atari-early/) *(Katja Grace)* (summarized by Rohin): With DeepMind\\'s Agent57 (summarized below), it seems that it is feasible to outperform professional game testers on all Atari games using no game-specific knowledge. Interestingly, in a 2016 survey, the median response put a small chance (10%) on this being feasible by 2021, and a medium chance (50%) of being feasible by 2026. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n OTHER PROGRESS IN AI\\n\\ufeff\\n\\n REINFORCEMENT LEARNING\\n[Agent57: Outperforming the human Atari benchmark](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark) *(Adri Puigdomnech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann et al)* (summarized by Sudhanshu): This blogpost and its associated [arxiv publication](https://arxiv.org/abs/2003.13350) present Agent57, DeepMind\\'s latest RL agent created for the purpose of achieving human-level performance in a suite of 57 Atari games. Notably, Agent57 is the first agent that is able to surpass average human performance, as measured by Human Normalized Score or HNS, on every individual game in the suite, with the same set of hyperparameters. The blogpost\\xa0details the evolution of DeepMind\\'s Atari agents from DQN up to Agent57, and the paper elaborates on the improvements made in Agent57.Specifically, Agent57 builds on a recent agent \\'Never Give Up\\' (NGU), which itself augments R2D2 with episodic memory for curiosity-driven exploration. Agent57 introduces (i) a new parameterization of state-action value function that decomposes into intrinsic and extrinsic rewards, and (ii) a meta-controller which selects which\\xa0of its numerous distributed policies to prioritize during learning, allowing the agent to control the exploration/exploitation trade-off.**Read more:** [Paper: Agent57: Outperforming the Atari Human Benchmark](https://arxiv.org/abs/2003.13350) |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Sudhanshu\\'s opinion:** On the one hand, this work feels like the achievement of an important milestone in DeepMind\\'s ongoing research agenda towards building more general agents. On the other hand, it has the flavour of engineered sophistry: a remarkable collection of building blocks arranged together to patch specific known weaknesses, but lacking in core insights about how to make agents more general,\\xa0without, say, making them more complex.The work is well presented and accessible, especially the blogpost that contains a snapshot of the functional development of deep reinforcement learning capabilities over time. There are several open questions from here on out; personally, I hope this progresses to a single instance of an agent that is proficient at multiple games, and to the design of agents that do not require extensive hyperparameter tuning. The scale of DeepMind\\'s experiments continues to grow, with 256 actors, and 10s of billions of frames, suggesting that, for now, this work is only suitable for simulated environments. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| [Massively Scaling Reinforcement Learning with SEED RL](https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html) *(Lasse Espeholt, Raphal Marinier, Piotr Stanczyk et al)* (summarized by Nicholas): Deep learning has [historically](https://blog.openai.com/ai-and-compute/) ([AN #7](https://mailchi.mp/3e550712419a/alignment-newsletter-7)) seen many improvements as a result of scaling to larger models with larger amounts of computation, as with the months-long training of [OpenAI Five](http://arxiv.org/abs/1912.06680) ([AN #82](https://mailchi.mp/7ba40faa7eed/an-82-how-openai-five-distributed-their-training-computation)) and [AlphaStar](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/) ([AN #43](https://mailchi.mp/768a8130013f/alignment-newsletter-43)). SEED RL redesigns the architecture of distributed RL to enable better machine utilization and communication and achieves an order of magnitude improvement in training speed. Current distributed architectures typically separate machines into *actors* and *learners*. *Actors* are typically CPUs that simulate the environment, and run inference to predict agent actions. They then send *trajectories* to the *learners*. *Learners* are typically accelerators (GPUs or TPUs), which are responsible for training the model. They then send the updated model parameters to the *actors*. SEED RL addresses 3 main issues in this setup:1. Inference could benefit from specialized accelerators2. Sending model parameters and states requires high bandwidth.3. Environment simulation and inference are very different tasks and having them on the same machine makes it hard to utilize the resource efficiently. The solution is to instead have actors **only** simulate the environment. After each step, they send the resulting observation to the *learner*, which is responsible for both training and inference, possibly split on separate hardware. It then sends back just the actions to the environment. This enables each piece of hardware to be used for its designed purpose. Since they now need to communicate at each step, they use gRPC to minimize latency.**Read more:** [Paper: SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference](https://arxiv.org/abs/1910.06591) |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Nicholas\\' opinion:** Given how compute-intensive deep RL is, I think it is quite useful to enable cheaper and faster training before these algorithms can be broadly useful. Their claimed speedup is quite impressive, and I like how well they can separate the training and inference from the simulation. I expect that specialized hardware for both training and inference will soon become the norm and SEED RL seems like it will scale well as those accelerators become faster. One thing to note is that this architecture seems very specifically tuned to the problem of games where CPUs can efficiently simulate the environment and it does not improve the sample efficiency for situations where we cant run lots of simulations.**Rohin\\'s opinion:** It was quite surprising to me that this worked as well as it did: this model requires communication across machines *at every timestep of the environment*, which intuitively means that latency should be a major bottleneck, while the standard model only requires communication once per batch of trajectories. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n DEEP LEARNING\\n[AutoML-Zero: Evolving Machine Learning Algorithms From Scratch](http://arxiv.org/abs/2003.03384) *(Esteban Real, Chen Liang et al)* (summarized by Sudhanshu): Most previous work in the area of automated machine learning, or AutoML, has focussed on narrow search spaces that are restricted to specific parts of the machine learning pipeline, e.g. the architecture of a neural network, or the optimizer in meta-learning. These spaces are often so constrained by the hand-engineered components around them that architectures and algorithms discovered, say, by evolutionary\\xa0search (ES), are only slightly better than random search (RS). This work aims to set up the problem with very weak constraints and a wide search space: a) a machine learning program has three component functions, *Setup*, *Predict*, and *Learn*, which start out empty, and b) are populated by RS or ES with procedural operations from over 50 arithmetic, trigonometric, linear algebra, probability, and pre-calculus operators.They demonstrate that with such a vast search space, RS fares very poorly in comparison to ES. They also report that ES finds several procedures that are recognizable as useful for machine learning, such as a simple neural network, gradient descent, gradient normalization, multiplicative interactions, noise augmentation, noisy dropout and learning rate decay. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Sudhanshu\\'s opinion:** This work empirically demonstrates that we now have sufficient methods and tricks in our ES toolkit that enable us to evolve machine learning algorithms from scratch. Additionally, this process produces computer code, which itself may yield to theoretical analysis furthering our knowledge of learning algorithms. I think that powerful AI systems of the future may employ such techniques to discover solutions. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n NEWS\\n[Announcing Web-TAISU, May 13-17](https://www.alignmentforum.org/posts/CMnMaTxNAhXfcEtgm/announcing-web-taisu-may-13-17) *(Linda Linsefors)* (summarized by Rohin): The [Technical AI Safety Unconference](https://www.lesswrong.com/events/yuMuDGnJ8omGhMx9y/taisu-technical-ai-safety-unconference) ([AN #57](https://mailchi.mp/392d2043e782/an-57why-we-should-focus-on-robustness-in-ai-safety-and-the-analogous-problems-in-programming)) will be held online from May 13-17. |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI\\'m always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2020 Alignment Newsletter, All rights reserved.*\\n\\n**'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '2102.04074v1',\n",
       "  'title': 'Learning Curve Theory',\n",
       "  'authors': ['Marcus Hutter'],\n",
       "  'date_published': '2021-02-08 09:25:31+00:00',\n",
       "  'data_last_modified': '2021-02-08 09:25:31+00:00',\n",
       "  'url': 'http://arxiv.org/abs/2102.04074v1',\n",
       "  'abstract': 'Recently a number of empirical \"universal\" scaling law papers have been published, most notably by OpenAI. `Scaling laws\\' refers to power-law decreases of training or test error w.r.t. more data, larger neural networks, and/or more compute. In this work we focus on scaling w.r.t. data size $n$. Theoretical understanding of this phenomenon is largely lacking, except in finite-dimensional models for which error typically decreases with $n^{-1/2}$ or $n^{-1}$, where $n$ is the sample size. We develop and theoretically analyse the simplest possible (toy) model that can exhibit $n^{-\\\\beta}$ learning curves for arbitrary power $\\\\beta>0$, and determine whether power laws are universal or depend on the data distribution.',\n",
       "  'author_comment': '26 pages, 6 Figures',\n",
       "  'journal_ref': 'Latest 2021 version at http://www.hutter1.net/publ/scaling.pdf',\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'stat.ML'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 1.0,\n",
       "  'main_tex_filename': '',\n",
       "  'text': '',\n",
       "  'bibliography_bbl': '',\n",
       "  'bibliography_bib': '',\n",
       "  'alignment_newsletter': {'source': 'alignment-newsletter',\n",
       "   'source_type': 'google-sheets',\n",
       "   'converted_with': 'python',\n",
       "   'venue': 'arXiv',\n",
       "   'newsletter_category': 'Deep learning',\n",
       "   'highlight': False,\n",
       "   'newsletter_number': 'AN #141',\n",
       "   'newsletter_url': 'https://mailchi.mp/93d2051b7d70/an-141the-case-for-practicing-alignment-work-on-gpt-3-and-other-large-models',\n",
       "   'summarizer': 'Rohin',\n",
       "   'summary': 'Like [last weeks highlight](https://arxiv.org/abs/2102.06701) ([AN #140](https://mailchi.mp/229fd666e06b/an-140-theoretical-models-that-predict-scaling-laws)), this paper proposes a theoretical model that could predict empirically observable scaling laws. The author considers a very simple online learning model, in which we are given a feature vector and must classify it into one of two categories. Well also consider a very simple tabular algorithm that just memorizes the classifications of all previously seen vectors and spits out the correct classification if it has been seen before, and otherwise says I dont know. How does the error incurred by this algorithm scale with data size?\\n\\nThe answer of course depends on the data distribution -- if we always see the same feature vector, then we never make an error after the first timestep, whereas if the vector is chosen uniformly at random, well always have maximal error. The author analyzes several possible data distributions in between these extremes.\\n\\nThe most interesting case is when the data is drawn from a Zipf distribution. In this case, when you order the feature vectors from most to least likely, the nth vector has probability proportional to n^(-(+1)). Then we see a power law for the scaling, n^(-), where  =  / (+1). This could explain the scaling laws observed in the wild.',\n",
       "   'opinion': 'As with last weeks paper, Im happy to see more work on understanding scaling laws. For this paper, the assumption on reality is in which data distribution we assume the data is drawn from. However, overall I feel less compelled by this paper than with the one from last week, for two reasons: First, it seems to me that using a tabular (memorization) algorithm is probably too coarse of a model; I would guess that there are facts about neural nets that are relevant to scaling that arent captured by tabular algorithms. Second, I prefer the assumption that the data are drawn from a low-dimensional manifold rather than that the data are drawn from some specific distribution like a Zipf distribution (or others discussed in the paper).',\n",
       "   'prerequisites': 'nan',\n",
       "   'read_more': 'nan',\n",
       "   'paper_version': '2102.04074v1',\n",
       "   'arxiv_id': '2102.04074',\n",
       "   'title': 'Learning Curve Theory',\n",
       "   'authors': ['Marcus Hutter'],\n",
       "   'date_published': '2021-02-08 09:25:31+00:00',\n",
       "   'data_last_modified': '2021-02-08 09:25:31+00:00',\n",
       "   'url': 'http://arxiv.org/abs/2102.04074v1',\n",
       "   'abstract': 'Recently a number of empirical \"universal\" scaling law papers have been published, most notably by OpenAI. `Scaling laws\\' refers to power-law decreases of training or test error w.r.t. more data, larger neural networks, and/or more compute. In this work we focus on scaling w.r.t. data size $n$. Theoretical understanding of this phenomenon is largely lacking, except in finite-dimensional models for which error typically decreases with $n^{-1/2}$ or $n^{-1}$, where $n$ is the sample size. We develop and theoretically analyse the simplest possible (toy) model that can exhibit $n^{-\\\\beta}$ learning curves for arbitrary power $\\\\beta>0$, and determine whether power laws are universal or depend on the data distribution.',\n",
       "   'author_comment': '26 pages, 6 Figures',\n",
       "   'journal_ref': 'Latest 2021 version at http://www.hutter1.net/publ/scaling.pdf',\n",
       "   'doi': 'None',\n",
       "   'primary_category': 'cs.LG',\n",
       "   'categories': \"['cs.LG', 'stat.ML']\",\n",
       "   'individual_summary': 'Title: Learning Curve Theory\\nAuthors: Marcus Hutter\\nPaper abstract: Recently a number of empirical \"universal\" scaling law papers have been published, most notably by OpenAI. `Scaling laws\\' refers to power-law decreases of training or test error w.r.t. more data, larger neural networks, and/or more compute. In this work we focus on scaling w.r.t. data size $n$. Theoretical understanding of this phenomenon is largely lacking, except in finite-dimensional models for which error typically decreases with $n^{-1/2}$ or $n^{-1}$, where $n$ is the sample size. We develop and theoretically analyse the simplest possible (toy) model that can exhibit $n^{-\\\\beta}$ learning curves for arbitrary power $\\\\beta>0$, and determine whether power laws are universal or depend on the data distribution.\\nSummary: Like [last weeks highlight](https://arxiv.org/abs/2102.06701) ([AN #140](https://mailchi.mp/229fd666e06b/an-140-theoretical-models-that-predict-scaling-laws)), this paper proposes a theoretical model that could predict empirically observable scaling laws. The author considers a very simple online learning model, in which we are given a feature vector and must classify it into one of two categories. Well also consider a very simple tabular algorithm that just memorizes the classifications of all previously seen vectors and spits out the correct classification if it has been seen before, and otherwise says I dont know. How does the error incurred by this algorithm scale with data size?\\n\\nThe answer of course depends on the data distribution -- if we always see the same feature vector, then we never make an error after the first timestep, whereas if the vector is chosen uniformly at random, well always have maximal error. The author analyzes several possible data distributions in between these extremes.\\n\\nThe most interesting case is when the data is drawn from a Zipf distribution. In this case, when you order the feature vectors from most to least likely, the nth vector has probability proportional to n^(-(+1)). Then we see a power law for the scaling, n^(-), where  =  / (+1). This could explain the scaling laws observed in the wild.\\nMy opinion: As with last weeks paper, Im happy to see more work on understanding scaling laws. For this paper, the assumption on reality is in which data distribution we assume the data is drawn from. However, overall I feel less compelled by this paper than with the one from last week, for two reasons: First, it seems to me that using a tabular (memorization) algorithm is probably too coarse of a model; I would guess that there are facts about neural nets that are relevant to scaling that arent captured by tabular algorithms. Second, I prefer the assumption that the data are drawn from a low-dimensional manifold rather than that the data are drawn from some specific distribution like a Zipf distribution (or others discussed in the paper).',\n",
       "   'paper_text': '',\n",
       "   'text': 'HIGHLIGHTS\\n[The case for aligning narrowly superhuman models](https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models) *(Ajeya Cotra)* (summarized by Rohin): One argument against work on AI safety is that [it is hard to do good work without feedback loops](https://www.jefftk.com/p/why-global-poverty). So how could we get feedback loops? The most obvious approach is to actually try to align strong models right now, in order to get practice with aligning models in the future. This post fleshes out what such an approach might look like. Note that I will not be covering all of the points mentioned in the post; if you find yourself skeptical, you may want to read the full post as your question might be answered there.The author specifically suggests that we work on **aligning narrowly superhuman models** to make them more useful. *Aligning* a model roughly means harnessing the full capabilities of the model and orienting these full capabilities towards helping humans. For example, GPT-3 presumably knows a lot about medicine and health. How can we get GPT-3 to apply this knowledge as best as possible to be maximally useful in answering user questions about health?*Narrowly superhuman* means that the model has more knowledge or latent capability than either its overseers or its users. In the example above, GPT-3 almost certainly has more medical knowledge than laypeople, so it is at least narrowly superhuman at giving medical advice relative to laypeople. (It might even be so relative to doctors, given how broad its knowledge is.)[Learning to Summarize with Human Feedback](https://openai.com/blog/learning-to-summarize-with-human-feedback/) ([AN #116](https://mailchi.mp/d31663e4d330/an-116-how-to-make-explanations-of-neurons-compositional)) is a good example of what this could look like: that paper attempted to bring out GPT-3s latent capability to write summaries, and outperformed the reference summaries written by humans. This sort of work will be needed for any new powerful model we train, and so it has a lot of potential for growing the field of people concerned about long-term risk.Note that the focus here is on aligning *existing* capabilities to make a model more useful, and so simply increasing capabilities doesnt count. As a concrete example, just scaling up the model capacity or training data or compute would *not* count as an example of aligning narrowly superhuman models, even though it might make the model more useful, since scaling increases raw capabilities without improving alignment. This makes it pretty different from what profit-maximizing companies would do by default: instead of baking in domain knowledge and simply scaling up models in order to solve the easiest profitable problems (as you would do if you wanted to maximize profit), work in this research area would look for general and scalable techniques, would not be allowed to scale up models, and would select interestingly difficult problems.Why is this a fruitful area of research? The author points out four main benefits:1. Most importantly, the more we align systems ahead of time, the more likely that researchers will be able to put thought and consideration into new issues like treacherous turns, rather than spending all their time putting out fires.2. We can build practical know-how and infrastructure for alignment techniques like learning from human feedback.3. As the world gets progressively faster and crazier, well have better AI assistants helping us to navigate the world.4. It improves our chances of discovering or verifying a long-term or full alignment solution.See also [MIRIs comments](https://www.alignmentforum.org/posts/AyfDnnAdjG7HHeD3d/miri-comments-on-cotra-s-case-for-aligning-narrowly), which were more positive than I expected.**Read more:** [MIRI comments](https://www.alignmentforum.org/posts/AyfDnnAdjG7HHeD3d/miri-comments-on-cotra-s-case-for-aligning-narrowly) |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** I am very sympathetic to the argument that we should be getting experience with aligning powerful models right now, and would be excited to see more work along these lines. As the post mentions, I personally see this sort of work as a strong baseline, and while I currently think that the conceptual work Im doing is more important, I wouldnt be surprised if I worked on a project in this vein within the next two years.I especially agree with the point that this is one of the most scalable forms of research, and am personally working on a [benchmark](https://docs.google.com/document/d/18MEmQ4aA1zdZHBKG5fLeISYoFAv1IfUozWZ5JYnq-bY/edit) meant to incentivize this sort of research for similar reasons. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n TECHNICAL AI ALIGNMENT\\n\\ufeff\\n\\n AGENT FOUNDATIONS\\n[A Semitechnical Introductory Dialogue on Solomonoff Induction](https://www.alignmentforum.org/posts/EL4HNa92Z95FKL9R2/a-semitechnical-introductory-dialogue-on-solomonoff-1) *(Eliezer Yudkowsky)* (summarized by Rohin): This post is a good introduction to Solomonoff induction and why its interesting (though note it is quite long).\\ufeff\\n\\n INTERPRETABILITY\\n[What mechanisms drive agent behaviour?](https://medium.com/@deepmindsafetyresearch/what-mechanisms-drive-agent-behaviour-e7b8d9aee88) *(Grgoire Dletang et al)* (summarized by Rohin): A common challenge when understanding the world is that it is very hard to infer causal structure from only observational data. Luckily, we arent limited to observational data in the case of AI systems: we can intervene on either the environment the agent is acting in, or the agent itself, and see what happens. In this paper, the authors present an agent debugger that helps with this, which has all the features youd normally expect in a debugger: you can set breakpoints, step forward or backward in the execution trace, and set or monitor variables.Lets consider an example where an agent is trained to go to a high reward apple. However, during training the location of the apple is correlated with the floor type (grass or sand). Suppose we now get an agent that does well in the training environment. How can we tell if the agent looks for the apple and goes there, rather than looking at the floor type and going to the location where the apple was during training?We cant distinguish between these possibilities with just observational data. However, with the agent debugger, we can simulate what the agent would do in the case where the floor type and apple location are different from how they were in training, which can then answer our question.We can go further: using the data collected from simulations using the agent debugger, we can also build a causal model that explains how the agent makes decisions. We do have to identify the features of interest (i.e. the nodes in the causal graph), but the probability tables can be computed automatically from the data from the agent debugger. The resulting causal model can then be thought of as an explanation for the behavior of the agent. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** I very much like the general idea that we really can look at counterfactuals for artificial agents, given that we can control their inputs and internal state. This is the same idea underlying [cross-examination](https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1) ([AN #86](https://mailchi.mp/598f425b1533/an-86-improving-debate-and-factored-cognition-through-human-experiments)), as well as various other kinds of interpretability research.In addition, one nice aspect of causal models as your form of explanation is that you can modulate the size of the causal model based on how many nodes you add to the graph. The full causal model for e.g. GPT-3 would be way too complex to understand, but perhaps we can get a high-level understanding with a causal model with higher-level concepts. Id be very interested to see research tackling these sorts of scaling challenges. |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n FORECASTING\\n[How does bee learning compare with machine learning?](https://www.alignmentforum.org/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning) *(Guilhermo Costa)* (summarized by Rohin): The [biological anchors approach](https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines) ([AN #121](https://mailchi.mp/41774b61e5f8/an-121forecasting-transformative-ai-timelines-using-biological-anchors)) to forecasting AI timelines estimates the compute needed for transformative AI based on the compute used by animals. One important parameter of the framework is needed to bridge between the two: if we find that an animal can do a specific task using X amount of compute, then what should we estimate as the amount of compute needed for an ML model to do the same task? This post aims to better estimate this parameter, by comparing few-shot image classification in bees to the same task in ML models. I wont go through the details here, but the upshot is that (after various approximations and judgment calls) ML models can reach the same performance as bees on few-shot image classification using 1,000 times less compute.If we plug this parameter into the biological anchors framework (without changing any of the other parameters), the median year for transformative AI according to the model changes from 2050 to 2035, though the author advises only updating to (say) 2045 since the results of the investigation are so uncertain. The author also sees this as generally validating the biological anchors approach to forecasting timelines. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** I really liked this post: the problem is important, the approach to tackle it makes sense, and most importantly its very easy to follow the reasoning. I dont think that directly substituting in the 1,000 number into the timelines calculation is the right approach; I think there are a few reasons (explained [here](https://www.alignmentforum.org/posts/yW3Tct2iyBMzYhTw7/how-does-bee-learning-compare-with-machine-learning?commentId=rcJuytMfdQNMb82rR), some of which were mentioned in the post) to think that the comparison was biased in favor of the ML models. I would instead wildly guess that this comparison suggests that a transformative model would use 20x less compute than a human, which still shortens timelines, probably to 2045 or so. (This is before incorporating uncertainty about the conclusions of the report as a whole.) |\\n\\n |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n MISCELLANEOUS (ALIGNMENT)\\n[On the alignment problem](https://80000hours.org/podcast/episodes/brian-christian-the-alignment-problem/?utm_campaign=Feed%3A+80000HoursPodcast+%2880%2C000+Hours+Podcast+with+Rob+Wiblin%29&utm_source=feedburner&utm_medium=feed) *(Rob Wiblin and Brian Christian)* (summarized by Rohin): This 80,000 Hours podcast goes over many of the examples from Brians book, [The Alignment Problem](https://www.amazon.com/Alignment-Problem-Machine-Learning-Values/dp/153669519X) ([AN #120](https://mailchi.mp/42ec72ef7e11/an-120tracing-the-intellectual-roots-of-ai-and-ai-alignment)). I recommend listening to it if you arent going to read the book itself; the examples and stories are fascinating. (Though note I only skimmed through the podcast.)[Epistemological Framing for AI Alignment Research](https://www.alignmentforum.org/posts/Y4YHTBziAscS5WPN7/epistemological-framing-for-ai-alignment-research) *(Adam Shimi)* (summarized by Rohin): This post recommends that we think about AI alignment research in the following framework:1. Defining the problem and its terms: for example, we might want to define agency, optimization, AI, and well-behaved.2. Exploring these definitions, to see what they entail.3. Solving the now well-defined problem.This is explicitly *not* a paradigm, but rather a framework in which we can think about possible paradigms for AI safety. A specific paradigm would choose a specific problem formulation and definition (or at least something significantly more concrete than solve AI safety). However, we are not yet sufficiently deconfused to be able to commit to a specific paradigm; hence this overarching framework. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n AI GOVERNANCE\\n[NSCAI Final Report](https://reports.nscai.gov/final-report/table-of-contents/) *(Eric Schmidt et al)* (summarized by Rohin): In the US, the National Security Commission on AI released their report to Congress. The [full pdf](https://www.nscai.gov/wp-content/uploads/2021/03/Full-Report-Digital-1.pdf) is over 750 pages long, so I have not read it myself, and instead Im adding in some commentary from others. In their [newsletter](https://cset.georgetown.edu/newsletters/), CSET says that highlights include:- A warning that the U.S. military could be at a competitive disadvantage within the next decade if it does not accelerate its AI adoption. The report recommends laying the foundation for widespread AI integration by 2025, comprising a DOD-wide digital ecosystem, a technically literate workforce, and more efficient business practices aided by AI.- A recommendation that the White House establish a new Technology Competitiveness Council, led by the vice president, to develop a comprehensive technology strategy and oversee its implementation.- A recommendation that the U.S. military explore using autonomous weapons systems, provided their use is authorized by human operators.- A proposal to establish a new Digital Service Academy and a civilian National Reserve to cultivate domestic AI talent.- A call to provide $35 billion in federal investment and incentives for domestic semiconductor manufacturing.- A recommendation to double non-defense AI R&D funding annually until it reaches $32 billion per year, and to triple the number of National AI Research Institutes.- A call for reformed export controls, coordinated with allies, on key technologies such as high-end semiconductor manufacturing equipment.- A recommendation that Congress pass a second National Defense Education Act and reform the U.S. immigration system to attract and retain AI students and workers from abroad.While none of the reports recommendations are legally binding, it has [reportedly been well-received by key members of both parties](https://apnews.com/article/ai-panel-urges-us-boost-tech-skills-95b210543d4a42bd6cd5347a46cb74d6).Matthew van der Merwe also summarizes the recommendations in [Import AI](https://jack-clark.net/2021/03/08/import-ai-239-china-trains-a-massive-10b-model-vicarious-does-pickplace-the-gchq-publishes-some-of-its-thoughts-on-ai/); this has a lot of overlap with the CSET summary so I won\\'t copy it here.Jeff Ding adds in [ChinAI #134](https://chinai.substack.com/p/chinai-134-weaponized-interdependence):[I]f you make it past the bluster in the beginning  or take it for what it is: obligatory marketing to cater to a DC audience hooked on a narrow vision of national security  theres some smart moderate policy ideas in the report (e.g. chapter 7 on establishing justified confidence in AI systems).In email correspondence, Jon Rodriguez adds some commentary on the safety implications:1. The report acknowledges the potential danger of AGI, and specifically calls for value alignment research to take place (pg. 36). To my knowledge, this is one of the first times a leading world government has called for value alignment.2. The report makes a clear statement that the US prohibits AI from authorizing the launch of nuclear weapons (pg. 98).3. The report calls for dialogues with China and Russia to ensure that military decisions made by military AI at \"machine speed\" does not lead to out-of-control conflict escalation which humans would not want (pg. 97). |\\n\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\ufeff\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| \\n\\n \\ufeff\\n\\n OTHER PROGRESS IN AI\\n\\ufeff\\n\\n DEEP LEARNING\\n[Learning Curve Theory](https://arxiv.org/abs/2102.04074) *(Marcus Hutter)* (summarized by Rohin): Like [last weeks highlight](https://arxiv.org/abs/2102.06701) ([AN #140](https://mailchi.mp/229fd666e06b/an-140-theoretical-models-that-predict-scaling-laws)), this paper proposes a theoretical model that could predict empirically observable scaling laws. The author considers a very simple online learning model, in which we are given a feature vector and must classify it into one of two categories. Well also consider a very simple tabular algorithm that just memorizes the classifications of all previously seen vectors and spits out the correct classification if it has been seen before, and otherwise says I dont know. How does the error incurred by this algorithm scale with data size?The answer of course depends on the data distribution -- if we always see the same feature vector, then we never make an error after the first timestep, whereas if the vector is chosen uniformly at random, well always have maximal error. The author analyzes several possible data distributions in between these extremes.The most interesting case is when the data is drawn from a Zipf distribution. In this case, when you order the feature vectors from most to least likely, the nth vector has probability proportional to n^(-(+1)). Then we see a power law for the scaling, n^(-), where  =  / (+1). This could explain the scaling laws observed in the wild. |\\n\\n\\n |\\n\\n\\ufeff\\n\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n| **Rohin\\'s opinion:** As with last weeks paper, Im happy to see more work on understanding scaling laws. For this paper, the assumption on reality is in which data distribution we assume the data is drawn from. However, overall I feel less compelled by this paper than with the one from last week, for two reasons. First, it seems to me that using a tabular (memorization) algorithm is probably too coarse of a model; I would guess that there are facts about neural nets that are relevant to scaling that arent captured by tabular algorithms. Second, I prefer the assumption that the data are drawn from a low-dimensional manifold, rather than that the data are drawn from some specific distribution like a Zipf distribution (or others discussed in the paper). |\\n\\n |\\n\\n\\n |\\n\\n |\\n\\n |\\n| \\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| FEEDBACK\\nI\\'m always happy to hear feedback; you can send it to me, [Rohin Shah](https://rohinshah.com/), by **replying to this email**.\\n  |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| PODCAST\\nAn audio podcast version of the **Alignment Newsletter** is available. This podcast is an audio version of the newsletter, recorded by [Robert Miles](http://robertskmiles.com).\\n**Subscribe here:**\\n\\n[RSS Feed](http://alignment-newsletter.libsyn.com/rss)[Google Podcasts](https://podcasts.google.com/?feed=aHR0cDovL2FsaWdubWVudC1uZXdzbGV0dGVyLmxpYnN5bi5jb20vcnNz)[Spotify Podcasts](https://open.spotify.com/show/5pwApVP0wr1Q61S4LmONuX)[Apple Podcasts](https://podcasts.apple.com/us/podcast/alignment-newsletter-podcast/id1489248000) |\\n\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n|  |\\n| --- |\\n|  |\\n\\n |\\n\\n\\n\\n|  |  |\\n| --- | --- |\\n| \\n\\n\\n|  |\\n| --- |\\n| *Copyright  2021 Alignment Newsletter, All rights reserved.*\\n\\n**'}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1907.03843v2',\n",
       "  'title': 'Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem',\n",
       "  'authors': ['Pedro Fernandes', 'Francisco C. Santos', 'Manuel Lopes'],\n",
       "  'date_published': '2019-06-26 10:18:19+00:00',\n",
       "  'data_last_modified': '2020-12-22 18:11:35+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1907.03843v2',\n",
       "  'abstract': 'The rise of artificial intelligence (A.I.) based systems is already offering substantial benefits to the society as a whole. However, these systems may also enclose potential conflicts and unintended consequences. Notably, people will tend to adopt an A.I. system if it confers them an advantage, at which point non-adopters might push for a strong regulation if that advantage for adopters is at a cost for them. Here we propose an agent-based game-theoretical model for these conflicts, where agents may decide to resort to A.I. to use and acquire additional information on the payoffs of a stochastic game, striving to bring insights from simulation to what has been, hitherto, a mostly philosophical discussion. We frame our results under the current discussion on ethical A.I. and the conflict between individual and societal gains: the societal value alignment problem. We test the arising equilibria in the adoption of A.I. technology under different norms followed by artificial agents, their ensuing benefits, and the emergent levels of wealth inequality. We show that without any regulation, purely selfish A.I. systems will have the strongest advantage, even when a utilitarian A.I. provides significant benefits for the individual and the society. Nevertheless, we show that it is possible to develop A.I. systems following human conscious policies that, when introduced in society, lead to an equilibrium where the gains for the adopters are not at a cost for non-adopters, thus increasing the overall wealth of the population and lowering inequality. However, as shown, a self-organised adoption of such policies would require external regulation.',\n",
       "  'author_comment': None,\n",
       "  'journal_ref': 'AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020',\n",
       "  'doi': '10.3233/AIC-201502',\n",
       "  'primary_category': 'cs.CY',\n",
       "  'categories': ['cs.CY', 'cs.AI'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'pos',\n",
       "  'confidence_score': 0.978026084,\n",
       "  'main_tex_filename': '',\n",
       "  'text': '',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{10}\\n\\n\\\\bibitem{asilomarCalifornia}\\nState of california endorses asilomar ai principles.\\n\\\\newblock Future of Life Institute. [Visited on 17/09/19]\\n  https://futureoflife.org/2018/08/31/state-of-california-endorses-asilomar-ai-principles/,\\n  2018.\\n\\n\\\\bibitem{allen2000prolegomena}\\nColin Allen, Gary Varner, and Jason Zinser.\\n\\\\newblock Prolegomena to any future artificial moral agent.\\n\\\\newblock {\\\\em Journal of Experimental \\\\& Theoretical Artificial Intelligence},\\n  12(3):251--261, 2000.\\n\\n\\\\bibitem{anderson2007machine}\\nMichael Anderson and Susan~Leigh Anderson.\\n\\\\newblock Machine ethics: Creating an ethical intelligent agent.\\n\\\\newblock {\\\\em AI Magazine}, 28(4):15, 2007.\\n\\n\\\\bibitem{armstrong2016racing}\\nStuart Armstrong, Nick Bostrom, and Carl Shulman.\\n\\\\newblock Racing to the precipice: a model of artificial intelligence\\n  development.\\n\\\\newblock {\\\\em AI \\\\& society}, 31(2):201--206, 2016.\\n\\n\\\\bibitem{ashton2015fly}\\nKevin Ashton.\\n\\\\newblock {\\\\em How to fly a horse: The secret history of creation, invention,\\n  and discovery}.\\n\\\\newblock Anchor, 2015.\\n\\n\\\\bibitem{asilomar2018principles}\\nAI~Asilomar.\\n\\\\newblock Principles.(2017).\\n\\\\newblock In {\\\\em Principles developed in conjunction with the 2017 Asilomar\\n  conference [Benevolent AI 2017]}, 2018.\\n\\n\\\\bibitem{asimov2004robot}\\nIsaac Asimov.\\n\\\\newblock {\\\\em I, robot}, volume~1.\\n\\\\newblock Spectra, 2004.\\n\\n\\\\bibitem{babcock2017guidelines}\\nJames Babcock, J{\\\\\\'a}nos Kram{\\\\\\'a}r, and Roman~V Yampolskiy.\\n\\\\newblock Guidelines for artificial intelligence containment.\\n\\\\newblock {\\\\em arXiv preprint arXiv:1707.08476}, 2017.\\n\\n\\\\bibitem{beauchamp2008principes}\\nTom~L Beauchamp and James~F Childress.\\n\\\\newblock {\\\\em Les principes de l\\'{\\\\\\'e}thique biom{\\\\\\'e}dicale}.\\n\\\\newblock Belles Lettres, 2008.\\n\\n\\\\bibitem{bernhard2006parochial}\\nHelen Bernhard, Urs Fischbacher, and Ernst Fehr.\\n\\\\newblock Parochial altruism in humans.\\n\\\\newblock {\\\\em Nature}, 442(7105):912, 2006.\\n\\n\\\\bibitem{bonnefon2016social}\\nJean-Fran{\\\\c{c}}ois Bonnefon, Azim Shariff, and Iyad Rahwan.\\n\\\\newblock The social dilemma of autonomous vehicles.\\n\\\\newblock {\\\\em Science}, 352(6293):1573--1576, 2016.\\n\\n\\\\bibitem{bostrom2003ethical}\\nNick Bostrom.\\n\\\\newblock Ethical issues in advanced artificial intelligence.\\n\\\\newblock {\\\\em Science Fiction and Philosophy: From Time Travel to\\n  Superintelligence}, pages 277--284, 2003.\\n\\n\\\\bibitem{bostromoracle}\\nNick Bostrom.\\n\\\\newblock Oracle ai, 2008.\\n\\n\\\\bibitem{bostrom2014superintelligence}\\nNick Bostrom.\\n\\\\newblock {\\\\em Superintelligence}.\\n\\\\newblock 2014.\\n\\n\\\\bibitem{technologyreview17brooksAINEWS}\\nRodney Brooks.\\n\\\\newblock The seven deadly sins of ai predictions.\\n\\\\newblock Technology Review. [Visited on 17/09/19]\\n  https://www.technologyreview.com/s/609048/the-seven-deadly-sins-of-ai-predictions/,\\n  2017.\\n\\n\\\\bibitem{brown2020language}\\nTom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\\n  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\\n  et~al.\\n\\\\newblock Language models are few-shot learners.\\n\\\\newblock {\\\\em arXiv preprint arXiv:2005.14165}, 2020.\\n\\n\\\\bibitem{brundage20184}\\nMiles Brundage.\\n\\\\newblock Scaling up humanity: The case for conditional optimism about\\n  artificial intelligence.\\n\\\\newblock {\\\\em Should we fear artificial intelligence?}, page~13, 2018.\\n\\n\\\\bibitem{chalmers2010singularity}\\nDavid Chalmers.\\n\\\\newblock The singularity: A philosophical analysis.\\n\\\\newblock {\\\\em Journal of Consciousness Studies}, 17(9-10):7--65, 2010.\\n\\n\\\\bibitem{choi2007coevolution}\\nJung-Kyoo Choi and Samuel Bowles.\\n\\\\newblock The coevolution of parochial altruism and war.\\n\\\\newblock {\\\\em Science}, 318(5850):636--640, 2007.\\n\\n\\\\bibitem{conitzer2017moral}\\nVincent Conitzer, Walter Sinnott-Armstrong, Jana~Schaich Borg, Yuan Deng, and\\n  Max Kramer.\\n\\\\newblock Moral decision making frameworks for artificial intelligence.\\n\\\\newblock In {\\\\em Thirty-first aaai conference on artificial intelligence},\\n  2017.\\n\\n\\\\bibitem{de2018social}\\nCelso~M de~Melo, Stacy Marsella, and Jonathan Gratch.\\n\\\\newblock Social decisions and fairness change when peoples interests are\\n  represented by autonomous agents.\\n\\\\newblock {\\\\em Autonomous Agents and Multi-Agent Systems}, 32(1):163--187,\\n  2018.\\n\\n\\\\bibitem{drexler1986engines}\\nK~Eric Drexler.\\n\\\\newblock {\\\\em Engines of creation}.\\n\\\\newblock Anchor, 1986.\\n\\n\\\\bibitem{forbes17airegulationNEWS}\\nAmitai Etzioni and Oren Etzioni.\\n\\\\newblock Why regulating ai is a mistake?\\n\\\\newblock Forbes. [Visited on 17/09/19]\\n  https://www.forbes.com/sites/ciocentral/2017/01/09/why-regulating-ai-is-a-mistake/,\\n  2017.\\n\\n\\\\bibitem{floridi2018an}\\nLuciano Floridi, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice\\n  Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo,\\n  Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena.\\n\\\\newblock An ethical framework for a good ai society: Opportunities, risks,\\n  principles, and recommendations.\\n\\\\newblock {\\\\em Minds and Machines}, 2018.\\n\\n\\\\bibitem{garcia2011evolution}\\nJuli{\\\\\\'a}n Garc{\\\\\\'\\\\i}a and Jeroen~CJM van~den Bergh.\\n\\\\newblock Evolution of parochial altruism by multilevel selection.\\n\\\\newblock {\\\\em Evolution and Human Behavior}, 32(4):277--287, 2011.\\n\\n\\\\bibitem{good1966speculations}\\nIrving~John Good.\\n\\\\newblock Speculations concerning the first ultraintelligent machine.\\n\\\\newblock In {\\\\em Advances in computers}, volume~6, pages 31--88. Elsevier,\\n  1966.\\n\\n\\\\bibitem{Han2019AIWar}\\nT.~A. Han, L.~M. Pereira, and T.~Lenaerts.\\n\\\\newblock Modelling and influencing the ai bidding war: A research agenda.\\n\\\\newblock In {\\\\em Proceedings of the AAAAI/ACM Conference on AI, Ethics, and\\n  Society, (AIES 2019).}, 2019.\\n\\n\\\\bibitem{guardian18cambridAIregulNEWS}\\nAlex Hern.\\n\\\\newblock Cambridge analytica scandal \\'highlights need for ai regulation\\'.\\n\\\\newblock The Guardian. [Visited on 17/09/19], 2018.\\n\\n\\\\bibitem{hleg2019ethics}\\nAI~HLEG.\\n\\\\newblock Ethics guidelines for trustworthy ai, 2019.\\n\\n\\\\bibitem{kalai1993rational}\\nEhud Kalai and Ehud Lehrer.\\n\\\\newblock Rational learning leads to nash equilibrium.\\n\\\\newblock {\\\\em Econometrica: Journal of the Econometric Society}, pages\\n  1019--1045, 1993.\\n\\n\\\\bibitem{mcnamara2016law}\\nSteven McNamara.\\n\\\\newblock The law and ethics of high-frequency trading.\\n\\\\newblock {\\\\em Minn. JL Sci. \\\\& Tech.}, 17:71, 2016.\\n\\n\\\\bibitem{mill2016utilitarianism}\\nJohn~Stuart Mill.\\n\\\\newblock Utilitarianism.\\n\\\\newblock In {\\\\em Seven masterpieces of philosophy}, pages 337--383. Routledge,\\n  2016.\\n\\n\\\\bibitem{nash1951non}\\nJohn Nash.\\n\\\\newblock Non-cooperative games.\\n\\\\newblock {\\\\em Annals of mathematics}, pages 286--295, 1951.\\n\\n\\\\bibitem{nash1950equilibrium}\\nJohn~F Nash et~al.\\n\\\\newblock Equilibrium points in n-person games.\\n\\\\newblock {\\\\em Proceedings of the national academy of sciences}, 36(1):48--49,\\n  1950.\\n\\n\\\\bibitem{nowak1992evolutionary}\\nMartin~A Nowak and Robert~M May.\\n\\\\newblock Evolutionary games and spatial chaos.\\n\\\\newblock {\\\\em Nature}, 359(6398):826, 1992.\\n\\n\\\\bibitem{nowak2005irreview}\\nMartin~A Nowak and Karl Sigmund.\\n\\\\newblock Evolution of indirect reciprocity.\\n\\\\newblock {\\\\em Nature}, 437(7063):1291, 2005.\\n\\n\\\\bibitem{oecd2019ethics}\\nOECD.\\n\\\\newblock Recommendation of the council on artificial intelligence, 2019.\\n\\n\\\\bibitem{pacheco2008evolutionary}\\nJorge~M Pacheco, Francisco~C Santos, Max~O Souza, and Brian Skyrms.\\n\\\\newblock Evolutionary dynamics of collective action in n-person stag hunt\\n  dilemmas.\\n\\\\newblock {\\\\em Proceedings of the Royal Society B: Biological Sciences},\\n  276(1655):315--321, 2008.\\n\\n\\\\bibitem{paiva2018engineering}\\nAna Paiva, Fernando~P Santos, and Francisco~C Santos.\\n\\\\newblock Engineering pro-sociality with autonomous agents.\\n\\\\newblock {\\\\em AAAI 18}, pages 7994--7999, 2018.\\n\\n\\\\bibitem{pereira2016programming}\\nLu{\\\\\\'\\\\i}s~Moniz Pereira and Ari Saptawijaya.\\n\\\\newblock {\\\\em Programming machine ethics}, volume~26.\\n\\\\newblock Springer, 2016.\\n\\n\\\\bibitem{rachels2012ethical}\\nJames Rachels.\\n\\\\newblock Ethical egoism.\\n\\\\newblock {\\\\em Ethical theory: an anthology}, 14:193, 2012.\\n\\n\\\\bibitem{rand2013human}\\nDavid~G Rand and Martin~A Nowak.\\n\\\\newblock Human cooperation.\\n\\\\newblock {\\\\em Trends in cognitive sciences}, 17(8):413--425, 2013.\\n\\n\\\\bibitem{rand2014static}\\nDavid~G Rand, Martin~A Nowak, James~H Fowler, and Nicholas~A Christakis.\\n\\\\newblock Static network structure can stabilize human cooperation.\\n\\\\newblock {\\\\em Proceedings of the National Academy of Sciences},\\n  111(48):17093--17098, 2014.\\n\\n\\\\bibitem{russell2015research}\\nStuart Russell, Daniel Dewey, and Max Tegmark.\\n\\\\newblock Research priorities for robust and beneficial artificial\\n  intelligence.\\n\\\\newblock {\\\\em Ai Magazine}, 36(4):105--114, 2015.\\n\\n\\\\bibitem{sanchez2020does}\\nJavier S{\\\\\\'a}nchez-Monedero, Lina Dencik, and Lilian Edwards.\\n\\\\newblock What does it mean to\\'solve\\'the problem of discrimination in hiring?\\n  social, technical and legal perspectives from the uk on automated hiring\\n  systems.\\n\\\\newblock In {\\\\em Proceedings of the 2020 Conference on Fairness,\\n  Accountability, and Transparency}, pages 458--468, 2020.\\n\\n\\\\bibitem{santos2018nature}\\nFernando~P Santos, Francisco~C Santos, and Jorge~M Pacheco.\\n\\\\newblock Social norm complexity and past reputations in the evolution of\\n  cooperation.\\n\\\\newblock {\\\\em Nature}, 555(7695):242, 2018.\\n\\n\\\\bibitem{santos2011risk}\\nFrancisco~C Santos and Jorge~M Pacheco.\\n\\\\newblock Risk of collective failure provides an escape from the tragedy of the\\n  commons.\\n\\\\newblock {\\\\em Proceedings of the National Academy of Sciences USA},\\n  108(26):10421--10425, 2011.\\n\\n\\\\bibitem{santos2006pnas}\\nFrancisco~C Santos, Jorge~M Pacheco, and Tom Lenaerts.\\n\\\\newblock Evolutionary dynamics of social dilemmas in structured heterogeneous\\n  populations.\\n\\\\newblock {\\\\em Proceedings of the National Academy of Sciences},\\n  103(9):3490--3494, 2006.\\n\\n\\\\bibitem{santos2008nature}\\nFrancisco~C Santos, Marta~D Santos, and Jorge~M Pacheco.\\n\\\\newblock Social diversity promotes the emergence of cooperation in public\\n  goods games.\\n\\\\newblock {\\\\em Nature}, 454(7201):213, 2008.\\n\\n\\\\bibitem{shapiro2002valuealignment}\\nDaniel Shapiro and Ross Shachter.\\n\\\\newblock User-agent value alignment.\\n\\\\newblock In {\\\\em Proc. of The 18th Nat. Conf. on Artif. Intell. AAAI}, 2002.\\n\\n\\\\bibitem{shen2012stock}\\nShunrong Shen, Haomiao Jiang, and Tongda Zhang.\\n\\\\newblock Stock market forecasting using machine learning algorithms.\\n\\\\newblock {\\\\em Department of Electrical Engineering, Stanford University,\\n  Stanford, CA}, pages 1--5, 2012.\\n\\n\\\\bibitem{sigmund2010calculus}\\nKarl Sigmund.\\n\\\\newblock {\\\\em The calculus of selfishness}, volume~6.\\n\\\\newblock Princeton University Press, 2010.\\n\\n\\\\bibitem{tan2018ai}\\nJoshua~Z Tan and Jeffrey Ding.\\n\\\\newblock Ai governance through \"ai\" markets.\\n\\\\newblock online www.joshuatan.com, 2018.\\n\\n\\\\bibitem{taylor2016alignment}\\nJessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch.\\n\\\\newblock Alignment for advanced machine learning systems.\\n\\\\newblock {\\\\em Machine Intelligence Research Institute}, 2016.\\n\\n\\\\bibitem{traulsen2006stochastic}\\nArne Traulsen, Martin~A Nowak, and Jorge~M Pacheco.\\n\\\\newblock Stochastic dynamics of invasion and fixation.\\n\\\\newblock {\\\\em Physical Review E}, 74(1):011909, 2006.\\n\\n\\\\bibitem{turing2009computing}\\nAlan~M Turing.\\n\\\\newblock Computing machinery and intelligence.\\n\\\\newblock {\\\\em Mind}, 49:433--460, 1950.\\n\\n\\\\bibitem{yampolskiy2013safety}\\nRoman Yampolskiy and Joshua Fox.\\n\\\\newblock Safety engineering for artificial general intelligence.\\n\\\\newblock {\\\\em Topoi}, 32(2):217--226, 2013.\\n\\n\\\\bibitem{yampolskiy2012leakproofing}\\nRoman~V Yampolskiy.\\n\\\\newblock Leakproofing singularity-artificial intelligence confinement problem.\\n\\\\newblock {\\\\em Journal of Consciousness Studies JCS}, 2012.\\n\\n\\\\bibitem{yampolskiy2013artificial}\\nRoman~V Yampolskiy.\\n\\\\newblock Artificial intelligence safety engineering: Why machine ethics is a\\n  wrong approach.\\n\\\\newblock In {\\\\em Philosophy and theory of artificial intelligence}, pages\\n  389--396. Springer, 2013.\\n\\n\\\\bibitem{yudkowsky2008artificial}\\nEliezer Yudkowsky.\\n\\\\newblock Artificial intelligence as a positive and negative factor in global\\n  risk.\\n\\\\newblock {\\\\em Global catastrophic risks}, 1(303):184, 2008.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '\\n@article{santos2006ploscb,\\n  title={Cooperation prevails when individuals adjust their social ties},\\n  author={Santos, Francisco C and Pacheco, Jorge M and Lenaerts, Tom},\\n  journal={PLoS computational biology},\\n  volume={2},\\n  number={10},\\n  pages={e140},\\n  year={2006},\\n  publisher={Public Library of Science}\\n}\\n\\n@article{rand2014static,\\n  title={Static network structure can stabilize human cooperation},\\n  author={Rand, David G and Nowak, Martin A and Fowler, James H and Christakis, Nicholas A},\\n  journal={Proceedings of the National Academy of Sciences},\\n  volume={111},\\n  number={48},\\n  pages={17093--17098},\\n  year={2014},\\n  publisher={National Acad Sciences}\\n}\\n\\n@article{nowak1992evolutionary,\\n  title={Evolutionary games and spatial chaos},\\n  author={Nowak, Martin A and May, Robert M},\\n  journal={Nature},\\n  volume={359},\\n  number={6398},\\n  pages={826},\\n  year={1992},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{nowak2005irreview,\\n  title={Evolution of indirect reciprocity},\\n  author={Nowak, Martin A and Sigmund, Karl},\\n  journal={Nature},\\n  volume={437},\\n  number={7063},\\n  pages={1291},\\n  year={2005},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{santos2018nature,\\n  title={Social norm complexity and past reputations in the evolution of cooperation},\\n  author={Santos, Fernando P and Santos, Francisco C and Pacheco, Jorge M},\\n  journal={Nature},\\n  volume={555},\\n  number={7695},\\n  pages={242},\\n  year={2018},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{santos2006pnas,\\n  title={Evolutionary dynamics of social dilemmas in structured heterogeneous populations},\\n  author={Santos, Francisco C and Pacheco, Jorge M and Lenaerts, Tom},\\n  journal={Proceedings of the National Academy of Sciences},\\n  volume={103},\\n  number={9},\\n  pages={3490--3494},\\n  year={2006},\\n  publisher={National Acad Sciences}\\n}\\n\\n@article{santos2008nature,\\n  title={Social diversity promotes the emergence of cooperation in public goods games},\\n  author={Santos, Francisco C and Santos, Marta D and Pacheco, Jorge M},\\n  journal={Nature},\\n  volume={454},\\n  number={7201},\\n  pages={213},\\n  year={2008},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{rand2013human,\\n  title={Human cooperation},\\n  author={Rand, David G and Nowak, Martin A},\\n  journal={Trends in cognitive sciences},\\n  volume={17},\\n  number={8},\\n  pages={413--425},\\n  year={2013},\\n  publisher={Elsevier}\\n}\\n\\n@article{choi2007coevolution,\\n  title={The coevolution of parochial altruism and war},\\n  author={Choi, Jung-Kyoo and Bowles, Samuel},\\n  journal={Science},\\n  volume={318},\\n  number={5850},\\n  pages={636--640},\\n  year={2007}\\n}\\n\\n@article{bernhard2006parochial,\\n  title={Parochial altruism in humans},\\n  author={Bernhard, Helen and Fischbacher, Urs and Fehr, Ernst},\\n  journal={Nature},\\n  volume={442},\\n  number={7105},\\n  pages={912},\\n  year={2006},\\n  publisher={Nature Publishing Group}\\n}\\n\\n@article{paiva2018engineering,\\n  title={Engineering Pro-Sociality with Autonomous Agents},\\n  author={Paiva, Ana and Santos, Fernando P and Santos, Francisco C},\\n  journal={AAAI 18},\\n  pages={7994--7999},\\n  year={2018}\\n}\\n\\n@article{garcia2011evolution,\\n  title={Evolution of parochial altruism by multilevel selection},\\n  author={Garc{\\\\\\'\\\\i}a, Juli{\\\\\\'a}n and van den Bergh, Jeroen CJM},\\n  journal={Evolution and Human Behavior},\\n  volume={32},\\n  number={4},\\n  pages={277--287},\\n  year={2011},\\n  publisher={Elsevier}\\n}\\n\\n@article{pacheco2008evolutionary,\\n  title={Evolutionary dynamics of collective action in N-person stag hunt dilemmas},\\n  author={Pacheco, Jorge M and Santos, Francisco C and Souza, Max O and Skyrms, Brian},\\n  journal={Proceedings of the Royal Society B: Biological Sciences},\\n  volume={276},\\n  number={1655},\\n  pages={315--321},\\n  year={2008},\\n  publisher={The Royal Society London}\\n}\\n\\n@article{santos2011risk,\\n  title={Risk of collective failure provides an escape from the tragedy of the commons},\\n  author={Santos, Francisco C and Pacheco, Jorge M},\\n  journal={Proceedings of the National Academy of Sciences USA},\\n  volume={108},\\n  number={26},\\n  pages={10421--10425},\\n  year={2011},\\n  publisher={National Acad Sciences}\\n}\\n\\n@book{nowak2006evolutionary,\\n  title={Evolutionary dynamics},\\n  author={Nowak, Martin A},\\n  year={2006},\\n  publisher={Harvard University Press}\\n}\\n\\n@book{sigmund2010calculus,\\n  title={The calculus of selfishness},\\n  author={Sigmund, Karl},\\n  volume={6},\\n  year={2010},\\n  publisher={Princeton University Press}\\n}\\n\\n@article{traulsen2006stochastic,\\n  title={Stochastic dynamics of invasion and fixation},\\n  author={Traulsen, Arne and Nowak, Martin A and Pacheco, Jorge M},\\n  journal={Physical Review E},\\n  volume={74},\\n  number={1},\\n  pages={011909},\\n  year={2006},\\n  publisher={APS}\\n}\\n\\n@article{armstrong2016racing,\\n  title={Racing to the precipice: a model of artificial intelligence development},\\n  author={Armstrong, Stuart and Bostrom, Nick and Shulman, Carl},\\n  journal={AI \\\\& society},\\n  volume={31},\\n  number={2},\\n  pages={201--206},\\n  year={2016},\\n  publisher={Springer}\\n}\\n\\n@INPROCEEDINGS{Ng00icml,\\n  author = {Andrew Y. Ng and Stuart J. Russell},\\n  title = {Algorithms for inverse reinforcement learning},\\n  booktitle = {Proc. 17th Int. Conf. Machine Learning},\\n  year = {2000},\\n  address = {USA}\\n}\\n\\n@INPROCEEDINGS{Ziebart11icml,\\n  author = {Kevin Waugh and Brian D. Ziebart and J. Andrew (Drew) Bagnell},\\n  title = {Computational Rationalization: The Inverse Equilibrium Problem},\\n  booktitle = {Inter. Conf. on Machine Learning},\\n  year = {2011}\\n}\\n\\n@book{bostrom2014superintelligence,\\n  title={Superintelligence},\\n  author={Bostrom, Nick},\\n  year={2014},\\n  publisher={}\\n}\\n\\n@book{bersini2017bigbrother,\\n  title={Big Brother is driving you: Br{\\\\\\'e}ves r{\\\\\\'e}flexions d\\'un informaticien obtus sur la soci{\\\\\\'e}t{\\\\\\'e} {\\\\\\'e} venir},\\n  author={Bersini, Hugues},\\n  year={2017},\\n  publisher={Academie Royale de Belgique}\\n}\\n\\n@misc{forbes17airegulationNEWS,\\n title={Why regulating ai is a mistake?},\\n howpublished = {Forbes. [Visited on 17/09/19] https://www.forbes.com/sites/ciocentral/2017/01/09/why-regulating-ai-is-a-mistake/},\\n author = {Amitai Etzioni and Oren Etzioni},\\n year = {2017}\\n}\\n\\n@misc{asilomarCalifornia,\\n title={State of California Endorses Asilomar AI Principles},\\n howpublished = {Future of Life Institute. [Visited on 17/09/19] https://futureoflife.org/2018/08/31/state-of-california-endorses-asilomar-ai-principles/},\\n year = {2018}\\n}\\n\\n\\n@misc{theverge17putinAINEWS,\\n title={Putin says the nation that leads in AI \\'will be the ruler of the world\\'},\\n howpublished = {The Verge},\\n url={https://www.theverge.com/2017/9/4/16251226/russia-ai-putin-rule-the-world},\\n year = {2017}\\n}\\n\\n@inproceedings{shapiro2002valuealignment,\\n  title={User-agent value alignment},\\n  author={Shapiro, Daniel and Shachter, Ross},\\n  booktitle={Proc. of The 18th Nat. Conf. on Artif. Intell. AAAI},\\n  year={2002}\\n}\\n\\n@book{pereira2016programming,\\n  title={Programming machine ethics},\\n  author={Pereira, Lu{\\\\\\'\\\\i}s Moniz and Saptawijaya, Ari},\\n  volume={26},\\n  year={2016},\\n  publisher={Springer}\\n}\\n\\n@inproceedings{Han2019AIWar,\\n  title={Modelling and Influencing the AI Bidding War: A Research Agenda},\\n  author={Han, T. A. and Pereira, L. M. and Lenaerts, T.},\\n  booktitle={Proceedings of the AAAAI/ACM Conference on AI, Ethics, and Society, (AIES 2019).},\\n  year={2019}\\n}\\n\\n@article{nowak2000fairness,\\n  title={Fairness versus reason in the ultimatum game},\\n  author={Nowak, Martin A and Page, Karen M and Sigmund, Karl},\\n  journal={Science},\\n  volume={289},\\n  number={5485},\\n  pages={1773--1775},\\n  year={2000},\\n  publisher={American Association for the Advancement of Science}\\n}\\n\\n@article{fischhoff1991value,\\n  title={Value elicitation: Is there anything in there?},\\n  author={Fischhoff, Baruch},\\n  journal={American psychologist},\\n  volume={46},\\n  number={8},\\n  pages={835},\\n  year={1991},\\n  publisher={American Psychological Association}\\n}\\n\\n\\n@article{smith2016bigdata,\\n  title={Big data: A report on algorithmic systems, opportunity, and civil rights},\\n  author={Smith, Megan and Patil, D and Mu{\\\\~n}oz, Cecilia},\\n  journal={White House Report, Executive Office of the President},\\n  year={2016}\\n}\\n\\n@misc{technologyreview17brooksAINEWS,\\n title={The Seven Deadly Sins of AI Predictions},\\n author={Rodney Brooks},\\n howpublished = {Technology Review. [Visited on 17/09/19] https://www.technologyreview.com/s/609048/the-seven-deadly-sins-of-ai-predictions/},\\n year = {2017}\\n}\\n\\n@book{ashton2015fly,\\n  title={How to fly a horse: The secret history of creation, invention, and discovery},\\n  author={Ashton, Kevin},\\n  year={2015},\\n  publisher={Anchor}\\n}\\n\\n\\n@misc{guardian18cambridAIregulNEWS,\\n title={Cambridge Analytica scandal \\'Highlights need for AI regulation\\'},\\n howpublished = {The Guardian. [Visited on 17/09/19]},\\n url={https://www.theguardian.com/},\\n author={Alex Hern},\\n year = {2018}\\n}\\n\\n@article{hadfield2016off,\\n  title={The off-switch game},\\n  author={Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},\\n  journal={arXiv preprint arXiv:1611.08219},\\n  year={2016}\\n}\\n\\n@article{kirchsteiger1994role,\\n  title={The role of envy in ultimatum games},\\n  author={Kirchsteiger, Georg},\\n  journal={Journal of economic behavior \\\\& organization},\\n  volume={25},\\n  number={3},\\n  pages={373--389},\\n  year={1994},\\n  publisher={Elsevier}\\n}\\n\\n@inproceedings{hadfield2016cooperative,\\n  title={Cooperative inverse reinforcement learning},\\n  author={Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},\\n  booktitle={Advances in neural information processing systems},\\n  pages={3909--3917},\\n  year={2016}\\n}\\n\\n@book{brandt2016handbook,\\n  title={Handbook of computational social choice},\\n  author={Brandt, Felix and Conitzer, Vincent and Endriss, Ulle and Lang, J{\\\\\\'e}r{\\\\^o}me and Procaccia, Ariel D},\\n  year={2016},\\n  publisher={Cambridge University Press}\\n}\\n\\n\\n\\n\\n\\n\\n\\n@article{de2018social,\\n  title={Social decisions and fairness change when peoples interests are represented by autonomous agents},\\n  author={de Melo, Celso M and Marsella, Stacy and Gratch, Jonathan},\\n  journal={Autonomous Agents and Multi-Agent Systems},\\n  volume={32},\\n  number={1},\\n  pages={163--187},\\n  year={2018},\\n  publisher={Springer}\\n}\\n\\n\\n@article{taylor2016alignment,\\n  title={Alignment for advanced machine learning systems},\\n  author={Taylor, Jessica and Yudkowsky, Eliezer and LaVictoire, Patrick and Critch, Andrew},\\n  journal={Machine Intelligence Research Institute},\\n  year={2016}\\n}\\n\\n\\n@article{vladeck2014machines,\\n  title={Machines without principals: liability rules and artificial intelligence},\\n  author={Vladeck, David C},\\n  journal={Wash. L. Rev.},\\n  volume={89},\\n  pages={117},\\n  year={2014},\\n  publisher={HeinOnline}\\n}\\n\\n\\n@article{mclaren2003extensionally,\\n  title={Extensionally defining principles and cases in ethics: An AI model},\\n  author={McLaren, Bruce M},\\n  journal={Artificial Intelligence},\\n  volume={150},\\n  number={1-2},\\n  pages={145--181},\\n  year={2003},\\n  publisher={Elsevier}\\n}\\n\\n\\n@article{weng2009toward,\\n  title={Toward the human--robot co-existence society: On safety intelligence for next generation robots},\\n  author={Weng, Yueh-Hsuan and Chen, Chien-Hsun and Sun, Chuen-Tsai},\\n  journal={International Journal of Social Robotics},\\n  volume={1},\\n  number={4},\\n  pages={267},\\n  year={2009},\\n  publisher={Springer}\\n}\\n\\n\\n@article{yampolskiy2013safety,\\n  title={Safety engineering for artificial general intelligence},\\n  author={Yampolskiy, Roman and Fox, Joshua},\\n  journal={Topoi},\\n  volume={32},\\n  number={2},\\n  pages={217--226},\\n  year={2013},\\n  publisher={Springer}\\n}\\n\\n\\n\\n@article{bostrom2014ethics,\\n  title={The ethics of artificial intelligence},\\n  author={Bostrom, Nick and Yudkowsky, Eliezer},\\n  journal={The Cambridge handbook of artificial intelligence},\\n  volume={316},\\n  pages={334},\\n  year={2014},\\n  publisher={Cambridge University Press Cambridge}\\n}\\n\\n\\n\\n@incollection{yampolskiy2013artificial,\\n  title={Artificial intelligence safety engineering: Why machine ethics is a wrong approach},\\n  author={Yampolskiy, Roman V},\\n  booktitle={Philosophy and theory of artificial intelligence},\\n  pages={389--396},\\n  year={2013},\\n  publisher={Springer}\\n}\\n\\n\\n@article{bostrom2003ethical,\\n  title={Ethical issues in advanced artificial intelligence},\\n  author={Bostrom, Nick},\\n  journal={Science Fiction and Philosophy: From Time Travel to Superintelligence},\\n  pages={277--284},\\n  year={2003},\\n  publisher={United Kingdom: Oxford University}\\n}\\n\\n\\n\\n@article{russell2015research,\\n  title={Research priorities for robust and beneficial artificial intelligence},\\n  author={Russell, Stuart and Dewey, Daniel and Tegmark, Max},\\n  journal={Ai Magazine},\\n  volume={36},\\n  number={4},\\n  pages={105--114},\\n  year={2015}\\n}\\n\\n\\n\\n@inproceedings{arnold2017value,\\n  title={Value alignment or misalignment--what will keep systems accountable},\\n  author={Arnold, Thomas and Kasenberg, Daniel and Scheutz, Matthias},\\n  booktitle={3rd International Workshop on AI, Ethics, and Society},\\n  year={2017}\\n}\\n\\n\\n\\n@article{yudkowsky2008artificial,\\n  title={Artificial intelligence as a positive and negative factor in global risk},\\n  author={Yudkowsky, Eliezer},\\n  journal={Global catastrophic risks},\\n  volume={1},\\n  number={303},\\n  pages={184},\\n  year={2008},\\n  publisher={Oxford University Press Oxford, UK}\\n}\\n\\n\\n@incollection{good1966speculations,\\n  title={Speculations concerning the first ultraintelligent machine},\\n  author={Good, Irving John},\\n  booktitle={Advances in computers},\\n  volume={6},\\n  pages={31--88},\\n  year={1966},\\n  publisher={Elsevier}\\n}\\n\\n\\n\\n@article{hadfield2018incomplete,\\n  title={Incomplete Contracting and AI Alignment},\\n  author={Hadfield-Menell, Dylan and Hadfield, Gillian K},\\n  year={2018}\\n}\\n\\n@article{wulfmeier2015maximum,\\n  title={Maximum entropy deep inverse reinforcement learning},\\n  author={Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},\\n  journal={arXiv preprint arXiv:1507.04888},\\n  year={2015}\\n}\\n\\n\\n@article{anderson2007machine,\\n  title={Machine ethics: Creating an ethical intelligent agent},\\n  author={Anderson, Michael and Anderson, Susan Leigh},\\n  journal={AI Magazine},\\n  volume={28},\\n  number={4},\\n  pages={15},\\n  year={2007}\\n}\\n\\n@book{asimov2004robot,\\n  title={I, robot},\\n  author={Asimov, Isaac},\\n  volume={1},\\n  year={2004},\\n  publisher={Spectra}\\n}\\n\\n@article{floridi2018an,\\n  title={An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and\\nRecommendations},\\n  author={Floridi, Luciano and Cowls, Josh and Beltrametti, Monica and Chatila, Raja and Chazerand, Patrice and Dignum, Virginia and Luetge, Christoph and Madelin,Robert and Pagallo, Ugo and Rossi, Francesca and Schafer, Burkhard and Valcke, Peggy and Vayena, Effy},\\n  journal={Minds and Machines},\\n  year={2018}\\n}\\n\\n\\n@article{allen2000prolegomena,\\n  title={Prolegomena to any future artificial moral agent},\\n  author={Allen, Colin and Varner, Gary and Zinser, Jason},\\n  journal={Journal of Experimental \\\\& Theoretical Artificial Intelligence},\\n  volume={12},\\n  number={3},\\n  pages={251--261},\\n  year={2000},\\n  publisher={Taylor \\\\& Francis}\\n}\\n\\n\\n\\n@article{turing2009computing,\\n  title={Computing machinery and intelligence},\\n  volume={49},\\n  author={Turing, Alan M},\\n  journal={Mind},\\n  pages={433-460},\\n  year={1950}\\n}\\n\\n\\n\\n@article{yampolskiy2012leakproofing,\\n  title={Leakproofing Singularity-Artificial Intelligence Confinement Problem},\\n  author={Yampolskiy, Roman V},\\n  journal={Journal of Consciousness Studies JCS},\\n  year={2012}\\n}\\n\\n\\n@book{drexler1986engines,\\n  title={Engines of creation},\\n  author={Drexler, K Eric},\\n  year={1986},\\n  publisher={Anchor}\\n}\\n\\n\\n@misc{bostromoracle,\\n  title={Oracle AI},\\n  author={Bostrom, Nick},\\n  year={2008}\\n}\\n\\n\\n@article{chalmers2010singularity,\\n  title={The singularity: A philosophical analysis},\\n  author={Chalmers, David},\\n  journal={Journal of Consciousness Studies},\\n  volume={17},\\n  number={9-10},\\n  pages={7--65},\\n  year={2010},\\n  publisher={Imprint Academic}\\n}\\n\\n@article{ramachandran2007bayesian,\\n  title={Bayesian inverse reinforcement learning},\\n  author={Ramachandran, Deepak and Amir, Eyal},\\n  journal={Urbana},\\n  volume={51},\\n  number={61801},\\n  pages={1--4},\\n  year={2007}\\n}\\n\\n\\n\\n@inproceedings{natarajan2010multi,\\n  title={Multi-agent inverse reinforcement learning},\\n  author={Natarajan, Sriraam and Kunapuli, Gautam and Judah, Kshitij and Tadepalli, Prasad and Kersting, Kristian and Shavlik, Jude},\\n  booktitle={2010 Ninth International Conference on Machine Learning and Applications},\\n  pages={395--400},\\n  year={2010},\\n  organization={IEEE}\\n}\\n\\n\\n@inproceedings{asilomar2018principles,\\n  title={Principles.(2017)},\\n  author={Asilomar, AI},\\n  booktitle={Principles developed in conjunction with the 2017 Asilomar conference [Benevolent AI 2017]},\\n  year={2018}\\n}\\n\\n\\n@book{beauchamp2008principes,\\n  title={Les principes de l\\'{\\\\\\'e}thique biom{\\\\\\'e}dicale},\\n  author={Beauchamp, Tom L and Childress, James F},\\n  year={2008},\\n  publisher={Belles Lettres}\\n}\\n\\n\\n@article{leike2018scalable,\\nauthor = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},\\nyear = {2018},\\nmonth = {11},\\npages = {},\\ntitle = {Scalable agent alignment via reward modeling: a research direction}\\n}\\n\\n@article{brundage20184,\\n  title={Scaling Up Humanity: The Case for Conditional Optimism about Artificial Intelligence},\\n  author={Brundage, Miles},\\n  journal={Should we fear artificial intelligence?},\\n  pages={13},\\n  year={2018}\\n}\\n\\n\\n@misc{tan2018ai,\\n  title={AI governance through \"AI\" markets},\\n  author={Tan, Joshua Z and Ding, Jeffrey},\\n\\thowpublished={online www.joshuatan.com},\\n  year={2018}\\n}\\n\\n@article{babcock2017guidelines,\\n  title={Guidelines for artificial intelligence containment},\\n  author={Babcock, James and Kram{\\\\\\'a}r, J{\\\\\\'a}nos and Yampolskiy, Roman V},\\n  journal={arXiv preprint arXiv:1707.08476},\\n  year={2017}\\n}\\n\\n\\n\\n\\n\\n@article{conitzer2010making,\\n  title={Making decisions based on the preferences of multiple agents},\\n  author={Conitzer, Vincent},\\n  journal={Communications of the ACM},\\n  volume={53},\\n  number={3},\\n  pages={84--94},\\n  year={2010},\\n  publisher={ACM}\\n}\\n\\n\\n\\n@article{bonnefon2016social,\\n  title={The social dilemma of autonomous vehicles},\\n  author={Bonnefon, Jean-Fran{\\\\c{c}}ois and Shariff, Azim and Rahwan, Iyad},\\n  journal={Science},\\n  volume={352},\\n  number={6293},\\n  pages={1573--1576},\\n  year={2016},\\n  publisher={American Association for the Advancement of Science}\\n}\\n\\n\\n@inproceedings{noothigattu2018voting,\\n  title={A voting-based system for ethical decision making},\\n  author={Noothigattu, Ritesh and Gaikwad, Snehalkumar S and Awad, Edmond and Dsouza, Sohan and Rahwan, Iyad and Ravikumar, Pradeep and Procaccia, Ariel D},\\n  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},\\n  year={2018}\\n}\\n\\n\\n@inproceedings{freedman2018adapting,\\n  title={Adapting a kidney exchange algorithm to align with human values},\\n  author={Freedman, Rachel and Borg, Jana Schaich and Sinnott-Armstrong, Walter and Dickerson, John P and Conitzer, Vincent},\\n  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},\\n  year={2018}\\n}\\n\\n@inproceedings{conitzer2017moral,\\n  title={Moral decision making frameworks for artificial intelligence},\\n  author={Conitzer, Vincent and Sinnott-Armstrong, Walter and Borg, Jana Schaich and Deng, Yuan and Kramer, Max},\\n  booktitle={Thirty-first aaai conference on artificial intelligence},\\n  year={2017}\\n}\\n\\n\\n\\n@misc{hleg2019ethics,\\n  title={Ethics guidelines for trustworthy AI},\\n  author={HLEG, AI},\\n  year={2019},\\n  publisher={Retrieved from High-Level Expert Group on Artificial Intelligence(AI HLEG~}\\n}\\n\\n\\n@misc{oecd2019ethics,\\n  title={Recommendation of the Council on Artificial Intelligence},\\n  author={OECD},\\n  year={2019}\\n}\\n\\n@article{nash1951non,\\n  title={Non-cooperative games},\\n  author={Nash, John},\\n  journal={Annals of mathematics},\\n  pages={286--295},\\n  year={1951},\\n  publisher={JSTOR}\\n}\\n\\n@article{nash1950equilibrium,\\n  title={Equilibrium points in n-person games},\\n  author={Nash, John F and others},\\n  journal={Proceedings of the national academy of sciences},\\n  volume={36},\\n  number={1},\\n  pages={48--49},\\n  year={1950},\\n  publisher={USA}\\n}\\n\\n@article{kalai1993rational,\\n  title={Rational learning leads to Nash equilibrium},\\n  author={Kalai, Ehud and Lehrer, Ehud},\\n  journal={Econometrica: Journal of the Econometric Society},\\n  pages={1019--1045},\\n  year={1993},\\n  publisher={JSTOR}\\n}\\n\\n@article{rachels2012ethical,\\n  title={Ethical egoism},\\n  author={Rachels, James},\\n  journal={Ethical theory: an anthology},\\n  volume={14},\\n  pages={193},\\n  year={2012},\\n  publisher={John Wiley \\\\& Sons}\\n}\\n\\n\\n@incollection{mill2016utilitarianism,\\n  title={Utilitarianism},\\n  author={Mill, John Stuart},\\n  booktitle={Seven masterpieces of philosophy},\\n  pages={337--383},\\n  year={2016},\\n  publisher={Routledge}\\n}\\n\\n\\n\\n@article{erdHos1960evolution,\\n  title={On the evolution of random graphs},\\n  author={Erd{\\\\H{o}}s, Paul and R{\\\\\\'e}nyi, Alfr{\\\\\\'e}d},\\n  journal={Publ. Math. Inst. Hung. Acad. Sci},\\n  volume={5},\\n  number={1},\\n  pages={17--60},\\n  year={1960}\\n}\\n\\n@article{barabasi1999emergence,\\n  title={Emergence of scaling in random networks},\\n  author={Barab{\\\\\\'a}si, Albert-L{\\\\\\'a}szl{\\\\\\'o} and Albert, R{\\\\\\'e}ka},\\n  journal={science},\\n  volume={286},\\n  number={5439},\\n  pages={509--512},\\n  year={1999},\\n  publisher={American Association for the Advancement of Science}\\n}\\n\\n\\n@article{barabasi2000scale,\\n  title={Scale-free characteristics of random networks: the topology of the world-wide web},\\n  author={Barab{\\\\\\'a}si, Albert-L{\\\\\\'a}szl{\\\\\\'o} and Albert, R{\\\\\\'e}ka and Jeong, Hawoong},\\n  journal={Physica A: statistical mechanics and its applications},\\n  volume={281},\\n  number={1-4},\\n  pages={69--77},\\n  year={2000},\\n  publisher={Elsevier}\\n}\\n\\n\\n\\n@article{albert2005scale,\\n  title={Scale-free networks in cell biology},\\n  author={Albert, Reka},\\n  journal={Journal of cell science},\\n  volume={118},\\n  number={21},\\n  pages={4947--4957},\\n  year={2005},\\n  publisher={The Company of Biologists Ltd}\\n}\\n\\n\\n@article{eguiluz2005scale,\\n  title={Scale-free brain functional networks},\\n  author={Eguiluz, Victor M and Chialvo, Dante R and Cecchi, Guillermo A and Baliki, Marwan and Apkarian, A Vania},\\n  journal={Physical review letters},\\n  volume={94},\\n  number={1},\\n  pages={018102},\\n  year={2005},\\n  publisher={APS}\\n}\\n\\n@article{ebel2002scale,\\n  title={Scale-free topology of e-mail networks},\\n  author={Ebel, Holger and Mielsch, Lutz-Ingo and Bornholdt, Stefan},\\n  journal={Physical review E},\\n  volume={66},\\n  number={3},\\n  pages={035103},\\n  year={2002},\\n  publisher={APS}\\n}\\n\\n@article{barabasi2003scale,\\n  title={Scale-free networks},\\n  author={Barab{\\\\\\'a}si, Albert-L{\\\\\\'a}szl{\\\\\\'o} and Bonabeau, Eric},\\n  journal={Scientific american},\\n  volume={288},\\n  number={5},\\n  pages={60--69},\\n  year={2003},\\n  publisher={JSTOR}\\n}\\n\\n@article{pastor2001epidemic,\\n  title={Epidemic spreading in scale-free networks},\\n  author={Pastor-Satorras, Romualdo and Vespignani, Alessandro},\\n  journal={Physical review letters},\\n  volume={86},\\n  number={14},\\n  pages={3200},\\n  year={2001},\\n  publisher={APS}\\n}\\n\\n@inproceedings{sanchez2020does,\\n  title={What does it mean to\\'solve\\'the problem of discrimination in hiring? social, technical and legal perspectives from the UK on automated hiring systems},\\n  author={S{\\\\\\'a}nchez-Monedero, Javier and Dencik, Lina and Edwards, Lilian},\\n  booktitle={Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},\\n  pages={458--468},\\n  year={2020}\\n}\\n\\n@article{mcnamara2016law,\\n  title={The law and ethics of high-frequency trading},\\n  author={McNamara, Steven},\\n  journal={Minn. JL Sci. \\\\& Tech.},\\n  volume={17},\\n  pages={71},\\n  year={2016},\\n  publisher={HeinOnline}\\n}\\n\\n\\n@article{shen2012stock,\\n  title={Stock market forecasting using machine learning algorithms},\\n  author={Shen, Shunrong and Jiang, Haomiao and Zhang, Tongda},\\n  journal={Department of Electrical Engineering, Stanford University, Stanford, CA},\\n  pages={1--5},\\n  year={2012}\\n}\\n\\n@article{brown2020language,\\n  title={Language models are few-shot learners},\\n  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},\\n  journal={arXiv preprint arXiv:2005.14165},\\n  year={2020}\\n}',\n",
       "  'arxiv_citations': {'1707.08476': True,\n",
       "   '2005.14165': True,\n",
       "   '1611.08219': True,\n",
       "   '1507.04888': True}},\n",
       " {'source': 'arxiv',\n",
       "  'source_type': 'latex',\n",
       "  'converted_with': 'pandoc',\n",
       "  'paper_version': '1611.01578v2',\n",
       "  'title': 'Neural Architecture Search with Reinforcement Learning',\n",
       "  'authors': ['Barret Zoph', 'Quoc V. Le'],\n",
       "  'date_published': '2016-11-05 00:41:37+00:00',\n",
       "  'data_last_modified': '2017-02-15 05:28:05+00:00',\n",
       "  'url': 'http://arxiv.org/abs/1611.01578v2',\n",
       "  'abstract': 'Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.',\n",
       "  'author_comment': None,\n",
       "  'journal_ref': None,\n",
       "  'doi': None,\n",
       "  'primary_category': 'cs.LG',\n",
       "  'categories': ['cs.LG', 'cs.AI', 'cs.NE'],\n",
       "  'citation_level': '0',\n",
       "  'alignment_text': 'unlabeled',\n",
       "  'confidence_score': 0.784875978,\n",
       "  'main_tex_filename': './iclr2017_conference.tex',\n",
       "  'text': '---\\nabstract: |\\n  Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of $3.65$, which is $0.09$ percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.\\nauthor:\\n- |\\n  Barret Zoph[^1], \\xa0Quoc V. Le\\\\\\n  Google Brain\\\\\\n  `{barretzoph,qvl}@google.com`\\\\\\nbibliography:\\n- iclr2017_conference.bib\\ntitle: |\\n  Neural Architecture Search with\\\\\\n  Reinforcement Learning\\n---\\n\\nIntroduction\\n============\\n\\nThe last few years have seen much success of deep neural networks in many challenging applications, such as speech recognition\\xa0[@hinton2012deep], image recognition\\xa0[@lecun1998gradient; @krizhevsky2012imagenet] and machine translation\\xa0[@sutskever2014sequence; @bahdanau2014neural; @wu2016google]. Along with this success is a paradigm shift from feature designing to architecture designing, i.e., from SIFT\\xa0[@lowe1999object], and HOG\\xa0[@dalal2005histograms], to AlexNet\\xa0[@krizhevsky2012imagenet], VGGNet\\xa0[@simonyan2014very], GoogleNet\\xa0[@szegedy2015going], and ResNet\\xa0[@he2015deep]. Although it has become easier, designing architectures still requires a lot of expert knowledge and takes ample time.\\n\\n![An overview of Neural Architecture Search.](NAS.eps){#figure:NAS width=\"0.6\\\\\\\\columnwidth\"}\\n\\nThis paper presents Neural Architecture Search, a gradient-based method for finding good architectures (see Figure\\xa0[1](#figure:NAS){reference-type=\"ref\" reference=\"figure:NAS\"}) . Our work is based on the observation that the structure and connectivity of a neural network can be typically specified by a variable-length string. It is therefore possible to use a recurrent network -- the controller -- to generate such string. Training the network specified by the string -- the \"child network\" -- on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.\\n\\nOur experiments show that Neural Architecture Search can design good models from scratch, an achievement considered not possible with other methods. On image recognition with CIFAR-10, Neural Architecture Search can find a novel ConvNet model that is better than most human-invented architectures. Our CIFAR-10 model achieves a 3.65 test set error, while being 1.05x faster than the current best model. On language modeling with Penn Treebank, Neural Architecture Search can design a novel recurrent cell that is also better than previous RNN and LSTM architectures. The cell that our model found achieves a test set perplexity of 62.4 on the Penn Treebank dataset, which is 3.6 perplexity better than the previous state-of-the-art.\\n\\nRelated Work\\n============\\n\\nHyperparameter optimization is an important research topic in machine learning, and is widely used in practice\\xa0[@bergstra2011algorithms; @bergstra2012random; @snoek2012practical; @snoek2015scalable; @saxena2016convolutional]. Despite their success, these methods are still limited in that they only search models from a fixed-length space. In other words, it is difficult to ask them to generate a variable-length configuration that specifies the structure and connectivity of a network. In practice, these methods often work better if they are supplied with a good initial model\\xa0[@bergstra2012random; @snoek2012practical; @snoek2015scalable]. There are Bayesian optimization methods that allow to search non fixed length architectures\\xa0[@bergstra2013making; @mendoza2016towards], but they are less general and less flexible than the method proposed in this paper.\\n\\nModern neuro-evolution algorithms, e.g.,\\xa0[@wierstra2005modeling; @floreano2008neuroevolution; @stanley2009hypercube], on the other hand, are much more flexible for composing novel models, yet they are usually less practical at a large scale. Their limitations lie in the fact that they are search-based methods, thus they are slow or require many heuristics to work well.\\n\\nNeural Architecture Search has some parallels to program synthesis and inductive programming, the idea of searching a program from examples\\xa0[@summers1977methodology; @biermann1978inference]. In machine learning, probabilistic program induction has been used successfully in many settings, such as learning to solve simple Q&A\\xa0[@liang2010learning; @neelakantan2015neural; @andreas2016learning], sort a list of numbers\\xa0[@reed2015neural], and learning with very few examples\\xa0[@lake2015human].\\n\\nThe controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning\\xa0[@sutskever2014sequence]. Unlike sequence to sequence learning, our method optimizes a non-differentiable metric, which is the accuracy of the child network. It is therefore similar to the work on BLEU optimization in Neural Machine Translation\\xa0[@ranzato2015sequence; @ShenCHHWSL15]. Unlike these approaches, our method learns directly from the reward signal without any supervised bootstrapping.\\n\\nAlso related to our work is the idea of learning to learn or meta-learning\\xa0[@thrun2012learning], a general framework of using information learned in one task to improve a future task. More closely related is the idea of using a neural network to learn the gradient descent updates for another network\\xa0[@andrychowicz2016learning] and the idea of using reinforcement learning to find update policies for another network\\xa0[@li2016learning].\\n\\nMethods {#sec:methods}\\n=======\\n\\nIn the following section, we will first describe a simple method of using a recurrent network to generate convolutional architectures. We will show how the recurrent network can be trained with a policy gradient method to maximize the expected accuracy of the sampled architectures. We will present several improvements of our core approach such as forming skip connections to increase model complexity and using a parameter server approach to speed up training. In the last part of the section, we will focus on generating recurrent architectures, which is another key contribution of our paper.\\n\\nArchitecture Descriptions\\n-------------------------\\n\\nAs stated earlier, our key observation is that many common neural architectures can be summarized in variable-length strings, which we also call \"architecture descriptions.\" Below is an example architecture description of a simple 2-layer convolutional network:\\n\\n        layer {\\n            name = layer_1\\n            type = conv\\n            connect_to = input\\n            filter_width = 2\\n            filter_height = 3\\n            stride_width = 4\\n            stride_height = 5\\n            num_filters = 6\\n        }\\n        layer {\\n            name = layer_2\\n            type = conv\\n            connect_to = layer_1\\n            filter_width = 7\\n            filter_ height = 8\\n            stride_width = 9\\n            stride_height = 10\\n            num_filters = 11\\n        }\\n        layer {\\n            name = classifier\\n            connect_to = layer_2\\n            num_class = 10\\n        }\\n\\nBy focusing on key hyperparameters, the above description can be compressed into:\\n\\n        2 3 4 5 6 7 8 9 10 11\\n\\nNotice that information regarding layer types and connectivity between layers are skipped entirely because we are only interested in feedforward convolutional networks without skip connections. The last layer is also skipped because we do not want to vary it. Although the new description does not have hyperparameter names such as \"filter_width\", \"filter_height\", they can be inferred given the order of the tokens. In the following section, we will show how we use a recurrent network to generate this type of architecture descriptions.\\n\\nGenerate Model Descriptions with a Controller Recurrent Neural Network {#sec:Controller_RNN}\\n----------------------------------------------------------------------\\n\\nIn Neural Architecture Search, we use a controller to generate architectural hyperparameters of neural networks. To be flexible, the controller is implemented as a recurrent neural network. Let\\'s suppose we would like to predict feedforward neural networks with only convolutional layers, we can use the controller to generate their hyperparameters as a sequence of tokens:\\n\\n![How our controller recurrent neural network samples a simple convolutional network. It predicts filter height, filter width, stride height, stride width, and number of filters for one layer and repeats. Every prediction is carried out by a softmax classifier and then fed into the next time step as input.](FIGURE1.eps){#figure:Controller_RNN width=\"0.7\\\\\\\\columnwidth\"}\\n\\nIn our experiments, the process of generating an architecture stops if the number of layers exceeds a certain value. This value follows a schedule where we increase it as training progresses. Once the controller RNN finishes generating an architecture, a neural network with this architecture is built and trained. At convergence, the accuracy of the network on a held-out validation set is recorded. The parameters of the controller RNN, $\\\\theta_c$, are then optimized in order to maximize the expected validation accuracy of the proposed architectures. In the next section, we will describe a policy gradient method which we use to update parameters $\\\\theta_c$ so that the controller RNN generates better architectures over time.\\n\\nTraining with REINFORCE\\n-----------------------\\n\\nThe list of tokens that the controller predicts can be viewed as a list of actions $a_{1:T}$ to design an architecture for a child network. At convergence, this child network will achieve an accuracy $R$ on a held-out dataset. We can use this accuracy $R$ as the reward signal and use reinforcement learning to train the controller. More concretely, to find the optimal architecture, we ask our controller to maximize its expected reward, represented by $J(\\\\theta_c)$:\\n\\n$$J(\\\\theta_c) = E_{P(a_{1:T};\\\\theta_c)}[R]$$\\n\\nSince the reward signal $R$ is non-differentiable, we need to use a policy gradient method to iteratively update $\\\\theta_c$. In this work, we use the REINFORCE rule from\\xa0[@Williams92simplestatistical]: $$\\\\bigtriangledown_{\\\\theta_c} J(\\\\theta_c) = \\\\sum_{t=1}^{T} E_{P(a_{1:T};\\\\theta_c)}\\\\big[\\\\bigtriangledown_{\\\\theta_c} \\\\log P(a_t|a_{(t-1):1};\\\\theta_c)R\\\\big]$$\\n\\nAn empirical approximation of the above quantity is: $$\\\\frac{1}{m} \\\\sum_{k=1}^{m} \\\\sum_{t=1}^{T} \\\\bigtriangledown_{\\\\theta_c} \\\\log P(a_t|a_{(t-1):1};\\\\theta_c)R_{k}$$\\n\\nWhere $m$ is the number of different architectures that the controller samples in one batch and $T$ is the number of hyperparameters our controller has to predict to design a neural network architecture. The validation accuracy that the $k$-th neural network architecture achieves after being trained on a training dataset is $R_k$.\\n\\nThe above update is an unbiased estimate for our gradient, but has a very high variance. In order to reduce the variance of this estimate we employ a baseline function:\\n\\n$$\\\\frac{1}{m} \\\\sum_{k=1}^{m} \\\\sum_{t=1}^{T} \\\\bigtriangledown_{\\\\theta_c} \\\\log P(a_t|a_{(t-1):1};\\\\theta_c)(R_{k} - b)$$\\n\\nAs long as the baseline function $b$ does not depend on the on the current action, then this is still an unbiased gradient estimate. In this work, our baseline $b$ is an exponential moving average of the previous architecture accuracies.\\n\\n#### Accelerate Training with Parallelism and Asynchronous Updates:\\n\\nIn Neural Architecture Search, each gradient update to the controller parameters $\\\\theta_c$ corresponds to training one child network to convergence. As training a child network can take hours, we use distributed training and asynchronous parameter updates in order to speed up the learning process of the controller\\xa0[@dean2012large]. We use a parameter-server scheme where we have a parameter server of $S$ shards, that store the shared parameters for $K$ controller replicas. Each controller replica samples $m$ different child architectures that are trained in parallel. The controller then collects gradients according to the results of that minibatch of $m$ architectures at convergence and sends them to the parameter server in order to update the weights across all controller replicas. In our implementation, convergence of each child network is reached when its training exceeds a certain number of epochs. This scheme of parallelism is summarized in Figure\\xa0[3](#figure:Dist_Setup){reference-type=\"ref\" reference=\"figure:Dist_Setup\"}.\\n\\n![Distributed training for Neural Architecture Search. We use a set of $S$ parameter servers to store and send parameters to $K$ controller replicas. Each controller replica then samples $m$ architectures and run the multiple child models in parallel. The accuracy of each child model is recorded to compute the gradients with respect to $\\\\theta_c$, which are then sent back to the parameter servers.](ps.eps){#figure:Dist_Setup width=\"0.8\\\\\\\\columnwidth\"}\\n\\nIncrease Architecture Complexity with Skip Connections and Other Layer Types {#sec:skip_connection}\\n----------------------------------------------------------------------------\\n\\nIn Section\\xa0[3.2](#sec:Controller_RNN){reference-type=\"ref\" reference=\"sec:Controller_RNN\"}, the search space does not have skip connections, or branching layers used in modern architectures such as GoogleNet\\xa0[@szegedy2015going], and Residual Net\\xa0[@he2015deep]. In this section we introduce a method that allows our controller to propose skip connections or branching layers, thereby widening the search space.\\n\\nTo enable the controller to predict such connections, we use a set-selection type attention\\xa0[@neelakantan2015neural] which was built upon the attention mechanism\\xa0[@bahdanau2014neural; @vinyals2015pointer]. At layer $N$, we add an anchor point which has $N-1$ content-based sigmoids to indicate the previous layers that need to be connected. Each sigmoid is a function of the current hiddenstate of the controller and the previous hiddenstates of the previous $N-1$ anchor points:\\n\\n$$\\\\mathrm{P(Layer \\\\ j \\\\ is \\\\ an \\\\ input \\\\ to \\\\ layer \\\\ i) = \\\\ } \\\\mathrm{sigmoid}(v^\\\\mathrm{T} \\\\mathrm{tanh}(W_{prev}*h_{j} + W_{curr} * h_{i})),$$ where $h_j$ represents the hiddenstate of the controller at anchor point for the $j$-th layer, where $j$ ranges from $0$ to $N-1$. We then sample from these sigmoids to decide what previous layers to be used as inputs to the current layer. The matrices $W_{prev}$, $W_{curr}$ and $v$ are trainable parameters. As these connections are also defined by probability distributions, the REINFORCE method still applies without any significant modifications. Figure\\xa0[4](#figure:Controller_RNN_PTR){reference-type=\"ref\" reference=\"figure:Controller_RNN_PTR\"} shows how the controller uses skip connections to decide what layers it wants as inputs to the current layer.\\n\\n![The controller uses anchor points, and set-selection attention to form skip connections.](FIGURE2.eps){#figure:Controller_RNN_PTR width=\"0.75\\\\\\\\columnwidth\"}\\n\\nIn our framework, if one layer has many input layers then all input layers are concatenated in the depth dimension. Skip connections can cause \"compilation failures\" where one layer is not compatible with another layer, or one layer may not have any input or output. To circumvent these issues, we employ three simple techniques. First, if a layer is not connected to any input layer then the image is used as the input layer. Second, at the final layer we take all layer outputs that have not been connected and concatenate them before sending this final hiddenstate to the classifier. Lastly, if input layers to be concatenated have different sizes, we pad the small layers with zeros so that the concatenated layers have the same sizes.\\n\\nFinally, in Section\\xa0[3.2](#sec:Controller_RNN){reference-type=\"ref\" reference=\"sec:Controller_RNN\"}, we do not predict the learning rate and we also assume that the architectures consist of only convolutional layers, which is also quite restrictive. It is possible to add the learning rate as one of the predictions. Additionally, it is also possible to predict pooling, local contrast normalization\\xa0[@jarrett2009best; @krizhevsky2012imagenet], and batchnorm\\xa0[@BatchNorm] in the architectures. To be able to add more types of layers, we need to add an additional step in the controller RNN to predict the layer type, then other hyperparameters associated with it.\\n\\nGenerate Recurrent Cell Architectures {#sec:recurrent_cell}\\n-------------------------------------\\n\\nIn this section, we will modify the above method to generate recurrent cells. At every time step $t$, the controller needs to find a functional form for $h_t$ that takes $x_t$ and $h_{t-1}$ as inputs. The simplest way is to have $h_t = \\\\tanh(W_1 * x_t + W_2 * h_{t-1})$, which is the formulation of a basic recurrent cell. A more complicated formulation is the widely-used LSTM recurrent cell\\xa0[@lstm].\\n\\nThe computations for basic RNN and LSTM cells can be generalized as a tree of steps that take $x_t$ and $h_{t-1}$ as inputs and produce $h_t$ as final output. The controller RNN needs to label each node in the tree with a combination method (addition, elementwise multiplication, etc.) and an activation function ($\\\\tanh$, $\\\\mathrm{sigmoid}$, etc.) to merge two inputs and produce one output. Two outputs are then fed as inputs to the next node in the tree. To allow the controller RNN to select these methods and functions, we index the nodes in the tree in an order so that the controller RNN can visit each node one by one and label the needed hyperparameters.\\n\\nInspired by the construction of the LSTM cell\\xa0[@lstm], we also need cell variables $c_{t-1}$ and $c_t$ to represent the memory states. To incorporate these variables, we need the controller RNN to predict what nodes in the tree to connect these two variables to. These predictions can be done in the last two blocks of the controller RNN.\\n\\n![An example of a recurrent cell constructed from a tree that has two leaf nodes (base 2) and one internal node. Left: the tree that defines the computation steps to be predicted by controller. Center: an example set of predictions made by the controller for each computation step in the tree. Right: the computation graph of the recurrent cell constructed from example predictions of the controller. ](tree.eps \"fig:\"){#figure:RNN_Method width=\"0.21\\\\\\\\columnwidth\"} ![An example of a recurrent cell constructed from a tree that has two leaf nodes (base 2) and one internal node. Left: the tree that defines the computation steps to be predicted by controller. Center: an example set of predictions made by the controller for each computation step in the tree. Right: the computation graph of the recurrent cell constructed from example predictions of the controller. ](lstm3.eps \"fig:\"){#figure:RNN_Method width=\"0.53\\\\\\\\columnwidth\"} ![An example of a recurrent cell constructed from a tree that has two leaf nodes (base 2) and one internal node. Left: the tree that defines the computation steps to be predicted by controller. Center: an example set of predictions made by the controller for each computation step in the tree. Right: the computation graph of the recurrent cell constructed from example predictions of the controller. ](example1.eps \"fig:\"){#figure:RNN_Method width=\"0.24\\\\\\\\columnwidth\"}\\n\\nTo make this process more clear, we show an example in Figure\\xa0[7](#figure:RNN_Method){reference-type=\"ref\" reference=\"figure:RNN_Method\"}, for a tree structure that has two leaf nodes and one internal node. The leaf nodes are indexed by 0 and 1, and the internal node is indexed by 2. The controller RNN needs to first predict 3 blocks, each block specifying a combination method and an activation function for each tree index. After that it needs to predict the last 2 blocks that specify how to connect $c_t$ and $c_{t-1}$ to temporary variables inside the tree. Specifically, according to the predictions of the controller RNN in this example, the following computation steps will occur:\\n\\n-   The controller predicts $Add$ and $Tanh$ for tree index 0, this means we need to compute $a_0 = \\\\tanh(W_1 * x_t + W_2 * h_{t-1})$.\\n\\n-   The controller predicts $Elem Mult$ and $ReLU$ for tree index 1, this means we need to compute $a_1 = \\\\mathrm{ReLU}\\\\big((W_3 * x_t) \\\\odot (W_4 * h_{t-1})\\\\big)$.\\n\\n-   The controller predicts 0 for the second element of the \"Cell Index\", $Add$ and $ReLU$ for elements in \"Cell Inject\", which means we need to compute $a_0^{new} = \\\\mathrm{ReLU}(a_0 + c_{t-1})$. Notice that we don\\'t have any learnable parameters for the internal nodes of the tree.\\n\\n-   The controller predicts $Elem Mult$ and $Sigmoid$ for tree index 2, this means we need to compute $a_2 = \\\\mathrm{sigmoid}( a_0^{new} \\\\odot a_1)$. Since the maximum index in the tree is 2, $h_t$ is set to $a_2$.\\n\\n-   The controller RNN predicts 1 for the first element of the \"Cell Index\", this means that we should set $c_t$ to the output of the tree at index 1 before the activation, i.e., $c_t = (W_3 * x_t) \\\\odot (W_4 * h_{t-1})$.\\n\\nIn the above example, the tree has two leaf nodes, thus it is called a \"base 2\" architecture. In our experiments, we use a base number of 8 to make sure that the cell is expressive.\\n\\nExperiments and Results {#others}\\n=======================\\n\\nWe apply our method to an image classification task with CIFAR-10 and a language modeling task with Penn Treebank, two of the most benchmarked datasets in deep learning. On CIFAR-10, our goal is to find a good convolutional architecture whereas on Penn Treebank our goal is to find a good recurrent cell. On each dataset, we have a separate held-out validation dataset to compute the reward signal. The reported performance on the test set is computed only once for the network that achieves the best result on the held-out validation dataset. More details about our experimental procedures and results are as follows.\\n\\nLearning Convolutional Architectures for CIFAR-10\\n-------------------------------------------------\\n\\n#### Dataset:\\n\\nIn these experiments we use the CIFAR-10 dataset with data preprocessing and augmentation procedures that are in line with other previous results. We first preprocess the data by whitening all the images. Additionally, we upsample each image then choose a random 32x32 crop of this upsampled image. Finally, we use random horizontal flips on this 32x32 cropped image.\\n\\n#### Search space:\\n\\nOur search space consists of convolutional architectures, with rectified linear units as non-linearities\\xa0[@nair2010rectified], batch normalization\\xa0[@BatchNorm] and skip connections between layers (Section\\xa0[3.4](#sec:skip_connection){reference-type=\"ref\" reference=\"sec:skip_connection\"}). For every convolutional layer, the controller RNN has to select a filter height in \\\\[1, 3, 5, 7\\\\], a filter width in \\\\[1, 3, 5, 7\\\\], and a number of filters in \\\\[24, 36, 48, 64\\\\]. For strides, we perform two sets of experiments, one where we fix the strides to be 1, and one where we allow the controller to predict the strides in \\\\[1, 2, 3\\\\].\\n\\n#### Training details:\\n\\nThe controller RNN is a two-layer LSTM with 35 hidden units on each layer. It is trained with the ADAM optimizer\\xa0[@ADAM] with a learning rate of 0.0006. The weights of the controller are initialized uniformly between -0.08 and 0.08. For the distributed training, we set the number of parameter server shards $S$ to 20, the number of controller replicas $K$ to 100 and the number of child replicas $m$ to 8, which means there are 800 networks being trained on 800 GPUs concurrently at any time.\\n\\nOnce the controller RNN samples an architecture, a child model is constructed and trained for 50 epochs. The reward used for updating the controller is the maximum validation accuracy of the last 5 epochs cubed. The validation set has 5,000 examples randomly sampled from the training set, the remaining 45,000 examples are used for training. The settings for training the CIFAR-10 child models are the same with those used in\\xa0[@Huang2016Densely]. We use the Momentum Optimizer with a learning rate of 0.1, weight decay of 1e-4, momentum of 0.9 and used Nesterov Momentum \\xa0[@icml2013_sutskever13].\\n\\nDuring the training of the controller, we use a schedule of increasing number of layers in the child networks as training progresses. On CIFAR-10, we ask the controller to increase the depth by 2 for the child models every 1,600 samples, starting at 6 layers.\\n\\n#### Results:\\n\\nAfter the controller trains 12,800 architectures, we find the architecture that achieves the best validation accuracy. We then run a small grid search over learning rate, weight decay, batchnorm epsilon and what epoch to decay the learning rate. The best model from this grid search is then run until convergence and we then compute the test accuracy of such model and summarize the results in Table\\xa0[\\\\[tab:cifar10\\\\]](#tab:cifar10){reference-type=\"ref\" reference=\"tab:cifar10\"}. As can be seen from the table, Neural Architecture Search can design several promising architectures that perform as well as some of the best models on this dataset.\\n\\nFirst, if we ask the controller to not predict stride or pooling, it can design a 15-layer architecture that achieves 5.50% error rate on the test set. This architecture has a good balance between accuracy and depth. In fact, it is the shallowest and perhaps the most inexpensive architecture among the top performing networks in this table. This architecture is shown in Appendix\\xa0[6](#sec:appendix){reference-type=\"ref\" reference=\"sec:appendix\"}, Figure\\xa0[9](#figure:strange_net){reference-type=\"ref\" reference=\"figure:strange_net\"}. A notable feature of this architecture is that it has many rectangular filters and it prefers larger filters at the top layers. Like residual networks\\xa0[@he2015deep], the architecture also has many one-step skip connections. This architecture is a local optimum in the sense that if we perturb it, its performance becomes worse. For example, if we densely connect all layers with skip connections, its performance becomes slightly worse: 5.56%. If we remove all skip connections, its performance drops to 7.97%.\\n\\nIn the second set of experiments, we ask the controller to predict strides in addition to other hyperparameters. As stated earlier, this is more challenging because the search space is larger. In this case, it finds a 20-layer architecture that achieves 6.01% error rate on the test set, which is not much worse than the first set of experiments.\\n\\nFinally, if we allow the controller to include 2 pooling layers at layer 13 and layer 24 of the architectures, the controller can design a 39-layer network that achieves 4.47% which is very close to the best human-invented architecture that achieves 3.74%. To limit the search space complexity we have our model predict 13 layers where each layer prediction is a fully connected block of 3 layers. Additionally, we change the number of filters our model can predict from \\\\[24, 36, 48, 64\\\\] to \\\\[6, 12, 24, 36\\\\]. Our result can be improved to 3.65% by adding 40 more filters to each layer of our architecture. Additionally this model with 40 filters added is 1.05x as fast as the DenseNet model that achieves 3.74%, while having better performance. The DenseNet model that achieves 3.46% error rate\\xa0[@Huang2016Densely2] uses 1x1 convolutions to reduce its total number of parameters, which we did not do, so it is not an exact comparison.\\n\\nLearning Recurrent Cells for Penn Treebank\\n------------------------------------------\\n\\n#### Dataset:\\n\\nWe apply Neural Architecture Search to the Penn Treebank dataset, a well-known benchmark for language modeling. On this task, LSTM architectures tend to excel\\xa0[@ZarembaReg; @Gal2015], and improving them is difficult\\xa0[@jozefowicz2015empirical]. As PTB is a small dataset, regularization methods are needed to avoid overfitting. First, we make use of the embedding dropout and recurrent dropout techniques proposed in\\xa0[@ZarembaReg] and\\xa0[@Gal2015]. We also try to combine them with the method of sharing Input and Output embeddings, e.g., [@bengio2003neural; @mnih2007three], especially [@SocherEmbedding] and [@shareEmbedding]. Results with this method are marked with \"shared embeddings.\"\\n\\n#### Search space:\\n\\nFollowing Section\\xa0[3.5](#sec:recurrent_cell){reference-type=\"ref\" reference=\"sec:recurrent_cell\"}, our controller sequentially predicts a combination method then an activation function for each node in the tree. For each node in the tree, the controller RNN needs to select a combination method in $[add, elem\\\\_mult]$ and an activation method in $[identity, tanh, sigmoid, relu]$. The number of input pairs to the RNN cell is called the \"base number\" and set to 8 in our experiments. When the base number is 8, the search space is has approximately $6 \\\\times 10^{16}$ architectures, which is much larger than 15,000, the number of architectures that we allow our controller to evaluate.\\n\\n  **Cell Type**    **Search Space Size**   **Number of Evaluations**  \\n  --------------- ----------------------- --------------------------- --\\n  Base $8$         $6.33 \\\\cdot 10^{16}$            $15,000$           \\n\\n  **RNN Cell Type**                              **Parameters**   **Test Perplexity**  \\n  --------------------------------------------- ---------------- --------------------- --\\n  @Gal2015 - Variational LSTM (large, untied)         66M               $75.2$         \\n  Controller Base 8 (1150)                            65M               $69.0$         \\n  Controller Base 8 max/sin V1 (1150)                 65M               $73.9$         \\n  Controller Base 16 (865)                            65M               $71.5$         \\n\\n#### Training details:\\n\\nThe controller and its training are almost identical to the CIFAR-10 experiments except for a few modifications: 1) the learning rate for the controller RNN is 0.0005, slightly smaller than that of the controller RNN in CIFAR-10, 2) in the distributed training, we set $S$ to 20, $K$ to 400 and $m$ to 1, which means there are 400 networks being trained on 400 CPUs concurrently at any time, 3) during asynchronous training we only do parameter updates to the parameter-server once 10 gradients from replicas have been accumulated.\\n\\nIn our experiments, every child model is constructed and trained for 35 epochs. Every child model has two layers, with the number of hidden units adjusted so that total number of learnable parameters approximately match the \"medium\" baselines\\xa0[@ZarembaReg; @Gal2015]. In these experiments we only have the controller predict the RNN cell structure and fix all other hyperparameters. The reward function is $\\\\frac{c}{\\\\textrm{(validation perplexity)}^2}$ where $c$ is a constant, usually set at 80.\\n\\nAfter the controller RNN is done training, we take the best RNN cell according to the lowest validation perplexity and then run a grid search over learning rate, weight initialization, dropout rates and decay epoch. The best cell found was then run with three different configurations and sizes to increase its capacity.\\n\\n#### Results:\\n\\nIn Table\\xa0[\\\\[table:PTBwordresults\\\\]](#table:PTBwordresults){reference-type=\"ref\" reference=\"table:PTBwordresults\"}, we provide a comprehensive list of architectures and their performance on the PTB dataset. As can be seen from the table, the models found by Neural Architecture Search outperform other state-of-the-art models on this dataset, and one of our best models achieves a gain of almost 3.6 perplexity. Not only is our cell is better, the model that achieves 64 perplexity is also more than two times faster because the previous best network requires running a cell 10 times per time step\\xa0[@Zilly2016].\\n\\n  **Model**                                                       **Parameters**   **Test Perplexity**  \\n  -------------------------------------------------------------- ---------------- --------------------- --\\n  @Mikolov2012 - KN-5                                             2M$^\\\\ddagger$          $141.2$        \\n  @Mikolov2012 - KN5 + cache                                      2M$^\\\\ddagger$          $125.7$        \\n  @Mikolov2012 - RNN                                              6M$^\\\\ddagger$          $124.7$        \\n  @Mikolov2012 - RNN-LDA                                          7M$^\\\\ddagger$          $113.7$        \\n  @Mikolov2012 - RNN-LDA + KN-5 + cache                           9M$^\\\\ddagger$          $92.0$         \\n  @Pascanu2013a - Deep RNN                                              6M               $107.5$        \\n  @Cheng2014 - Sum-Prod Net                                       5M$^\\\\ddagger$          $100.0$        \\n  @ZarembaReg - LSTM (medium)                                          20M               $82.7$         \\n  @ZarembaReg - LSTM (large)                                           66M               $78.4$         \\n  @Gal2015 - Variational LSTM (medium, untied)                         20M               $79.7$         \\n  @Gal2015 - Variational LSTM (medium, untied, MC)                     20M               $78.6$         \\n  @Gal2015 - Variational LSTM (large, untied)                          66M               $75.2$         \\n  @Gal2015 - Variational LSTM (large, untied, MC)                      66M               $73.4$         \\n  @Kim2016 - CharCNN                                                   19M               $78.9$         \\n  @shareEmbedding - Variational LSTM, shared embeddings                51M               $73.2$         \\n  @Socher2016 - Zoneout + Variational LSTM (medium)                    20M               $80.6$         \\n  @Socher2016 - Pointer Sentinel-LSTM (medium)                         21M               $70.9$         \\n  @SocherEmbedding - VD-LSTM + REAL (large)                            51M               $68.5$         \\n  @Zilly2016 - Variational RHN, shared embeddings                      24M               $66.0$         \\n  Neural Architecture Search with base 8                               32M               $67.9$         \\n  Neural Architecture Search with base 8 and shared embeddings         25M               $64.0$         \\n  Neural Architecture Search with base 8 and shared embeddings         54M               $62.4$         \\n\\nThe newly discovered cell is visualized in Figure\\xa0[11](#fig:strange_cell){reference-type=\"ref\" reference=\"fig:strange_cell\"} in Appendix\\xa0[6](#sec:appendix){reference-type=\"ref\" reference=\"sec:appendix\"}. The visualization reveals that the new cell has many similarities to the LSTM cell in the first few steps, such as it likes to compute $W_1 * h_{t-1} + W_2 * x_t$ several times and send them to different components in the cell.\\n\\n#### Transfer Learning Results:\\n\\nTo understand whether the cell can generalize to a different task, we apply it to the character language modeling task on the same dataset. We use an experimental setup that is similar to\\xa0[@ha2016hypernetworks], but use variational dropout by [@Gal2015]. We also train our own LSTM with our setup to get a fair LSTM baseline. Models are trained for 80K steps and the best test set perplexity is taken according to the step where validation set perplexity is the best. The results on the test set of our method and state-of-art methods are reported in Table\\xa0[\\\\[table:charPTB\\\\]](#table:charPTB){reference-type=\"ref\" reference=\"table:charPTB\"}. The results on small settings with 5-6M parameters confirm that the new cell does indeed generalize, and is better than the LSTM cell.\\n\\n  **RNN Cell Type**                                               **Parameters**   **Test Bits Per Character**\\n  -------------------------------------------------------------- ---------------- -----------------------------\\n  @ha2016hypernetworks - Layer Norm HyperLSTM                         4.92M                   1.250\\n  @ha2016hypernetworks - Layer Norm HyperLSTM Large Embeddings        5.06M                   1.233\\n  @ha2016hypernetworks - 2-Layer Norm HyperLSTM                       14.41M                  1.219\\n  Two layer LSTM                                                      6.57M                   1.243\\n  Two Layer with New Cell                                             6.57M                   1.228\\n  Two Layer with New Cell                                             16.28M                  1.214\\n\\nAdditionally, we carry out a larger experiment where the model has 16.28M parameters. This model has a weight decay rate of $1e-4$, was trained for 600K steps (longer than the above models) and the test perplexity is taken where the validation set perplexity is highest. We use dropout rates of 0.2 and 0.5 as described in [@Gal2015], but do not use embedding dropout. We use the ADAM optimizer with a learning rate of 0.001 and an input embedding size of 128. Our model had two layers with 800 hidden units. We used a minibatch size of 32 and BPTT length of 100. With this setting, our model achieves 1.214 perplexity, which is the new state-of-the-art result on this task.\\n\\nFinally, we also drop our cell into the GNMT framework\\xa0[@wu2016google], which was previously tuned for LSTM cells, and train an WMT14 English $\\\\rightarrow$ German translation model. The GNMT network has 8 layers in the encoder, 8 layers in the decoder. The first layer of the encoder has bidirectional connections. The attention module is a neural network with 1 hidden layer. When a LSTM cell is used, the number of hidden units in each layer is 1024. The model is trained in a distributed setting with a parameter sever and 12 workers. Additionally, each worker uses 8 GPUs and a minibatch of 128. We use Adam with a learning rate of 0.0002 in the first 60K training steps, and SGD with a learning rate of 0.5 until 400K steps. After that the learning rate is annealed by dividing by 2 after every 100K steps until it reaches 0.1. Training is stopped at 800K steps. More details can be found in\\xa0[@wu2016google].\\n\\nIn our experiment with the new cell, we make no change to the above settings except for dropping in the new cell and adjusting the hyperparameters so that the new model should have the same computational complexity with the base model. The result shows that our cell, with the same computational complexity, achieves an improvement of 0.5 test set BLEU than the default LSTM cell. Though this improvement is not huge, the fact that the new cell can be used without any tuning on the existing GNMT framework is encouraging. We expect further tuning can help our cell perform better.\\n\\n#### Control Experiment 1 -- Adding more functions in the search space:\\n\\nTo test the robustness of Neural Architecture Search, we add $max$ to the list of combination functions and $sin$ to the list of activation functions and rerun our experiments. The results show that even with a bigger search space, the model can achieve somewhat comparable performance. The best architecture with $max$ and $sin$ is shown in Figure\\xa0[11](#fig:strange_cell){reference-type=\"ref\" reference=\"fig:strange_cell\"} in Appendix\\xa0[6](#sec:appendix){reference-type=\"ref\" reference=\"sec:appendix\"}.\\n\\n#### Control Experiment 2 -- Comparison against Random Search:\\n\\nInstead of policy gradient, one can use random search to find the best network. Although this baseline seems simple, it is often very hard to surpass\\xa0[@bergstra2012random]. We report the perplexity improvements using policy gradient against random search as training progresses in Figure\\xa0[8](#fig:learning_curve){reference-type=\"ref\" reference=\"fig:learning_curve\"}. The results show that not only the best model using policy gradient is better than the best model using random search, but also the average of top models is also much better.\\n\\n![Improvement of Neural Architecture Search over random search over time. We plot the difference between the average of the top $k$ models our controller finds vs. random search every 400 models run.](learning_curve.eps){#fig:learning_curve width=\"48%\"}\\n\\nConclusion\\n==========\\n\\nIn this paper we introduce Neural Architecture Search, an idea of using a recurrent neural network to compose neural network architectures. By using recurrent network as the controller, our method is flexible so that it can search variable-length architecture space. Our method has strong empirical performance on very challenging benchmarks and presents a new research direction for automatically finding good neural network architectures. The code for running the models found by the controller on CIFAR-10 and PTB will be released at https://github.com/tensorflow/models . Additionally, we have added the RNN cell found using our method under the name NASCell into TensorFlow, so others can easily use it.\\n\\n### Acknowledgments {#acknowledgments .unnumbered}\\n\\nWe thank Greg Corrado, Jeff Dean, David Ha, Lukasz Kaiser and the Google Brain team for their help with the project.\\n\\nAppendix {#sec:appendix}\\n========\\n\\n![Convolutional architecture discovered by our method, when the search space does not have strides or pooling layers. FH is filter height, FW is filter width and N is number of filters. Note that the skip connections are not residual connections. If one layer has many input layers then all input layers are concatenated in the depth dimension.](Conv_Structure.pdf){#figure:strange_net}\\n\\n![A comparison of the original LSTM cell vs. two good cells our model found. Top left: LSTM cell. Top right: Cell found by our model when the search space does not include $max$ and $sin$. Bottom: Cell found by our model when the search space includes $max$ and $sin$ (the controller did not choose to use the $sin$ function).](newcell.eps \"fig:\"){#fig:strange_cell width=\"100%\"} ![A comparison of the original LSTM cell vs. two good cells our model found. Top left: LSTM cell. Top right: Cell found by our model when the search space does not include $max$ and $sin$. Bottom: Cell found by our model when the search space includes $max$ and $sin$ (the controller did not choose to use the $sin$ function).](newcell2.eps \"fig:\"){#fig:strange_cell width=\"50%\"}\\n\\n[^1]: Work done as a member of the Google Brain Residency program ([g.co/brainresidency](g.co/brainresidency).)\\n',\n",
       "  'bibliography_bbl': '\\\\begin{thebibliography}{66}\\n\\\\providecommand{\\\\natexlab}[1]{#1}\\n\\\\providecommand{\\\\url}[1]{\\\\texttt{#1}}\\n\\\\expandafter\\\\ifx\\\\csname urlstyle\\\\endcsname\\\\relax\\n  \\\\providecommand{\\\\doi}[1]{doi: #1}\\\\else\\n  \\\\providecommand{\\\\doi}{doi: \\\\begingroup \\\\urlstyle{rm}\\\\Url}\\\\fi\\n\\n\\\\bibitem[Andreas et~al.(2016)Andreas, Rohrbach, Darrell, and\\n  Klein]{andreas2016learning}\\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.\\n\\\\newblock Learning to compose neural networks for question answering.\\n\\\\newblock In \\\\emph{NAACL}, 2016.\\n\\n\\\\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,\\n  Schaul, and de~Freitas]{andrychowicz2016learning}\\nMarcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,\\n  Tom Schaul, and Nando de~Freitas.\\n\\\\newblock Learning to learn by gradient descent by gradient descent.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1606.04474}, 2016.\\n\\n\\\\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2014neural}\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\\n\\\\newblock Neural machine translation by jointly learning to align and\\n  translate.\\n\\\\newblock In \\\\emph{ICLR}, 2015.\\n\\n\\\\bibitem[Bengio et~al.(2003)Bengio, Ducharme, Vincent, and\\n  Jauvin]{bengio2003neural}\\nYoshua Bengio, R{\\\\\\'e}jean Ducharme, Pascal Vincent, and Christian Jauvin.\\n\\\\newblock A neural probabilistic language model.\\n\\\\newblock \\\\emph{JMLR}, 2003.\\n\\n\\\\bibitem[Bergstra \\\\& Bengio(2012)Bergstra and Bengio]{bergstra2012random}\\nJames Bergstra and Yoshua Bengio.\\n\\\\newblock Random search for hyper-parameter optimization.\\n\\\\newblock \\\\emph{JMLR}, 2012.\\n\\n\\\\bibitem[Bergstra et~al.(2011)Bergstra, Bardenet, Bengio, and\\n  K{\\\\\\'e}gl]{bergstra2011algorithms}\\nJames Bergstra, R{\\\\\\'e}mi Bardenet, Yoshua Bengio, and Bal{\\\\\\'a}zs K{\\\\\\'e}gl.\\n\\\\newblock Algorithms for hyper-parameter optimization.\\n\\\\newblock In \\\\emph{NIPS}, 2011.\\n\\n\\\\bibitem[Bergstra et~al.(2013)Bergstra, Yamins, and Cox]{bergstra2013making}\\nJames Bergstra, Daniel Yamins, and David~D Cox.\\n\\\\newblock Making a science of model search: Hyperparameter optimization in\\n  hundreds of dimensions for vision architectures.\\n\\\\newblock \\\\emph{ICML}, 2013.\\n\\n\\\\bibitem[Biermann(1978)]{biermann1978inference}\\nAlan~W. Biermann.\\n\\\\newblock The inference of regular {LISP} programs from examples.\\n\\\\newblock \\\\emph{IEEE transactions on Systems, Man, and Cybernetics}, 1978.\\n\\n\\\\bibitem[Cheng et~al.(2014)Cheng, Kok, Pham, Chieu, and Chai]{Cheng2014}\\nWei-Chen Cheng, Stanley Kok, Hoai~Vu Pham, Hai~Leong Chieu, and Kian Ming~Adam\\n  Chai.\\n\\\\newblock Language modeling with sum-product networks.\\n\\\\newblock In \\\\emph{INTERSPEECH}, 2014.\\n\\n\\\\bibitem[Dalal \\\\& Triggs(2005)Dalal and Triggs]{dalal2005histograms}\\nNavneet Dalal and Bill Triggs.\\n\\\\newblock Histograms of oriented gradients for human detection.\\n\\\\newblock In \\\\emph{CVPR}, 2005.\\n\\n\\\\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,\\n  Tucker, Yang, Le, et~al.]{dean2012large}\\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,\\n  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V. Le, et~al.\\n\\\\newblock Large scale distributed deep networks.\\n\\\\newblock In \\\\emph{NIPS}, 2012.\\n\\n\\\\bibitem[Floreano et~al.(2008)Floreano, D{\\\\\"u}rr, and\\n  Mattiussi]{floreano2008neuroevolution}\\nDario Floreano, Peter D{\\\\\"u}rr, and Claudio Mattiussi.\\n\\\\newblock Neuroevolution: from architectures to learning.\\n\\\\newblock \\\\emph{Evolutionary Intelligence}, 2008.\\n\\n\\\\bibitem[Gal(2015)]{Gal2015}\\nYarin Gal.\\n\\\\newblock A theoretically grounded application of dropout in recurrent neural\\n  networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1512.05287}, 2015.\\n\\n\\\\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{ha2016hypernetworks}\\nDavid Ha, Andrew Dai, and Quoc~V. Le.\\n\\\\newblock Hypernetworks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1609.09106}, 2016.\\n\\n\\\\bibitem[He et~al.(2016{\\\\natexlab{a}})He, Zhang, Ren, and Sun]{he2015deep}\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\n\\\\newblock Deep residual learning for image recognition.\\n\\\\newblock In \\\\emph{CVPR}, 2016{\\\\natexlab{a}}.\\n\\n\\\\bibitem[He et~al.(2016{\\\\natexlab{b}})He, Zhang, Ren, and\\n  Sun]{identity-mappings}\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\n\\\\newblock Identity mappings in deep residual networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1603.05027}, 2016{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,\\n  Vanhoucke, Nguyen, Sainath, et~al.]{hinton2012deep}\\nGeoffrey Hinton, Li~Deng, Dong Yu, George~E. Dahl, Abdel-rahman Mohamed,\\n  Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N.\\n  Sainath, et~al.\\n\\\\newblock Deep neural networks for acoustic modeling in speech recognition: The\\n  shared views of four research groups.\\n\\\\newblock \\\\emph{IEEE Signal Processing Magazine}, 2012.\\n\\n\\\\bibitem[Hochreiter \\\\& Schmidhuber(1997)Hochreiter and Schmidhuber]{lstm}\\nSepp Hochreiter and Juergen Schmidhuber.\\n\\\\newblock Long short-term memory.\\n\\\\newblock \\\\emph{Neural Computation}, 1997.\\n\\n\\\\bibitem[Huang et~al.(2016{\\\\natexlab{a}})Huang, Liu, and\\n  Weinberger]{Huang2016Densely}\\nGao Huang, Zhuang Liu, and Kilian~Q. Weinberger.\\n\\\\newblock Densely connected convolutional networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1608.06993}, 2016{\\\\natexlab{a}}.\\n\\n\\\\bibitem[Huang et~al.(2016{\\\\natexlab{b}})Huang, Liu, Weinberger, and van~der\\n  Maaten]{Huang2016Densely2}\\nGao Huang, Zhuang Liu, Kilian~Q. Weinberger, and Laurens van~der Maaten.\\n\\\\newblock Densely connected convolutional networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1608.06993}, 2016{\\\\natexlab{b}}.\\n\\n\\\\bibitem[Huang et~al.(2016{\\\\natexlab{c}})Huang, Sun, Liu, Sedra, and\\n  Weinberger]{stochastic}\\nGao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger.\\n\\\\newblock Deep networks with stochastic depth.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1603.09382}, 2016{\\\\natexlab{c}}.\\n\\n\\\\bibitem[Inan et~al.(2016)Inan, Khosravi, and Socher]{SocherEmbedding}\\nHakan Inan, Khashayar Khosravi, and Richard Socher.\\n\\\\newblock Tying word vectors and word classifiers: A loss framework for\\n  language modeling.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1611.01462}, 2016.\\n\\n\\\\bibitem[Ioffe \\\\& Szegedy(2015)Ioffe and Szegedy]{BatchNorm}\\nSergey Ioffe and Christian Szegedy.\\n\\\\newblock Batch normalization: Accelerating deep network training by reducing\\n  internal covariate shift.\\n\\\\newblock In \\\\emph{ICML}, 2015.\\n\\n\\\\bibitem[Jarrett et~al.(2009)Jarrett, Kavukcuoglu, Lecun,\\n  et~al.]{jarrett2009best}\\nKevin Jarrett, Koray Kavukcuoglu, Yann Lecun, et~al.\\n\\\\newblock What is the best multi-stage architecture for object recognition?\\n\\\\newblock In \\\\emph{ICCV}, 2009.\\n\\n\\\\bibitem[Jozefowicz et~al.(2015)Jozefowicz, Zaremba, and\\n  Sutskever]{jozefowicz2015empirical}\\nRafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.\\n\\\\newblock An empirical exploration of recurrent network architectures.\\n\\\\newblock In \\\\emph{ICML}, 2015.\\n\\n\\\\bibitem[Kim et~al.(2015)Kim, Jernite, Sontag, and Rush]{Kim2016}\\nYoon Kim, Yacine Jernite, David Sontag, and Alexander~M. Rush.\\n\\\\newblock Character-aware neural language models.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1508.06615}, 2015.\\n\\n\\\\bibitem[Kingma \\\\& Ba(2015)Kingma and Ba]{ADAM}\\nDiederik~P. Kingma and Jimmy Ba.\\n\\\\newblock Adam: {A} method for stochastic optimization.\\n\\\\newblock In \\\\emph{ICLR}, 2015.\\n\\n\\\\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and\\n  Hinton]{krizhevsky2012imagenet}\\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.\\n\\\\newblock Imagenet classification with deep convolutional neural networks.\\n\\\\newblock In \\\\emph{NIPS}, 2012.\\n\\n\\\\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}\\nBrenden~M. Lake, Ruslan Salakhutdinov, and Joshua~B. Tenenbaum.\\n\\\\newblock Human-level concept learning through probabilistic program induction.\\n\\\\newblock \\\\emph{Science}, 2015.\\n\\n\\\\bibitem[Larsson et~al.(2016)Larsson, Maire, and Shakhnarovich]{fractalnet}\\nGustav Larsson, Michael Maire, and Gregory Shakhnarovich.\\n\\\\newblock Fractalnet: Ultra-deep neural networks without residuals.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1605.07648}, 2016.\\n\\n\\\\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and\\n  Haffner]{lecun1998gradient}\\nYann LeCun, L{\\\\\\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.\\n\\\\newblock Gradient-based learning applied to document recognition.\\n\\\\newblock \\\\emph{Proceedings of the IEEE}, 1998.\\n\\n\\\\bibitem[Lee et~al.(2015)Lee, Xie, Gallagher, Zhang, and Tu]{dsn}\\nChen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.\\n\\\\newblock Deeply-supervised nets.\\n\\\\newblock In \\\\emph{AISTATS}, 2015.\\n\\n\\\\bibitem[Li \\\\& Malik(2016)Li and Malik]{li2016learning}\\nKe~Li and Jitendra Malik.\\n\\\\newblock Learning to optimize.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1606.01885}, 2016.\\n\\n\\\\bibitem[Liang et~al.(2010)Liang, Jordan, and Klein]{liang2010learning}\\nPercy Liang, Michael~I. Jordan, and Dan Klein.\\n\\\\newblock Learning programs: A hierarchical {B}ayesian approach.\\n\\\\newblock In \\\\emph{ICML}, 2010.\\n\\n\\\\bibitem[Lin et~al.(2013)Lin, Chen, and Yan]{netinnet}\\nMin Lin, Qiang Chen, and Shuicheng Yan.\\n\\\\newblock Network in network.\\n\\\\newblock In \\\\emph{ICLR}, 2013.\\n\\n\\\\bibitem[Lowe(1999)]{lowe1999object}\\nDavid~G. Lowe.\\n\\\\newblock Object recognition from local scale-invariant features.\\n\\\\newblock In \\\\emph{CVPR}, 1999.\\n\\n\\\\bibitem[Mendoza et~al.(2016)Mendoza, Klein, Feurer, Springenberg, and\\n  Hutter]{mendoza2016towards}\\nHector Mendoza, Aaron Klein, Matthias Feurer, Jost~Tobias Springenberg, and\\n  Frank Hutter.\\n\\\\newblock Towards automatically-tuned neural networks.\\n\\\\newblock In \\\\emph{Proceedings of the 2016 Workshop on Automatic Machine\\n  Learning}, pp.\\\\  58--65, 2016.\\n\\n\\\\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{Socher2016}\\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.\\n\\\\newblock Pointer sentinel mixture models.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1609.07843}, 2016.\\n\\n\\\\bibitem[Mikolov \\\\& Zweig(2012)Mikolov and Zweig]{Mikolov2012}\\nTomas Mikolov and Geoffrey Zweig.\\n\\\\newblock Context dependent recurrent neural network language model.\\n\\\\newblock In \\\\emph{SLT}, pp.\\\\  234--239, 2012.\\n\\n\\\\bibitem[Mnih \\\\& Hinton(2007)Mnih and Hinton]{mnih2007three}\\nAndriy Mnih and Geoffrey Hinton.\\n\\\\newblock Three new graphical models for statistical language modelling.\\n\\\\newblock In \\\\emph{ICML}, 2007.\\n\\n\\\\bibitem[Nair \\\\& Hinton(2010)Nair and Hinton]{nair2010rectified}\\nVinod Nair and Geoffrey~E. Hinton.\\n\\\\newblock Rectified linear units improve restricted {B}oltzmann machines.\\n\\\\newblock In \\\\emph{ICML}, 2010.\\n\\n\\\\bibitem[Neelakantan et~al.(2015)Neelakantan, Le, and\\n  Sutskever]{neelakantan2015neural}\\nArvind Neelakantan, Quoc~V. Le, and Ilya Sutskever.\\n\\\\newblock Neural programmer: Inducing latent programs with gradient descent.\\n\\\\newblock In \\\\emph{ICLR}, 2015.\\n\\n\\\\bibitem[Pascanu et~al.(2013)Pascanu, Gulcehre, Cho, and Bengio]{Pascanu2013a}\\nRazvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio.\\n\\\\newblock How to construct deep recurrent neural networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1312.6026}, 2013.\\n\\n\\\\bibitem[Press \\\\& Wolf(2016)Press and Wolf]{shareEmbedding}\\nOfir Press and Lior Wolf.\\n\\\\newblock Using the output embedding to improve language models.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1608.05859}, 2016.\\n\\n\\\\bibitem[Ranzato et~al.(2015)Ranzato, Chopra, Auli, and\\n  Zaremba]{ranzato2015sequence}\\nMarc\\'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba.\\n\\\\newblock Sequence level training with recurrent neural networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1511.06732}, 2015.\\n\\n\\\\bibitem[Reed \\\\& de~Freitas(2015)Reed and de~Freitas]{reed2015neural}\\nScott Reed and Nando de~Freitas.\\n\\\\newblock Neural programmer-interpreters.\\n\\\\newblock In \\\\emph{ICLR}, 2015.\\n\\n\\\\bibitem[Saxena \\\\& Verbeek(2016)Saxena and Verbeek]{saxena2016convolutional}\\nShreyas Saxena and Jakob Verbeek.\\n\\\\newblock Convolutional neural fabrics.\\n\\\\newblock In \\\\emph{NIPS}, 2016.\\n\\n\\\\bibitem[Shen et~al.(2016)Shen, Cheng, He, He, Wu, Sun, and Liu]{ShenCHHWSL15}\\nShiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.\\n\\\\newblock Minimum risk training for neural machine translation.\\n\\\\newblock In \\\\emph{ACL}, 2016.\\n\\n\\\\bibitem[Simonyan \\\\& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}\\nKaren Simonyan and Andrew Zisserman.\\n\\\\newblock Very deep convolutional networks for large-scale image recognition.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1409.1556}, 2014.\\n\\n\\\\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}\\nJasper Snoek, Hugo Larochelle, and Ryan~P. Adams.\\n\\\\newblock Practical {B}ayesian optimization of machine learning algorithms.\\n\\\\newblock In \\\\emph{NIPS}, 2012.\\n\\n\\\\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,\\n  Patwary, Ali, Adams, et~al.]{snoek2015scalable}\\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish,\\n  Narayanan Sundaram, Mostofa Patwary, Mostofa Ali, Ryan~P. Adams, et~al.\\n\\\\newblock Scalable bayesian optimization using deep neural networks.\\n\\\\newblock In \\\\emph{ICML}, 2015.\\n\\n\\\\bibitem[Springenberg et~al.(2014)Springenberg, Dosovitskiy, Brox, and\\n  Riedmiller]{allcnn}\\nJost~Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin\\n  Riedmiller.\\n\\\\newblock Striving for simplicity: The all convolutional net.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1412.6806}, 2014.\\n\\n\\\\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and Schmidhuber]{highway}\\nRupesh~Kumar Srivastava, Klaus Greff, and J{\\\\\"u}rgen Schmidhuber.\\n\\\\newblock Highway networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1505.00387}, 2015.\\n\\n\\\\bibitem[Stanley et~al.(2009)Stanley, D\\'Ambrosio, and\\n  Gauci]{stanley2009hypercube}\\nKenneth~O. Stanley, David~B. D\\'Ambrosio, and Jason Gauci.\\n\\\\newblock A hypercube-based encoding for evolving large-scale neural networks.\\n\\\\newblock \\\\emph{Artificial {L}ife}, 2009.\\n\\n\\\\bibitem[Summers(1977)]{summers1977methodology}\\nPhillip~D. Summers.\\n\\\\newblock A methodology for {LISP} program construction from examples.\\n\\\\newblock \\\\emph{Journal of the ACM}, 1977.\\n\\n\\\\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and\\n  Hinton]{icml2013_sutskever13}\\nIlya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.\\n\\\\newblock On the importance of initialization and momentum in deep learning.\\n\\\\newblock In \\\\emph{ICML}, 2013.\\n\\n\\\\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and\\n  Le]{sutskever2014sequence}\\nIlya Sutskever, Oriol Vinyals, and Quoc~V. Le.\\n\\\\newblock Sequence to sequence learning with neural networks.\\n\\\\newblock In \\\\emph{NIPS}, 2014.\\n\\n\\\\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,\\n  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}\\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\\n  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.\\n\\\\newblock Going deeper with convolutions.\\n\\\\newblock In \\\\emph{CVPR}, 2015.\\n\\n\\\\bibitem[Thrun \\\\& Pratt(2012)Thrun and Pratt]{thrun2012learning}\\nSebastian Thrun and Lorien Pratt.\\n\\\\newblock \\\\emph{Learning to learn}.\\n\\\\newblock Springer Science \\\\& Business Media, 2012.\\n\\n\\\\bibitem[Vinyals et~al.(2015)Vinyals, Fortunato, and\\n  Jaitly]{vinyals2015pointer}\\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\\n\\\\newblock Pointer networks.\\n\\\\newblock In \\\\emph{NIPS}, 2015.\\n\\n\\\\bibitem[Wierstra et~al.(2005)Wierstra, Gomez, and\\n  Schmidhuber]{wierstra2005modeling}\\nDaan Wierstra, Faustino~J Gomez, and J{\\\\\"u}rgen Schmidhuber.\\n\\\\newblock Modeling systems with internal state using evolino.\\n\\\\newblock In \\\\emph{GECCO}, 2005.\\n\\n\\\\bibitem[Williams(1992)]{Williams92simplestatistical}\\nRonald~J. Williams.\\n\\\\newblock Simple statistical gradient-following algorithms for connectionist\\n  reinforcement learning.\\n\\\\newblock In \\\\emph{Machine Learning}, 1992.\\n\\n\\\\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, et~al.]{wu2016google}\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V. Le, Mohammad Norouzi, et~al.\\n\\\\newblock Google\\'s neural machine translation system: Bridging the gap between\\n  human and machine translation.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1609.08144}, 2016.\\n\\n\\\\bibitem[Zagoruyko \\\\& Komodakis(2016)Zagoruyko and Komodakis]{wide}\\nSergey Zagoruyko and Nikos Komodakis.\\n\\\\newblock Wide residual networks.\\n\\\\newblock In \\\\emph{BMVC}, 2016.\\n\\n\\\\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and Vinyals]{ZarembaReg}\\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\\n\\\\newblock Recurrent neural network regularization.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1409.2329}, 2014.\\n\\n\\\\bibitem[Zilly et~al.(2016)Zilly, Srivastava, Koutn{\\\\\\'\\\\i}k, and\\n  Schmidhuber]{Zilly2016}\\nJulian~Georg Zilly, Rupesh~Kumar Srivastava, Jan Koutn{\\\\\\'\\\\i}k, and J{\\\\\"u}rgen\\n  Schmidhuber.\\n\\\\newblock Recurrent highway networks.\\n\\\\newblock \\\\emph{arXiv preprint arXiv:1607.03474}, 2016.\\n\\n\\\\end{thebibliography}\\n',\n",
       "  'bibliography_bib': '',\n",
       "  'arxiv_citations': {'1606.04474': True,\n",
       "   '1512.05287': True,\n",
       "   '1609.09106': True,\n",
       "   '1603.05027': True,\n",
       "   '1608.06993': True,\n",
       "   '1603.09382': True,\n",
       "   '1611.01462': True,\n",
       "   '1508.06615': True,\n",
       "   '1605.07648': True,\n",
       "   '1606.01885': True,\n",
       "   '1609.07843': True,\n",
       "   '1312.6026': True,\n",
       "   '1608.05859': True,\n",
       "   '1511.06732': True,\n",
       "   '1409.1556': True,\n",
       "   '1412.6806': True,\n",
       "   '1505.00387': True,\n",
       "   '1609.08144': True,\n",
       "   '1409.2329': True,\n",
       "   '1607.03474': True}}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markdown(clusters, data):\n",
    "    topic_dict = {}\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        if cluster not in topic_dict:\n",
    "            topic_dict[cluster] = []\n",
    "        topic_dict[cluster].append(data[idx])\n",
    "    \n",
    "    with open('output.md', 'w') as f:\n",
    "        for topic, texts in topic_dict.items():\n",
    "            f.write(f\"## Topic {topic}\\n\")\n",
    "            for text in texts:\n",
    "                f.write(f\"- {text}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "# embeddings = get_embeddings(data)\n",
    "# num_clusters = 5  # Adjust the number of clusters as needed\n",
    "# clusters = cluster_data(embeddings, num_clusters)\n",
    "# generate_markdown(clusters, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's an attempt at extracting major tags in markdown format based on the papers provided:\\n\\nMechanistic Interpretability:\\n- Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\\n- Quantifying Perceptual Distortion of Adversarial Examples\\n- Fooling the primate brain with minimal, targeted image manipulation\\n\\nRed Teaming:\\n- Transfer of Adversarial Robustness Between Perturbation Types\\n- Verifiably Safe Exploration for End-to-End Reinforcement Learning\\n\\nAI Governance:\\n- Unsolved Problems in ML Safety\\n- Predictability and Surprise in Large Generative Models\\n- Accounting for the Neglected Dimensions of AI Progress\\n\\nValue Alignment:\\n- An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning\\n- Robot Planning with Mathematical Models of Human State and Action\\n- Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem\\n\\nAgent Alignment:\\n- Learning What To Do by Simulating the Past\\n- Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective\\n- Making Efficient Use of Demonstrations to Solve Hard Exploration Problems\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(anthropic, \n",
    "            f'Can you extract the major tags into a markdown format like Tag1: Paper1, Paper 2, Tag2: Paper 2, Paper 3 from the following papers? {combined_entries} - for example - mechanistic interpretability, red teaming, AI governance, value alignment, agent alignment - the papers could belong to more than 1 tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://aipulse.org/feed/', 'value': 'Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://aipulse.org/webinar-with-congressman-ro-khanna-challenges-in-it-law-and-governance/'}], 'link': 'https://aipulse.org/webinar-with-congressman-ro-khanna-challenges-in-it-law-and-governance/', 'authors': [{'name': 'Laura Elbaum'}], 'author': 'Laura Elbaum', 'author_detail': {'name': 'Laura Elbaum'}, 'published': 'Mon, 22 Feb 2021 16:40:08 +0000', 'published_parsed': [2021, 2, 22, 16, 40, 8, 0, 53, 0], 'tags': [{'term': 'Webinars', 'scheme': None, 'label': None}], 'id': 'https://aipulse.org/?p=712', 'guidislink': False, 'summary': '<p>On Friday, February 19, the AI Pulse project hosted a web conversation on current issues in IT Governance with Congressman Ro Khanna, a leading progressive thinker on a wide range of law and technology issues in the United States Congress.</p>\\nThe post <a href=\"https://aipulse.org/webinar-with-congressman-ro-khanna-challenges-in-it-law-and-governance/\">Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance</a> first appeared on <a href=\"https://aipulse.org\">AI Pulse</a>.', 'summary_detail': {'type': 'text/html', 'language': None, 'base': 'https://aipulse.org/feed/', 'value': '<p>On Friday, February 19, the AI Pulse project hosted a web conversation on current issues in IT Governance with Congressman Ro Khanna, a leading progressive thinker on a wide range of law and technology issues in the United States Congress.</p>\\nThe post <a href=\"https://aipulse.org/webinar-with-congressman-ro-khanna-challenges-in-it-law-and-governance/\">Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance</a> first appeared on <a href=\"https://aipulse.org\">AI Pulse</a>.'}, 'content': [{'type': 'text/html', 'language': None, 'base': 'https://aipulse.org/feed/', 'value': '<div class=\"dkpdf-button-container\" style=\"text-align: center ;\">\\n\\n\\t\\t<a class=\"dkpdf-button\" href=\"https://aipulse.org/feed/?pdf=712\" target=\"_blank\"><span class=\"dkpdf-button-icon\"><i class=\"fa fa-file-pdf-o\"></i></span> Download as PDF</a>\\n\\n\\t</div>\\n\\n\\n\\n\\n\\n<p>On Friday, February 19, the AI Pulse project hosted a web conversation on current issues in IT Governance with Congressman Ro Khanna, a leading progressive thinker on a wide range of law and technology issues in the United States Congress. Rep. Khanna represents Californias 17th district in the House of Representatives, where he chairs the Environment Subcommittee of the House Committee on Oversight and Reform, and serves as Deputy Whip of the Congressional Progressive Caucus. He is a passionate advocate of using technology to bring economic opportunity to rural and small-town America. In 2018, at the request of Speaker Pelosi, he authored a widely praised set of principles for an Internet Bill of Rights. Prior to serving in Congress, Rep. Khanna worked as an intellectual-property lawyer and served in the Obama Administration as Deputy Assistant Secretary of Commerce. He holds an undergraduate degree in Economics from the University of Chicago and a J.D. from Yale.</p>\\n<p>Joining Rep. Khanna in conversation were Professors Eugene Volokh and Ted Parson of the UCLA School of Law. The conversation ranged over a wide set of law and technology issues, including market concentration and antitrust in the IT sector, first-amendment issues associated with online speech and content moderation, and online privacy regulation and related consumer online rights.</p>\\n<p>The recording of the event is available <a href=\"https://www.youtube.com/watch?v=tAKzefppQbo&amp;feature=youtu.be\" rel=\"noopener\" target=\"_blank\">here</a>.</p>\\n<p>We plan a continuing series of informal web conversations on various interesting and fun questions related to the societal impacts, governance, and ethics of AI/ML and related data and computational technologies. Stay tuned for these  or to be added to our mailing list for announcements of future events, please send an email to aipulse@law.ucla.edu.</p>The post <a href=\"https://aipulse.org/webinar-with-congressman-ro-khanna-challenges-in-it-law-and-governance/\">Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance</a> first appeared on <a href=\"https://aipulse.org\">AI Pulse</a>.'}], 'post-id': '712', 'text': 'Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance\\n\\n\\n Download as PDF\\n\\nOn Friday, February 19, the AI Pulse project hosted a web conversation on current issues in IT Governance with Congressman Ro Khanna, a leading progressive thinker on a wide range of law and technology issues in the United States Congress. Rep. Khanna represents Californias 17th district in the House of Representatives, where he chairs the Environment Subcommittee of the House Committee on Oversight and Reform, and serves as Deputy Whip of the Congressional Progressive Caucus. He is a passionate advocate of using technology to bring economic opportunity to rural and small-town America. In 2018, at the request of Speaker Pelosi, he authored a widely praised set of principles for an Internet Bill of Rights. Prior to serving in Congress, Rep. Khanna worked as an intellectual-property lawyer and served in the Obama Administration as Deputy Assistant Secretary of Commerce. He holds an undergraduate degree in Economics from the University of Chicago and a J.D. from Yale.\\nJoining Rep. Khanna in conversation were Professors Eugene Volokh and Ted Parson of the UCLA School of Law. The conversation ranged over a wide set of law and technology issues, including market concentration and antitrust in the IT sector, first-amendment issues associated with online speech and content moderation, and online privacy regulation and related consumer online rights.\\nThe recording of the event is available here.\\nWe plan a continuing series of informal web conversations on various interesting and fun questions related to the societal impacts, governance, and ethics of AI/ML and related data and computational technologies. Stay tuned for these  or to be added to our mailing list for announcements of future events, please send an email to aipulse@law.ucla.edu.The post Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance first appeared on AI Pulse.', 'source': 'https://aipulse.org', 'source_type': 'wordpress-blog'}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\n\\nMechanistic Interpretability:\\n- Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift\\n- Quantifying Perceptual Distortion of Adversarial Examples\\n- Fooling the primate brain with minimal, targeted image manipulation\\n\n",
    "\n",
    "\\nRed Teaming:\\n- Transfer of Adversarial Robustness Between Perturbation Types\\n- Verifiably Safe Exploration for End-to-End Reinforcement Learning\\n\n",
    "\n",
    "\\nAI Governance:\\n- Unsolved Problems in ML Safety\\n- Predictability and Surprise in Large Generative Models\\n- Accounting for the Neglected Dimensions of AI Progress\\n\\nValue Alignment:\\n- An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning\\n- Robot Planning with Mathematical Models of Human State and Action\\n- Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem\\n\n",
    "\n",
    "\\nAgent Alignment:\\n- Learning What To Do by Simulating the Past\\n- Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective\\n- Making Efficient Use of Demonstrations to Solve Hard Exploration Problems\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pain points\n",
    "- Feed whole dataset\n",
    "- Get exact markdown format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
